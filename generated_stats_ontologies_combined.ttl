@prefix ds: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix stats: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/stats_exeKGOntology.ttl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

stats:hasCentralTendencyMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:CentralTendencyMeasure ;
    rdfs:range stats:Average,
        stats:Mean,
        stats:Median ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:hasDependencyMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:DependencyMeasure ;
    rdfs:range stats:Corrcoef,
        stats:Cov ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:hasDispersionMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:DispersionMeasure ;
    rdfs:range stats:Std,
        stats:Var ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:hasParamAweights a owl:DatatypeProperty ;
    rdfs:domain stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamAxis a owl:DatatypeProperty ;
    rdfs:domain stats:Average,
        stats:Kurtosis,
        stats:Mean,
        stats:Median,
        stats:Percentile,
        stats:Ptp,
        stats:Quantile,
        stats:Skew,
        stats:Std,
        stats:Var ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamBias a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov,
        stats:Kurtosis,
        stats:Skew ;
    rdfs:range xsd:boolean,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamDdof a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov,
        stats:Std,
        stats:Var ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamDtype a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov,
        stats:Mean,
        stats:Std,
        stats:Var ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamFisher a owl:DatatypeProperty ;
    rdfs:domain stats:Kurtosis ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamFweights a owl:DatatypeProperty ;
    rdfs:domain stats:Cov ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamInterpolation a owl:DatatypeProperty ;
    rdfs:domain stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamKeepdims a owl:DatatypeProperty ;
    rdfs:domain stats:Average,
        stats:Kurtosis,
        stats:Mean,
        stats:Median,
        stats:Percentile,
        stats:Ptp,
        stats:Quantile,
        stats:Skew,
        stats:Std,
        stats:Var ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamM a owl:DatatypeProperty ;
    rdfs:domain stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamMethod a owl:DatatypeProperty ;
    rdfs:domain stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamNanPolicy a owl:DatatypeProperty ;
    rdfs:domain stats:Kurtosis,
        stats:Skew ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamOverwriteInput a owl:DatatypeProperty ;
    rdfs:domain stats:Median,
        stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamQ a owl:DatatypeProperty ;
    rdfs:domain stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamReturned a owl:DatatypeProperty ;
    rdfs:domain stats:Average ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamRowvar a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamWeights a owl:DatatypeProperty ;
    rdfs:domain stats:Average ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamWhere a owl:DatatypeProperty ;
    rdfs:domain stats:Mean,
        stats:Std,
        stats:Var ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamX a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamY a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasPositionMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:PositionMeasure ;
    rdfs:range stats:Percentile,
        stats:Quantile ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:hasShapeMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:ShapeMeasure ;
    rdfs:range stats:Kurtosis,
        stats:Ptp,
        stats:Skew ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:CentralTendencyMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:CoreModule a owl:Class ;
    rdfs:subClassOf stats:NumpyModule .

stats:DependencyMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:DispersionMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:LibModule a owl:Class ;
    rdfs:subClassOf stats:NumpyModule .

stats:PositionMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:ScipyModule a owl:Class ;
    rdfs:subClassOf ds:Module .

stats:ShapeMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:NumpyModule a owl:Class ;
    rdfs:subClassOf ds:Module .

stats:StatsModule a owl:Class ;
    rdfs:subClassOf stats:ScipyModule .

stats:Ptp a owl:Class ;
    rdfs:comment """Range of values (maximum - minimum) along an axis.

The name of the function comes from the acronym for 'peak to peak'.

.. warning::
    `ptp` preserves the data type of the array. This means the
    return value for an input of signed integers with n bits
    (e.g. `np.int8`, `np.int16`, etc) is also a signed integer
    with n bits.  In that case, peak-to-peak values greater than
    ``2**(n-1)-1`` will be returned as negative values. An example
    with a work-around is shown below.

Parameters
----------
a : array_like
    Input values.
axis : None or int or tuple of ints, optional
    Axis along which to find the peaks.  By default, flatten the
    array.  `axis` may be negative, in
    which case it counts from the last to the first axis.

    .. versionadded:: 1.15.0

    If this is a tuple of ints, a reduction is performed on multiple
    axes, instead of a single axis or all the axes as before.
out : array_like
    Alternative output array in which to place the result. It must
    have the same shape and buffer length as the expected output,
    but the type of the output values will be cast if necessary.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `ptp` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

Returns
-------
ptp : ndarray or scalar
    The range of a given array - `scalar` if array is one-dimensional
    or a new array holding the result along the given axis

Examples
--------
>>> x = np.array([[4, 9, 2, 10],
...               [6, 9, 7, 12]])

>>> np.ptp(x, axis=1)
array([8, 6])

>>> np.ptp(x, axis=0)
array([2, 0, 5, 2])

>>> np.ptp(x)
10

This example shows that a negative value can be returned when
the input is an array of signed integers.

>>> y = np.array([[1, 127],
...               [0, 127],
...               [-1, 127],
...               [-2, 127]], dtype=np.int8)
>>> np.ptp(y, axis=1)
array([ 126,  127, -128, -127], dtype=int8)

A work-around is to use the `view()` method to view the result as
unsigned integers with the same bit width:

>>> np.ptp(y, axis=1).view(np.uint8)
array([126, 127, 128, 129], dtype=uint8)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FromnumericModule,
        stats:StatisticCalculationMethod .

stats:FromnumericModule a owl:Class ;
    rdfs:subClassOf stats:CoreModule .

stats:Median a owl:Class ;
    rdfs:comment """Compute the median along the specified axis.

Returns the median of the array elements.

Parameters
----------
a : array_like
    Input array or object that can be converted to an array.
axis : {int, sequence of int, None}, optional
    Axis or axes along which the medians are computed. The default
    is to compute the median along a flattened version of the array.
    A sequence of axes is supported since version 1.9.0.
out : ndarray, optional
    Alternative output array in which to place the result. It must
    have the same shape and buffer length as the expected output,
    but the type (of the output) will be cast if necessary.
overwrite_input : bool, optional
   If True, then allow use of memory of input array `a` for
   calculations. The input array will be modified by the call to
   `median`. This will save memory when you do not need to preserve
   the contents of the input array. Treat the input as undefined,
   but it will probably be fully or partially sorted. Default is
   False. If `overwrite_input` is ``True`` and `a` is not already an
   `ndarray`, an error will be raised.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the original `arr`.

    .. versionadded:: 1.9.0

Returns
-------
median : ndarray
    A new array holding the result. If the input contains integers
    or floats smaller than ``float64``, then the output data-type is
    ``np.float64``.  Otherwise, the data-type of the output is the
    same as that of the input. If `out` is specified, that array is
    returned instead.

See Also
--------
mean, percentile

Notes
-----
Given a vector ``V`` of length ``N``, the median of ``V`` is the
middle value of a sorted copy of ``V``, ``V_sorted`` - i
e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the
two middle values of ``V_sorted`` when ``N`` is even.

Examples
--------
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> np.median(a)
3.5
>>> np.median(a, axis=0)
array([6.5, 4.5, 2.5])
>>> np.median(a, axis=1)
array([7.,  2.])
>>> m = np.median(a, axis=0)
>>> out = np.zeros_like(m)
>>> np.median(a, axis=0, out=m)
array([6.5,  4.5,  2.5])
>>> m
array([6.5,  4.5,  2.5])
>>> b = a.copy()
>>> np.median(b, axis=1, overwrite_input=True)
array([7.,  2.])
>>> assert not np.all(a==b)
>>> b = a.copy()
>>> np.median(b, axis=None, overwrite_input=True)
3.5
>>> assert not np.all(a==b)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FunctionBaseModule,
        stats:StatisticCalculationMethod .

stats:Average a owl:Class ;
    rdfs:comment """Compute the weighted average along the specified axis.

Parameters
----------
a : array_like
    Array containing data to be averaged. If `a` is not an array, a
    conversion is attempted.
axis : None or int or tuple of ints, optional
    Axis or axes along which to average `a`.  The default,
    axis=None, will average over all of the elements of the input array.
    If axis is negative it counts from the last to the first axis.

    .. versionadded:: 1.7.0

    If axis is a tuple of ints, averaging is performed on all of the axes
    specified in the tuple instead of a single axis or all the axes as
    before.
weights : array_like, optional
    An array of weights associated with the values in `a`. Each value in
    `a` contributes to the average according to its associated weight.
    The weights array can either be 1-D (in which case its length must be
    the size of `a` along the given axis) or of the same shape as `a`.
    If `weights=None`, then all data in `a` are assumed to have a
    weight equal to one.  The 1-D calculation is::

        avg = sum(a * weights) / sum(weights)

    The only constraint on `weights` is that `sum(weights)` must not be 0.
returned : bool, optional
    Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`)
    is returned, otherwise only the average is returned.
    If `weights=None`, `sum_of_weights` is equivalent to the number of
    elements over which the average is taken.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the original `a`.
    *Note:* `keepdims` will not work with instances of `numpy.matrix`
    or other classes whose methods do not support `keepdims`.

    .. versionadded:: 1.23.0

Returns
-------
retval, [sum_of_weights] : array_type or double
    Return the average along the specified axis. When `returned` is `True`,
    return a tuple with the average as the first element and the sum
    of the weights as the second element. `sum_of_weights` is of the
    same type as `retval`. The result dtype follows a genereal pattern.
    If `weights` is None, the result dtype will be that of `a` , or ``float64``
    if `a` is integral. Otherwise, if `weights` is not None and `a` is non-
    integral, the result type will be the type of lowest precision capable of
    representing values of both `a` and `weights`. If `a` happens to be
    integral, the previous rules still applies but the result dtype will
    at least be ``float64``.

Raises
------
ZeroDivisionError
    When all weights along axis are zero. See `numpy.ma.average` for a
    version robust to this type of error.
TypeError
    When the length of 1D `weights` is not the same as the shape of `a`
    along axis.

See Also
--------
mean

ma.average : average for masked arrays -- useful if your data contains
             "missing" values
numpy.result_type : Returns the type that results from applying the
                    numpy type promotion rules to the arguments.

Examples
--------
>>> data = np.arange(1, 5)
>>> data
array([1, 2, 3, 4])
>>> np.average(data)
2.5
>>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1))
4.0

>>> data = np.arange(6).reshape((3, 2))
>>> data
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> np.average(data, axis=1, weights=[1./4, 3./4])
array([0.75, 2.75, 4.75])
>>> np.average(data, weights=[1./4, 3./4])
Traceback (most recent call last):
    ...
TypeError: Axis must be specified when shapes of a and weights differ.

>>> a = np.ones(5, dtype=np.float128)
>>> w = np.ones(5, dtype=np.complex64)
>>> avg = np.average(a, weights=w)
>>> print(avg.dtype)
complex256

With ``keepdims=True``, the following result has shape (3, 1).

>>> np.average(data, axis=1, keepdims=True)
array([[0.5],
       [2.5],
       [4.5]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FunctionBaseModule,
        stats:StatisticCalculationMethod .

stats:Mean a owl:Class ;
    rdfs:comment """Compute the arithmetic mean along the specified axis.

Returns the average of the array elements.  The average is taken over
the flattened array by default, otherwise over the specified axis.
`float64` intermediate and return values are used for integer inputs.

Parameters
----------
a : array_like
    Array containing numbers whose mean is desired. If `a` is not an
    array, a conversion is attempted.
axis : None or int or tuple of ints, optional
    Axis or axes along which the means are computed. The default is to
    compute the mean of the flattened array.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, a mean is performed over multiple axes,
    instead of a single axis or all the axes as before.
dtype : data-type, optional
    Type to use in computing the mean.  For integer inputs, the default
    is `float64`; for floating point inputs, it is the same as the
    input dtype.
out : ndarray, optional
    Alternate output array in which to place the result.  The default
    is ``None``; if provided, it must have the same shape as the
    expected output, but the type will be cast if necessary.
    See :ref:`ufuncs-output-type` for more details.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `mean` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

where : array_like of bool, optional
    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.

    .. versionadded:: 1.20.0

Returns
-------
m : ndarray, see dtype parameter above
    If `out=None`, returns a new array containing the mean values,
    otherwise a reference to the output array is returned.

See Also
--------
average : Weighted average
std, var, nanmean, nanstd, nanvar

Notes
-----
The arithmetic mean is the sum of the elements along the axis divided
by the number of elements.

Note that for floating-point input, the mean is computed using the
same precision the input has.  Depending on the input data, this can
cause the results to be inaccurate, especially for `float32` (see
example below).  Specifying a higher-precision accumulator using the
`dtype` keyword can alleviate this issue.

By default, `float16` results are computed using `float32` intermediates
for extra precision.

Examples
--------
>>> a = np.array([[1, 2], [3, 4]])
>>> np.mean(a)
2.5
>>> np.mean(a, axis=0)
array([2., 3.])
>>> np.mean(a, axis=1)
array([1.5, 3.5])

In single precision, `mean` can be inaccurate:

>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.mean(a)
0.54999924

Computing the mean in float64 is more accurate:

>>> np.mean(a, dtype=np.float64)
0.55000000074505806 # may vary

Specifying a where argument:

>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
>>> np.mean(a)
12.0
>>> np.mean(a, where=[[True], [False], [False]])
9.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FromnumericModule,
        stats:StatisticCalculationMethod .

stats:Skew a owl:Class ;
    rdfs:comment """Compute the sample skewness of a data set.

For normally distributed data, the skewness should be about zero. For
unimodal continuous distributions, a skewness value greater than zero means
that there is more weight in the right tail of the distribution. The
function `skewtest` can be used to determine if the skewness value
is close enough to zero, statistically speaking.

Parameters
----------
a : ndarray
    Input array.
axis : int or None, default: 0
    If an int, the axis of the input along which to compute the statistic.
    The statistic of each axis-slice (e.g. row) of the input will appear in a
    corresponding element of the output.
    If ``None``, the input will be raveled before computing the statistic.
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.
nan_policy : {'propagate', 'omit', 'raise'}
    Defines how to handle input NaNs.
    
    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along
      which the  statistic is computed, the corresponding entry of the output
      will be NaN.
    - ``omit``: NaNs will be omitted when performing the calculation.
      If insufficient data remains in the axis slice along which the
      statistic is computed, the corresponding entry of the output will be
      NaN.
    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.
keepdims : bool, default: False
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns
-------
skewness : ndarray
    The skewness of values along an axis, returning NaN where all values
    are equal.

Notes
-----
The sample skewness is computed as the Fisher-Pearson coefficient
of skewness, i.e.

.. math::

    g_1=\\frac{m_3}{m_2^{3/2}}

where

.. math::

    m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i

is the biased sample :math:`i\\texttt{th}` central moment, and
:math:`\\bar{x}` is
the sample mean.  If ``bias`` is False, the calculations are
corrected for bias and the value computed is the adjusted
Fisher-Pearson standardized moment coefficient, i.e.

.. math::

    G_1=\\frac{k_3}{k_2^{3/2}}=
        \\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}.

Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new
code) are converted to ``np.ndarray`` before the calculation is performed. In
this case, the output will be a scalar or ``np.ndarray`` of appropriate shape
rather than a 2D ``np.matrix``. Similarly, while masked elements of masked
arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a
masked array with ``mask=False``.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.
   Section 2.2.24.1

Examples
--------
>>> from scipy.stats import skew
>>> skew([1, 2, 3, 4, 5])
0.0
>>> skew([2, 8, 0, 4, 1, 9, 9, 0])
0.2650554122698573""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod,
        stats:StatsModule .

stats:StatisticCalculation a owl:Class .

stats:FunctionBaseModule a owl:Class ;
    rdfs:subClassOf stats:LibModule .

stats:Kurtosis a owl:Class ;
    rdfs:comment """Compute the kurtosis (Fisher or Pearson) of a dataset.

Kurtosis is the fourth central moment divided by the square of the
variance. If Fisher's definition is used, then 3.0 is subtracted from
the result to give 0.0 for a normal distribution.

If bias is False then the kurtosis is calculated using k statistics to
eliminate bias coming from biased moment estimators

Use `kurtosistest` to see if result is close enough to normal.

Parameters
----------
a : array
    Data for which the kurtosis is calculated.
axis : int or None, default: 0
    If an int, the axis of the input along which to compute the statistic.
    The statistic of each axis-slice (e.g. row) of the input will appear in a
    corresponding element of the output.
    If ``None``, the input will be raveled before computing the statistic.
fisher : bool, optional
    If True, Fisher's definition is used (normal ==> 0.0). If False,
    Pearson's definition is used (normal ==> 3.0).
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.
nan_policy : {'propagate', 'omit', 'raise'}
    Defines how to handle input NaNs.
    
    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along
      which the  statistic is computed, the corresponding entry of the output
      will be NaN.
    - ``omit``: NaNs will be omitted when performing the calculation.
      If insufficient data remains in the axis slice along which the
      statistic is computed, the corresponding entry of the output will be
      NaN.
    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.
keepdims : bool, default: False
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns
-------
kurtosis : array
    The kurtosis of values along an axis, returning NaN where all values
    are equal.

Notes
-----

Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new
code) are converted to ``np.ndarray`` before the calculation is performed. In
this case, the output will be a scalar or ``np.ndarray`` of appropriate shape
rather than a 2D ``np.matrix``. Similarly, while masked elements of masked
arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a
masked array with ``mask=False``.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.

Examples
--------
In Fisher's definition, the kurtosis of the normal distribution is zero.
In the following example, the kurtosis is close to zero, because it was
calculated from the dataset, not from the continuous distribution.

>>> import numpy as np
>>> from scipy.stats import norm, kurtosis
>>> data = norm.rvs(size=1000, random_state=3)
>>> kurtosis(data)
-0.06928694200380558

The distribution with a higher kurtosis has a heavier tail.
The zero valued kurtosis of the normal distribution in Fisher's definition
can serve as a reference point.

>>> import matplotlib.pyplot as plt
>>> import scipy.stats as stats
>>> from scipy.stats import kurtosis

>>> x = np.linspace(-5, 5, 100)
>>> ax = plt.subplot()
>>> distnames = ['laplace', 'norm', 'uniform']

>>> for distname in distnames:
...     if distname == 'uniform':
...         dist = getattr(stats, distname)(loc=-2, scale=4)
...     else:
...         dist = getattr(stats, distname)
...     data = dist.rvs(size=1000)
...     kur = kurtosis(data, fisher=True)
...     y = dist.pdf(x)
...     ax.plot(x, y, label="{}, {}".format(distname, round(kur, 3)))
...     ax.legend()

The Laplace distribution has a heavier tail than the normal distribution.
The uniform distribution (which has negative kurtosis) has the thinnest
tail.""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod,
        stats:StatsModule .

stats:Std a owl:Class ;
    rdfs:comment """Compute the standard deviation along the specified axis.

Returns the standard deviation, a measure of the spread of a distribution,
of the array elements. The standard deviation is computed for the
flattened array by default, otherwise over the specified axis.

Parameters
----------
a : array_like
    Calculate the standard deviation of these values.
axis : None or int or tuple of ints, optional
    Axis or axes along which the standard deviation is computed. The
    default is to compute the standard deviation of the flattened array.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, a standard deviation is performed over
    multiple axes, instead of a single axis or all the axes as before.
dtype : dtype, optional
    Type to use in computing the standard deviation. For arrays of
    integer type the default is float64, for arrays of float types it is
    the same as the array type.
out : ndarray, optional
    Alternative output array in which to place the result. It must have
    the same shape as the expected output but the type (of the calculated
    values) will be cast if necessary.
ddof : int, optional
    Means Delta Degrees of Freedom.  The divisor used in calculations
    is ``N - ddof``, where ``N`` represents the number of elements.
    By default `ddof` is zero.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `std` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

where : array_like of bool, optional
    Elements to include in the standard deviation.
    See `~numpy.ufunc.reduce` for details.

    .. versionadded:: 1.20.0

Returns
-------
standard_deviation : ndarray, see dtype parameter above.
    If `out` is None, return a new array containing the standard deviation,
    otherwise return a reference to the output array.

See Also
--------
var, mean, nanmean, nanstd, nanvar
:ref:`ufuncs-output-type`

Notes
-----
The standard deviation is the square root of the average of the squared
deviations from the mean, i.e., ``std = sqrt(mean(x))``, where
``x = abs(a - a.mean())**2``.

The average squared deviation is typically calculated as ``x.sum() / N``,
where ``N = len(x)``. If, however, `ddof` is specified, the divisor
``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``
provides an unbiased estimator of the variance of the infinite population.
``ddof=0`` provides a maximum likelihood estimate of the variance for
normally distributed variables. The standard deviation computed in this
function is the square root of the estimated variance, so even with
``ddof=1``, it will not be an unbiased estimate of the standard deviation
per se.

Note that, for complex numbers, `std` takes the absolute
value before squaring, so that the result is always real and nonnegative.

For floating-point input, the *std* is computed using the same
precision the input has. Depending on the input data, this can cause
the results to be inaccurate, especially for float32 (see example below).
Specifying a higher-accuracy accumulator using the `dtype` keyword can
alleviate this issue.

Examples
--------
>>> a = np.array([[1, 2], [3, 4]])
>>> np.std(a)
1.1180339887498949 # may vary
>>> np.std(a, axis=0)
array([1.,  1.])
>>> np.std(a, axis=1)
array([0.5,  0.5])

In single precision, std() can be inaccurate:

>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.std(a)
0.45000005

Computing the standard deviation in float64 is more accurate:

>>> np.std(a, dtype=np.float64)
0.44999999925494177 # may vary

Specifying a where argument:

>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])
>>> np.std(a)
2.614064523559687 # may vary
>>> np.std(a, where=[[True], [True], [False]])
2.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FromnumericModule,
        stats:StatisticCalculationMethod .

stats:Var a owl:Class ;
    rdfs:comment """Compute the variance along the specified axis.

Returns the variance of the array elements, a measure of the spread of a
distribution.  The variance is computed for the flattened array by
default, otherwise over the specified axis.

Parameters
----------
a : array_like
    Array containing numbers whose variance is desired.  If `a` is not an
    array, a conversion is attempted.
axis : None or int or tuple of ints, optional
    Axis or axes along which the variance is computed.  The default is to
    compute the variance of the flattened array.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, a variance is performed over multiple axes,
    instead of a single axis or all the axes as before.
dtype : data-type, optional
    Type to use in computing the variance.  For arrays of integer type
    the default is `float64`; for arrays of float types it is the same as
    the array type.
out : ndarray, optional
    Alternate output array in which to place the result.  It must have
    the same shape as the expected output, but the type is cast if
    necessary.
ddof : int, optional
    "Delta Degrees of Freedom": the divisor used in the calculation is
    ``N - ddof``, where ``N`` represents the number of elements. By
    default `ddof` is zero.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `var` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

where : array_like of bool, optional
    Elements to include in the variance. See `~numpy.ufunc.reduce` for
    details.

    .. versionadded:: 1.20.0

Returns
-------
variance : ndarray, see dtype parameter above
    If ``out=None``, returns a new array containing the variance;
    otherwise, a reference to the output array is returned.

See Also
--------
std, mean, nanmean, nanstd, nanvar
:ref:`ufuncs-output-type`

Notes
-----
The variance is the average of the squared deviations from the mean,
i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.

The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``.
If, however, `ddof` is specified, the divisor ``N - ddof`` is used
instead.  In standard statistical practice, ``ddof=1`` provides an
unbiased estimator of the variance of a hypothetical infinite population.
``ddof=0`` provides a maximum likelihood estimate of the variance for
normally distributed variables.

Note that for complex numbers, the absolute value is taken before
squaring, so that the result is always real and nonnegative.

For floating-point input, the variance is computed using the same
precision the input has.  Depending on the input data, this can cause
the results to be inaccurate, especially for `float32` (see example
below).  Specifying a higher-accuracy accumulator using the ``dtype``
keyword can alleviate this issue.

Examples
--------
>>> a = np.array([[1, 2], [3, 4]])
>>> np.var(a)
1.25
>>> np.var(a, axis=0)
array([1.,  1.])
>>> np.var(a, axis=1)
array([0.25,  0.25])

In single precision, var() can be inaccurate:

>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.var(a)
0.20250003

Computing the variance in float64 is more accurate:

>>> np.var(a, dtype=np.float64)
0.20249999932944759 # may vary
>>> ((1-0.55)**2 + (0.1-0.55)**2)/2
0.2025

Specifying a where argument:

>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])
>>> np.var(a)
6.833333333333333 # may vary
>>> np.var(a, where=[[True], [True], [False]])
4.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FromnumericModule,
        stats:StatisticCalculationMethod .

stats:Corrcoef a owl:Class ;
    rdfs:comment """Return Pearson product-moment correlation coefficients.

Please refer to the documentation for `cov` for more detail.  The
relationship between the correlation coefficient matrix, `R`, and the
covariance matrix, `C`, is

.. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }

The values of `R` are between -1 and 1, inclusive.

Parameters
----------
x : array_like
    A 1-D or 2-D array containing multiple variables and observations.
    Each row of `x` represents a variable, and each column a single
    observation of all those variables. Also see `rowvar` below.
y : array_like, optional
    An additional set of variables and observations. `y` has the same
    shape as `x`.
rowvar : bool, optional
    If `rowvar` is True (default), then each row represents a
    variable, with observations in the columns. Otherwise, the relationship
    is transposed: each column represents a variable, while the rows
    contain observations.
bias : _NoValue, optional
    Has no effect, do not use.

    .. deprecated:: 1.10.0
ddof : _NoValue, optional
    Has no effect, do not use.

    .. deprecated:: 1.10.0
dtype : data-type, optional
    Data-type of the result. By default, the return data-type will have
    at least `numpy.float64` precision.

    .. versionadded:: 1.20

Returns
-------
R : ndarray
    The correlation coefficient matrix of the variables.

See Also
--------
cov : Covariance matrix

Notes
-----
Due to floating point rounding the resulting array may not be Hermitian,
the diagonal elements may not be 1, and the elements may not satisfy the
inequality abs(a) <= 1. The real and imaginary parts are clipped to the
interval [-1,  1] in an attempt to improve on that situation but is not
much help in the complex case.

This function accepts but discards arguments `bias` and `ddof`.  This is
for backwards compatibility with previous versions of this function.  These
arguments had no effect on the return values of the function and can be
safely ignored in this and previous versions of numpy.

Examples
--------
In this example we generate two random arrays, ``xarr`` and ``yarr``, and
compute the row-wise and column-wise Pearson correlation coefficients,
``R``. Since ``rowvar`` is  true by  default, we first find the row-wise
Pearson correlation coefficients between the variables of ``xarr``.

>>> import numpy as np
>>> rng = np.random.default_rng(seed=42)
>>> xarr = rng.random((3, 3))
>>> xarr
array([[0.77395605, 0.43887844, 0.85859792],
       [0.69736803, 0.09417735, 0.97562235],
       [0.7611397 , 0.78606431, 0.12811363]])
>>> R1 = np.corrcoef(xarr)
>>> R1
array([[ 1.        ,  0.99256089, -0.68080986],
       [ 0.99256089,  1.        , -0.76492172],
       [-0.68080986, -0.76492172,  1.        ]])

If we add another set of variables and observations ``yarr``, we can
compute the row-wise Pearson correlation coefficients between the
variables in ``xarr`` and ``yarr``.

>>> yarr = rng.random((3, 3))
>>> yarr
array([[0.45038594, 0.37079802, 0.92676499],
       [0.64386512, 0.82276161, 0.4434142 ],
       [0.22723872, 0.55458479, 0.06381726]])
>>> R2 = np.corrcoef(xarr, yarr)
>>> R2
array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,
        -0.99004057],
       [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,
        -0.99981569],
       [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,
         0.77714685],
       [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,
        -0.83571711],
       [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,
         0.97517215],
       [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,
         1.        ]])

Finally if we use the option ``rowvar=False``, the columns are now
being treated as the variables and we will find the column-wise Pearson
correlation coefficients between variables in ``xarr`` and ``yarr``.

>>> R3 = np.corrcoef(xarr, yarr, rowvar=False)
>>> R3
array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,
         0.22423734],
       [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,
        -0.44069024],
       [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,
         0.75137473],
       [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,
         0.47536961],
       [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,
        -0.46666491],
       [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,
         1.        ]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FunctionBaseModule,
        stats:StatisticCalculationMethod .

stats:Percentile a owl:Class ;
    rdfs:comment """Compute the q-th percentile of the data along the specified axis.

Returns the q-th percentile(s) of the array elements.

Parameters
----------
a : array_like of real numbers
    Input array or object that can be converted to an array.
q : array_like of float
    Percentage or sequence of percentages for the percentiles to compute.
    Values must be between 0 and 100 inclusive.
axis : {int, tuple of int, None}, optional
    Axis or axes along which the percentiles are computed. The
    default is to compute the percentile(s) along a flattened
    version of the array.

    .. versionchanged:: 1.9.0
        A tuple of axes is supported
out : ndarray, optional
    Alternative output array in which to place the result. It must
    have the same shape and buffer length as the expected output,
    but the type (of the output) will be cast if necessary.
overwrite_input : bool, optional
    If True, then allow the input array `a` to be modified by intermediate
    calculations, to save memory. In this case, the contents of the input
    `a` after this function completes is undefined.
method : str, optional
    This parameter specifies the method to use for estimating the
    percentile.  There are many different methods, some unique to NumPy.
    See the notes for explanation.  The options sorted by their R type
    as summarized in the H&F paper [1]_ are:

    1. 'inverted_cdf'
    2. 'averaged_inverted_cdf'
    3. 'closest_observation'
    4. 'interpolated_inverted_cdf'
    5. 'hazen'
    6. 'weibull'
    7. 'linear'  (default)
    8. 'median_unbiased'
    9. 'normal_unbiased'

    The first three methods are discontinuous.  NumPy further defines the
    following discontinuous variations of the default 'linear' (7.) option:

    * 'lower'
    * 'higher',
    * 'midpoint'
    * 'nearest'

    .. versionchanged:: 1.22.0
        This argument was previously called "interpolation" and only
        offered the "linear" default and last four options.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left in
    the result as dimensions with size one. With this option, the
    result will broadcast correctly against the original array `a`.

    .. versionadded:: 1.9.0

interpolation : str, optional
    Deprecated name for the method keyword argument.

    .. deprecated:: 1.22.0

Returns
-------
percentile : scalar or ndarray
    If `q` is a single percentile and `axis=None`, then the result
    is a scalar. If multiple percentiles are given, first axis of
    the result corresponds to the percentiles. The other axes are
    the axes that remain after the reduction of `a`. If the input
    contains integers or floats smaller than ``float64``, the output
    data-type is ``float64``. Otherwise, the output data-type is the
    same as that of the input. If `out` is specified, that array is
    returned instead.

See Also
--------
mean
median : equivalent to ``percentile(..., 50)``
nanpercentile
quantile : equivalent to percentile, except q in the range [0, 1].

Notes
-----
Given a vector ``V`` of length ``n``, the q-th percentile of ``V`` is
the value ``q/100`` of the way from the minimum to the maximum in a
sorted copy of ``V``. The values and distances of the two nearest
neighbors as well as the `method` parameter will determine the
percentile if the normalized ranking does not match the location of
``q`` exactly. This function is the same as the median if ``q=50``, the
same as the minimum if ``q=0`` and the same as the maximum if
``q=100``.

The optional `method` parameter specifies the method to use when the
desired percentile lies between two indexes ``i`` and ``j = i + 1``.
In that case, we first determine ``i + g``, a virtual index that lies
between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the
fractional part of the index. The final result is, then, an interpolation
of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``,
``i`` and ``j`` are modified using correction constants ``alpha`` and
``beta`` whose choices depend on the ``method`` used. Finally, note that
since Python uses 0-based indexing, the code subtracts another 1 from the
index internally.

The following formula determines the virtual index ``i + g``, the location
of the percentile in the sorted sample:

.. math::
    i + g = (q / 100) * ( n - alpha - beta + 1 ) + alpha

The different methods then work as follows

inverted_cdf:
    method 1 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then take i

averaged_inverted_cdf:
    method 2 of H&F [1]_.
    This method give discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then average between bounds

closest_observation:
    method 3 of H&F [1]_.
    This method give discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 and index is odd ; then take j
    * if g = 0 and index is even ; then take i

interpolated_inverted_cdf:
    method 4 of H&F [1]_.
    This method give continuous results using:

    * alpha = 0
    * beta = 1

hazen:
    method 5 of H&F [1]_.
    This method give continuous results using:

    * alpha = 1/2
    * beta = 1/2

weibull:
    method 6 of H&F [1]_.
    This method give continuous results using:

    * alpha = 0
    * beta = 0

linear:
    method 7 of H&F [1]_.
    This method give continuous results using:

    * alpha = 1
    * beta = 1

median_unbiased:
    method 8 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is unknown (see reference).
    This method give continuous results using:

    * alpha = 1/3
    * beta = 1/3

normal_unbiased:
    method 9 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is known to be normal.
    This method give continuous results using:

    * alpha = 3/8
    * beta = 3/8

lower:
    NumPy method kept for backwards compatibility.
    Takes ``i`` as the interpolation point.

higher:
    NumPy method kept for backwards compatibility.
    Takes ``j`` as the interpolation point.

nearest:
    NumPy method kept for backwards compatibility.
    Takes ``i`` or ``j``, whichever is nearest.

midpoint:
    NumPy method kept for backwards compatibility.
    Uses ``(i + j) / 2``.

Examples
--------
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> np.percentile(a, 50)
3.5
>>> np.percentile(a, 50, axis=0)
array([6.5, 4.5, 2.5])
>>> np.percentile(a, 50, axis=1)
array([7.,  2.])
>>> np.percentile(a, 50, axis=1, keepdims=True)
array([[7.],
       [2.]])

>>> m = np.percentile(a, 50, axis=0)
>>> out = np.zeros_like(m)
>>> np.percentile(a, 50, axis=0, out=out)
array([6.5, 4.5, 2.5])
>>> m
array([6.5, 4.5, 2.5])

>>> b = a.copy()
>>> np.percentile(b, 50, axis=1, overwrite_input=True)
array([7.,  2.])
>>> assert not np.all(a == b)

The different methods can be visualized graphically:

.. plot::

    import matplotlib.pyplot as plt

    a = np.arange(4)
    p = np.linspace(0, 100, 6001)
    ax = plt.gca()
    lines = [
        ('linear', '-', 'C0'),
        ('inverted_cdf', ':', 'C1'),
        # Almost the same as `inverted_cdf`:
        ('averaged_inverted_cdf', '-.', 'C1'),
        ('closest_observation', ':', 'C2'),
        ('interpolated_inverted_cdf', '--', 'C1'),
        ('hazen', '--', 'C3'),
        ('weibull', '-.', 'C4'),
        ('median_unbiased', '--', 'C5'),
        ('normal_unbiased', '-.', 'C6'),
        ]
    for method, style, color in lines:
        ax.plot(
            p, np.percentile(a, p, method=method),
            label=method, linestyle=style, color=color)
    ax.set(
        title='Percentiles for different methods and data: ' + str(a),
        xlabel='Percentile',
        ylabel='Estimated percentile value',
        yticks=a)
    ax.legend(bbox_to_anchor=(1.03, 1))
    plt.tight_layout()
    plt.show()

References
----------
.. [1] R. J. Hyndman and Y. Fan,
   "Sample quantiles in statistical packages,"
   The American Statistician, 50(4), pp. 361-365, 1996""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FunctionBaseModule,
        stats:StatisticCalculationMethod .

stats:Quantile a owl:Class ;
    rdfs:comment """Compute the q-th quantile of the data along the specified axis.

.. versionadded:: 1.15.0

Parameters
----------
a : array_like of real numbers
    Input array or object that can be converted to an array.
q : array_like of float
    Probability or sequence of probabilities for the quantiles to compute.
    Values must be between 0 and 1 inclusive.
axis : {int, tuple of int, None}, optional
    Axis or axes along which the quantiles are computed. The default is
    to compute the quantile(s) along a flattened version of the array.
out : ndarray, optional
    Alternative output array in which to place the result. It must have
    the same shape and buffer length as the expected output, but the
    type (of the output) will be cast if necessary.
overwrite_input : bool, optional
    If True, then allow the input array `a` to be modified by
    intermediate calculations, to save memory. In this case, the
    contents of the input `a` after this function completes is
    undefined.
method : str, optional
    This parameter specifies the method to use for estimating the
    quantile.  There are many different methods, some unique to NumPy.
    See the notes for explanation.  The options sorted by their R type
    as summarized in the H&F paper [1]_ are:

    1. 'inverted_cdf'
    2. 'averaged_inverted_cdf'
    3. 'closest_observation'
    4. 'interpolated_inverted_cdf'
    5. 'hazen'
    6. 'weibull'
    7. 'linear'  (default)
    8. 'median_unbiased'
    9. 'normal_unbiased'

    The first three methods are discontinuous.  NumPy further defines the
    following discontinuous variations of the default 'linear' (7.) option:

    * 'lower'
    * 'higher',
    * 'midpoint'
    * 'nearest'

    .. versionchanged:: 1.22.0
        This argument was previously called "interpolation" and only
        offered the "linear" default and last four options.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left in
    the result as dimensions with size one. With this option, the
    result will broadcast correctly against the original array `a`.

interpolation : str, optional
    Deprecated name for the method keyword argument.

    .. deprecated:: 1.22.0

Returns
-------
quantile : scalar or ndarray
    If `q` is a single probability and `axis=None`, then the result
    is a scalar. If multiple probabilies levels are given, first axis of
    the result corresponds to the quantiles. The other axes are
    the axes that remain after the reduction of `a`. If the input
    contains integers or floats smaller than ``float64``, the output
    data-type is ``float64``. Otherwise, the output data-type is the
    same as that of the input. If `out` is specified, that array is
    returned instead.

See Also
--------
mean
percentile : equivalent to quantile, but with q in the range [0, 100].
median : equivalent to ``quantile(..., 0.5)``
nanquantile

Notes
-----
Given a vector ``V`` of length ``n``, the q-th quantile of ``V`` is
the value ``q`` of the way from the minimum to the maximum in a
sorted copy of ``V``. The values and distances of the two nearest
neighbors as well as the `method` parameter will determine the
quantile if the normalized ranking does not match the location of
``q`` exactly. This function is the same as the median if ``q=0.5``, the
same as the minimum if ``q=0.0`` and the same as the maximum if
``q=1.0``.

The optional `method` parameter specifies the method to use when the
desired quantile lies between two indexes ``i`` and ``j = i + 1``.
In that case, we first determine ``i + g``, a virtual index that lies
between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the
fractional part of the index. The final result is, then, an interpolation
of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``,
``i`` and ``j`` are modified using correction constants ``alpha`` and
``beta`` whose choices depend on the ``method`` used. Finally, note that
since Python uses 0-based indexing, the code subtracts another 1 from the
index internally.

The following formula determines the virtual index ``i + g``, the location
of the quantile in the sorted sample:

.. math::
    i + g = q * ( n - alpha - beta + 1 ) + alpha

The different methods then work as follows

inverted_cdf:
    method 1 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then take i

averaged_inverted_cdf:
    method 2 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then average between bounds

closest_observation:
    method 3 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 and index is odd ; then take j
    * if g = 0 and index is even ; then take i

interpolated_inverted_cdf:
    method 4 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 0
    * beta = 1

hazen:
    method 5 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 1/2
    * beta = 1/2

weibull:
    method 6 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 0
    * beta = 0

linear:
    method 7 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 1
    * beta = 1

median_unbiased:
    method 8 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is unknown (see reference).
    This method gives continuous results using:

    * alpha = 1/3
    * beta = 1/3

normal_unbiased:
    method 9 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is known to be normal.
    This method gives continuous results using:

    * alpha = 3/8
    * beta = 3/8

lower:
    NumPy method kept for backwards compatibility.
    Takes ``i`` as the interpolation point.

higher:
    NumPy method kept for backwards compatibility.
    Takes ``j`` as the interpolation point.

nearest:
    NumPy method kept for backwards compatibility.
    Takes ``i`` or ``j``, whichever is nearest.

midpoint:
    NumPy method kept for backwards compatibility.
    Uses ``(i + j) / 2``.

Examples
--------
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> np.quantile(a, 0.5)
3.5
>>> np.quantile(a, 0.5, axis=0)
array([6.5, 4.5, 2.5])
>>> np.quantile(a, 0.5, axis=1)
array([7.,  2.])
>>> np.quantile(a, 0.5, axis=1, keepdims=True)
array([[7.],
       [2.]])
>>> m = np.quantile(a, 0.5, axis=0)
>>> out = np.zeros_like(m)
>>> np.quantile(a, 0.5, axis=0, out=out)
array([6.5, 4.5, 2.5])
>>> m
array([6.5, 4.5, 2.5])
>>> b = a.copy()
>>> np.quantile(b, 0.5, axis=1, overwrite_input=True)
array([7.,  2.])
>>> assert not np.all(a == b)

See also `numpy.percentile` for a visualization of most methods.

References
----------
.. [1] R. J. Hyndman and Y. Fan,
   "Sample quantiles in statistical packages,"
   The American Statistician, 50(4), pp. 361-365, 1996""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FunctionBaseModule,
        stats:StatisticCalculationMethod .

stats:Cov a owl:Class ;
    rdfs:comment """Estimate a covariance matrix, given data and weights.

Covariance indicates the level to which two variables vary together.
If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,
then the covariance matrix element :math:`C_{ij}` is the covariance of
:math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
of :math:`x_i`.

See the notes for an outline of the algorithm.

Parameters
----------
m : array_like
    A 1-D or 2-D array containing multiple variables and observations.
    Each row of `m` represents a variable, and each column a single
    observation of all those variables. Also see `rowvar` below.
y : array_like, optional
    An additional set of variables and observations. `y` has the same form
    as that of `m`.
rowvar : bool, optional
    If `rowvar` is True (default), then each row represents a
    variable, with observations in the columns. Otherwise, the relationship
    is transposed: each column represents a variable, while the rows
    contain observations.
bias : bool, optional
    Default normalization (False) is by ``(N - 1)``, where ``N`` is the
    number of observations given (unbiased estimate). If `bias` is True,
    then normalization is by ``N``. These values can be overridden by using
    the keyword ``ddof`` in numpy versions >= 1.5.
ddof : int, optional
    If not ``None`` the default value implied by `bias` is overridden.
    Note that ``ddof=1`` will return the unbiased estimate, even if both
    `fweights` and `aweights` are specified, and ``ddof=0`` will return
    the simple average. See the notes for the details. The default value
    is ``None``.

    .. versionadded:: 1.5
fweights : array_like, int, optional
    1-D array of integer frequency weights; the number of times each
    observation vector should be repeated.

    .. versionadded:: 1.10
aweights : array_like, optional
    1-D array of observation vector weights. These relative weights are
    typically large for observations considered "important" and smaller for
    observations considered less "important". If ``ddof=0`` the array of
    weights can be used to assign probabilities to observation vectors.

    .. versionadded:: 1.10
dtype : data-type, optional
    Data-type of the result. By default, the return data-type will have
    at least `numpy.float64` precision.

    .. versionadded:: 1.20

Returns
-------
out : ndarray
    The covariance matrix of the variables.

See Also
--------
corrcoef : Normalized covariance matrix

Notes
-----
Assume that the observations are in the columns of the observation
array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The
steps to compute the weighted covariance are as follows::

    >>> m = np.arange(10, dtype=np.float64)
    >>> f = np.arange(10) * 2
    >>> a = np.arange(10) ** 2.
    >>> ddof = 1
    >>> w = f * a
    >>> v1 = np.sum(w)
    >>> v2 = np.sum(w * a)
    >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1
    >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)

Note that when ``a == 1``, the normalization factor
``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``
as it should.

Examples
--------
Consider two variables, :math:`x_0` and :math:`x_1`, which
correlate perfectly, but in opposite directions:

>>> x = np.array([[0, 2], [1, 1], [2, 0]]).T
>>> x
array([[0, 1, 2],
       [2, 1, 0]])

Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance
matrix shows this clearly:

>>> np.cov(x)
array([[ 1., -1.],
       [-1.,  1.]])

Note that element :math:`C_{0,1}`, which shows the correlation between
:math:`x_0` and :math:`x_1`, is negative.

Further, note how `x` and `y` are combined:

>>> x = [-2.1, -1,  4.3]
>>> y = [3,  1.1,  0.12]
>>> X = np.stack((x, y), axis=0)
>>> np.cov(X)
array([[11.71      , -4.286     ], # may vary
       [-4.286     ,  2.144133]])
>>> np.cov(x, y)
array([[11.71      , -4.286     ], # may vary
       [-4.286     ,  2.144133]])
>>> np.cov(x)
array(11.71)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:FunctionBaseModule,
        stats:StatisticCalculationMethod .

