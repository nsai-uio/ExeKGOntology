@prefix ds: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix stats: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/stats_exeKGOntology.ttl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

stats:hasCentralTendencyMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:CentralTendencyMeasure ;
    rdfs:range stats:Average,
        stats:Median ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:hasDependencyMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:DependencyMeasure ;
    rdfs:range stats:Corrcoef,
        stats:Cov ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:hasParamAweights a owl:DatatypeProperty ;
    rdfs:domain stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamAxis a owl:DatatypeProperty ;
    rdfs:domain stats:Average,
        stats:Median,
        stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamBias a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:boolean,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamDdof a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamDtype a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamFweights a owl:DatatypeProperty ;
    rdfs:domain stats:Cov ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamInterpolation a owl:DatatypeProperty ;
    rdfs:domain stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamKeepdims a owl:DatatypeProperty ;
    rdfs:domain stats:Average,
        stats:Median,
        stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamM a owl:DatatypeProperty ;
    rdfs:domain stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamMethod a owl:DatatypeProperty ;
    rdfs:domain stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamOverwriteInput a owl:DatatypeProperty ;
    rdfs:domain stats:Median,
        stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamQ a owl:DatatypeProperty ;
    rdfs:domain stats:Percentile,
        stats:Quantile ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamReturned a owl:DatatypeProperty ;
    rdfs:domain stats:Average ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamRowvar a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamWeights a owl:DatatypeProperty ;
    rdfs:domain stats:Average ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamX a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasParamY a owl:DatatypeProperty ;
    rdfs:domain stats:Corrcoef,
        stats:Cov ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

stats:hasPositionMeasureMethod a owl:ObjectProperty ;
    rdfs:domain stats:PositionMeasure ;
    rdfs:range stats:Percentile,
        stats:Quantile ;
    rdfs:subPropertyOf stats:hasStatisticCalculationMethod .

stats:CentralTendencyMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:DependencyMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:LibModule a owl:Class ;
    rdfs:subClassOf stats:NumpyModule .

stats:NumpyModule a owl:Class ;
    rdfs:subClassOf ds:Module .

stats:PositionMeasure a owl:Class ;
    rdfs:subClassOf ds:AtomicTask,
        stats:StatisticCalculation .

stats:StatisticCalculation a owl:Class .

stats:Median a owl:Class ;
    rdfs:comment """Compute the median along the specified axis.

Returns the median of the array elements.

Parameters
----------
a : array_like
    Input array or object that can be converted to an array.
axis : {int, sequence of int, None}, optional
    Axis or axes along which the medians are computed. The default
    is to compute the median along a flattened version of the array.
    A sequence of axes is supported since version 1.9.0.
out : ndarray, optional
    Alternative output array in which to place the result. It must
    have the same shape and buffer length as the expected output,
    but the type (of the output) will be cast if necessary.
overwrite_input : bool, optional
   If True, then allow use of memory of input array `a` for
   calculations. The input array will be modified by the call to
   `median`. This will save memory when you do not need to preserve
   the contents of the input array. Treat the input as undefined,
   but it will probably be fully or partially sorted. Default is
   False. If `overwrite_input` is ``True`` and `a` is not already an
   `ndarray`, an error will be raised.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the original `arr`.

    .. versionadded:: 1.9.0

Returns
-------
median : ndarray
    A new array holding the result. If the input contains integers
    or floats smaller than ``float64``, then the output data-type is
    ``np.float64``.  Otherwise, the data-type of the output is the
    same as that of the input. If `out` is specified, that array is
    returned instead.

See Also
--------
mean, percentile

Notes
-----
Given a vector ``V`` of length ``N``, the median of ``V`` is the
middle value of a sorted copy of ``V``, ``V_sorted`` - i
e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the
two middle values of ``V_sorted`` when ``N`` is even.

Examples
--------
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> np.median(a)
3.5
>>> np.median(a, axis=0)
array([6.5, 4.5, 2.5])
>>> np.median(a, axis=1)
array([7.,  2.])
>>> m = np.median(a, axis=0)
>>> out = np.zeros_like(m)
>>> np.median(a, axis=0, out=m)
array([6.5,  4.5,  2.5])
>>> m
array([6.5,  4.5,  2.5])
>>> b = a.copy()
>>> np.median(b, axis=1, overwrite_input=True)
array([7.,  2.])
>>> assert not np.all(a==b)
>>> b = a.copy()
>>> np.median(b, axis=None, overwrite_input=True)
3.5
>>> assert not np.all(a==b)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod ;
    ds:hasModule stats:FunctionBaseModule .

stats:Average a owl:Class ;
    rdfs:comment """Compute the weighted average along the specified axis.

Parameters
----------
a : array_like
    Array containing data to be averaged. If `a` is not an array, a
    conversion is attempted.
axis : None or int or tuple of ints, optional
    Axis or axes along which to average `a`.  The default,
    axis=None, will average over all of the elements of the input array.
    If axis is negative it counts from the last to the first axis.

    .. versionadded:: 1.7.0

    If axis is a tuple of ints, averaging is performed on all of the axes
    specified in the tuple instead of a single axis or all the axes as
    before.
weights : array_like, optional
    An array of weights associated with the values in `a`. Each value in
    `a` contributes to the average according to its associated weight.
    The weights array can either be 1-D (in which case its length must be
    the size of `a` along the given axis) or of the same shape as `a`.
    If `weights=None`, then all data in `a` are assumed to have a
    weight equal to one.  The 1-D calculation is::

        avg = sum(a * weights) / sum(weights)

    The only constraint on `weights` is that `sum(weights)` must not be 0.
returned : bool, optional
    Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`)
    is returned, otherwise only the average is returned.
    If `weights=None`, `sum_of_weights` is equivalent to the number of
    elements over which the average is taken.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the original `a`.
    *Note:* `keepdims` will not work with instances of `numpy.matrix`
    or other classes whose methods do not support `keepdims`.

    .. versionadded:: 1.23.0

Returns
-------
retval, [sum_of_weights] : array_type or double
    Return the average along the specified axis. When `returned` is `True`,
    return a tuple with the average as the first element and the sum
    of the weights as the second element. `sum_of_weights` is of the
    same type as `retval`. The result dtype follows a genereal pattern.
    If `weights` is None, the result dtype will be that of `a` , or ``float64``
    if `a` is integral. Otherwise, if `weights` is not None and `a` is non-
    integral, the result type will be the type of lowest precision capable of
    representing values of both `a` and `weights`. If `a` happens to be
    integral, the previous rules still applies but the result dtype will
    at least be ``float64``.

Raises
------
ZeroDivisionError
    When all weights along axis are zero. See `numpy.ma.average` for a
    version robust to this type of error.
TypeError
    When the length of 1D `weights` is not the same as the shape of `a`
    along axis.

See Also
--------
mean

ma.average : average for masked arrays -- useful if your data contains
             "missing" values
numpy.result_type : Returns the type that results from applying the
                    numpy type promotion rules to the arguments.

Examples
--------
>>> data = np.arange(1, 5)
>>> data
array([1, 2, 3, 4])
>>> np.average(data)
2.5
>>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1))
4.0

>>> data = np.arange(6).reshape((3, 2))
>>> data
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> np.average(data, axis=1, weights=[1./4, 3./4])
array([0.75, 2.75, 4.75])
>>> np.average(data, weights=[1./4, 3./4])
Traceback (most recent call last):
    ...
TypeError: Axis must be specified when shapes of a and weights differ.

>>> a = np.ones(5, dtype=np.float128)
>>> w = np.ones(5, dtype=np.complex64)
>>> avg = np.average(a, weights=w)
>>> print(avg.dtype)
complex256

With ``keepdims=True``, the following result has shape (3, 1).

>>> np.average(data, axis=1, keepdims=True)
array([[0.5],
       [2.5],
       [4.5]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod ;
    ds:hasModule stats:FunctionBaseModule .

stats:FunctionBaseModule a owl:Class ;
    rdfs:subClassOf stats:LibModule .

stats:Corrcoef a owl:Class ;
    rdfs:comment """Return Pearson product-moment correlation coefficients.

Please refer to the documentation for `cov` for more detail.  The
relationship between the correlation coefficient matrix, `R`, and the
covariance matrix, `C`, is

.. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }

The values of `R` are between -1 and 1, inclusive.

Parameters
----------
x : array_like
    A 1-D or 2-D array containing multiple variables and observations.
    Each row of `x` represents a variable, and each column a single
    observation of all those variables. Also see `rowvar` below.
y : array_like, optional
    An additional set of variables and observations. `y` has the same
    shape as `x`.
rowvar : bool, optional
    If `rowvar` is True (default), then each row represents a
    variable, with observations in the columns. Otherwise, the relationship
    is transposed: each column represents a variable, while the rows
    contain observations.
bias : _NoValue, optional
    Has no effect, do not use.

    .. deprecated:: 1.10.0
ddof : _NoValue, optional
    Has no effect, do not use.

    .. deprecated:: 1.10.0
dtype : data-type, optional
    Data-type of the result. By default, the return data-type will have
    at least `numpy.float64` precision.

    .. versionadded:: 1.20

Returns
-------
R : ndarray
    The correlation coefficient matrix of the variables.

See Also
--------
cov : Covariance matrix

Notes
-----
Due to floating point rounding the resulting array may not be Hermitian,
the diagonal elements may not be 1, and the elements may not satisfy the
inequality abs(a) <= 1. The real and imaginary parts are clipped to the
interval [-1,  1] in an attempt to improve on that situation but is not
much help in the complex case.

This function accepts but discards arguments `bias` and `ddof`.  This is
for backwards compatibility with previous versions of this function.  These
arguments had no effect on the return values of the function and can be
safely ignored in this and previous versions of numpy.

Examples
--------
In this example we generate two random arrays, ``xarr`` and ``yarr``, and
compute the row-wise and column-wise Pearson correlation coefficients,
``R``. Since ``rowvar`` is  true by  default, we first find the row-wise
Pearson correlation coefficients between the variables of ``xarr``.

>>> import numpy as np
>>> rng = np.random.default_rng(seed=42)
>>> xarr = rng.random((3, 3))
>>> xarr
array([[0.77395605, 0.43887844, 0.85859792],
       [0.69736803, 0.09417735, 0.97562235],
       [0.7611397 , 0.78606431, 0.12811363]])
>>> R1 = np.corrcoef(xarr)
>>> R1
array([[ 1.        ,  0.99256089, -0.68080986],
       [ 0.99256089,  1.        , -0.76492172],
       [-0.68080986, -0.76492172,  1.        ]])

If we add another set of variables and observations ``yarr``, we can
compute the row-wise Pearson correlation coefficients between the
variables in ``xarr`` and ``yarr``.

>>> yarr = rng.random((3, 3))
>>> yarr
array([[0.45038594, 0.37079802, 0.92676499],
       [0.64386512, 0.82276161, 0.4434142 ],
       [0.22723872, 0.55458479, 0.06381726]])
>>> R2 = np.corrcoef(xarr, yarr)
>>> R2
array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,
        -0.99004057],
       [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,
        -0.99981569],
       [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,
         0.77714685],
       [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,
        -0.83571711],
       [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,
         0.97517215],
       [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,
         1.        ]])

Finally if we use the option ``rowvar=False``, the columns are now
being treated as the variables and we will find the column-wise Pearson
correlation coefficients between variables in ``xarr`` and ``yarr``.

>>> R3 = np.corrcoef(xarr, yarr, rowvar=False)
>>> R3
array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,
         0.22423734],
       [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,
        -0.44069024],
       [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,
         0.75137473],
       [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,
         0.47536961],
       [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,
        -0.46666491],
       [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,
         1.        ]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod ;
    ds:hasModule stats:FunctionBaseModule .

stats:Percentile a owl:Class ;
    rdfs:comment """Compute the q-th percentile of the data along the specified axis.

Returns the q-th percentile(s) of the array elements.

Parameters
----------
a : array_like of real numbers
    Input array or object that can be converted to an array.
q : array_like of float
    Percentage or sequence of percentages for the percentiles to compute.
    Values must be between 0 and 100 inclusive.
axis : {int, tuple of int, None}, optional
    Axis or axes along which the percentiles are computed. The
    default is to compute the percentile(s) along a flattened
    version of the array.

    .. versionchanged:: 1.9.0
        A tuple of axes is supported
out : ndarray, optional
    Alternative output array in which to place the result. It must
    have the same shape and buffer length as the expected output,
    but the type (of the output) will be cast if necessary.
overwrite_input : bool, optional
    If True, then allow the input array `a` to be modified by intermediate
    calculations, to save memory. In this case, the contents of the input
    `a` after this function completes is undefined.
method : str, optional
    This parameter specifies the method to use for estimating the
    percentile.  There are many different methods, some unique to NumPy.
    See the notes for explanation.  The options sorted by their R type
    as summarized in the H&F paper [1]_ are:

    1. 'inverted_cdf'
    2. 'averaged_inverted_cdf'
    3. 'closest_observation'
    4. 'interpolated_inverted_cdf'
    5. 'hazen'
    6. 'weibull'
    7. 'linear'  (default)
    8. 'median_unbiased'
    9. 'normal_unbiased'

    The first three methods are discontinuous.  NumPy further defines the
    following discontinuous variations of the default 'linear' (7.) option:

    * 'lower'
    * 'higher',
    * 'midpoint'
    * 'nearest'

    .. versionchanged:: 1.22.0
        This argument was previously called "interpolation" and only
        offered the "linear" default and last four options.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left in
    the result as dimensions with size one. With this option, the
    result will broadcast correctly against the original array `a`.

    .. versionadded:: 1.9.0

interpolation : str, optional
    Deprecated name for the method keyword argument.

    .. deprecated:: 1.22.0

Returns
-------
percentile : scalar or ndarray
    If `q` is a single percentile and `axis=None`, then the result
    is a scalar. If multiple percentiles are given, first axis of
    the result corresponds to the percentiles. The other axes are
    the axes that remain after the reduction of `a`. If the input
    contains integers or floats smaller than ``float64``, the output
    data-type is ``float64``. Otherwise, the output data-type is the
    same as that of the input. If `out` is specified, that array is
    returned instead.

See Also
--------
mean
median : equivalent to ``percentile(..., 50)``
nanpercentile
quantile : equivalent to percentile, except q in the range [0, 1].

Notes
-----
Given a vector ``V`` of length ``n``, the q-th percentile of ``V`` is
the value ``q/100`` of the way from the minimum to the maximum in a
sorted copy of ``V``. The values and distances of the two nearest
neighbors as well as the `method` parameter will determine the
percentile if the normalized ranking does not match the location of
``q`` exactly. This function is the same as the median if ``q=50``, the
same as the minimum if ``q=0`` and the same as the maximum if
``q=100``.

The optional `method` parameter specifies the method to use when the
desired percentile lies between two indexes ``i`` and ``j = i + 1``.
In that case, we first determine ``i + g``, a virtual index that lies
between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the
fractional part of the index. The final result is, then, an interpolation
of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``,
``i`` and ``j`` are modified using correction constants ``alpha`` and
``beta`` whose choices depend on the ``method`` used. Finally, note that
since Python uses 0-based indexing, the code subtracts another 1 from the
index internally.

The following formula determines the virtual index ``i + g``, the location
of the percentile in the sorted sample:

.. math::
    i + g = (q / 100) * ( n - alpha - beta + 1 ) + alpha

The different methods then work as follows

inverted_cdf:
    method 1 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then take i

averaged_inverted_cdf:
    method 2 of H&F [1]_.
    This method give discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then average between bounds

closest_observation:
    method 3 of H&F [1]_.
    This method give discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 and index is odd ; then take j
    * if g = 0 and index is even ; then take i

interpolated_inverted_cdf:
    method 4 of H&F [1]_.
    This method give continuous results using:

    * alpha = 0
    * beta = 1

hazen:
    method 5 of H&F [1]_.
    This method give continuous results using:

    * alpha = 1/2
    * beta = 1/2

weibull:
    method 6 of H&F [1]_.
    This method give continuous results using:

    * alpha = 0
    * beta = 0

linear:
    method 7 of H&F [1]_.
    This method give continuous results using:

    * alpha = 1
    * beta = 1

median_unbiased:
    method 8 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is unknown (see reference).
    This method give continuous results using:

    * alpha = 1/3
    * beta = 1/3

normal_unbiased:
    method 9 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is known to be normal.
    This method give continuous results using:

    * alpha = 3/8
    * beta = 3/8

lower:
    NumPy method kept for backwards compatibility.
    Takes ``i`` as the interpolation point.

higher:
    NumPy method kept for backwards compatibility.
    Takes ``j`` as the interpolation point.

nearest:
    NumPy method kept for backwards compatibility.
    Takes ``i`` or ``j``, whichever is nearest.

midpoint:
    NumPy method kept for backwards compatibility.
    Uses ``(i + j) / 2``.

Examples
--------
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> np.percentile(a, 50)
3.5
>>> np.percentile(a, 50, axis=0)
array([6.5, 4.5, 2.5])
>>> np.percentile(a, 50, axis=1)
array([7.,  2.])
>>> np.percentile(a, 50, axis=1, keepdims=True)
array([[7.],
       [2.]])

>>> m = np.percentile(a, 50, axis=0)
>>> out = np.zeros_like(m)
>>> np.percentile(a, 50, axis=0, out=out)
array([6.5, 4.5, 2.5])
>>> m
array([6.5, 4.5, 2.5])

>>> b = a.copy()
>>> np.percentile(b, 50, axis=1, overwrite_input=True)
array([7.,  2.])
>>> assert not np.all(a == b)

The different methods can be visualized graphically:

.. plot::

    import matplotlib.pyplot as plt

    a = np.arange(4)
    p = np.linspace(0, 100, 6001)
    ax = plt.gca()
    lines = [
        ('linear', '-', 'C0'),
        ('inverted_cdf', ':', 'C1'),
        # Almost the same as `inverted_cdf`:
        ('averaged_inverted_cdf', '-.', 'C1'),
        ('closest_observation', ':', 'C2'),
        ('interpolated_inverted_cdf', '--', 'C1'),
        ('hazen', '--', 'C3'),
        ('weibull', '-.', 'C4'),
        ('median_unbiased', '--', 'C5'),
        ('normal_unbiased', '-.', 'C6'),
        ]
    for method, style, color in lines:
        ax.plot(
            p, np.percentile(a, p, method=method),
            label=method, linestyle=style, color=color)
    ax.set(
        title='Percentiles for different methods and data: ' + str(a),
        xlabel='Percentile',
        ylabel='Estimated percentile value',
        yticks=a)
    ax.legend(bbox_to_anchor=(1.03, 1))
    plt.tight_layout()
    plt.show()

References
----------
.. [1] R. J. Hyndman and Y. Fan,
   "Sample quantiles in statistical packages,"
   The American Statistician, 50(4), pp. 361-365, 1996""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod ;
    ds:hasModule stats:FunctionBaseModule .

stats:Quantile a owl:Class ;
    rdfs:comment """Compute the q-th quantile of the data along the specified axis.

.. versionadded:: 1.15.0

Parameters
----------
a : array_like of real numbers
    Input array or object that can be converted to an array.
q : array_like of float
    Probability or sequence of probabilities for the quantiles to compute.
    Values must be between 0 and 1 inclusive.
axis : {int, tuple of int, None}, optional
    Axis or axes along which the quantiles are computed. The default is
    to compute the quantile(s) along a flattened version of the array.
out : ndarray, optional
    Alternative output array in which to place the result. It must have
    the same shape and buffer length as the expected output, but the
    type (of the output) will be cast if necessary.
overwrite_input : bool, optional
    If True, then allow the input array `a` to be modified by
    intermediate calculations, to save memory. In this case, the
    contents of the input `a` after this function completes is
    undefined.
method : str, optional
    This parameter specifies the method to use for estimating the
    quantile.  There are many different methods, some unique to NumPy.
    See the notes for explanation.  The options sorted by their R type
    as summarized in the H&F paper [1]_ are:

    1. 'inverted_cdf'
    2. 'averaged_inverted_cdf'
    3. 'closest_observation'
    4. 'interpolated_inverted_cdf'
    5. 'hazen'
    6. 'weibull'
    7. 'linear'  (default)
    8. 'median_unbiased'
    9. 'normal_unbiased'

    The first three methods are discontinuous.  NumPy further defines the
    following discontinuous variations of the default 'linear' (7.) option:

    * 'lower'
    * 'higher',
    * 'midpoint'
    * 'nearest'

    .. versionchanged:: 1.22.0
        This argument was previously called "interpolation" and only
        offered the "linear" default and last four options.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left in
    the result as dimensions with size one. With this option, the
    result will broadcast correctly against the original array `a`.

interpolation : str, optional
    Deprecated name for the method keyword argument.

    .. deprecated:: 1.22.0

Returns
-------
quantile : scalar or ndarray
    If `q` is a single probability and `axis=None`, then the result
    is a scalar. If multiple probabilies levels are given, first axis of
    the result corresponds to the quantiles. The other axes are
    the axes that remain after the reduction of `a`. If the input
    contains integers or floats smaller than ``float64``, the output
    data-type is ``float64``. Otherwise, the output data-type is the
    same as that of the input. If `out` is specified, that array is
    returned instead.

See Also
--------
mean
percentile : equivalent to quantile, but with q in the range [0, 100].
median : equivalent to ``quantile(..., 0.5)``
nanquantile

Notes
-----
Given a vector ``V`` of length ``n``, the q-th quantile of ``V`` is
the value ``q`` of the way from the minimum to the maximum in a
sorted copy of ``V``. The values and distances of the two nearest
neighbors as well as the `method` parameter will determine the
quantile if the normalized ranking does not match the location of
``q`` exactly. This function is the same as the median if ``q=0.5``, the
same as the minimum if ``q=0.0`` and the same as the maximum if
``q=1.0``.

The optional `method` parameter specifies the method to use when the
desired quantile lies between two indexes ``i`` and ``j = i + 1``.
In that case, we first determine ``i + g``, a virtual index that lies
between ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the
fractional part of the index. The final result is, then, an interpolation
of ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``,
``i`` and ``j`` are modified using correction constants ``alpha`` and
``beta`` whose choices depend on the ``method`` used. Finally, note that
since Python uses 0-based indexing, the code subtracts another 1 from the
index internally.

The following formula determines the virtual index ``i + g``, the location
of the quantile in the sorted sample:

.. math::
    i + g = q * ( n - alpha - beta + 1 ) + alpha

The different methods then work as follows

inverted_cdf:
    method 1 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then take i

averaged_inverted_cdf:
    method 2 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 ; then average between bounds

closest_observation:
    method 3 of H&F [1]_.
    This method gives discontinuous results:

    * if g > 0 ; then take j
    * if g = 0 and index is odd ; then take j
    * if g = 0 and index is even ; then take i

interpolated_inverted_cdf:
    method 4 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 0
    * beta = 1

hazen:
    method 5 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 1/2
    * beta = 1/2

weibull:
    method 6 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 0
    * beta = 0

linear:
    method 7 of H&F [1]_.
    This method gives continuous results using:

    * alpha = 1
    * beta = 1

median_unbiased:
    method 8 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is unknown (see reference).
    This method gives continuous results using:

    * alpha = 1/3
    * beta = 1/3

normal_unbiased:
    method 9 of H&F [1]_.
    This method is probably the best method if the sample
    distribution function is known to be normal.
    This method gives continuous results using:

    * alpha = 3/8
    * beta = 3/8

lower:
    NumPy method kept for backwards compatibility.
    Takes ``i`` as the interpolation point.

higher:
    NumPy method kept for backwards compatibility.
    Takes ``j`` as the interpolation point.

nearest:
    NumPy method kept for backwards compatibility.
    Takes ``i`` or ``j``, whichever is nearest.

midpoint:
    NumPy method kept for backwards compatibility.
    Uses ``(i + j) / 2``.

Examples
--------
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> np.quantile(a, 0.5)
3.5
>>> np.quantile(a, 0.5, axis=0)
array([6.5, 4.5, 2.5])
>>> np.quantile(a, 0.5, axis=1)
array([7.,  2.])
>>> np.quantile(a, 0.5, axis=1, keepdims=True)
array([[7.],
       [2.]])
>>> m = np.quantile(a, 0.5, axis=0)
>>> out = np.zeros_like(m)
>>> np.quantile(a, 0.5, axis=0, out=out)
array([6.5, 4.5, 2.5])
>>> m
array([6.5, 4.5, 2.5])
>>> b = a.copy()
>>> np.quantile(b, 0.5, axis=1, overwrite_input=True)
array([7.,  2.])
>>> assert not np.all(a == b)

See also `numpy.percentile` for a visualization of most methods.

References
----------
.. [1] R. J. Hyndman and Y. Fan,
   "Sample quantiles in statistical packages,"
   The American Statistician, 50(4), pp. 361-365, 1996""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod ;
    ds:hasModule stats:FunctionBaseModule .

stats:Cov a owl:Class ;
    rdfs:comment """Estimate a covariance matrix, given data and weights.

Covariance indicates the level to which two variables vary together.
If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,
then the covariance matrix element :math:`C_{ij}` is the covariance of
:math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
of :math:`x_i`.

See the notes for an outline of the algorithm.

Parameters
----------
m : array_like
    A 1-D or 2-D array containing multiple variables and observations.
    Each row of `m` represents a variable, and each column a single
    observation of all those variables. Also see `rowvar` below.
y : array_like, optional
    An additional set of variables and observations. `y` has the same form
    as that of `m`.
rowvar : bool, optional
    If `rowvar` is True (default), then each row represents a
    variable, with observations in the columns. Otherwise, the relationship
    is transposed: each column represents a variable, while the rows
    contain observations.
bias : bool, optional
    Default normalization (False) is by ``(N - 1)``, where ``N`` is the
    number of observations given (unbiased estimate). If `bias` is True,
    then normalization is by ``N``. These values can be overridden by using
    the keyword ``ddof`` in numpy versions >= 1.5.
ddof : int, optional
    If not ``None`` the default value implied by `bias` is overridden.
    Note that ``ddof=1`` will return the unbiased estimate, even if both
    `fweights` and `aweights` are specified, and ``ddof=0`` will return
    the simple average. See the notes for the details. The default value
    is ``None``.

    .. versionadded:: 1.5
fweights : array_like, int, optional
    1-D array of integer frequency weights; the number of times each
    observation vector should be repeated.

    .. versionadded:: 1.10
aweights : array_like, optional
    1-D array of observation vector weights. These relative weights are
    typically large for observations considered "important" and smaller for
    observations considered less "important". If ``ddof=0`` the array of
    weights can be used to assign probabilities to observation vectors.

    .. versionadded:: 1.10
dtype : data-type, optional
    Data-type of the result. By default, the return data-type will have
    at least `numpy.float64` precision.

    .. versionadded:: 1.20

Returns
-------
out : ndarray
    The covariance matrix of the variables.

See Also
--------
corrcoef : Normalized covariance matrix

Notes
-----
Assume that the observations are in the columns of the observation
array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The
steps to compute the weighted covariance are as follows::

    >>> m = np.arange(10, dtype=np.float64)
    >>> f = np.arange(10) * 2
    >>> a = np.arange(10) ** 2.
    >>> ddof = 1
    >>> w = f * a
    >>> v1 = np.sum(w)
    >>> v2 = np.sum(w * a)
    >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1
    >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)

Note that when ``a == 1``, the normalization factor
``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``
as it should.

Examples
--------
Consider two variables, :math:`x_0` and :math:`x_1`, which
correlate perfectly, but in opposite directions:

>>> x = np.array([[0, 2], [1, 1], [2, 0]]).T
>>> x
array([[0, 1, 2],
       [2, 1, 0]])

Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance
matrix shows this clearly:

>>> np.cov(x)
array([[ 1., -1.],
       [-1.,  1.]])

Note that element :math:`C_{0,1}`, which shows the correlation between
:math:`x_0` and :math:`x_1`, is negative.

Further, note how `x` and `y` are combined:

>>> x = [-2.1, -1,  4.3]
>>> y = [3,  1.1,  0.12]
>>> X = np.stack((x, y), axis=0)
>>> np.cov(X)
array([[11.71      , -4.286     ], # may vary
       [-4.286     ,  2.144133]])
>>> np.cov(x, y)
array([[11.71      , -4.286     ], # may vary
       [-4.286     ,  2.144133]])
>>> np.cov(x)
array(11.71)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        stats:StatisticCalculationMethod ;
    ds:hasModule stats:FunctionBaseModule .

