@prefix ds: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#> .
@prefix ml: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

ml:hasBinaryClassificationMethod a owl:ObjectProperty ;
    rdfs:domain ml:BinaryClassification ;
    rdfs:range ml:AdaBoostClassifierMethod,
        ml:BaggingClassifierMethod,
        ml:ExtraTreesClassifierMethod,
        ml:GradientBoostingClassifierMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:RandomForestClassifierMethod,
        ml:StackingClassifierMethod,
        ml:VotingClassifierMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasBoostingRegressionMethod a owl:ObjectProperty ;
    rdfs:domain ml:BoostingRegression ;
    rdfs:range ml:AdaBoostRegressorMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasClusteringMethod a owl:ObjectProperty ;
    rdfs:domain ml:Clustering ;
    rdfs:range ml:IsolationForestMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasDataProcessingMethod a owl:ObjectProperty ;
    rdfs:domain ml:DataProcessing ;
    rdfs:range ml:RandomTreesEmbeddingMethod ;
    rdfs:subPropertyOf ml:hasPrepareTransformerMethod .

ml:hasMulticlassClassificationMethod a owl:ObjectProperty ;
    rdfs:domain ml:MulticlassClassification ;
    rdfs:range ml:AdaBoostClassifierMethod,
        ml:ExtraTreesClassifierMethod,
        ml:GradientBoostingClassifierMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:RandomForestClassifierMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasMultilabelClassificationMethod a owl:ObjectProperty ;
    rdfs:domain ml:MultilabelClassification ;
    rdfs:range ml:AdaBoostClassifierMethod,
        ml:ExtraTreesClassifierMethod,
        ml:GradientBoostingClassifierMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:RandomForestClassifierMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasParamAlgorithm a owl:DatatypeProperty ;
    rdfs:domain ml:AdaBoostClassifierMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAlpha a owl:DatatypeProperty ;
    rdfs:domain ml:GradientBoostingRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBootstrap a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBootstrapFeatures a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCategoricalFeatures a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCcpAlpha a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamClassWeight a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:RandomForestClassifierMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamContamination a owl:DatatypeProperty ;
    rdfs:domain ml:IsolationForestMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCriterion a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCv a owl:DatatypeProperty ;
    rdfs:domain ml:StackingClassifierMethod,
        ml:StackingRegressorMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEarlyStopping a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEstimator a owl:DatatypeProperty ;
    rdfs:domain ml:AdaBoostClassifierMethod,
        ml:AdaBoostRegressorMethod,
        ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEstimators a owl:DatatypeProperty ;
    rdfs:domain ml:StackingClassifierMethod,
        ml:StackingRegressorMethod,
        ml:VotingClassifierMethod,
        ml:VotingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamFinalEstimator a owl:DatatypeProperty ;
    rdfs:domain ml:StackingClassifierMethod,
        ml:StackingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamFlattenTransform a owl:DatatypeProperty ;
    rdfs:domain ml:VotingClassifierMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamInit a owl:DatatypeProperty ;
    rdfs:domain ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamInteractionCst a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamL2Regularization a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamLearningRate a owl:DatatypeProperty ;
    rdfs:domain ml:AdaBoostClassifierMethod,
        ml:AdaBoostRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamLoss a owl:DatatypeProperty ;
    rdfs:domain ml:AdaBoostRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxBins a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxDepth a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxFeatures a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:range xsd:float,
        xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxIter a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxLeafNodes a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxSamples a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinImpurityDecrease a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinSamplesLeaf a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinSamplesSplit a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinWeightFractionLeaf a owl:DatatypeProperty ;
    rdfs:domain ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMonotonicCst a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNEstimators a owl:DatatypeProperty ;
    rdfs:domain ml:AdaBoostClassifierMethod,
        ml:AdaBoostRegressorMethod,
        ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNIterNoChange a owl:DatatypeProperty ;
    rdfs:domain ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNJobs a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod,
        ml:StackingClassifierMethod,
        ml:StackingRegressorMethod,
        ml:VotingClassifierMethod,
        ml:VotingRegressorMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamOobScore a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:range xsd:boolean,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPassthrough a owl:DatatypeProperty ;
    rdfs:domain ml:StackingClassifierMethod,
        ml:StackingRegressorMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamQuantile a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamRandomState a owl:DatatypeProperty ;
    rdfs:domain ml:AdaBoostClassifierMethod,
        ml:AdaBoostRegressorMethod,
        ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamScoring a owl:DatatypeProperty ;
    rdfs:domain ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSparseOutput a owl:DatatypeProperty ;
    rdfs:domain ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamStackMethod a owl:DatatypeProperty ;
    rdfs:domain ml:StackingClassifierMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSubsample a owl:DatatypeProperty ;
    rdfs:domain ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamTol a owl:DatatypeProperty ;
    rdfs:domain ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:float ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamValidationFraction a owl:DatatypeProperty ;
    rdfs:domain ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod ;
    rdfs:range xsd:float,
        xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamVerbose a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod,
        ml:StackingClassifierMethod,
        ml:StackingRegressorMethod,
        ml:VotingClassifierMethod,
        ml:VotingRegressorMethod ;
    rdfs:range xsd:boolean,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamVoting a owl:DatatypeProperty ;
    rdfs:domain ml:VotingClassifierMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamWarmStart a owl:DatatypeProperty ;
    rdfs:domain ml:BaggingClassifierMethod,
        ml:BaggingRegressorMethod,
        ml:ExtraTreesClassifierMethod,
        ml:ExtraTreesRegressorMethod,
        ml:GradientBoostingClassifierMethod,
        ml:GradientBoostingRegressorMethod,
        ml:HistGradientBoostingClassifierMethod,
        ml:HistGradientBoostingRegressorMethod,
        ml:IsolationForestMethod,
        ml:RandomForestClassifierMethod,
        ml:RandomForestRegressorMethod,
        ml:RandomTreesEmbeddingMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasRandomForestRegressionMethod a owl:ObjectProperty ;
    rdfs:domain ml:RandomForestRegression ;
    rdfs:range ml:ExtraTreesRegressorMethod,
        ml:RandomForestRegressorMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasRegularizedRegressionMethod a owl:ObjectProperty ;
    rdfs:domain ml:RegularizedRegression ;
    rdfs:range ml:BaggingRegressorMethod,
        ml:StackingRegressorMethod,
        ml:VotingRegressorMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:SklearnModule a owl:Class ;
    rdfs:subClassOf ds:Module .

ml:GradientBoostingModule a owl:Class ;
    rdfs:subClassOf ml:EnsembleModule .

ml:VotingRegressorMethod a owl:Class ;
    rdfs:comment """Prediction voting regressor for unfitted estimators.

A voting regressor is an ensemble meta-estimator that fits several base
regressors, each on the whole dataset. Then it averages the individual
predictions to form a final prediction.

Read more in the :ref:`User Guide <voting_regressor>`.

.. versionadded:: 0.21

Parameters
----------
estimators : list of (str, estimator) tuples
    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones
    of those original estimators that will be stored in the class attribute
    ``self.estimators_``. An estimator can be set to ``'drop'`` using
    :meth:`set_params`.

    .. versionchanged:: 0.21
        ``'drop'`` is accepted. Using None was deprecated in 0.22 and
        support was removed in 0.24.

weights : array-like of shape (n_regressors,), default=None
    Sequence of weights (`float` or `int`) to weight the occurrences of
    predicted values before averaging. Uses uniform weights if `None`.

n_jobs : int, default=None
    The number of jobs to run in parallel for ``fit``.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : bool, default=False
    If True, the time elapsed while fitting will be printed as it
    is completed.

    .. versionadded:: 0.23

Attributes
----------
estimators_ : list of regressors
    The collection of fitted sub-estimators as defined in ``estimators``
    that are not 'drop'.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

    .. versionadded:: 0.20

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying regressor exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

See Also
--------
VotingClassifier : Soft Voting/Majority Rule classifier.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.ensemble import VotingRegressor
>>> from sklearn.neighbors import KNeighborsRegressor
>>> r1 = LinearRegression()
>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)
>>> r3 = KNeighborsRegressor()
>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])
>>> y = np.array([2, 6, 12, 20, 30, 42])
>>> er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])
>>> print(er.fit(X, y).predict(X))
[ 6.8...  8.4... 12.5... 17.8... 26...  34...]

In the following example, we drop the `'lr'` estimator with
:meth:`~VotingRegressor.set_params` and fit the remaining two estimators:

>>> er = er.set_params(lr='drop')
>>> er = er.fit(X, y)
>>> len(er.estimators_)
2""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:AdaBoostRegressorMethod a owl:Class ;
    rdfs:comment """An AdaBoost regressor.

An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.

This class implements the algorithm known as AdaBoost.R2 [2].

Read more in the :ref:`User Guide <adaboost>`.

.. versionadded:: 0.14

Parameters
----------
estimator : object, default=None
    The base estimator from which the boosted ensemble is built.
    If ``None``, then the base estimator is
    :class:`~sklearn.tree.DecisionTreeRegressor` initialized with
    `max_depth=3`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=50
    The maximum number of estimators at which boosting is terminated.
    In case of perfect fit, the learning procedure is stopped early.
    Values must be in the range `[1, inf)`.

learning_rate : float, default=1.0
    Weight applied to each regressor at each boosting iteration. A higher
    learning rate increases the contribution of each regressor. There is
    a trade-off between the `learning_rate` and `n_estimators` parameters.
    Values must be in the range `(0.0, inf)`.

loss : {'linear', 'square', 'exponential'}, default='linear'
    The loss function to use when updating the weights after each
    boosting iteration.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given at each `estimator` at each
    boosting iteration.
    Thus, it is only used when `estimator` exposes a `random_state`.
    In addition, it controls the bootstrap of the weights used to train the
    `estimator` at each boosting iteration.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of regressors
    The collection of fitted sub-estimators.

estimator_weights_ : ndarray of floats
    Weights for each estimator in the boosted ensemble.

estimator_errors_ : ndarray of floats
    Regression error for each estimator in the boosted ensemble.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances if supported by the
    ``estimator`` (when based on decision trees).

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
AdaBoostClassifier : An AdaBoost classifier.
GradientBoostingRegressor : Gradient Boosting Classification Tree.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.

References
----------
.. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
       on-Line Learning and an Application to Boosting", 1995.

.. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.

Examples
--------
>>> from sklearn.ensemble import AdaBoostRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
>>> regr.fit(X, y)
AdaBoostRegressor(n_estimators=100, random_state=0)
>>> regr.predict([[0, 0, 0, 0]])
array([4.7972...])
>>> regr.score(X, y)
0.9771...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:VotingClassifierMethod a owl:Class ;
    rdfs:comment """Soft Voting/Majority Rule classifier for unfitted estimators.

Read more in the :ref:`User Guide <voting_classifier>`.

.. versionadded:: 0.17

Parameters
----------
estimators : list of (str, estimator) tuples
    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones
    of those original estimators that will be stored in the class attribute
    ``self.estimators_``. An estimator can be set to ``'drop'`` using
    :meth:`set_params`.

    .. versionchanged:: 0.21
        ``'drop'`` is accepted. Using None was deprecated in 0.22 and
        support was removed in 0.24.

voting : {'hard', 'soft'}, default='hard'
    If 'hard', uses predicted class labels for majority rule voting.
    Else if 'soft', predicts the class label based on the argmax of
    the sums of the predicted probabilities, which is recommended for
    an ensemble of well-calibrated classifiers.

weights : array-like of shape (n_classifiers,), default=None
    Sequence of weights (`float` or `int`) to weight the occurrences of
    predicted class labels (`hard` voting) or class probabilities
    before averaging (`soft` voting). Uses uniform weights if `None`.

n_jobs : int, default=None
    The number of jobs to run in parallel for ``fit``.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionadded:: 0.18

flatten_transform : bool, default=True
    Affects shape of transform output only when voting='soft'
    If voting='soft' and flatten_transform=True, transform method returns
    matrix with shape (n_samples, n_classifiers * n_classes). If
    flatten_transform=False, it returns
    (n_classifiers, n_samples, n_classes).

verbose : bool, default=False
    If True, the time elapsed while fitting will be printed as it
    is completed.

    .. versionadded:: 0.23

Attributes
----------
estimators_ : list of classifiers
    The collection of fitted sub-estimators as defined in ``estimators``
    that are not 'drop'.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

    .. versionadded:: 0.20

le_ : :class:`~sklearn.preprocessing.LabelEncoder`
    Transformer used to encode the labels during fit and decode during
    prediction.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying classifier exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

See Also
--------
VotingRegressor : Prediction voting regressor.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)
>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
>>> clf3 = GaussianNB()
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> eclf1 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
>>> eclf1 = eclf1.fit(X, y)
>>> print(eclf1.predict(X))
[1 1 1 2 2 2]
>>> np.array_equal(eclf1.named_estimators_.lr.predict(X),
...                eclf1.named_estimators_['lr'].predict(X))
True
>>> eclf2 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...         voting='soft')
>>> eclf2 = eclf2.fit(X, y)
>>> print(eclf2.predict(X))
[1 1 1 2 2 2]

To drop an estimator, :meth:`set_params` can be used to remove it. Here we
dropped one of the estimators, resulting in 2 fitted estimators:

>>> eclf2 = eclf2.set_params(lr='drop')
>>> eclf2 = eclf2.fit(X, y)
>>> len(eclf2.estimators_)
2

Setting `flatten_transform=True` with `voting='soft'` flattens output shape of
`transform`:

>>> eclf3 = VotingClassifier(estimators=[
...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...        voting='soft', weights=[2,1,1],
...        flatten_transform=True)
>>> eclf3 = eclf3.fit(X, y)
>>> print(eclf3.predict(X))
[1 1 1 2 2 2]
>>> print(eclf3.transform(X).shape)
(6, 6)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:StackingRegressorMethod a owl:Class ;
    rdfs:comment """Stack of estimators with a final regressor.

Stacked generalization consists in stacking the output of individual
estimator and use a regressor to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.

Note that `estimators_` are fitted on the full `X` while `final_estimator_`
is trained using cross-validated predictions of the base estimators using
`cross_val_predict`.

Read more in the :ref:`User Guide <stacking>`.

.. versionadded:: 0.22

Parameters
----------
estimators : list of (str, estimator)
    Base estimators which will be stacked together. Each element of the
    list is defined as a tuple of string (i.e. name) and an estimator
    instance. An estimator can be set to 'drop' using `set_params`.

final_estimator : estimator, default=None
    A regressor which will be used to combine the base estimators.
    The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.

cv : int, cross-validation generator, iterable, or "prefit", default=None
    Determines the cross-validation splitting strategy used in
    `cross_val_predict` to train `final_estimator`. Possible inputs for
    cv are:

    * None, to use the default 5-fold cross validation,
    * integer, to specify the number of folds in a (Stratified) KFold,
    * An object to be used as a cross-validation generator,
    * An iterable yielding train, test splits.
    * "prefit" to assume the `estimators` are prefit, and skip cross validation

    For integer/None inputs, if the estimator is a classifier and y is
    either binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used.
    In all other cases, :class:`~sklearn.model_selection.KFold` is used.
    These splitters are instantiated with `shuffle=False` so the splits
    will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    If "prefit" is passed, it is assumed that all `estimators` have
    been fitted already. The `final_estimator_` is trained on the `estimators`
    predictions on the full training set and are **not** cross validated
    predictions. Please note that if the models have been trained on the same
    data to train the stacking model, there is a very high risk of overfitting.

    .. versionadded:: 1.1
        The 'prefit' option was added in 1.1

    .. note::
       A larger number of split will provide no benefits if the number
       of training samples is large enough. Indeed, the training time
       will increase. ``cv`` is not used for model evaluation but for
       prediction.

n_jobs : int, default=None
    The number of jobs to run in parallel for `fit` of all `estimators`.
    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means
    using all processors. See Glossary for more details.

passthrough : bool, default=False
    When False, only the predictions of estimators will be used as
    training data for `final_estimator`. When True, the
    `final_estimator` is trained on the predictions as well as the
    original training data.

verbose : int, default=0
    Verbosity level.

Attributes
----------
estimators_ : list of estimator
    The elements of the `estimators` parameter, having been fitted on the
    training data. If an estimator has been set to `'drop'`, it
    will not appear in `estimators_`. When `cv="prefit"`, `estimators_`
    is set to `estimators` and is not fitted again.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying regressor exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

final_estimator_ : estimator
    The regressor to stacked the base estimators fitted.

stack_method_ : list of str
    The method used by each base estimator.

See Also
--------
StackingClassifier : Stack of estimators with a final classifier.

References
----------
.. [1] Wolpert, David H. "Stacked generalization." Neural networks 5.2
   (1992): 241-259.

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.linear_model import RidgeCV
>>> from sklearn.svm import LinearSVR
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.ensemble import StackingRegressor
>>> X, y = load_diabetes(return_X_y=True)
>>> estimators = [
...     ('lr', RidgeCV()),
...     ('svr', LinearSVR(dual="auto", random_state=42))
... ]
>>> reg = StackingRegressor(
...     estimators=estimators,
...     final_estimator=RandomForestRegressor(n_estimators=10,
...                                           random_state=42)
... )
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=42
... )
>>> reg.fit(X_train, y_train).score(X_test, y_test)
0.3...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:AdaBoostClassifierMethod a owl:Class ;
    rdfs:comment """An AdaBoost classifier.

An AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.

This class implements the algorithm based on [2]_.

Read more in the :ref:`User Guide <adaboost>`.

.. versionadded:: 0.14

Parameters
----------
estimator : object, default=None
    The base estimator from which the boosted ensemble is built.
    Support for sample weighting is required, as well as proper
    ``classes_`` and ``n_classes_`` attributes. If ``None``, then
    the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`
    initialized with `max_depth=1`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=50
    The maximum number of estimators at which boosting is terminated.
    In case of perfect fit, the learning procedure is stopped early.
    Values must be in the range `[1, inf)`.

learning_rate : float, default=1.0
    Weight applied to each classifier at each boosting iteration. A higher
    learning rate increases the contribution of each classifier. There is
    a trade-off between the `learning_rate` and `n_estimators` parameters.
    Values must be in the range `(0.0, inf)`.

algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'
    If 'SAMME.R' then use the SAMME.R real boosting algorithm.
    ``estimator`` must support calculation of class probabilities.
    If 'SAMME' then use the SAMME discrete boosting algorithm.
    The SAMME.R algorithm typically converges faster than SAMME,
    achieving a lower test error with fewer boosting iterations.

    .. deprecated:: 1.4
        `"SAMME.R"` is deprecated and will be removed in version 1.6.
        '"SAMME"' will become the default.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given at each `estimator` at each
    boosting iteration.
    Thus, it is only used when `estimator` exposes a `random_state`.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of classifiers
    The collection of fitted sub-estimators.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_classes_ : int
    The number of classes.

estimator_weights_ : ndarray of floats
    Weights for each estimator in the boosted ensemble.

estimator_errors_ : ndarray of floats
    Classification error for each estimator in the boosted
    ensemble.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances if supported by the
    ``estimator`` (when based on decision trees).

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
AdaBoostRegressor : An AdaBoost regressor that begins by fitting a
    regressor on the original dataset and then fits additional copies of
    the regressor on the same dataset but where the weights of instances
    are adjusted according to the error of the current prediction.

GradientBoostingClassifier : GB builds an additive model in a forward
    stage-wise fashion. Regression trees are fit on the negative gradient
    of the binomial or multinomial deviance loss function. Binary
    classification is a special case where only a single regression tree is
    induced.

sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning
    method used for classification.
    Creates a model that predicts the value of a target variable by
    learning simple decision rules inferred from the data features.

References
----------
.. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
       on-Line Learning and an Application to Boosting", 1995.

.. [2] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class adaboost."
       Statistics and its Interface 2.3 (2009): 349-360.
       <10.4310/SII.2009.v2.n3.a8>`

Examples
--------
>>> from sklearn.ensemble import AdaBoostClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = AdaBoostClassifier(n_estimators=100, algorithm="SAMME", random_state=0)
>>> clf.fit(X, y)
AdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)
>>> clf.predict([[0, 0, 0, 0]])
array([1])
>>> clf.score(X, y)
0.96...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:StackingClassifierMethod a owl:Class ;
    rdfs:comment """Stack of estimators with a final classifier.

Stacked generalization consists in stacking the output of individual
estimator and use a classifier to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.

Note that `estimators_` are fitted on the full `X` while `final_estimator_`
is trained using cross-validated predictions of the base estimators using
`cross_val_predict`.

Read more in the :ref:`User Guide <stacking>`.

.. versionadded:: 0.22

Parameters
----------
estimators : list of (str, estimator)
    Base estimators which will be stacked together. Each element of the
    list is defined as a tuple of string (i.e. name) and an estimator
    instance. An estimator can be set to 'drop' using `set_params`.

    The type of estimator is generally expected to be a classifier.
    However, one can pass a regressor for some use case (e.g. ordinal
    regression).

final_estimator : estimator, default=None
    A classifier which will be used to combine the base estimators.
    The default classifier is a
    :class:`~sklearn.linear_model.LogisticRegression`.

cv : int, cross-validation generator, iterable, or "prefit", default=None
    Determines the cross-validation splitting strategy used in
    `cross_val_predict` to train `final_estimator`. Possible inputs for
    cv are:

    * None, to use the default 5-fold cross validation,
    * integer, to specify the number of folds in a (Stratified) KFold,
    * An object to be used as a cross-validation generator,
    * An iterable yielding train, test splits,
    * `"prefit"` to assume the `estimators` are prefit. In this case, the
      estimators will not be refitted.

    For integer/None inputs, if the estimator is a classifier and y is
    either binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used.
    In all other cases, :class:`~sklearn.model_selection.KFold` is used.
    These splitters are instantiated with `shuffle=False` so the splits
    will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    If "prefit" is passed, it is assumed that all `estimators` have
    been fitted already. The `final_estimator_` is trained on the `estimators`
    predictions on the full training set and are **not** cross validated
    predictions. Please note that if the models have been trained on the same
    data to train the stacking model, there is a very high risk of overfitting.

    .. versionadded:: 1.1
        The 'prefit' option was added in 1.1

    .. note::
       A larger number of split will provide no benefits if the number
       of training samples is large enough. Indeed, the training time
       will increase. ``cv`` is not used for model evaluation but for
       prediction.

stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'
    Methods called for each base estimator. It can be:

    * if 'auto', it will try to invoke, for each estimator,
      `'predict_proba'`, `'decision_function'` or `'predict'` in that
      order.
    * otherwise, one of `'predict_proba'`, `'decision_function'` or
      `'predict'`. If the method is not implemented by the estimator, it
      will raise an error.

n_jobs : int, default=None
    The number of jobs to run in parallel all `estimators` `fit`.
    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means
    using all processors. See Glossary for more details.

passthrough : bool, default=False
    When False, only the predictions of estimators will be used as
    training data for `final_estimator`. When True, the
    `final_estimator` is trained on the predictions as well as the
    original training data.

verbose : int, default=0
    Verbosity level.

Attributes
----------
classes_ : ndarray of shape (n_classes,) or list of ndarray if `y`         is of type `"multilabel-indicator"`.
    Class labels.

estimators_ : list of estimators
    The elements of the `estimators` parameter, having been fitted on the
    training data. If an estimator has been set to `'drop'`, it
    will not appear in `estimators_`. When `cv="prefit"`, `estimators_`
    is set to `estimators` and is not fitted again.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying classifier exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

final_estimator_ : estimator
    The classifier which predicts given the output of `estimators_`.

stack_method_ : list of str
    The method used by each base estimator.

See Also
--------
StackingRegressor : Stack of estimators with a final regressor.

Notes
-----
When `predict_proba` is used by each estimator (i.e. most of the time for
`stack_method='auto'` or specifically for `stack_method='predict_proba'`),
The first column predicted by each estimator will be dropped in the case
of a binary classification problem. Indeed, both feature will be perfectly
collinear.

In some cases (e.g. ordinal regression), one can pass regressors as the
first layer of the :class:`StackingClassifier`. However, note that `y` will
be internally encoded in a numerically increasing order or lexicographic
order. If this ordering is not adequate, one should manually numerically
encode the classes in the desired order.

References
----------
.. [1] Wolpert, David H. "Stacked generalization." Neural networks 5.2
   (1992): 241-259.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.svm import LinearSVC
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.ensemble import StackingClassifier
>>> X, y = load_iris(return_X_y=True)
>>> estimators = [
...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
...     ('svr', make_pipeline(StandardScaler(),
...                           LinearSVC(dual="auto", random_state=42)))
... ]
>>> clf = StackingClassifier(
...     estimators=estimators, final_estimator=LogisticRegression()
... )
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, stratify=y, random_state=42
... )
>>> clf.fit(X_train, y_train).score(X_test, y_test)
0.9...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:IsolationForestMethod a owl:Class ;
    rdfs:comment """Isolation Forest Algorithm.

Return the anomaly score of each sample using the IsolationForest algorithm

The IsolationForest 'isolates' observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.

Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.

This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.

Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.

Read more in the :ref:`User Guide <isolation_forest>`.

.. versionadded:: 0.18

Parameters
----------
n_estimators : int, default=100
    The number of base estimators in the ensemble.

max_samples : "auto", int or float, default="auto"
    The number of samples to draw from X to train each base estimator.
        - If int, then draw `max_samples` samples.
        - If float, then draw `max_samples * X.shape[0]` samples.
        - If "auto", then `max_samples=min(256, n_samples)`.

    If max_samples is larger than the number of samples provided,
    all samples will be used for all trees (no sampling).

contamination : 'auto' or float, default='auto'
    The amount of contamination of the data set, i.e. the proportion
    of outliers in the data set. Used when fitting to define the threshold
    on the scores of the samples.

        - If 'auto', the threshold is determined as in the
          original paper.
        - If float, the contamination should be in the range (0, 0.5].

    .. versionchanged:: 0.22
       The default value of ``contamination`` changed from 0.1
       to ``'auto'``.

max_features : int or float, default=1.0
    The number of features to draw from X to train each base estimator.

        - If int, then draw `max_features` features.
        - If float, then draw `max(1, int(max_features * n_features_in_))` features.

    Note: using a float number less than 1.0 or integer less than number of
    features will enable feature subsampling and leads to a longer runtime.

bootstrap : bool, default=False
    If True, individual trees are fit on random subsets of the training
    data sampled with replacement. If False, sampling without replacement
    is performed.

n_jobs : int, default=None
    The number of jobs to run in parallel for both :meth:`fit` and
    :meth:`predict`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo-randomness of the selection of the feature
    and split values for each branching step and each tree in the forest.

    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    Controls the verbosity of the tree building process.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`the Glossary <warm_start>`.

    .. versionadded:: 0.21

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
    The child estimator template used to create the collection of
    fitted sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of ExtraTreeRegressor instances
    The collection of fitted sub-estimators.

estimators_features_ : list of ndarray
    The subset of drawn features for each base estimator.

estimators_samples_ : list of ndarray
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator.

max_samples_ : int
    The actual number of samples.

offset_ : float
    Offset used to define the decision function from the raw scores. We
    have the relation: ``decision_function = score_samples - offset_``.
    ``offset_`` is defined as follows. When the contamination parameter is
    set to "auto", the offset is equal to -0.5 as the scores of inliers are
    close to 0 and the scores of outliers are close to -1. When a
    contamination parameter different than "auto" is provided, the offset
    is defined in such a way we obtain the expected number of outliers
    (samples with decision function < 0) in training.

    .. versionadded:: 0.20

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a
    Gaussian distributed dataset.
sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.
    Estimate the support of a high-dimensional distribution.
    The implementation is based on libsvm.
sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection
    using Local Outlier Factor (LOF).

Notes
-----
The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to ``ceil(log_2(n))`` where
:math:`n` is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).

References
----------
.. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. "Isolation forest."
       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.
.. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. "Isolation-based
       anomaly detection." ACM Transactions on Knowledge Discovery from
       Data (TKDD) 6.1 (2012): 3.

Examples
--------
>>> from sklearn.ensemble import IsolationForest
>>> X = [[-1.1], [0.3], [0.5], [100]]
>>> clf = IsolationForest(random_state=0).fit(X)
>>> clf.predict([[0.1], [0], [90]])
array([ 1,  1, -1])

For an example of using isolation forest for anomaly detection see
:ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py`.""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:BaggingClassifierMethod a owl:Class ;
    rdfs:comment """A Bagging classifier.

A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.

This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting [1]_. If samples are drawn with
replacement, then the method is known as Bagging [2]_. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces [3]_. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches [4]_.

Read more in the :ref:`User Guide <bagging>`.

.. versionadded:: 0.15

Parameters
----------
estimator : object, default=None
    The base estimator to fit on random subsets of the dataset.
    If None, then the base estimator is a
    :class:`~sklearn.tree.DecisionTreeClassifier`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=10
    The number of base estimators in the ensemble.

max_samples : int or float, default=1.0
    The number of samples to draw from X to train each base estimator (with
    replacement by default, see `bootstrap` for more details).

    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples.

max_features : int or float, default=1.0
    The number of features to draw from X to train each base estimator (
    without replacement by default, see `bootstrap_features` for more
    details).

    - If int, then draw `max_features` features.
    - If float, then draw `max(1, int(max_features * n_features_in_))` features.

bootstrap : bool, default=True
    Whether samples are drawn with replacement. If False, sampling
    without replacement is performed.

bootstrap_features : bool, default=False
    Whether features are drawn with replacement.

oob_score : bool, default=False
    Whether to use out-of-bag samples to estimate
    the generalization error. Only available if bootstrap=True.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit
    a whole new ensemble. See :term:`the Glossary <warm_start>`.

    .. versionadded:: 0.17
       *warm_start* constructor parameter.

n_jobs : int, default=None
    The number of jobs to run in parallel for both :meth:`fit` and
    :meth:`predict`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the random resampling of the original dataset
    (sample wise and feature wise).
    If the base estimator accepts a `random_state` attribute, a different
    seed is generated for each instance in the ensemble.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

estimators_ : list of estimators
    The collection of fitted base estimators.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

estimators_features_ : list of arrays
    The subset of drawn features for each base estimator.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_classes_ : int or list
    The number of classes.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_decision_function_ : ndarray of shape (n_samples, n_classes)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_decision_function_` might contain NaN. This attribute exists
    only when ``oob_score`` is True.

See Also
--------
BaggingRegressor : A Bagging regressor.

References
----------

.. [1] L. Breiman, "Pasting small votes for classification in large
       databases and on-line", Machine Learning, 36(1), 85-103, 1999.

.. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
       1996.

.. [3] T. Ho, "The random subspace method for constructing decision
       forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
       1998.

.. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
       Learning and Knowledge Discovery in Databases, 346-361, 2012.

Examples
--------
>>> from sklearn.svm import SVC
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=100, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = BaggingClassifier(estimator=SVC(),
...                         n_estimators=10, random_state=0).fit(X, y)
>>> clf.predict([[0, 0, 0, 0]])
array([1])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:BaggingRegressorMethod a owl:Class ;
    rdfs:comment """A Bagging regressor.

A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.

This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting [1]_. If samples are drawn with
replacement, then the method is known as Bagging [2]_. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces [3]_. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches [4]_.

Read more in the :ref:`User Guide <bagging>`.

.. versionadded:: 0.15

Parameters
----------
estimator : object, default=None
    The base estimator to fit on random subsets of the dataset.
    If None, then the base estimator is a
    :class:`~sklearn.tree.DecisionTreeRegressor`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=10
    The number of base estimators in the ensemble.

max_samples : int or float, default=1.0
    The number of samples to draw from X to train each base estimator (with
    replacement by default, see `bootstrap` for more details).

    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples.

max_features : int or float, default=1.0
    The number of features to draw from X to train each base estimator (
    without replacement by default, see `bootstrap_features` for more
    details).

    - If int, then draw `max_features` features.
    - If float, then draw `max(1, int(max_features * n_features_in_))` features.

bootstrap : bool, default=True
    Whether samples are drawn with replacement. If False, sampling
    without replacement is performed.

bootstrap_features : bool, default=False
    Whether features are drawn with replacement.

oob_score : bool, default=False
    Whether to use out-of-bag samples to estimate
    the generalization error. Only available if bootstrap=True.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit
    a whole new ensemble. See :term:`the Glossary <warm_start>`.

n_jobs : int, default=None
    The number of jobs to run in parallel for both :meth:`fit` and
    :meth:`predict`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the random resampling of the original dataset
    (sample wise and feature wise).
    If the base estimator accepts a `random_state` attribute, a different
    seed is generated for each instance in the ensemble.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

estimators_ : list of estimators
    The collection of fitted sub-estimators.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

estimators_features_ : list of arrays
    The subset of drawn features for each base estimator.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_prediction_ : ndarray of shape (n_samples,)
    Prediction computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_prediction_` might contain NaN. This attribute exists only
    when ``oob_score`` is True.

See Also
--------
BaggingClassifier : A Bagging classifier.

References
----------

.. [1] L. Breiman, "Pasting small votes for classification in large
       databases and on-line", Machine Learning, 36(1), 85-103, 1999.

.. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
       1996.

.. [3] T. Ho, "The random subspace method for constructing decision
       forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
       1998.

.. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
       Learning and Knowledge Discovery in Databases, 346-361, 2012.

Examples
--------
>>> from sklearn.svm import SVR
>>> from sklearn.ensemble import BaggingRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_samples=100, n_features=4,
...                        n_informative=2, n_targets=1,
...                        random_state=0, shuffle=False)
>>> regr = BaggingRegressor(estimator=SVR(),
...                         n_estimators=10, random_state=0).fit(X, y)
>>> regr.predict([[0, 0, 0, 0]])
array([-2.8720...])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:RandomTreesEmbeddingMethod a owl:Class ;
    rdfs:comment """An ensemble of totally random trees.

An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as there are trees in
the forest.

The dimensionality of the resulting representation is
``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.

Read more in the :ref:`User Guide <random_trees_embedding>`.

Parameters
----------
n_estimators : int, default=100
    Number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

max_depth : int, default=5
    The maximum depth of each tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` is the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` is the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

sparse_output : bool, default=True
    Whether or not to return a sparse CSR matrix, as default behavior,
    or to return a dense array compatible with dense pipeline operators.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the generation of the random `y` used to fit the trees
    and the draw of the splits for each feature at the trees' nodes.
    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances
    The collection of fitted sub-estimators.

feature_importances_ : ndarray of shape (n_features,)
    The feature importances (the higher, the more important the feature).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

one_hot_encoder_ : OneHotEncoder instance
    One-hot encoder used to create the sparse embedding.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
ExtraTreesClassifier : An extra-trees classifier.
ExtraTreesRegressor : An extra-trees regressor.
RandomForestClassifier : A random forest classifier.
RandomForestRegressor : A random forest regressor.
sklearn.tree.ExtraTreeClassifier: An extremely randomized
    tree classifier.
sklearn.tree.ExtraTreeRegressor : An extremely randomized
    tree regressor.

References
----------
.. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
       Machine Learning, 63(1), 3-42, 2006.
.. [2] Moosmann, F. and Triggs, B. and Jurie, F.  "Fast discriminative
       visual codebooks using randomized clustering forests"
       NIPS 2007

Examples
--------
>>> from sklearn.ensemble import RandomTreesEmbedding
>>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]
>>> random_trees = RandomTreesEmbedding(
...    n_estimators=5, random_state=0, max_depth=1).fit(X)
>>> X_sparse_embedding = random_trees.transform(X)
>>> X_sparse_embedding.toarray()
array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],
       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:PrepareTransformerMethod .

ml:ExtraTreesRegressorMethod a owl:Class ;
    rdfs:comment """An extra-trees regressor.

This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {"squared_error", "absolute_error", "friedman_mse", "poisson"},             default="squared_error"
    The function to measure the quality of a split. Supported criteria
    are "squared_error" for the mean squared error, which is equal to
    variance reduction as feature selection criterion and minimizes the L2
    loss using the mean of each terminal node, "friedman_mse", which uses
    mean squared error with Friedman's improvement score for potential
    splits, "absolute_error" for the mean absolute error, which minimizes
    the L1 loss using the median of each terminal node, and "poisson" which
    uses reduction in Poisson deviance to find splits.
    Training using "absolute_error" is significantly slower
    than when using "squared_error".

    .. versionadded:: 0.18
       Mean Absolute Error (MAE) criterion.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {"sqrt", "log2", None}, int or float, default=1.0
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If "sqrt", then `max_features=sqrt(n_features)`.
    - If "log2", then `max_features=log2(n_features)`.
    - If None or 1.0, then `max_features=n_features`.

    .. note::
        The default of 1.0 is equivalent to bagged trees and more
        randomness can be achieved by setting smaller values, e.g. 0.3.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `"auto"` to 1.0.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=False
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.r2_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls 3 sources of randomness:

    - the bootstrapping of the samples used when building trees
      (if ``bootstrap=True``)
    - the sampling of the features to consider when looking for the best
      split at each node (if ``max_features < n_features``)
    - the draw of the splits for each of the `max_features`

    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonically increasing
      - 0: no constraint
      - -1: monotonically decreasing

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multioutput regressions (i.e. when `n_outputs_ > 1`),
      - regressions trained on data with missing values.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
    Prediction computed with out-of-bag estimate on the training set.
    This attribute exists only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
ExtraTreesClassifier : An extra-trees classifier with random splits.
RandomForestClassifier : A random forest classifier with optimal splits.
RandomForestRegressor : Ensemble regressor using trees with optimal splits.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------
.. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
       Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.ensemble import ExtraTreesRegressor
>>> X, y = load_diabetes(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(
...    X_train, y_train)
>>> reg.score(X_test, y_test)
0.2727...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:RandomForestRegressorMethod a owl:Class ;
    rdfs:comment """A random forest regressor.

A random forest is a meta estimator that fits a number of decision tree
regressors on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
Trees in the forest use the best split strategy, i.e. equivalent to passing
`splitter="best"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.
The sub-sample size is controlled with the `max_samples` parameter if
`bootstrap=True` (default), otherwise the whole dataset is used to build
each tree.

For a comparison between tree-based ensemble models see the example
:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {"squared_error", "absolute_error", "friedman_mse", "poisson"},             default="squared_error"
    The function to measure the quality of a split. Supported criteria
    are "squared_error" for the mean squared error, which is equal to
    variance reduction as feature selection criterion and minimizes the L2
    loss using the mean of each terminal node, "friedman_mse", which uses
    mean squared error with Friedman's improvement score for potential
    splits, "absolute_error" for the mean absolute error, which minimizes
    the L1 loss using the median of each terminal node, and "poisson" which
    uses reduction in Poisson deviance to find splits.
    Training using "absolute_error" is significantly slower
    than when using "squared_error".

    .. versionadded:: 0.18
       Mean Absolute Error (MAE) criterion.

    .. versionadded:: 1.0
       Poisson criterion.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {"sqrt", "log2", None}, int or float, default=1.0
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If "sqrt", then `max_features=sqrt(n_features)`.
    - If "log2", then `max_features=log2(n_features)`.
    - If None or 1.0, then `max_features=n_features`.

    .. note::
        The default of 1.0 is equivalent to bagged trees and more
        randomness can be achieved by setting smaller values, e.g. 0.3.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `"auto"` to 1.0.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=True
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.r2_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls both the randomness of the bootstrapping of the samples used
    when building trees (if ``bootstrap=True``) and the sampling of the
    features to consider when looking for the best split at each node
    (if ``max_features < n_features``).
    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonically increasing
      - 0: no constraint
      - -1: monotonically decreasing

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multioutput regressions (i.e. when `n_outputs_ > 1`),
      - regressions trained on data with missing values.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
    Prediction computed with out-of-bag estimate on the training set.
    This attribute exists only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized
    tree regressors.
sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient
    Boosting Regression Tree, very fast for big datasets (n_samples >=
    10_000).

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
``max_features=n_features`` and ``bootstrap=False``, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, ``random_state`` has to be fixed.

The default value ``max_features=1.0`` uses ``n_features``
rather than ``n_features / 3``. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].

References
----------
.. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.

.. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
       trees", Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = RandomForestRegressor(max_depth=2, random_state=0)
>>> regr.fit(X, y)
RandomForestRegressor(...)
>>> print(regr.predict([[0, 0, 0, 0]]))
[-8.32987858]""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:EnsembleModule a owl:Class ;
    rdfs:subClassOf ml:SklearnModule .

ml:ExtraTreesClassifierMethod a owl:Class ;
    rdfs:comment """An extra-trees classifier.

This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {"gini", "entropy", "log_loss"}, default="gini"
    The function to measure the quality of a split. Supported criteria are
    "gini" for the Gini impurity and "log_loss" and "entropy" both for the
    Shannon information gain, see :ref:`tree_mathematical_formulation`.
    Note: This parameter is tree-specific.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {"sqrt", "log2", None}, int or float, default="sqrt"
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If "sqrt", then `max_features=sqrt(n_features)`.
    - If "log2", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `"auto"` to `"sqrt"`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=False
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.accuracy_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls 3 sources of randomness:

    - the bootstrapping of the samples used when building trees
      (if ``bootstrap=True``)
    - the sampling of the features to consider when looking for the best
      split at each node (if ``max_features < n_features``)
    - the draw of the splits for each of the `max_features`

    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts,             default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    The "balanced_subsample" mode is the same as "balanced" except that
    weights are computed based on the bootstrap sample for every tree
    grown.

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonically increasing
      - 0: no constraint
      - -1: monotonically decreasing

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multiclass classifications (i.e. when `n_classes > 2`),
      - multioutput classifications (i.e. when `n_outputs_ > 1`),
      - classifications trained on data with missing values.

    The constraints hold over the probability of the positive class.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeClassifier`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeClassifier
    The collection of fitted sub-estimators.

classes_ : ndarray of shape (n_classes,) or a list of such arrays
    The classes labels (single output problem), or a list of arrays of
    class labels (multi-output problem).

n_classes_ : int or list
    The number of classes (single output problem), or a list containing the
    number of classes for each output (multi-output problem).

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_decision_function_` might contain NaN. This attribute exists
    only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
ExtraTreesRegressor : An extra-trees regressor with random splits.
RandomForestClassifier : A random forest classifier with optimal splits.
RandomForestRegressor : Ensemble regressor using trees with optimal splits.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------
.. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
       trees", Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_features=4, random_state=0)
>>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)
>>> clf.fit(X, y)
ExtraTreesClassifier(random_state=0)
>>> clf.predict([[0, 0, 0, 0]])
array([1])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:RandomForestClassifierMethod a owl:Class ;
    rdfs:comment """A random forest classifier.

A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
Trees in the forest use the best split strategy, i.e. equivalent to passing
`splitter="best"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.
The sub-sample size is controlled with the `max_samples` parameter if
`bootstrap=True` (default), otherwise the whole dataset is used to build
each tree.

For a comparison between tree-based ensemble models see the example
:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {"gini", "entropy", "log_loss"}, default="gini"
    The function to measure the quality of a split. Supported criteria are
    "gini" for the Gini impurity and "log_loss" and "entropy" both for the
    Shannon information gain, see :ref:`tree_mathematical_formulation`.
    Note: This parameter is tree-specific.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {"sqrt", "log2", None}, int or float, default="sqrt"
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If "sqrt", then `max_features=sqrt(n_features)`.
    - If "log2", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `"auto"` to `"sqrt"`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=True
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.accuracy_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls both the randomness of the bootstrapping of the samples used
    when building trees (if ``bootstrap=True``) and the sampling of the
    features to consider when looking for the best split at each node
    (if ``max_features < n_features``).
    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts,             default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    The "balanced_subsample" mode is the same as "balanced" except that
    weights are computed based on the bootstrap sample for every tree
    grown.

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonic increase
      - 0: no constraint
      - -1: monotonic decrease

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multiclass classifications (i.e. when `n_classes > 2`),
      - multioutput classifications (i.e. when `n_outputs_ > 1`),
      - classifications trained on data with missing values.

    The constraints hold over the probability of the positive class.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeClassifier
    The collection of fitted sub-estimators.

classes_ : ndarray of shape (n_classes,) or a list of such arrays
    The classes labels (single output problem), or a list of arrays of
    class labels (multi-output problem).

n_classes_ : int or list
    The number of classes (single output problem), or a list containing the
    number of classes for each output (multi-output problem).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_decision_function_` might contain NaN. This attribute exists
    only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized
    tree classifiers.
sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient
    Boosting Classification Tree, very fast for big datasets (n_samples >=
    10_000).

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
``max_features=n_features`` and ``bootstrap=False``, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, ``random_state`` has to be fixed.

References
----------
.. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.

Examples
--------
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = RandomForestClassifier(max_depth=2, random_state=0)
>>> clf.fit(X, y)
RandomForestClassifier(...)
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:GradientBoostingRegressorMethod a owl:Class ;
    rdfs:comment """Gradient Boosting for regression.

This estimator builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage a regression tree is fit on the negative gradient of the given
loss function.

:class:`sklearn.ensemble.HistGradientBoostingRegressor` is a much faster
variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).

Read more in the :ref:`User Guide <gradient_boosting>`.

Parameters
----------
loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'
    Loss function to be optimized. 'squared_error' refers to the squared
    error for regression. 'absolute_error' refers to the absolute error of
    regression and is a robust loss function. 'huber' is a
    combination of the two. 'quantile' allows quantile regression (use
    `alpha` to specify the quantile).

learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by `learning_rate`.
    There is a trade-off between learning_rate and n_estimators.
    Values must be in the range `[0.0, inf)`.

n_estimators : int, default=100
    The number of boosting stages to perform. Gradient boosting
    is fairly robust to over-fitting so a large number usually
    results in better performance.
    Values must be in the range `[1, inf)`.

subsample : float, default=1.0
    The fraction of samples to be used for fitting the individual base
    learners. If smaller than 1.0 this results in Stochastic Gradient
    Boosting. `subsample` interacts with the parameter `n_estimators`.
    Choosing `subsample < 1.0` leads to a reduction of variance
    and an increase in bias.
    Values must be in the range `(0.0, 1.0]`.

criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'
    The function to measure the quality of a split. Supported criteria are
    "friedman_mse" for the mean squared error with improvement score by
    Friedman, "squared_error" for mean squared error. The default value of
    "friedman_mse" is generally the best as it can provide a better
    approximation in some cases.

    .. versionadded:: 0.18

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, values must be in the range `[2, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`
      will be `ceil(min_samples_split * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`
      will be `ceil(min_samples_leaf * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.
    Values must be in the range `[0.0, 0.5]`.

max_depth : int or None, default=3
    Maximum depth of the individual regression estimators. The maximum
    depth limits the number of nodes in the tree. Tune this parameter
    for best performance; the best value depends on the interaction
    of the input variables. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.
    If int, values must be in the range `[1, inf)`.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.
    Values must be in the range `[0.0, inf)`.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

init : estimator or 'zero', default=None
    An estimator object that is used to compute the initial predictions.
    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the
    initial raw predictions are set to zero. By default a
    ``DummyEstimator`` is used, predicting either the average target value
    (for loss='squared_error'), or a quantile for the other losses.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given to each Tree estimator at each
    boosting iteration.
    In addition, it controls the random permutation of the features at
    each split (see Notes for more details).
    It also controls the random splitting of the training data to obtain a
    validation set if `n_iter_no_change` is not None.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

max_features : {'sqrt', 'log2'}, int or float, default=None
    The number of features to consider when looking for the best split:

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and the features
      considered at each split will be `max(1, int(max_features * n_features_in_))`.
    - If "sqrt", then `max_features=sqrt(n_features)`.
    - If "log2", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    Choosing `max_features < n_features` leads to a reduction of variance
    and an increase in bias.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

alpha : float, default=0.9
    The alpha-quantile of the huber loss function and the quantile
    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
    Values must be in the range `(0.0, 1.0)`.

verbose : int, default=0
    Enable verbose output. If 1 then it prints progress and performance
    once in a while (the more trees the lower the frequency). If greater
    than 1 then it prints progress and performance for every tree.
    Values must be in the range `[0, inf)`.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    Values must be in the range `[2, inf)`.
    If None, then unlimited number of leaf nodes.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just erase the
    previous solution. See :term:`the Glossary <warm_start>`.

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Values must be in the range `(0.0, 1.0)`.
    Only used if ``n_iter_no_change`` is set to an integer.

    .. versionadded:: 0.20

n_iter_no_change : int, default=None
    ``n_iter_no_change`` is used to decide if early stopping will be used
    to terminate training when validation score is not improving. By
    default it is set to None to disable early stopping. If set to a
    number, it will set aside ``validation_fraction`` size of the training
    data as validation and terminate training when validation score is not
    improving in all of the previous ``n_iter_no_change`` numbers of
    iterations.
    Values must be in the range `[1, inf)`.
    See
    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.

    .. versionadded:: 0.20

tol : float, default=1e-4
    Tolerance for the early stopping. When the loss is not improving
    by at least tol for ``n_iter_no_change`` iterations (if set to a
    number), the training stops.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.20

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed.
    Values must be in the range `[0.0, inf)`.
    See :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

Attributes
----------
n_estimators_ : int
    The number of estimators as selected by early stopping (if
    ``n_iter_no_change`` is specified). Otherwise it is set to
    ``n_estimators``.

n_trees_per_iteration_ : int
    The number of trees that are built at each iteration. For regressors, this is
    always 1.

    .. versionadded:: 1.4.0

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

oob_improvement_ : ndarray of shape (n_estimators,)
    The improvement in loss on the out-of-bag samples
    relative to the previous iteration.
    ``oob_improvement_[0]`` is the improvement in
    loss of the first stage over the ``init`` estimator.
    Only available if ``subsample < 1.0``.

oob_scores_ : ndarray of shape (n_estimators,)
    The full history of the loss values on the out-of-bag
    samples. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

oob_score_ : float
    The last value of the loss on the out-of-bag samples. It is
    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

train_score_ : ndarray of shape (n_estimators,)
    The i-th score ``train_score_[i]`` is the loss of the
    model at iteration ``i`` on the in-bag sample.
    If ``subsample == 1`` this is the loss on the training data.

init_ : estimator
    The estimator that provides the initial predictions. Set via the ``init``
    argument.

estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)
    The collection of fitted sub-estimators.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

max_features_ : int
    The inferred value of max_features.

See Also
--------
HistGradientBoostingRegressor : Histogram-based Gradient Boosting
    Classification Tree.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
sklearn.ensemble.RandomForestRegressor : A random forest regressor.

Notes
-----
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
``max_features=n_features``, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
``random_state`` has to be fixed.

References
----------
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

J. Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.

Examples
--------
>>> from sklearn.datasets import make_regression
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_regression(random_state=0)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = GradientBoostingRegressor(random_state=0)
>>> reg.fit(X_train, y_train)
GradientBoostingRegressor(random_state=0)
>>> reg.predict(X_test[1:2])
array([-61...])
>>> reg.score(X_test, y_test)
0.4...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:HistGradientBoostingRegressorMethod a owl:Class ;
    rdfs:comment """Histogram-based Gradient Boosting Regression Tree.

This estimator is much faster than
:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`
for big datasets (n_samples >= 10 000).

This estimator has native support for missing values (NaNs). During
training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are
assigned to the left or right child consequently. If no missing values
were encountered for a given feature during training, then samples with
missing values are mapped to whichever child has the most samples.

This implementation is inspired by
`LightGBM <https://github.com/Microsoft/LightGBM>`_.

Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.

.. versionadded:: 0.21

Parameters
----------
loss : {'squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile'},             default='squared_error'
    The loss function to use in the boosting process. Note that the
    "squared error", "gamma" and "poisson" losses actually implement
    "half least squares loss", "half gamma deviance" and "half poisson
    deviance" to simplify the computation of the gradient. Furthermore,
    "gamma" and "poisson" losses internally use a log-link, "gamma"
    requires ``y > 0`` and "poisson" requires ``y >= 0``.
    "quantile" uses the pinball loss.

    .. versionchanged:: 0.23
       Added option 'poisson'.

    .. versionchanged:: 1.1
       Added option 'quantile'.

    .. versionchanged:: 1.3
       Added option 'gamma'.

quantile : float, default=None
    If loss is "quantile", this parameter specifies which quantile to be estimated
    and must be between 0 and 1.
learning_rate : float, default=0.1
    The learning rate, also known as *shrinkage*. This is used as a
    multiplicative factor for the leaves values. Use ``1`` for no
    shrinkage.
max_iter : int, default=100
    The maximum number of iterations of the boosting process, i.e. the
    maximum number of trees.
max_leaf_nodes : int or None, default=31
    The maximum number of leaves for each tree. Must be strictly greater
    than 1. If None, there is no maximum limit.
max_depth : int or None, default=None
    The maximum depth of each tree. The depth of a tree is the number of
    edges to go from the root to the deepest leaf.
    Depth isn't constrained by default.
min_samples_leaf : int, default=20
    The minimum number of samples per leaf. For small datasets with less
    than a few hundred samples, it is recommended to lower this value
    since only very shallow trees would be built.
l2_regularization : float, default=0
    The L2 regularization parameter. Use ``0`` for no regularization (default).
max_features : float, default=1.0
    Proportion of randomly chosen features in each and every node split.
    This is a form of regularization, smaller values make the trees weaker
    learners and might prevent overfitting.
    If interaction constraints from `interaction_cst` are present, only allowed
    features are taken into account for the subsampling.

    .. versionadded:: 1.4

max_bins : int, default=255
    The maximum number of bins to use for non-missing values. Before
    training, each feature of the input array `X` is binned into
    integer-valued bins, which allows for a much faster training stage.
    Features with a small number of unique values may use less than
    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
    is always reserved for missing values. Must be no larger than 255.
categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None
    Indicates the categorical features.

    - None : no feature will be considered categorical.
    - boolean array-like : boolean mask indicating categorical features.
    - integer array-like : integer indices indicating categorical
      features.
    - str array-like: names of categorical features (assuming the training
      data has feature names).
    - `"from_dtype"`: dataframe columns with dtype "category" are
      considered to be categorical features. The input must be an object
      exposing a ``__dataframe__`` method such as pandas or polars
      DataFrames to use this feature.

    For each categorical feature, there must be at most `max_bins` unique
    categories. Negative values for categorical features encoded as numeric
    dtypes are treated as missing values. All categorical values are
    converted to floating point numbers. This means that categorical values
    of 1.0 and 1 are treated as the same category.

    Read more in the :ref:`User Guide <categorical_support_gbdt>`.

    .. versionadded:: 0.24

    .. versionchanged:: 1.2
       Added support for feature names.

    .. versionchanged:: 1.4
       Added `"from_dtype"` option. The default will change to `"from_dtype"` in
       v1.6.

monotonic_cst : array-like of int of shape (n_features) or dict, default=None
    Monotonic constraint to enforce on each feature are specified using the
    following integer values:

    - 1: monotonic increase
    - 0: no constraint
    - -1: monotonic decrease

    If a dict with str keys, map feature to monotonic constraints by name.
    If an array, the features are mapped to constraints by position. See
    :ref:`monotonic_cst_features_names` for a usage example.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 0.23

    .. versionchanged:: 1.2
       Accept dict of constraints with feature names as keys.

interaction_cst : {"pairwise", "no_interactions"} or sequence of lists/tuples/sets             of int, default=None
    Specify interaction constraints, the sets of features which can
    interact with each other in child node splits.

    Each item specifies the set of feature indices that are allowed
    to interact with each other. If there are more features than
    specified in these constraints, they are treated as if they were
    specified as an additional set.

    The strings "pairwise" and "no_interactions" are shorthands for
    allowing only pairwise or no interactions, respectively.

    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`
    is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,
    and specifies that each branch of a tree will either only split
    on features 0 and 1 or only split on features 2, 3 and 4.

    .. versionadded:: 1.2

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble. For results to be valid, the
    estimator should be re-trained on the same data only.
    See :term:`the Glossary <warm_start>`.
early_stopping : 'auto' or bool, default='auto'
    If 'auto', early stopping is enabled if the sample size is larger than
    10000. If True, early stopping is enabled, otherwise early stopping is
    disabled.

    .. versionadded:: 0.23

scoring : str or callable or None, default='loss'
    Scoring parameter to use for early stopping. It can be a single
    string (see :ref:`scoring_parameter`) or a callable (see
    :ref:`scoring`). If None, the estimator's default scorer is used. If
    ``scoring='loss'``, early stopping is checked w.r.t the loss value.
    Only used if early stopping is performed.
validation_fraction : int or float or None, default=0.1
    Proportion (or absolute size) of training data to set aside as
    validation data for early stopping. If None, early stopping is done on
    the training data. Only used if early stopping is performed.
n_iter_no_change : int, default=10
    Used to determine when to "early stop". The fitting process is
    stopped when none of the last ``n_iter_no_change`` scores are better
    than the ``n_iter_no_change - 1`` -th-to-last one, up to some
    tolerance. Only used if early stopping is performed.
tol : float, default=1e-7
    The absolute tolerance to use when comparing scores during early
    stopping. The higher the tolerance, the more likely we are to early
    stop: higher tolerance means that it will be harder for subsequent
    iterations to be considered an improvement upon the reference score.
verbose : int, default=0
    The verbosity level. If not zero, print some information about the
    fitting process.
random_state : int, RandomState instance or None, default=None
    Pseudo-random number generator to control the subsampling in the
    binning process, and the train/validation data split if early stopping
    is enabled.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
do_early_stopping_ : bool
    Indicates whether early stopping is used during training.
n_iter_ : int
    The number of iterations as selected by early stopping, depending on
    the `early_stopping` parameter. Otherwise it corresponds to max_iter.
n_trees_per_iteration_ : int
    The number of tree that are built at each iteration. For regressors,
    this is always 1.
train_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the training data. The first entry
    is the score of the ensemble before the first iteration. Scores are
    computed according to the ``scoring`` parameter. If ``scoring`` is
    not 'loss', scores are computed on a subset of at most 10 000
    samples. Empty if no early stopping.
validation_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the held-out validation data. The
    first entry is the score of the ensemble before the first iteration.
    Scores are computed according to the ``scoring`` parameter. Empty if
    no early stopping or if ``validation_fraction`` is None.
is_categorical_ : ndarray, shape (n_features, ) or None
    Boolean mask for the categorical features. ``None`` if there are no
    categorical features.
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
GradientBoostingRegressor : Exact gradient boosting method that does not
    scale as good on datasets with a large number of samples.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
RandomForestRegressor : A meta-estimator that fits a number of decision
    tree regressors on various sub-samples of the dataset and uses
    averaging to improve the statistical performance and control
    over-fitting.
AdaBoostRegressor : A meta-estimator that begins by fitting a regressor
    on the original dataset and then fits additional copies of the
    regressor on the same dataset but where the weights of instances are
    adjusted according to the error of the current prediction. As such,
    subsequent regressors focus more on difficult cases.

Examples
--------
>>> from sklearn.ensemble import HistGradientBoostingRegressor
>>> from sklearn.datasets import load_diabetes
>>> X, y = load_diabetes(return_X_y=True)
>>> est = HistGradientBoostingRegressor().fit(X, y)
>>> est.score(X, y)
0.92...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:GradientBoostingModule,
        ml:TrainMethod .

ml:GradientBoostingClassifierMethod a owl:Class ;
    rdfs:comment """Gradient Boosting for classification.

This algorithm builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage ``n_classes_`` regression trees are fit on the negative gradient
of the loss function, e.g. binary or multiclass log loss. Binary
classification is a special case where only a single regression tree is
induced.

:class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster
variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).

Read more in the :ref:`User Guide <gradient_boosting>`.

Parameters
----------
loss : {'log_loss', 'exponential'}, default='log_loss'
    The loss function to be optimized. 'log_loss' refers to binomial and
    multinomial deviance, the same as used in logistic regression.
    It is a good choice for classification with probabilistic outputs.
    For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.

learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by `learning_rate`.
    There is a trade-off between learning_rate and n_estimators.
    Values must be in the range `[0.0, inf)`.

n_estimators : int, default=100
    The number of boosting stages to perform. Gradient boosting
    is fairly robust to over-fitting so a large number usually
    results in better performance.
    Values must be in the range `[1, inf)`.

subsample : float, default=1.0
    The fraction of samples to be used for fitting the individual base
    learners. If smaller than 1.0 this results in Stochastic Gradient
    Boosting. `subsample` interacts with the parameter `n_estimators`.
    Choosing `subsample < 1.0` leads to a reduction of variance
    and an increase in bias.
    Values must be in the range `(0.0, 1.0]`.

criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'
    The function to measure the quality of a split. Supported criteria are
    'friedman_mse' for the mean squared error with improvement score by
    Friedman, 'squared_error' for mean squared error. The default value of
    'friedman_mse' is generally the best as it can provide a better
    approximation in some cases.

    .. versionadded:: 0.18

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, values must be in the range `[2, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`
      will be `ceil(min_samples_split * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`
      will be `ceil(min_samples_leaf * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.
    Values must be in the range `[0.0, 0.5]`.

max_depth : int or None, default=3
    Maximum depth of the individual regression estimators. The maximum
    depth limits the number of nodes in the tree. Tune this parameter
    for best performance; the best value depends on the interaction
    of the input variables. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.
    If int, values must be in the range `[1, inf)`.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.
    Values must be in the range `[0.0, inf)`.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

init : estimator or 'zero', default=None
    An estimator object that is used to compute the initial predictions.
    ``init`` has to provide :term:`fit` and :term:`predict_proba`. If
    'zero', the initial raw predictions are set to zero. By default, a
    ``DummyEstimator`` predicting the classes priors is used.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given to each Tree estimator at each
    boosting iteration.
    In addition, it controls the random permutation of the features at
    each split (see Notes for more details).
    It also controls the random splitting of the training data to obtain a
    validation set if `n_iter_no_change` is not None.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

max_features : {'sqrt', 'log2'}, int or float, default=None
    The number of features to consider when looking for the best split:

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and the features
      considered at each split will be `max(1, int(max_features * n_features_in_))`.
    - If 'sqrt', then `max_features=sqrt(n_features)`.
    - If 'log2', then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    Choosing `max_features < n_features` leads to a reduction of variance
    and an increase in bias.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

verbose : int, default=0
    Enable verbose output. If 1 then it prints progress and performance
    once in a while (the more trees the lower the frequency). If greater
    than 1 then it prints progress and performance for every tree.
    Values must be in the range `[0, inf)`.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    Values must be in the range `[2, inf)`.
    If `None`, then unlimited number of leaf nodes.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just erase the
    previous solution. See :term:`the Glossary <warm_start>`.

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Values must be in the range `(0.0, 1.0)`.
    Only used if ``n_iter_no_change`` is set to an integer.

    .. versionadded:: 0.20

n_iter_no_change : int, default=None
    ``n_iter_no_change`` is used to decide if early stopping will be used
    to terminate training when validation score is not improving. By
    default it is set to None to disable early stopping. If set to a
    number, it will set aside ``validation_fraction`` size of the training
    data as validation and terminate training when validation score is not
    improving in all of the previous ``n_iter_no_change`` numbers of
    iterations. The split is stratified.
    Values must be in the range `[1, inf)`.
    See
    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.

    .. versionadded:: 0.20

tol : float, default=1e-4
    Tolerance for the early stopping. When the loss is not improving
    by at least tol for ``n_iter_no_change`` iterations (if set to a
    number), the training stops.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.20

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed.
    Values must be in the range `[0.0, inf)`.
    See :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

Attributes
----------
n_estimators_ : int
    The number of estimators as selected by early stopping (if
    ``n_iter_no_change`` is specified). Otherwise it is set to
    ``n_estimators``.

    .. versionadded:: 0.20

n_trees_per_iteration_ : int
    The number of trees that are built at each iteration. For binary classifiers,
    this is always 1.

    .. versionadded:: 1.4.0

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

oob_improvement_ : ndarray of shape (n_estimators,)
    The improvement in loss on the out-of-bag samples
    relative to the previous iteration.
    ``oob_improvement_[0]`` is the improvement in
    loss of the first stage over the ``init`` estimator.
    Only available if ``subsample < 1.0``.

oob_scores_ : ndarray of shape (n_estimators,)
    The full history of the loss values on the out-of-bag
    samples. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

oob_score_ : float
    The last value of the loss on the out-of-bag samples. It is
    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

train_score_ : ndarray of shape (n_estimators,)
    The i-th score ``train_score_[i]`` is the loss of the
    model at iteration ``i`` on the in-bag sample.
    If ``subsample == 1`` this is the loss on the training data.

init_ : estimator
    The estimator that provides the initial predictions. Set via the ``init``
    argument.

estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``n_trees_per_iteration_``)
    The collection of fitted sub-estimators. ``n_trees_per_iteration_`` is 1 for
    binary classification, otherwise ``n_classes``.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_classes_ : int
    The number of classes.

max_features_ : int
    The inferred value of max_features.

See Also
--------
HistGradientBoostingClassifier : Histogram-based Gradient Boosting
    Classification Tree.
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
RandomForestClassifier : A meta-estimator that fits a number of decision
    tree classifiers on various sub-samples of the dataset and uses
    averaging to improve the predictive accuracy and control over-fitting.
AdaBoostClassifier : A meta-estimator that begins by fitting a classifier
    on the original dataset and then fits additional copies of the
    classifier on the same dataset where the weights of incorrectly
    classified instances are adjusted such that subsequent classifiers
    focus more on difficult cases.

Notes
-----
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
``max_features=n_features``, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
``random_state`` has to be fixed.

References
----------
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

J. Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.

Examples
--------
The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.

>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]

>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:TrainMethod .

ml:HistGradientBoostingClassifierMethod a owl:Class ;
    rdfs:comment """Histogram-based Gradient Boosting Classification Tree.

This estimator is much faster than
:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`
for big datasets (n_samples >= 10 000).

This estimator has native support for missing values (NaNs). During
training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are
assigned to the left or right child consequently. If no missing values
were encountered for a given feature during training, then samples with
missing values are mapped to whichever child has the most samples.

This implementation is inspired by
`LightGBM <https://github.com/Microsoft/LightGBM>`_.

Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.

.. versionadded:: 0.21

Parameters
----------
loss : {'log_loss'}, default='log_loss'
    The loss function to use in the boosting process.

    For binary classification problems, 'log_loss' is also known as logistic loss,
    binomial deviance or binary crossentropy. Internally, the model fits one tree
    per boosting iteration and uses the logistic sigmoid function (expit) as
    inverse link function to compute the predicted positive class probability.

    For multiclass classification problems, 'log_loss' is also known as multinomial
    deviance or categorical crossentropy. Internally, the model fits one tree per
    boosting iteration and per class and uses the softmax function as inverse link
    function to compute the predicted probabilities of the classes.

learning_rate : float, default=0.1
    The learning rate, also known as *shrinkage*. This is used as a
    multiplicative factor for the leaves values. Use ``1`` for no
    shrinkage.
max_iter : int, default=100
    The maximum number of iterations of the boosting process, i.e. the
    maximum number of trees for binary classification. For multiclass
    classification, `n_classes` trees per iteration are built.
max_leaf_nodes : int or None, default=31
    The maximum number of leaves for each tree. Must be strictly greater
    than 1. If None, there is no maximum limit.
max_depth : int or None, default=None
    The maximum depth of each tree. The depth of a tree is the number of
    edges to go from the root to the deepest leaf.
    Depth isn't constrained by default.
min_samples_leaf : int, default=20
    The minimum number of samples per leaf. For small datasets with less
    than a few hundred samples, it is recommended to lower this value
    since only very shallow trees would be built.
l2_regularization : float, default=0
    The L2 regularization parameter. Use ``0`` for no regularization (default).
max_features : float, default=1.0
    Proportion of randomly chosen features in each and every node split.
    This is a form of regularization, smaller values make the trees weaker
    learners and might prevent overfitting.
    If interaction constraints from `interaction_cst` are present, only allowed
    features are taken into account for the subsampling.

    .. versionadded:: 1.4

max_bins : int, default=255
    The maximum number of bins to use for non-missing values. Before
    training, each feature of the input array `X` is binned into
    integer-valued bins, which allows for a much faster training stage.
    Features with a small number of unique values may use less than
    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
    is always reserved for missing values. Must be no larger than 255.
categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None
    Indicates the categorical features.

    - None : no feature will be considered categorical.
    - boolean array-like : boolean mask indicating categorical features.
    - integer array-like : integer indices indicating categorical
      features.
    - str array-like: names of categorical features (assuming the training
      data has feature names).
    - `"from_dtype"`: dataframe columns with dtype "category" are
      considered to be categorical features. The input must be an object
      exposing a ``__dataframe__`` method such as pandas or polars
      DataFrames to use this feature.

    For each categorical feature, there must be at most `max_bins` unique
    categories. Negative values for categorical features encoded as numeric
    dtypes are treated as missing values. All categorical values are
    converted to floating point numbers. This means that categorical values
    of 1.0 and 1 are treated as the same category.

    Read more in the :ref:`User Guide <categorical_support_gbdt>`.

    .. versionadded:: 0.24

    .. versionchanged:: 1.2
       Added support for feature names.

    .. versionchanged:: 1.4
       Added `"from_dtype"` option. The default will change to `"from_dtype"` in
       v1.6.

monotonic_cst : array-like of int of shape (n_features) or dict, default=None
    Monotonic constraint to enforce on each feature are specified using the
    following integer values:

    - 1: monotonic increase
    - 0: no constraint
    - -1: monotonic decrease

    If a dict with str keys, map feature to monotonic constraints by name.
    If an array, the features are mapped to constraints by position. See
    :ref:`monotonic_cst_features_names` for a usage example.

    The constraints are only valid for binary classifications and hold
    over the probability of the positive class.
    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 0.23

    .. versionchanged:: 1.2
       Accept dict of constraints with feature names as keys.

interaction_cst : {"pairwise", "no_interactions"} or sequence of lists/tuples/sets             of int, default=None
    Specify interaction constraints, the sets of features which can
    interact with each other in child node splits.

    Each item specifies the set of feature indices that are allowed
    to interact with each other. If there are more features than
    specified in these constraints, they are treated as if they were
    specified as an additional set.

    The strings "pairwise" and "no_interactions" are shorthands for
    allowing only pairwise or no interactions, respectively.

    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`
    is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,
    and specifies that each branch of a tree will either only split
    on features 0 and 1 or only split on features 2, 3 and 4.

    .. versionadded:: 1.2

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble. For results to be valid, the
    estimator should be re-trained on the same data only.
    See :term:`the Glossary <warm_start>`.
early_stopping : 'auto' or bool, default='auto'
    If 'auto', early stopping is enabled if the sample size is larger than
    10000. If True, early stopping is enabled, otherwise early stopping is
    disabled.

    .. versionadded:: 0.23

scoring : str or callable or None, default='loss'
    Scoring parameter to use for early stopping. It can be a single
    string (see :ref:`scoring_parameter`) or a callable (see
    :ref:`scoring`). If None, the estimator's default scorer
    is used. If ``scoring='loss'``, early stopping is checked
    w.r.t the loss value. Only used if early stopping is performed.
validation_fraction : int or float or None, default=0.1
    Proportion (or absolute size) of training data to set aside as
    validation data for early stopping. If None, early stopping is done on
    the training data. Only used if early stopping is performed.
n_iter_no_change : int, default=10
    Used to determine when to "early stop". The fitting process is
    stopped when none of the last ``n_iter_no_change`` scores are better
    than the ``n_iter_no_change - 1`` -th-to-last one, up to some
    tolerance. Only used if early stopping is performed.
tol : float, default=1e-7
    The absolute tolerance to use when comparing scores. The higher the
    tolerance, the more likely we are to early stop: higher tolerance
    means that it will be harder for subsequent iterations to be
    considered an improvement upon the reference score.
verbose : int, default=0
    The verbosity level. If not zero, print some information about the
    fitting process.
random_state : int, RandomState instance or None, default=None
    Pseudo-random number generator to control the subsampling in the
    binning process, and the train/validation data split if early stopping
    is enabled.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.
class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form `{class_label: weight}`.
    If not given, all classes are supposed to have weight one.
    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as `n_samples / (n_classes * np.bincount(y))`.
    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if `sample_weight` is specified.

    .. versionadded:: 1.2

Attributes
----------
classes_ : array, shape = (n_classes,)
    Class labels.
do_early_stopping_ : bool
    Indicates whether early stopping is used during training.
n_iter_ : int
    The number of iterations as selected by early stopping, depending on
    the `early_stopping` parameter. Otherwise it corresponds to max_iter.
n_trees_per_iteration_ : int
    The number of tree that are built at each iteration. This is equal to 1
    for binary classification, and to ``n_classes`` for multiclass
    classification.
train_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the training data. The first entry
    is the score of the ensemble before the first iteration. Scores are
    computed according to the ``scoring`` parameter. If ``scoring`` is
    not 'loss', scores are computed on a subset of at most 10 000
    samples. Empty if no early stopping.
validation_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the held-out validation data. The
    first entry is the score of the ensemble before the first iteration.
    Scores are computed according to the ``scoring`` parameter. Empty if
    no early stopping or if ``validation_fraction`` is None.
is_categorical_ : ndarray, shape (n_features, ) or None
    Boolean mask for the categorical features. ``None`` if there are no
    categorical features.
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
GradientBoostingClassifier : Exact gradient boosting method that does not
    scale as good on datasets with a large number of samples.
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
RandomForestClassifier : A meta-estimator that fits a number of decision
    tree classifiers on various sub-samples of the dataset and uses
    averaging to improve the predictive accuracy and control over-fitting.
AdaBoostClassifier : A meta-estimator that begins by fitting a classifier
    on the original dataset and then fits additional copies of the
    classifier on the same dataset where the weights of incorrectly
    classified instances are adjusted such that subsequent classifiers
    focus more on difficult cases.

Examples
--------
>>> from sklearn.ensemble import HistGradientBoostingClassifier
>>> from sklearn.datasets import load_iris
>>> X, y = load_iris(return_X_y=True)
>>> clf = HistGradientBoostingClassifier().fit(X, y)
>>> clf.score(X, y)
1.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:EnsembleModule,
        ml:GradientBoostingModule,
        ml:TrainMethod .

