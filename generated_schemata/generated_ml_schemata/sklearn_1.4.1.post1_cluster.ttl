@prefix ds: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#> .
@prefix ml: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

ml:hasClusteringMethod a owl:ObjectProperty ;
    rdfs:domain ml:Clustering ;
    rdfs:range ml:AffinityPropagationMethod,
        ml:AgglomerativeClusteringMethod,
        ml:BirchMethod,
        ml:BisectingKMeansMethod,
        ml:DBSCANMethod,
        ml:HDBSCANMethod,
        ml:KMeansMethod,
        ml:MeanShiftMethod,
        ml:MiniBatchKMeansMethod,
        ml:OPTICSMethod,
        ml:SpectralBiclusteringMethod,
        ml:SpectralClusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:subPropertyOf ml:hasTrainMethod .

ml:hasDecompositionMethod a owl:ObjectProperty ;
    rdfs:domain ml:Decomposition ;
    rdfs:range ml:FeatureAgglomerationMethod ;
    rdfs:subPropertyOf ml:hasPrepareTransformerMethod .

ml:hasParamAffinity a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod,
        ml:SpectralClusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAlgorithm a owl:DatatypeProperty ;
    rdfs:domain ml:BisectingKMeansMethod,
        ml:DBSCANMethod,
        ml:HDBSCANMethod,
        ml:KMeansMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAllowSingleCluster a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAlpha a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAssignLabels a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBandwidth a owl:DatatypeProperty ;
    rdfs:domain ml:MeanShiftMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBatchSize a owl:DatatypeProperty ;
    rdfs:domain ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBinSeeding a owl:DatatypeProperty ;
    rdfs:domain ml:MeanShiftMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBisectingStrategy a owl:DatatypeProperty ;
    rdfs:domain ml:BisectingKMeansMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBranchingFactor a owl:DatatypeProperty ;
    rdfs:domain ml:BirchMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamClusterAll a owl:DatatypeProperty ;
    rdfs:domain ml:MeanShiftMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamClusterMethod a owl:DatatypeProperty ;
    rdfs:domain ml:OPTICSMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamClusterSelectionEpsilon a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamClusterSelectionMethod a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCoef0 a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamComputeDistances a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:FeatureAgglomerationMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamComputeFullTree a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:FeatureAgglomerationMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamComputeLabels a owl:DatatypeProperty ;
    rdfs:domain ml:BirchMethod,
        ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamConnectivity a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:FeatureAgglomerationMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamConvergenceIter a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCopy a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod,
        ml:BirchMethod,
        ml:HDBSCANMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCopyX a owl:DatatypeProperty ;
    rdfs:domain ml:BisectingKMeansMethod,
        ml:KMeansMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamDamping a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamDegree a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamDistanceThreshold a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:FeatureAgglomerationMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEigenSolver a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEigenTol a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEps a owl:DatatypeProperty ;
    rdfs:domain ml:DBSCANMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamGamma a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamInit a owl:DatatypeProperty ;
    rdfs:domain ml:BisectingKMeansMethod,
        ml:KMeansMethod,
        ml:MiniBatchKMeansMethod,
        ml:SpectralBiclusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamInitSize a owl:DatatypeProperty ;
    rdfs:domain ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamKernelParams a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamLeafSize a owl:DatatypeProperty ;
    rdfs:domain ml:DBSCANMethod,
        ml:HDBSCANMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamLinkage a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:FeatureAgglomerationMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxClusterSize a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxEps a owl:DatatypeProperty ;
    rdfs:domain ml:OPTICSMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxIter a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod,
        ml:BisectingKMeansMethod,
        ml:KMeansMethod,
        ml:MeanShiftMethod,
        ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMaxNoImprovement a owl:DatatypeProperty ;
    rdfs:domain ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMemory a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:FeatureAgglomerationMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMethod a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralBiclusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMetric a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:DBSCANMethod,
        ml:FeatureAgglomerationMethod,
        ml:HDBSCANMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMetricParams a owl:DatatypeProperty ;
    rdfs:domain ml:DBSCANMethod,
        ml:HDBSCANMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinBinFreq a owl:DatatypeProperty ;
    rdfs:domain ml:MeanShiftMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinClusterSize a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMinSamples a owl:DatatypeProperty ;
    rdfs:domain ml:DBSCANMethod,
        ml:HDBSCANMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMiniBatch a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralBiclusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNBest a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralBiclusteringMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNClusters a owl:DatatypeProperty ;
    rdfs:domain ml:AgglomerativeClusteringMethod,
        ml:BirchMethod,
        ml:BisectingKMeansMethod,
        ml:FeatureAgglomerationMethod,
        ml:KMeansMethod,
        ml:MiniBatchKMeansMethod,
        ml:SpectralBiclusteringMethod,
        ml:SpectralClusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNComponents a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralBiclusteringMethod,
        ml:SpectralClusteringMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNInit a owl:DatatypeProperty ;
    rdfs:domain ml:BisectingKMeansMethod,
        ml:KMeansMethod,
        ml:MiniBatchKMeansMethod,
        ml:SpectralBiclusteringMethod,
        ml:SpectralClusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNJobs a owl:DatatypeProperty ;
    rdfs:domain ml:DBSCANMethod,
        ml:HDBSCANMethod,
        ml:MeanShiftMethod,
        ml:OPTICSMethod,
        ml:SpectralClusteringMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNNeighbors a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralClusteringMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNSvdVecs a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralBiclusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamP a owl:DatatypeProperty ;
    rdfs:domain ml:DBSCANMethod,
        ml:OPTICSMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPoolingFunc a owl:DatatypeProperty ;
    rdfs:domain ml:FeatureAgglomerationMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPredecessorCorrection a owl:DatatypeProperty ;
    rdfs:domain ml:OPTICSMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPreference a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamRandomState a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod,
        ml:BisectingKMeansMethod,
        ml:KMeansMethod,
        ml:MiniBatchKMeansMethod,
        ml:SpectralBiclusteringMethod,
        ml:SpectralClusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamReassignmentRatio a owl:DatatypeProperty ;
    rdfs:domain ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSeeds a owl:DatatypeProperty ;
    rdfs:domain ml:MeanShiftMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamStoreCenters a owl:DatatypeProperty ;
    rdfs:domain ml:HDBSCANMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSvdMethod a owl:DatatypeProperty ;
    rdfs:domain ml:SpectralBiclusteringMethod,
        ml:SpectralCoclusteringMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamThreshold a owl:DatatypeProperty ;
    rdfs:domain ml:BirchMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamTol a owl:DatatypeProperty ;
    rdfs:domain ml:BisectingKMeansMethod,
        ml:KMeansMethod,
        ml:MiniBatchKMeansMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamVerbose a owl:DatatypeProperty ;
    rdfs:domain ml:AffinityPropagationMethod,
        ml:BisectingKMeansMethod,
        ml:KMeansMethod,
        ml:MiniBatchKMeansMethod,
        ml:SpectralClusteringMethod ;
    rdfs:range xsd:boolean,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:HdbscanModule a owl:Class ;
    rdfs:subClassOf ml:ClusterModule .

ml:SklearnModule a owl:Class ;
    rdfs:subClassOf ds:Module .

ml:BirchMethod a owl:Class ;
    rdfs:comment """Implements the BIRCH clustering algorithm.

It is a memory-efficient, online-learning algorithm provided as an
alternative to :class:`MiniBatchKMeans`. It constructs a tree
data structure with the cluster centroids being read off the leaf.
These can be either the final cluster centroids or can be provided as input
to another clustering algorithm such as :class:`AgglomerativeClustering`.

Read more in the :ref:`User Guide <birch>`.

.. versionadded:: 0.16

Parameters
----------
threshold : float, default=0.5
    The radius of the subcluster obtained by merging a new sample and the
    closest subcluster should be lesser than the threshold. Otherwise a new
    subcluster is started. Setting this value to be very low promotes
    splitting and vice-versa.

branching_factor : int, default=50
    Maximum number of CF subclusters in each node. If a new samples enters
    such that the number of subclusters exceed the branching_factor then
    that node is split into two nodes with the subclusters redistributed
    in each. The parent subcluster of that node is removed and two new
    subclusters are added as parents of the 2 split nodes.

n_clusters : int, instance of sklearn.cluster model or None, default=3
    Number of clusters after the final clustering step, which treats the
    subclusters from the leaves as new samples.

    - `None` : the final clustering step is not performed and the
      subclusters are returned as they are.

    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model
      is fit treating the subclusters as new samples and the initial data
      is mapped to the label of the closest subcluster.

    - `int` : the model fit is :class:`AgglomerativeClustering` with
      `n_clusters` set to be equal to the int.

compute_labels : bool, default=True
    Whether or not to compute labels for each fit.

copy : bool, default=True
    Whether or not to make a copy of the given data. If set to False,
    the initial data will be overwritten.

Attributes
----------
root_ : _CFNode
    Root of the CFTree.

dummy_leaf_ : _CFNode
    Start pointer to all the leaves.

subcluster_centers_ : ndarray
    Centroids of all subclusters read directly from the leaves.

subcluster_labels_ : ndarray
    Labels assigned to the centroids of the subclusters after
    they are clustered globally.

labels_ : ndarray of shape (n_samples,)
    Array of labels assigned to the input data.
    if partial_fit is used instead of fit, they are assigned to the
    last batch of data.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MiniBatchKMeans : Alternative implementation that does incremental updates
    of the centers' positions using mini-batches.

Notes
-----
The tree data structure consists of nodes with each node consisting of
a number of subclusters. The maximum number of subclusters in a node
is determined by the branching factor. Each subcluster maintains a
linear sum, squared sum and the number of samples in that subcluster.
In addition, each subcluster can also have a node as its child, if the
subcluster is not a member of a leaf node.

For a new point entering the root, it is merged with the subcluster closest
to it and the linear sum, squared sum and the number of samples of that
subcluster are updated. This is done recursively till the properties of
the leaf node are updated.

References
----------
* Tian Zhang, Raghu Ramakrishnan, Maron Livny
  BIRCH: An efficient data clustering method for large databases.
  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf

* Roberto Perdisci
  JBirch - Java implementation of BIRCH clustering algorithm
  https://code.google.com/archive/p/jbirch

Examples
--------
>>> from sklearn.cluster import Birch
>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]
>>> brc = Birch(n_clusters=None)
>>> brc.fit(X)
Birch(n_clusters=None)
>>> brc.predict(X)
array([0, 0, 0, 1, 1, 1])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:MeanShiftMethod a owl:Class ;
    rdfs:comment """Mean shift clustering using a flat kernel.

Mean shift clustering aims to discover "blobs" in a smooth density of
samples. It is a centroid-based algorithm, which works by updating
candidates for centroids to be the mean of the points within a given
region. These candidates are then filtered in a post-processing stage to
eliminate near-duplicates to form the final set of centroids.

Seeding is performed using a binning technique for scalability.

Read more in the :ref:`User Guide <mean_shift>`.

Parameters
----------
bandwidth : float, default=None
    Bandwidth used in the flat kernel.

    If not given, the bandwidth is estimated using
    sklearn.cluster.estimate_bandwidth; see the documentation for that
    function for hints on scalability (see also the Notes, below).

seeds : array-like of shape (n_samples, n_features), default=None
    Seeds used to initialize kernels. If not set,
    the seeds are calculated by clustering.get_bin_seeds
    with bandwidth as the grid size and default values for
    other parameters.

bin_seeding : bool, default=False
    If true, initial kernel locations are not locations of all
    points, but rather the location of the discretized version of
    points, where points are binned onto a grid whose coarseness
    corresponds to the bandwidth. Setting this option to True will speed
    up the algorithm because fewer seeds will be initialized.
    The default value is False.
    Ignored if seeds argument is not None.

min_bin_freq : int, default=1
   To speed up the algorithm, accept only those bins with at least
   min_bin_freq points as seeds.

cluster_all : bool, default=True
    If true, then all points are clustered, even those orphans that are
    not within any kernel. Orphans are assigned to the nearest kernel.
    If false, then orphans are given cluster label -1.

n_jobs : int, default=None
    The number of jobs to use for the computation. The following tasks benefit
    from the parallelization:

    - The search of nearest neighbors for bandwidth estimation and label
      assignments. See the details in the docstring of the
      ``NearestNeighbors`` class.
    - Hill-climbing optimization for all seeds.

    See :term:`Glossary <n_jobs>` for more details.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

max_iter : int, default=300
    Maximum number of iterations, per seed point before the clustering
    operation terminates (for that seed point), if has not converged yet.

    .. versionadded:: 0.22

Attributes
----------
cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers.

labels_ : ndarray of shape (n_samples,)
    Labels of each point.

n_iter_ : int
    Maximum number of iterations performed on each seed.

    .. versionadded:: 0.22

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
KMeans : K-Means clustering.

Notes
-----

Scalability:

Because this implementation uses a flat kernel and
a Ball Tree to look up members of each kernel, the complexity will tend
towards O(T*n*log(n)) in lower dimensions, with n the number of samples
and T the number of points. In higher dimensions the complexity will
tend towards O(T*n^2).

Scalability can be boosted by using fewer seeds, for example by using
a higher value of min_bin_freq in the get_bin_seeds function.

Note that the estimate_bandwidth function is much less scalable than the
mean shift algorithm and will be the bottleneck if it is used.

References
----------

Dorin Comaniciu and Peter Meer, "Mean Shift: A robust approach toward
feature space analysis". IEEE Transactions on Pattern Analysis and
Machine Intelligence. 2002. pp. 603-619.

Examples
--------
>>> from sklearn.cluster import MeanShift
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = MeanShift(bandwidth=2).fit(X)
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])
>>> clustering.predict([[0, 0], [5, 5]])
array([1, 0])
>>> clustering
MeanShift(bandwidth=2)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:SpectralCoclusteringMethod a owl:Class ;
    rdfs:comment """Spectral Co-Clustering algorithm (Dhillon, 2001).

Clusters rows and columns of an array `X` to solve the relaxed
normalized cut of the bipartite graph created from `X` as follows:
the edge between row vertex `i` and column vertex `j` has weight
`X[i, j]`.

The resulting bicluster structure is block-diagonal, since each
row and each column belongs to exactly one bicluster.

Supports sparse matrices, as long as they are nonnegative.

Read more in the :ref:`User Guide <spectral_coclustering>`.

Parameters
----------
n_clusters : int, default=3
    The number of biclusters to find.

svd_method : {'randomized', 'arpack'}, default='randomized'
    Selects the algorithm for finding singular vectors. May be
    'randomized' or 'arpack'. If 'randomized', use
    :func:`sklearn.utils.extmath.randomized_svd`, which may be faster
    for large matrices. If 'arpack', use
    :func:`scipy.sparse.linalg.svds`, which is more accurate, but
    possibly slower in some cases.

n_svd_vecs : int, default=None
    Number of vectors to use in calculating the SVD. Corresponds
    to `ncv` when `svd_method=arpack` and `n_oversamples` when
    `svd_method` is 'randomized`.

mini_batch : bool, default=False
    Whether to use mini-batch k-means, which is faster but may get
    different results.

init : {'k-means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++'
    Method for initialization of k-means algorithm; defaults to
    'k-means++'.

n_init : int, default=10
    Number of random initializations that are tried with the
    k-means algorithm.

    If mini-batch k-means is used, the best initialization is
    chosen and the algorithm runs once. Otherwise, the algorithm
    is run for each initialization and the best solution chosen.

random_state : int, RandomState instance, default=None
    Used for randomizing the singular value decomposition and the k-means
    initialization. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

Attributes
----------
rows_ : array-like of shape (n_row_clusters, n_rows)
    Results of the clustering. `rows[i, r]` is True if
    cluster `i` contains row `r`. Available only after calling ``fit``.

columns_ : array-like of shape (n_column_clusters, n_columns)
    Results of the clustering, like `rows`.

row_labels_ : array-like of shape (n_rows,)
    The bicluster label of each row.

column_labels_ : array-like of shape (n_cols,)
    The bicluster label of each column.

biclusters_ : tuple of two ndarrays
    The tuple contains the `rows_` and `columns_` arrays.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SpectralBiclustering : Partitions rows and columns under the assumption
    that the data has an underlying checkerboard structure.

References
----------
* :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using
  bipartite spectral graph partitioning.
  <10.1145/502512.502550>`

Examples
--------
>>> from sklearn.cluster import SpectralCoclustering
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)
>>> clustering.row_labels_ #doctest: +SKIP
array([0, 1, 1, 0, 0, 0], dtype=int32)
>>> clustering.column_labels_ #doctest: +SKIP
array([0, 0], dtype=int32)
>>> clustering
SpectralCoclustering(n_clusters=2, random_state=0)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:AffinityPropagationMethod a owl:Class ;
    rdfs:comment """Perform Affinity Propagation Clustering of data.

Read more in the :ref:`User Guide <affinity_propagation>`.

Parameters
----------
damping : float, default=0.5
    Damping factor in the range `[0.5, 1.0)` is the extent to
    which the current value is maintained relative to
    incoming values (weighted 1 - damping). This in order
    to avoid numerical oscillations when updating these
    values (messages).

max_iter : int, default=200
    Maximum number of iterations.

convergence_iter : int, default=15
    Number of iterations with no change in the number
    of estimated clusters that stops the convergence.

copy : bool, default=True
    Make a copy of input data.

preference : array-like of shape (n_samples,) or float, default=None
    Preferences for each point - points with larger values of
    preferences are more likely to be chosen as exemplars. The number
    of exemplars, ie of clusters, is influenced by the input
    preferences value. If the preferences are not passed as arguments,
    they will be set to the median of the input similarities.

affinity : {'euclidean', 'precomputed'}, default='euclidean'
    Which affinity to use. At the moment 'precomputed' and
    ``euclidean`` are supported. 'euclidean' uses the
    negative squared euclidean distance between points.

verbose : bool, default=False
    Whether to be verbose.

random_state : int, RandomState instance or None, default=None
    Pseudo-random number generator to control the starting state.
    Use an int for reproducible results across function calls.
    See the :term:`Glossary <random_state>`.

    .. versionadded:: 0.23
        this parameter was previously hardcoded as 0.

Attributes
----------
cluster_centers_indices_ : ndarray of shape (n_clusters,)
    Indices of cluster centers.

cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Cluster centers (if affinity != ``precomputed``).

labels_ : ndarray of shape (n_samples,)
    Labels of each point.

affinity_matrix_ : ndarray of shape (n_samples, n_samples)
    Stores the affinity matrix used in ``fit``.

n_iter_ : int
    Number of iterations taken to converge.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
AgglomerativeClustering : Recursively merges the pair of
    clusters that minimally increases a given linkage distance.
FeatureAgglomeration : Similar to AgglomerativeClustering,
    but recursively merges features instead of samples.
KMeans : K-Means clustering.
MiniBatchKMeans : Mini-Batch K-Means clustering.
MeanShift : Mean shift clustering using a flat kernel.
SpectralClustering : Apply clustering to a projection
    of the normalized Laplacian.

Notes
-----
For an example, see :ref:`examples/cluster/plot_affinity_propagation.py
<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.

The algorithmic complexity of affinity propagation is quadratic
in the number of points.

When the algorithm does not converge, it will still return a arrays of
``cluster_center_indices`` and labels if there are any exemplars/clusters,
however they may be degenerate and should be used with caution.

When ``fit`` does not converge, ``cluster_centers_`` is still populated
however it may be degenerate. In such a case, proceed with caution.
If ``fit`` does not converge and fails to produce any ``cluster_centers_``
then ``predict`` will label every sample as ``-1``.

When all training samples have equal similarities and equal preferences,
the assignment of cluster centers and labels depends on the preference.
If the preference is smaller than the similarities, ``fit`` will result in
a single cluster center and label ``0`` for every sample. Otherwise, every
training sample becomes its own cluster center and is assigned a unique
label.

References
----------

Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages
Between Data Points", Science Feb. 2007

Examples
--------
>>> from sklearn.cluster import AffinityPropagation
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 4], [4, 0]])
>>> clustering = AffinityPropagation(random_state=5).fit(X)
>>> clustering
AffinityPropagation(random_state=5)
>>> clustering.labels_
array([0, 0, 0, 1, 1, 1])
>>> clustering.predict([[0, 0], [4, 4]])
array([0, 1])
>>> clustering.cluster_centers_
array([[1, 2],
       [4, 2]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:AgglomerativeClusteringMethod a owl:Class ;
    rdfs:comment """Agglomerative Clustering.

Recursively merges pair of clusters of sample data; uses linkage distance.

Read more in the :ref:`User Guide <hierarchical_clustering>`.

Parameters
----------
n_clusters : int or None, default=2
    The number of clusters to find. It must be ``None`` if
    ``distance_threshold`` is not ``None``.

metric : str or callable, default="euclidean"
    Metric used to compute the linkage. Can be "euclidean", "l1", "l2",
    "manhattan", "cosine", or "precomputed". If linkage is "ward", only
    "euclidean" is accepted. If "precomputed", a distance matrix is needed
    as input for the fit method.

    .. versionadded:: 1.2

    .. deprecated:: 1.4
       `metric=None` is deprecated in 1.4 and will be removed in 1.6.
       Let `metric` be the default value (i.e. `"euclidean"`) instead.

memory : str or object with the joblib.Memory interface, default=None
    Used to cache the output of the computation of the tree.
    By default, no caching is done. If a string is given, it is the
    path to the caching directory.

connectivity : array-like or callable, default=None
    Connectivity matrix. Defines for each sample the neighboring
    samples following a given structure of the data.
    This can be a connectivity matrix itself or a callable that transforms
    the data into a connectivity matrix, such as derived from
    `kneighbors_graph`. Default is ``None``, i.e, the
    hierarchical clustering algorithm is unstructured.

compute_full_tree : 'auto' or bool, default='auto'
    Stop early the construction of the tree at ``n_clusters``. This is
    useful to decrease computation time if the number of clusters is not
    small compared to the number of samples. This option is useful only
    when specifying a connectivity matrix. Note also that when varying the
    number of clusters and using caching, it may be advantageous to compute
    the full tree. It must be ``True`` if ``distance_threshold`` is not
    ``None``. By default `compute_full_tree` is "auto", which is equivalent
    to `True` when `distance_threshold` is not `None` or that `n_clusters`
    is inferior to the maximum between 100 or `0.02 * n_samples`.
    Otherwise, "auto" is equivalent to `False`.

linkage : {'ward', 'complete', 'average', 'single'}, default='ward'
    Which linkage criterion to use. The linkage criterion determines which
    distance to use between sets of observation. The algorithm will merge
    the pairs of cluster that minimize this criterion.

    - 'ward' minimizes the variance of the clusters being merged.
    - 'average' uses the average of the distances of each observation of
      the two sets.
    - 'complete' or 'maximum' linkage uses the maximum distances between
      all observations of the two sets.
    - 'single' uses the minimum of the distances between all observations
      of the two sets.

    .. versionadded:: 0.20
        Added the 'single' option

distance_threshold : float, default=None
    The linkage distance threshold at or above which clusters will not be
    merged. If not ``None``, ``n_clusters`` must be ``None`` and
    ``compute_full_tree`` must be ``True``.

    .. versionadded:: 0.21

compute_distances : bool, default=False
    Computes distances between clusters even if `distance_threshold` is not
    used. This can be used to make dendrogram visualization, but introduces
    a computational and memory overhead.

    .. versionadded:: 0.24

Attributes
----------
n_clusters_ : int
    The number of clusters found by the algorithm. If
    ``distance_threshold=None``, it will be equal to the given
    ``n_clusters``.

labels_ : ndarray of shape (n_samples)
    Cluster labels for each point.

n_leaves_ : int
    Number of leaves in the hierarchical tree.

n_connected_components_ : int
    The estimated number of connected components in the graph.

    .. versionadded:: 0.21
        ``n_connected_components_`` was added to replace ``n_components_``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

children_ : array-like of shape (n_samples-1, 2)
    The children of each non-leaf node. Values less than `n_samples`
    correspond to leaves of the tree which are the original samples.
    A node `i` greater than or equal to `n_samples` is a non-leaf
    node and has children `children_[i - n_samples]`. Alternatively
    at the i-th iteration, children[i][0] and children[i][1]
    are merged to form node `n_samples + i`.

distances_ : array-like of shape (n_nodes-1,)
    Distances between nodes in the corresponding place in `children_`.
    Only computed if `distance_threshold` is used or `compute_distances`
    is set to `True`.

See Also
--------
FeatureAgglomeration : Agglomerative clustering but for features instead of
    samples.
ward_tree : Hierarchical clustering with ward linkage.

Examples
--------
>>> from sklearn.cluster import AgglomerativeClustering
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 4], [4, 0]])
>>> clustering = AgglomerativeClustering().fit(X)
>>> clustering
AgglomerativeClustering()
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:DBSCANMethod a owl:Class ;
    rdfs:comment """Perform DBSCAN clustering from vector array or distance matrix.

DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them.
Good for data which contains clusters of similar density.

The worst case memory complexity of DBSCAN is :math:`O({n}^2)`, which can
occur when the `eps` param is large and `min_samples` is low.

Read more in the :ref:`User Guide <dbscan>`.

Parameters
----------
eps : float, default=0.5
    The maximum distance between two samples for one to be considered
    as in the neighborhood of the other. This is not a maximum bound
    on the distances of points within a cluster. This is the most
    important DBSCAN parameter to choose appropriately for your data set
    and distance function.

min_samples : int, default=5
    The number of samples (or total weight) in a neighborhood for a point to
    be considered as a core point. This includes the point itself. If
    `min_samples` is set to a higher value, DBSCAN will find denser clusters,
    whereas if it is set to a lower value, the found clusters will be more
    sparse.

metric : str, or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string or callable, it must be one of
    the options allowed by :func:`sklearn.metrics.pairwise_distances` for
    its metric parameter.
    If metric is "precomputed", X is assumed to be a distance matrix and
    must be square. X may be a :term:`sparse graph`, in which
    case only "nonzero" elements may be considered neighbors for DBSCAN.

    .. versionadded:: 0.17
       metric *precomputed* to accept precomputed sparse matrix.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

    .. versionadded:: 0.19

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    The algorithm to be used by the NearestNeighbors module
    to compute pointwise distances and find nearest neighbors.
    See NearestNeighbors module documentation for details.

leaf_size : int, default=30
    Leaf size passed to BallTree or cKDTree. This can affect the speed
    of the construction and query, as well as the memory required
    to store the tree. The optimal value depends
    on the nature of the problem.

p : float, default=None
    The power of the Minkowski metric to be used to calculate distance
    between points. If None, then ``p=2`` (equivalent to the Euclidean
    distance).

n_jobs : int, default=None
    The number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
core_sample_indices_ : ndarray of shape (n_core_samples,)
    Indices of core samples.

components_ : ndarray of shape (n_core_samples, n_features)
    Copy of each core sample found by training.

labels_ : ndarray of shape (n_samples)
    Cluster labels for each point in the dataset given to fit().
    Noisy samples are given the label -1.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
OPTICS : A similar clustering at multiple values of eps. Our implementation
    is optimized for memory usage.

Notes
-----
For an example, see :ref:`examples/cluster/plot_dbscan.py
<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.

This implementation bulk-computes all neighborhood queries, which increases
the memory complexity to O(n.d) where d is the average number of neighbors,
while original DBSCAN had memory complexity O(n). It may attract a higher
memory complexity when querying these nearest neighborhoods, depending
on the ``algorithm``.

One way to avoid the query complexity is to pre-compute sparse
neighborhoods in chunks using
:func:`NearestNeighbors.radius_neighbors_graph
<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
``mode='distance'``, then using ``metric='precomputed'`` here.

Another way to reduce memory and computation time is to remove
(near-)duplicate points and use ``sample_weight`` instead.

:class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory
usage.

References
----------
Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `"A Density-Based
Algorithm for Discovering Clusters in Large Spatial Databases with Noise"
<https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.
In: Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996

Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
:doi:`"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN."
<10.1145/3068335>`
ACM Transactions on Database Systems (TODS), 42(3), 19.

Examples
--------
>>> from sklearn.cluster import DBSCAN
>>> import numpy as np
>>> X = np.array([[1, 2], [2, 2], [2, 3],
...               [8, 7], [8, 8], [25, 80]])
>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
>>> clustering.labels_
array([ 0,  0,  0,  1,  1, -1])
>>> clustering
DBSCAN(eps=3, min_samples=2)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:FeatureAgglomerationMethod a owl:Class ;
    rdfs:comment """Agglomerate features.

Recursively merges pair of clusters of features.

Read more in the :ref:`User Guide <hierarchical_clustering>`.

Parameters
----------
n_clusters : int or None, default=2
    The number of clusters to find. It must be ``None`` if
    ``distance_threshold`` is not ``None``.

metric : str or callable, default="euclidean"
    Metric used to compute the linkage. Can be "euclidean", "l1", "l2",
    "manhattan", "cosine", or "precomputed". If linkage is "ward", only
    "euclidean" is accepted. If "precomputed", a distance matrix is needed
    as input for the fit method.

    .. versionadded:: 1.2

    .. deprecated:: 1.4
       `metric=None` is deprecated in 1.4 and will be removed in 1.6.
       Let `metric` be the default value (i.e. `"euclidean"`) instead.

memory : str or object with the joblib.Memory interface, default=None
    Used to cache the output of the computation of the tree.
    By default, no caching is done. If a string is given, it is the
    path to the caching directory.

connectivity : array-like or callable, default=None
    Connectivity matrix. Defines for each feature the neighboring
    features following a given structure of the data.
    This can be a connectivity matrix itself or a callable that transforms
    the data into a connectivity matrix, such as derived from
    `kneighbors_graph`. Default is `None`, i.e, the
    hierarchical clustering algorithm is unstructured.

compute_full_tree : 'auto' or bool, default='auto'
    Stop early the construction of the tree at `n_clusters`. This is useful
    to decrease computation time if the number of clusters is not small
    compared to the number of features. This option is useful only when
    specifying a connectivity matrix. Note also that when varying the
    number of clusters and using caching, it may be advantageous to compute
    the full tree. It must be ``True`` if ``distance_threshold`` is not
    ``None``. By default `compute_full_tree` is "auto", which is equivalent
    to `True` when `distance_threshold` is not `None` or that `n_clusters`
    is inferior to the maximum between 100 or `0.02 * n_samples`.
    Otherwise, "auto" is equivalent to `False`.

linkage : {"ward", "complete", "average", "single"}, default="ward"
    Which linkage criterion to use. The linkage criterion determines which
    distance to use between sets of features. The algorithm will merge
    the pairs of cluster that minimize this criterion.

    - "ward" minimizes the variance of the clusters being merged.
    - "complete" or maximum linkage uses the maximum distances between
      all features of the two sets.
    - "average" uses the average of the distances of each feature of
      the two sets.
    - "single" uses the minimum of the distances between all features
      of the two sets.

pooling_func : callable, default=np.mean
    This combines the values of agglomerated features into a single
    value, and should accept an array of shape [M, N] and the keyword
    argument `axis=1`, and reduce it to an array of size [M].

distance_threshold : float, default=None
    The linkage distance threshold at or above which clusters will not be
    merged. If not ``None``, ``n_clusters`` must be ``None`` and
    ``compute_full_tree`` must be ``True``.

    .. versionadded:: 0.21

compute_distances : bool, default=False
    Computes distances between clusters even if `distance_threshold` is not
    used. This can be used to make dendrogram visualization, but introduces
    a computational and memory overhead.

    .. versionadded:: 0.24

Attributes
----------
n_clusters_ : int
    The number of clusters found by the algorithm. If
    ``distance_threshold=None``, it will be equal to the given
    ``n_clusters``.

labels_ : array-like of (n_features,)
    Cluster labels for each feature.

n_leaves_ : int
    Number of leaves in the hierarchical tree.

n_connected_components_ : int
    The estimated number of connected components in the graph.

    .. versionadded:: 0.21
        ``n_connected_components_`` was added to replace ``n_components_``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

children_ : array-like of shape (n_nodes-1, 2)
    The children of each non-leaf node. Values less than `n_features`
    correspond to leaves of the tree which are the original samples.
    A node `i` greater than or equal to `n_features` is a non-leaf
    node and has children `children_[i - n_features]`. Alternatively
    at the i-th iteration, children[i][0] and children[i][1]
    are merged to form node `n_features + i`.

distances_ : array-like of shape (n_nodes-1,)
    Distances between nodes in the corresponding place in `children_`.
    Only computed if `distance_threshold` is used or `compute_distances`
    is set to `True`.

See Also
--------
AgglomerativeClustering : Agglomerative clustering samples instead of
    features.
ward_tree : Hierarchical clustering with ward linkage.

Examples
--------
>>> import numpy as np
>>> from sklearn import datasets, cluster
>>> digits = datasets.load_digits()
>>> images = digits.images
>>> X = np.reshape(images, (len(images), -1))
>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)
>>> agglo.fit(X)
FeatureAgglomeration(n_clusters=32)
>>> X_reduced = agglo.transform(X)
>>> X_reduced.shape
(1797, 32)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:PrepareTransformerMethod .

ml:KMeansMethod a owl:Class ;
    rdfs:comment """K-Means clustering.

Read more in the :ref:`User Guide <k_means>`.

Parameters
----------

n_clusters : int, default=8
    The number of clusters to form as well as the number of
    centroids to generate.

    For an example of how to choose an optimal value for `n_clusters` refer to
    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.

init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'
    Method for initialization:

    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is "greedy k-means++". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.

    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.

    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.

    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.

    For an example of how to use the different `init` strategy, see the example
    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.

n_init : 'auto' or int, default='auto'
    Number of times the k-means algorithm is run with different centroid
    seeds. The final results is the best output of `n_init` consecutive runs
    in terms of inertia. Several runs are recommended for sparse
    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).

    When `n_init='auto'`, the number of runs depends on the value of init:
    10 if using `init='random'` or `init` is a callable;
    1 if using `init='k-means++'` or `init` is an array-like.

    .. versionadded:: 1.2
       Added 'auto' option for `n_init`.

    .. versionchanged:: 1.4
       Default value for `n_init` changed to `'auto'`.

max_iter : int, default=300
    Maximum number of iterations of the k-means algorithm for a
    single run.

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations to declare
    convergence.

verbose : int, default=0
    Verbosity mode.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization. Use
    an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

copy_x : bool, default=True
    When pre-computing distances it is more numerically accurate to center
    the data first. If copy_x is True (default), then the original data is
    not modified. If False, the original data is modified, and put back
    before the function returns, but small numerical differences may be
    introduced by subtracting and then adding the data mean. Note that if
    the original data is not C-contiguous, a copy will be made even if
    copy_x is False. If the original data is sparse, but not in CSR format,
    a copy will be made even if copy_x is False.

algorithm : {"lloyd", "elkan"}, default="lloyd"
    K-means algorithm to use. The classical EM-style algorithm is `"lloyd"`.
    The `"elkan"` variation can be more efficient on some datasets with
    well-defined clusters, by using the triangle inequality. However it's
    more memory intensive due to the allocation of an extra array of shape
    `(n_samples, n_clusters)`.

    .. versionchanged:: 0.18
        Added Elkan algorithm

    .. versionchanged:: 1.1
        Renamed "full" to "lloyd", and deprecated "auto" and "full".
        Changed "auto" to use "lloyd" instead of "elkan".

Attributes
----------
cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers. If the algorithm stops before fully
    converging (see ``tol`` and ``max_iter``), these will not be
    consistent with ``labels_``.

labels_ : ndarray of shape (n_samples,)
    Labels of each point

inertia_ : float
    Sum of squared distances of samples to their closest cluster center,
    weighted by the sample weights if provided.

n_iter_ : int
    Number of iterations run.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MiniBatchKMeans : Alternative online implementation that does incremental
    updates of the centers positions using mini-batches.
    For large scale learning (say n_samples > 10k) MiniBatchKMeans is
    probably much faster than the default batch implementation.

Notes
-----
The k-means problem is solved using either Lloyd's or Elkan's algorithm.

The average complexity is given by O(k n T), where n is the number of
samples and T is the number of iteration.

The worst case complexity is given by O(n^(k+2/p)) with
n = n_samples, p = n_features.
Refer to :doi:`"How slow is the k-means method?" D. Arthur and S. Vassilvitskii -
SoCG2006.<10.1145/1137856.1137880>` for more details.

In practice, the k-means algorithm is very fast (one of the fastest
clustering algorithms available), but it falls in local minima. That's why
it can be useful to restart it several times.

If the algorithm stops before fully converging (because of ``tol`` or
``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,
i.e. the ``cluster_centers_`` will not be the means of the points in each
cluster. Also, the estimator will reassign ``labels_`` after the last
iteration to make ``labels_`` consistent with ``predict`` on the training
set.

Examples
--------

>>> from sklearn.cluster import KMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [10, 2], [10, 4], [10, 0]])
>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init="auto").fit(X)
>>> kmeans.labels_
array([1, 1, 1, 0, 0, 0], dtype=int32)
>>> kmeans.predict([[0, 0], [12, 3]])
array([1, 0], dtype=int32)
>>> kmeans.cluster_centers_
array([[10.,  2.],
       [ 1.,  2.]])

For a more detailed example of K-Means using the iris dataset see
:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.

For examples of common problems with K-Means and how to address them see
:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.

For an example of how to use K-Means to perform color quantization see
:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.

For a demonstration of how K-Means can be used to cluster text documents see
:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.

For a comparison between K-Means and MiniBatchKMeans refer to example
:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:BisectingKMeansMethod a owl:Class ;
    rdfs:comment """Bisecting K-Means clustering.

Read more in the :ref:`User Guide <bisect_k_means>`.

.. versionadded:: 1.1

Parameters
----------
n_clusters : int, default=8
    The number of clusters to form as well as the number of
    centroids to generate.

init : {'k-means++', 'random'} or callable, default='random'
    Method for initialization:

    'k-means++' : selects initial cluster centers for k-mean
    clustering in a smart way to speed up convergence. See section
    Notes in k_init for more details.

    'random': choose `n_clusters` observations (rows) at random from data
    for the initial centroids.

    If a callable is passed, it should take arguments X, n_clusters and a
    random state and return an initialization.

n_init : int, default=1
    Number of time the inner k-means algorithm will be run with different
    centroid seeds in each bisection.
    That will result producing for each bisection best output of n_init
    consecutive runs in terms of inertia.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization
    in inner K-Means. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

max_iter : int, default=300
    Maximum number of iterations of the inner k-means algorithm at each
    bisection.

verbose : int, default=0
    Verbosity mode.

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations  to declare
    convergence. Used in inner k-means algorithm at each bisection to pick
    best possible clusters.

copy_x : bool, default=True
    When pre-computing distances it is more numerically accurate to center
    the data first. If copy_x is True (default), then the original data is
    not modified. If False, the original data is modified, and put back
    before the function returns, but small numerical differences may be
    introduced by subtracting and then adding the data mean. Note that if
    the original data is not C-contiguous, a copy will be made even if
    copy_x is False. If the original data is sparse, but not in CSR format,
    a copy will be made even if copy_x is False.

algorithm : {"lloyd", "elkan"}, default="lloyd"
    Inner K-means algorithm used in bisection.
    The classical EM-style algorithm is `"lloyd"`.
    The `"elkan"` variation can be more efficient on some datasets with
    well-defined clusters, by using the triangle inequality. However it's
    more memory intensive due to the allocation of an extra array of shape
    `(n_samples, n_clusters)`.

bisecting_strategy : {"biggest_inertia", "largest_cluster"},            default="biggest_inertia"
    Defines how bisection should be performed:

     - "biggest_inertia" means that BisectingKMeans will always check
        all calculated cluster for cluster with biggest SSE
        (Sum of squared errors) and bisect it. This approach concentrates on
        precision, but may be costly in terms of execution time (especially for
        larger amount of data points).

     - "largest_cluster" - BisectingKMeans will always split cluster with
        largest amount of points assigned to it from all clusters
        previously calculated. That should work faster than picking by SSE
        ('biggest_inertia') and may produce similar results in most cases.

Attributes
----------
cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers. If the algorithm stops before fully
    converging (see ``tol`` and ``max_iter``), these will not be
    consistent with ``labels_``.

labels_ : ndarray of shape (n_samples,)
    Labels of each point.

inertia_ : float
    Sum of squared distances of samples to their closest cluster center,
    weighted by the sample weights if provided.

n_features_in_ : int
    Number of features seen during :term:`fit`.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

See Also
--------
KMeans : Original implementation of K-Means algorithm.

Notes
-----
It might be inefficient when n_cluster is less than 3, due to unnecessary
calculations for that case.

Examples
--------
>>> from sklearn.cluster import BisectingKMeans
>>> import numpy as np
>>> X = np.array([[1, 1], [10, 1], [3, 1],
...               [10, 0], [2, 1], [10, 2],
...               [10, 8], [10, 9], [10, 10]])
>>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)
>>> bisect_means.labels_
array([0, 2, 0, 2, 0, 2, 1, 1, 1], dtype=int32)
>>> bisect_means.predict([[0, 0], [12, 3]])
array([0, 2], dtype=int32)
>>> bisect_means.cluster_centers_
array([[ 2., 1.],
       [10., 9.],
       [10., 1.]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:SpectralBiclusteringMethod a owl:Class ;
    rdfs:comment """Spectral biclustering (Kluger, 2003).

Partitions rows and columns under the assumption that the data has
an underlying checkerboard structure. For instance, if there are
two row partitions and three column partitions, each row will
belong to three biclusters, and each column will belong to two
biclusters. The outer product of the corresponding row and column
label vectors gives this checkerboard structure.

Read more in the :ref:`User Guide <spectral_biclustering>`.

Parameters
----------
n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3
    The number of row and column clusters in the checkerboard
    structure.

method : {'bistochastic', 'scale', 'log'}, default='bistochastic'
    Method of normalizing and converting singular vectors into
    biclusters. May be one of 'scale', 'bistochastic', or 'log'.
    The authors recommend using 'log'. If the data is sparse,
    however, log normalization will not work, which is why the
    default is 'bistochastic'.

    .. warning::
       if `method='log'`, the data must not be sparse.

n_components : int, default=6
    Number of singular vectors to check.

n_best : int, default=3
    Number of best singular vectors to which to project the data
    for clustering.

svd_method : {'randomized', 'arpack'}, default='randomized'
    Selects the algorithm for finding singular vectors. May be
    'randomized' or 'arpack'. If 'randomized', uses
    :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster
    for large matrices. If 'arpack', uses
    `scipy.sparse.linalg.svds`, which is more accurate, but
    possibly slower in some cases.

n_svd_vecs : int, default=None
    Number of vectors to use in calculating the SVD. Corresponds
    to `ncv` when `svd_method=arpack` and `n_oversamples` when
    `svd_method` is 'randomized`.

mini_batch : bool, default=False
    Whether to use mini-batch k-means, which is faster but may get
    different results.

init : {'k-means++', 'random'} or ndarray of shape (n_clusters, n_features),             default='k-means++'
    Method for initialization of k-means algorithm; defaults to
    'k-means++'.

n_init : int, default=10
    Number of random initializations that are tried with the
    k-means algorithm.

    If mini-batch k-means is used, the best initialization is
    chosen and the algorithm runs once. Otherwise, the algorithm
    is run for each initialization and the best solution chosen.

random_state : int, RandomState instance, default=None
    Used for randomizing the singular value decomposition and the k-means
    initialization. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

Attributes
----------
rows_ : array-like of shape (n_row_clusters, n_rows)
    Results of the clustering. `rows[i, r]` is True if
    cluster `i` contains row `r`. Available only after calling ``fit``.

columns_ : array-like of shape (n_column_clusters, n_columns)
    Results of the clustering, like `rows`.

row_labels_ : array-like of shape (n_rows,)
    Row partition labels.

column_labels_ : array-like of shape (n_cols,)
    Column partition labels.

biclusters_ : tuple of two ndarrays
    The tuple contains the `rows_` and `columns_` arrays.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SpectralCoclustering : Spectral Co-Clustering algorithm (Dhillon, 2001).

References
----------

* :doi:`Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray
  data: coclustering genes and conditions.
  <10.1101/gr.648603>`

Examples
--------
>>> from sklearn.cluster import SpectralBiclustering
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)
>>> clustering.row_labels_
array([1, 1, 1, 0, 0, 0], dtype=int32)
>>> clustering.column_labels_
array([1, 0], dtype=int32)
>>> clustering
SpectralBiclustering(n_clusters=2, random_state=0)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:OPTICSMethod a owl:Class ;
    rdfs:comment """Estimate clustering structure from vector array.

OPTICS (Ordering Points To Identify the Clustering Structure), closely
related to DBSCAN, finds core sample of high density and expands clusters
from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable
neighborhood radius. Better suited for usage on large datasets than the
current sklearn implementation of DBSCAN.

Clusters are then extracted using a DBSCAN-like method
(cluster_method = 'dbscan') or an automatic
technique proposed in [1]_ (cluster_method = 'xi').

This implementation deviates from the original OPTICS by first performing
k-nearest-neighborhood searches on all points to identify core sizes, then
computing only the distances to unprocessed points when constructing the
cluster order. Note that we do not employ a heap to manage the expansion
candidates, so the time complexity will be O(n^2).

Read more in the :ref:`User Guide <optics>`.

Parameters
----------
min_samples : int > 1 or float between 0 and 1, default=5
    The number of samples in a neighborhood for a point to be considered as
    a core point. Also, up and down steep regions can't have more than
    ``min_samples`` consecutive non-steep points. Expressed as an absolute
    number or a fraction of the number of samples (rounded to be at least
    2).

max_eps : float, default=np.inf
    The maximum distance between two samples for one to be considered as
    in the neighborhood of the other. Default value of ``np.inf`` will
    identify clusters across all scales; reducing ``max_eps`` will result
    in shorter run times.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.

    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string. If metric is
    "precomputed", `X` is assumed to be a distance matrix and must be
    square.

    Valid values for metric are:

    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
      'manhattan']

    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
      'yule']

    Sparse matrices are only supported by scikit-learn metrics.
    See the documentation for scipy.spatial.distance for details on these
    metrics.

    .. note::
       `'kulsinski'` is deprecated from SciPy 1.9 and will removed in SciPy 1.11.

p : float, default=2
    Parameter for the Minkowski metric from
    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

cluster_method : str, default='xi'
    The extraction method used to extract clusters using the calculated
    reachability and ordering. Possible values are "xi" and "dbscan".

eps : float, default=None
    The maximum distance between two samples for one to be considered as
    in the neighborhood of the other. By default it assumes the same value
    as ``max_eps``.
    Used only when ``cluster_method='dbscan'``.

xi : float between 0 and 1, default=0.05
    Determines the minimum steepness on the reachability plot that
    constitutes a cluster boundary. For example, an upwards point in the
    reachability plot is defined by the ratio from one point to its
    successor being at most 1-xi.
    Used only when ``cluster_method='xi'``.

predecessor_correction : bool, default=True
    Correct clusters according to the predecessors calculated by OPTICS
    [2]_. This parameter has minimal effect on most datasets.
    Used only when ``cluster_method='xi'``.

min_cluster_size : int > 1 or float between 0 and 1, default=None
    Minimum number of samples in an OPTICS cluster, expressed as an
    absolute number or a fraction of the number of samples (rounded to be
    at least 2). If ``None``, the value of ``min_samples`` is used instead.
    Used only when ``cluster_method='xi'``.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.
    - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.
    - 'brute' will use a brute-force search.
    - 'auto' (default) will attempt to decide the most appropriate
      algorithm based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to :class:`~sklearn.neighbors.BallTree` or
    :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the
    construction and query, as well as the memory required to store the
    tree. The optimal value depends on the nature of the problem.

memory : str or object with the joblib.Memory interface, default=None
    Used to cache the output of the computation of the tree.
    By default, no caching is done. If a string is given, it is the
    path to the caching directory.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
labels_ : ndarray of shape (n_samples,)
    Cluster labels for each point in the dataset given to fit().
    Noisy samples and points which are not included in a leaf cluster
    of ``cluster_hierarchy_`` are labeled as -1.

reachability_ : ndarray of shape (n_samples,)
    Reachability distances per sample, indexed by object order. Use
    ``clust.reachability_[clust.ordering_]`` to access in cluster order.

ordering_ : ndarray of shape (n_samples,)
    The cluster ordered list of sample indices.

core_distances_ : ndarray of shape (n_samples,)
    Distance at which each sample becomes a core point, indexed by object
    order. Points which will never be core have a distance of inf. Use
    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.

predecessor_ : ndarray of shape (n_samples,)
    Point that a sample was reached from, indexed by object order.
    Seed points have a predecessor of -1.

cluster_hierarchy_ : ndarray of shape (n_clusters, 2)
    The list of clusters in the form of ``[start, end]`` in each row, with
    all indices inclusive. The clusters are ordered according to
    ``(end, -start)`` (ascending) so that larger clusters encompassing
    smaller clusters come after those smaller ones. Since ``labels_`` does
    not reflect the hierarchy, usually
    ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also
    note that these indices are of the ``ordering_``, i.e.
    ``X[ordering_][start:end + 1]`` form a cluster.
    Only available when ``cluster_method='xi'``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
DBSCAN : A similar clustering for a specified neighborhood radius (eps).
    Our implementation is optimized for runtime.

References
----------
.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,
   and Jörg Sander. "OPTICS: ordering points to identify the clustering
   structure." ACM SIGMOD Record 28, no. 2 (1999): 49-60.

.. [2] Schubert, Erich, Michael Gertz.
   "Improving the Cluster Structure Extracted from OPTICS Plots." Proc. of
   the Conference "Lernen, Wissen, Daten, Analysen" (LWDA) (2018): 318-329.

Examples
--------
>>> from sklearn.cluster import OPTICS
>>> import numpy as np
>>> X = np.array([[1, 2], [2, 5], [3, 6],
...               [8, 7], [8, 8], [7, 3]])
>>> clustering = OPTICS(min_samples=2).fit(X)
>>> clustering.labels_
array([0, 0, 0, 1, 1, 1])

For a more detailed example see
:ref:`sphx_glr_auto_examples_cluster_plot_optics.py`.""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:MiniBatchKMeansMethod a owl:Class ;
    rdfs:comment """Mini-Batch K-Means clustering.

Read more in the :ref:`User Guide <mini_batch_kmeans>`.

Parameters
----------

n_clusters : int, default=8
    The number of clusters to form as well as the number of
    centroids to generate.

init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'
    Method for initialization:

    'k-means++' : selects initial cluster centroids using sampling based on
    an empirical probability distribution of the points' contribution to the
    overall inertia. This technique speeds up convergence. The algorithm
    implemented is "greedy k-means++". It differs from the vanilla k-means++
    by making several trials at each sampling step and choosing the best centroid
    among them.

    'random': choose `n_clusters` observations (rows) at random from data
    for the initial centroids.

    If an array is passed, it should be of shape (n_clusters, n_features)
    and gives the initial centers.

    If a callable is passed, it should take arguments X, n_clusters and a
    random state and return an initialization.

max_iter : int, default=100
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion heuristics.

batch_size : int, default=1024
    Size of the mini batches.
    For faster computations, you can set the ``batch_size`` greater than
    256 * number of cores to enable parallelism on all cores.

    .. versionchanged:: 1.0
       `batch_size` default changed from 100 to 1024.

verbose : int, default=0
    Verbosity mode.

compute_labels : bool, default=True
    Compute label assignment and inertia for the complete dataset
    once the minibatch optimization has converged in fit.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization and
    random reassignment. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

tol : float, default=0.0
    Control early stopping based on the relative center changes as
    measured by a smoothed, variance-normalized of the mean center
    squared position changes. This early stopping heuristics is
    closer to the one used for the batch variant of the algorithms
    but induces a slight computational and memory overhead over the
    inertia heuristic.

    To disable convergence detection based on normalized center
    change, set tol to 0.0 (default).

max_no_improvement : int, default=10
    Control early stopping based on the consecutive number of mini
    batches that does not yield an improvement on the smoothed inertia.

    To disable convergence detection based on inertia, set
    max_no_improvement to None.

init_size : int, default=None
    Number of samples to randomly sample for speeding up the
    initialization (sometimes at the expense of accuracy): the
    only algorithm is initialized by running a batch KMeans on a
    random subset of the data. This needs to be larger than n_clusters.

    If `None`, the heuristic is `init_size = 3 * batch_size` if
    `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.

n_init : 'auto' or int, default="auto"
    Number of random initializations that are tried.
    In contrast to KMeans, the algorithm is only run once, using the best of
    the `n_init` initializations as measured by inertia. Several runs are
    recommended for sparse high-dimensional problems (see
    :ref:`kmeans_sparse_high_dim`).

    When `n_init='auto'`, the number of runs depends on the value of init:
    3 if using `init='random'` or `init` is a callable;
    1 if using `init='k-means++'` or `init` is an array-like.

    .. versionadded:: 1.2
       Added 'auto' option for `n_init`.

    .. versionchanged:: 1.4
       Default value for `n_init` changed to `'auto'` in version.

reassignment_ratio : float, default=0.01
    Control the fraction of the maximum number of counts for a center to
    be reassigned. A higher value means that low count centers are more
    easily reassigned, which means that the model will take longer to
    converge, but should converge in a better clustering. However, too high
    a value may cause convergence issues, especially with a small batch
    size.

Attributes
----------

cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers.

labels_ : ndarray of shape (n_samples,)
    Labels of each point (if compute_labels is set to True).

inertia_ : float
    The value of the inertia criterion associated with the chosen
    partition if compute_labels is set to True. If compute_labels is set to
    False, it's an approximation of the inertia based on an exponentially
    weighted average of the batch inertiae.
    The inertia is defined as the sum of square distances of samples to
    their cluster center, weighted by the sample weights if provided.

n_iter_ : int
    Number of iterations over the full dataset.

n_steps_ : int
    Number of minibatches processed.

    .. versionadded:: 1.0

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
KMeans : The classic implementation of the clustering method based on the
    Lloyd's algorithm. It consumes the whole set of input data at each
    iteration.

Notes
-----
See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf

When there are too few points in the dataset, some centers may be
duplicated, which means that a proper clustering in terms of the number
of requesting clusters and the number of returned clusters will not
always match. One solution is to set `reassignment_ratio=0`, which
prevents reassignments of clusters that are too small.

Examples
--------
>>> from sklearn.cluster import MiniBatchKMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 0], [4, 4],
...               [4, 5], [0, 1], [2, 2],
...               [3, 2], [5, 5], [1, -1]])
>>> # manually fit on batches
>>> kmeans = MiniBatchKMeans(n_clusters=2,
...                          random_state=0,
...                          batch_size=6,
...                          n_init="auto")
>>> kmeans = kmeans.partial_fit(X[0:6,:])
>>> kmeans = kmeans.partial_fit(X[6:12,:])
>>> kmeans.cluster_centers_
array([[3.375, 3.  ],
       [0.75 , 0.5 ]])
>>> kmeans.predict([[0, 0], [4, 4]])
array([1, 0], dtype=int32)
>>> # fit on the whole data
>>> kmeans = MiniBatchKMeans(n_clusters=2,
...                          random_state=0,
...                          batch_size=6,
...                          max_iter=10,
...                          n_init="auto").fit(X)
>>> kmeans.cluster_centers_
array([[3.55102041, 2.48979592],
       [1.06896552, 1.        ]])
>>> kmeans.predict([[0, 0], [4, 4]])
array([1, 0], dtype=int32)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

ml:ClusterModule a owl:Class ;
    rdfs:subClassOf ml:SklearnModule .

ml:HDBSCANMethod a owl:Class ;
    rdfs:comment """Cluster data using hierarchical density-based clustering.

HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications
with Noise. Performs :class:`~sklearn.cluster.DBSCAN` over varying epsilon
values and integrates the result to find a clustering that gives the best
stability over epsilon.
This allows HDBSCAN to find clusters of varying densities (unlike
:class:`~sklearn.cluster.DBSCAN`), and be more robust to parameter selection.
Read more in the :ref:`User Guide <hdbscan>`.

For an example of how to use HDBSCAN, as well as a comparison to
:class:`~sklearn.cluster.DBSCAN`, please see the :ref:`plotting demo
<sphx_glr_auto_examples_cluster_plot_hdbscan.py>`.

.. versionadded:: 1.3

Parameters
----------
min_cluster_size : int, default=5
    The minimum number of samples in a group for that group to be
    considered a cluster; groupings smaller than this size will be left
    as noise.

min_samples : int, default=None
    The number of samples in a neighborhood for a point
    to be considered as a core point. This includes the point itself.
    When `None`, defaults to `min_cluster_size`.

cluster_selection_epsilon : float, default=0.0
    A distance threshold. Clusters below this value will be merged.
    See [5]_ for more information.

max_cluster_size : int, default=None
    A limit to the size of clusters returned by the `"eom"` cluster
    selection algorithm. There is no limit when `max_cluster_size=None`.
    Has no effect if `cluster_selection_method="leaf"`.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array.

    - If metric is a string or callable, it must be one of
      the options allowed by :func:`~sklearn.metrics.pairwise_distances`
      for its metric parameter.

    - If metric is "precomputed", X is assumed to be a distance matrix and
      must be square.

metric_params : dict, default=None
    Arguments passed to the distance metric.

alpha : float, default=1.0
    A distance scaling parameter as used in robust single linkage.
    See [3]_ for more information.

algorithm : {"auto", "brute", "kd_tree", "ball_tree"}, default="auto"
    Exactly which algorithm to use for computing core distances; By default
    this is set to `"auto"` which attempts to use a
    :class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses
    a :class:`~sklearn.neighbors.BallTree` tree. Both `"kd_tree"` and
    `"ball_tree"` algorithms use the
    :class:`~sklearn.neighbors.NearestNeighbors` estimator.

    If the `X` passed during `fit` is sparse or `metric` is invalid for
    both :class:`~sklearn.neighbors.KDTree` and
    :class:`~sklearn.neighbors.BallTree`, then it resolves to use the
    `"brute"` algorithm.

    .. deprecated:: 1.4
       The `'kdtree'` option was deprecated in version 1.4,
       and will be renamed to `'kd_tree'` in 1.6.

    .. deprecated:: 1.4
       The `'balltree'` option was deprecated in version 1.4,
       and will be renamed to `'ball_tree'` in 1.6.

leaf_size : int, default=40
    Leaf size for trees responsible for fast nearest neighbour queries when
    a KDTree or a BallTree are used as core-distance algorithms. A large
    dataset size and small `leaf_size` may induce excessive memory usage.
    If you are running out of memory consider increasing the `leaf_size`
    parameter. Ignored for `algorithm="brute"`.

n_jobs : int, default=None
    Number of jobs to run in parallel to calculate distances.
    `None` means 1 unless in a :obj:`joblib.parallel_backend` context.
    `-1` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

cluster_selection_method : {"eom", "leaf"}, default="eom"
    The method used to select clusters from the condensed tree. The
    standard approach for HDBSCAN* is to use an Excess of Mass (`"eom"`)
    algorithm to find the most persistent clusters. Alternatively you can
    instead select the clusters at the leaves of the tree -- this provides
    the most fine grained and homogeneous clusters.

allow_single_cluster : bool, default=False
    By default HDBSCAN* will not produce a single cluster, setting this
    to True will override this and allow single cluster results in
    the case that you feel this is a valid result for your dataset.

store_centers : str, default=None
    Which, if any, cluster centers to compute and store. The options are:

    - `None` which does not compute nor store any centers.
    - `"centroid"` which calculates the center by taking the weighted
      average of their positions. Note that the algorithm uses the
      euclidean metric and does not guarantee that the output will be
      an observed data point.
    - `"medoid"` which calculates the center by taking the point in the
      fitted data which minimizes the distance to all other points in
      the cluster. This is slower than "centroid" since it requires
      computing additional pairwise distances between points of the
      same cluster but guarantees the output is an observed data point.
      The medoid is also well-defined for arbitrary metrics, and does not
      depend on a euclidean metric.
    - `"both"` which computes and stores both forms of centers.

copy : bool, default=False
    If `copy=True` then any time an in-place modifications would be made
    that would overwrite data passed to :term:`fit`, a copy will first be
    made, guaranteeing that the original data will be unchanged.
    Currently, it only applies when `metric="precomputed"`, when passing
    a dense array or a CSR sparse matrix and when `algorithm="brute"`.

Attributes
----------
labels_ : ndarray of shape (n_samples,)
    Cluster labels for each point in the dataset given to :term:`fit`.
    Outliers are labeled as follows:

    - Noisy samples are given the label -1.
    - Samples with infinite elements (+/- np.inf) are given the label -2.
    - Samples with missing data are given the label -3, even if they
      also have infinite elements.

probabilities_ : ndarray of shape (n_samples,)
    The strength with which each sample is a member of its assigned
    cluster.

    - Clustered samples have probabilities proportional to the degree that
      they persist as part of the cluster.
    - Noisy samples have probability zero.
    - Samples with infinite elements (+/- np.inf) have probability 0.
    - Samples with missing data have probability `np.nan`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

centroids_ : ndarray of shape (n_clusters, n_features)
    A collection containing the centroid of each cluster calculated under
    the standard euclidean metric. The centroids may fall "outside" their
    respective clusters if the clusters themselves are non-convex.

    Note that `n_clusters` only counts non-outlier clusters. That is to
    say, the `-1, -2, -3` labels for the outlier clusters are excluded.

medoids_ : ndarray of shape (n_clusters, n_features)
    A collection containing the medoid of each cluster calculated under
    the whichever metric was passed to the `metric` parameter. The
    medoids are points in the original cluster which minimize the average
    distance to all other points in that cluster under the chosen metric.
    These can be thought of as the result of projecting the `metric`-based
    centroid back onto the cluster.

    Note that `n_clusters` only counts non-outlier clusters. That is to
    say, the `-1, -2, -3` labels for the outlier clusters are excluded.

See Also
--------
DBSCAN : Density-Based Spatial Clustering of Applications
    with Noise.
OPTICS : Ordering Points To Identify the Clustering Structure.
Birch : Memory-efficient, online-learning algorithm.

References
----------

.. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering
  based on hierarchical density estimates.
  <10.1007/978-3-642-37456-2_14>`
.. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.
   Hierarchical density estimates for data clustering, visualization,
   and outlier detection.<10.1145/2733381>`

.. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the
   cluster tree.
   <https://papers.nips.cc/paper/2010/hash/
   b534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_

.. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and
   Sander, J. Density-Based Clustering Validation.
   <https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_

.. [5] :arxiv:`Malzer, C., & Baum, M. "A Hybrid Approach To Hierarchical
   Density-based Cluster Selection."<1911.02282>`.

Examples
--------
>>> from sklearn.cluster import HDBSCAN
>>> from sklearn.datasets import load_digits
>>> X, _ = load_digits(return_X_y=True)
>>> hdb = HDBSCAN(min_cluster_size=20)
>>> hdb.fit(X)
HDBSCAN(min_cluster_size=20)
>>> hdb.labels_
array([ 2,  6, -1, ..., -1, -1, -1])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:HdbscanModule,
        ml:TrainMethod .

ml:SpectralClusteringMethod a owl:Class ;
    rdfs:comment """Apply clustering to a projection of the normalized Laplacian.

In practice Spectral Clustering is very useful when the structure of
the individual clusters is highly non-convex, or more generally when
a measure of the center and spread of the cluster is not a suitable
description of the complete cluster, such as when clusters are
nested circles on the 2D plane.

If the affinity matrix is the adjacency matrix of a graph, this method
can be used to find normalized graph cuts [1]_, [2]_.

When calling ``fit``, an affinity matrix is constructed using either
a kernel function such the Gaussian (aka RBF) kernel with Euclidean
distance ``d(X, X)``::

        np.exp(-gamma * d(X,X) ** 2)

or a k-nearest neighbors connectivity matrix.

Alternatively, a user-provided affinity matrix can be specified by
setting ``affinity='precomputed'``.

Read more in the :ref:`User Guide <spectral_clustering>`.

Parameters
----------
n_clusters : int, default=8
    The dimension of the projection subspace.

eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None
    The eigenvalue decomposition strategy to use. AMG requires pyamg
    to be installed. It can be faster on very large, sparse problems,
    but may also lead to instabilities. If None, then ``'arpack'`` is
    used. See [4]_ for more details regarding `'lobpcg'`.

n_components : int, default=None
    Number of eigenvectors to use for the spectral embedding. If None,
    defaults to `n_clusters`.

random_state : int, RandomState instance, default=None
    A pseudo random number generator used for the initialization
    of the lobpcg eigenvectors decomposition when `eigen_solver ==
    'amg'`, and for the K-Means initialization. Use an int to make
    the results deterministic across calls (See
    :term:`Glossary <random_state>`).

    .. note::
        When using `eigen_solver == 'amg'`,
        it is necessary to also fix the global numpy seed with
        `np.random.seed(int)` to get deterministic results. See
        https://github.com/pyamg/pyamg/issues/139 for further
        information.

n_init : int, default=10
    Number of time the k-means algorithm will be run with different
    centroid seeds. The final results will be the best output of n_init
    consecutive runs in terms of inertia. Only used if
    ``assign_labels='kmeans'``.

gamma : float, default=1.0
    Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.
    Ignored for ``affinity='nearest_neighbors'``.

affinity : str or callable, default='rbf'
    How to construct the affinity matrix.
     - 'nearest_neighbors': construct the affinity matrix by computing a
       graph of nearest neighbors.
     - 'rbf': construct the affinity matrix using a radial basis function
       (RBF) kernel.
     - 'precomputed': interpret ``X`` as a precomputed affinity matrix,
       where larger values indicate greater similarity between instances.
     - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph
       of precomputed distances, and construct a binary affinity matrix
       from the ``n_neighbors`` nearest neighbors of each instance.
     - one of the kernels supported by
       :func:`~sklearn.metrics.pairwise.pairwise_kernels`.

    Only kernels that produce similarity scores (non-negative values that
    increase with similarity) should be used. This property is not checked
    by the clustering algorithm.

n_neighbors : int, default=10
    Number of neighbors to use when constructing the affinity matrix using
    the nearest neighbors method. Ignored for ``affinity='rbf'``.

eigen_tol : float, default="auto"
    Stopping criterion for eigen decomposition of the Laplacian matrix.
    If `eigen_tol="auto"` then the passed tolerance will depend on the
    `eigen_solver`:

    - If `eigen_solver="arpack"`, then `eigen_tol=0.0`;
    - If `eigen_solver="lobpcg"` or `eigen_solver="amg"`, then
      `eigen_tol=None` which configures the underlying `lobpcg` solver to
      automatically resolve the value according to their heuristics. See,
      :func:`scipy.sparse.linalg.lobpcg` for details.

    Note that when using `eigen_solver="lobpcg"` or `eigen_solver="amg"`
    values of `tol<1e-5` may lead to convergence issues and should be
    avoided.

    .. versionadded:: 1.2
       Added 'auto' option.

assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'
    The strategy for assigning labels in the embedding space. There are two
    ways to assign labels after the Laplacian embedding. k-means is a
    popular choice, but it can be sensitive to initialization.
    Discretization is another approach which is less sensitive to random
    initialization [3]_.
    The cluster_qr method [5]_ directly extract clusters from eigenvectors
    in spectral clustering. In contrast to k-means and discretization, cluster_qr
    has no tuning parameters and runs no iterations, yet may outperform
    k-means and discretization in terms of both quality and speed.

    .. versionchanged:: 1.1
       Added new labeling method 'cluster_qr'.

degree : float, default=3
    Degree of the polynomial kernel. Ignored by other kernels.

coef0 : float, default=1
    Zero coefficient for polynomial and sigmoid kernels.
    Ignored by other kernels.

kernel_params : dict of str to any, default=None
    Parameters (keyword arguments) and values for kernel passed as
    callable object. Ignored by other kernels.

n_jobs : int, default=None
    The number of parallel jobs to run when `affinity='nearest_neighbors'`
    or `affinity='precomputed_nearest_neighbors'`. The neighbors search
    will be done in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : bool, default=False
    Verbosity mode.

    .. versionadded:: 0.24

Attributes
----------
affinity_matrix_ : array-like of shape (n_samples, n_samples)
    Affinity matrix used for clustering. Available only after calling
    ``fit``.

labels_ : ndarray of shape (n_samples,)
    Labels of each point

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.cluster.KMeans : K-Means clustering.
sklearn.cluster.DBSCAN : Density-Based Spatial Clustering of
    Applications with Noise.

Notes
-----
A distance matrix for which 0 indicates identical elements and high values
indicate very dissimilar elements can be transformed into an affinity /
similarity matrix that is well-suited for the algorithm by
applying the Gaussian (aka RBF, heat) kernel::

    np.exp(- dist_matrix ** 2 / (2. * delta ** 2))

where ``delta`` is a free parameter representing the width of the Gaussian
kernel.

An alternative is to take a symmetric version of the k-nearest neighbors
connectivity matrix of the points.

If the pyamg package is installed, it is used: this greatly
speeds up computation.

References
----------
.. [1] :doi:`Normalized cuts and image segmentation, 2000
       Jianbo Shi, Jitendra Malik
       <10.1109/34.868688>`

.. [2] :doi:`A Tutorial on Spectral Clustering, 2007
       Ulrike von Luxburg
       <10.1007/s11222-007-9033-z>`

.. [3] `Multiclass spectral clustering, 2003
       Stella X. Yu, Jianbo Shi
       <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_

.. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:
       Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001
       A. V. Knyazev
       SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.
       <10.1137/S1064827500366124>`

.. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019
       Anil Damle, Victor Minden, Lexing Ying
       <10.1093/imaiai/iay008>`

Examples
--------
>>> from sklearn.cluster import SpectralClustering
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = SpectralClustering(n_clusters=2,
...         assign_labels='discretize',
...         random_state=0).fit(X)
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])
>>> clustering
SpectralClustering(assign_labels='discretize', n_clusters=2,
    random_state=0)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:ClusterModule,
        ml:TrainMethod .

