@prefix ds: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#> .
@prefix ml: <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

ml:hasParamA a owl:DatatypeProperty ;
    rdfs:domain ml:ConsensusScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAdjusted a owl:DatatypeProperty ;
    rdfs:domain ml:BalancedAccuracyScoreMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAllowNone a owl:DatatypeProperty ;
    rdfs:domain ml:CheckScoringMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAlpha a owl:DatatypeProperty ;
    rdfs:domain ml:D2PinballScoreMethod,
        ml:MeanPinballLossMethod ;
    rdfs:range xsd:float,
        xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAverage a owl:DatatypeProperty ;
    rdfs:domain ml:AveragePrecisionScoreMethod,
        ml:F1ScoreMethod,
        ml:FbetaScoreMethod,
        ml:JaccardScoreMethod,
        ml:PrecisionRecallFscoreSupportMethod,
        ml:PrecisionScoreMethod,
        ml:RecallScoreMethod,
        ml:RocAucScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAverageMethod a owl:DatatypeProperty ;
    rdfs:domain ml:AdjustedMutualInfoScoreMethod,
        ml:NormalizedMutualInfoScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamAxis a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesArgminMethod,
        ml:PairwiseDistancesArgminMinMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamB a owl:DatatypeProperty ;
    rdfs:domain ml:ConsensusScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamBeta a owl:DatatypeProperty ;
    rdfs:domain ml:FbetaScoreMethod,
        ml:HomogeneityCompletenessVMeasureMethod,
        ml:PrecisionRecallFscoreSupportMethod,
        ml:VMeasureScoreMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamContingency a owl:DatatypeProperty ;
    rdfs:domain ml:MutualInfoScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamCopy a owl:DatatypeProperty ;
    rdfs:domain ml:NanEuclideanDistancesMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamDigits a owl:DatatypeProperty ;
    rdfs:domain ml:ClassificationReportMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamDropIntermediate a owl:DatatypeProperty ;
    rdfs:domain ml:PrecisionRecallCurveMethod,
        ml:RocCurveMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamEps a owl:DatatypeProperty ;
    rdfs:domain ml:LogLossMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamFilterParams a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseKernelsMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamForceAllFinite a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamForceFinite a owl:DatatypeProperty ;
    rdfs:domain ml:ExplainedVarianceScoreMethod,
        ml:R2ScoreMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamGreaterIsBetter a owl:DatatypeProperty ;
    rdfs:domain ml:MakeScorerMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamIgnoreTies a owl:DatatypeProperty ;
    rdfs:domain ml:DcgScoreMethod,
        ml:NdcgScoreMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamK a owl:DatatypeProperty ;
    rdfs:domain ml:DcgScoreMethod,
        ml:NdcgScoreMethod,
        ml:TopKAccuracyScoreMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamLogBase a owl:DatatypeProperty ;
    rdfs:domain ml:DcgScoreMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMetric a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesArgminMethod,
        ml:PairwiseDistancesArgminMinMethod,
        ml:PairwiseDistancesChunkedMethod,
        ml:PairwiseDistancesMethod,
        ml:PairwiseKernelsMethod,
        ml:SilhouetteSamplesMethod,
        ml:SilhouetteScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMetricKwargs a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesArgminMethod,
        ml:PairwiseDistancesArgminMinMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMissingValues a owl:DatatypeProperty ;
    rdfs:domain ml:NanEuclideanDistancesMethod ;
    rdfs:range xsd:float,
        xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMultiClass a owl:DatatypeProperty ;
    rdfs:domain ml:RocAucScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamMultioutput a owl:DatatypeProperty ;
    rdfs:domain ml:D2AbsoluteErrorScoreMethod,
        ml:D2PinballScoreMethod,
        ml:ExplainedVarianceScoreMethod,
        ml:MeanAbsoluteErrorMethod,
        ml:MeanAbsolutePercentageErrorMethod,
        ml:MeanPinballLossMethod,
        ml:MeanSquaredErrorMethod,
        ml:MeanSquaredLogErrorMethod,
        ml:MedianAbsoluteErrorMethod,
        ml:R2ScoreMethod,
        ml:RootMeanSquaredErrorMethod,
        ml:RootMeanSquaredLogErrorMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNJobs a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesChunkedMethod,
        ml:PairwiseDistancesMethod,
        ml:PairwiseKernelsMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNeedsProba a owl:DatatypeProperty ;
    rdfs:domain ml:MakeScorerMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNeedsThreshold a owl:DatatypeProperty ;
    rdfs:domain ml:MakeScorerMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamNormalize a owl:DatatypeProperty ;
    rdfs:domain ml:AccuracyScoreMethod,
        ml:ConfusionMatrixMethod,
        ml:LogLossMethod,
        ml:TopKAccuracyScoreMethod,
        ml:ZeroOneLossMethod ;
    rdfs:range xsd:boolean,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamOutputDict a owl:DatatypeProperty ;
    rdfs:domain ml:ClassificationReportMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPosLabel a owl:DatatypeProperty ;
    rdfs:domain ml:AveragePrecisionScoreMethod,
        ml:BrierScoreLossMethod,
        ml:DetCurveMethod,
        ml:F1ScoreMethod,
        ml:FbetaScoreMethod,
        ml:JaccardScoreMethod,
        ml:PrecisionRecallCurveMethod,
        ml:PrecisionRecallFscoreSupportMethod,
        ml:PrecisionScoreMethod,
        ml:RecallScoreMethod,
        ml:RocCurveMethod ;
    rdfs:range xsd:boolean,
        xsd:float,
        xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPower a owl:DatatypeProperty ;
    rdfs:domain ml:D2TweedieScoreMethod,
        ml:MeanTweedieDevianceMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamPredDecision a owl:DatatypeProperty ;
    rdfs:domain ml:HingeLossMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamRaiseWarning a owl:DatatypeProperty ;
    rdfs:domain ml:ClassLikelihoodRatiosMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamRandomState a owl:DatatypeProperty ;
    rdfs:domain ml:SilhouetteScoreMethod ;
    rdfs:range xsd:int,
        xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamReduceFunc a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesChunkedMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamResponseMethod a owl:DatatypeProperty ;
    rdfs:domain ml:MakeScorerMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSampleSize a owl:DatatypeProperty ;
    rdfs:domain ml:SilhouetteScoreMethod ;
    rdfs:range xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSamplewise a owl:DatatypeProperty ;
    rdfs:domain ml:MultilabelConfusionMatrixMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamScoreFunc a owl:DatatypeProperty ;
    rdfs:domain ml:MakeScorerMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamScoring a owl:DatatypeProperty ;
    rdfs:domain ml:CheckScoringMethod,
        ml:GetScorerMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSimilarity a owl:DatatypeProperty ;
    rdfs:domain ml:ConsensusScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSparse a owl:DatatypeProperty ;
    rdfs:domain ml:FowlkesMallowsScoreMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamSquared a owl:DatatypeProperty ;
    rdfs:domain ml:EuclideanDistancesMethod,
        ml:MeanSquaredErrorMethod,
        ml:MeanSquaredLogErrorMethod,
        ml:NanEuclideanDistancesMethod ;
    rdfs:range xsd:boolean ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamWarnFor a owl:DatatypeProperty ;
    rdfs:domain ml:PrecisionRecallFscoreSupportMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamWeights a owl:DatatypeProperty ;
    rdfs:domain ml:CohenKappaScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamWorkingMemory a owl:DatatypeProperty ;
    rdfs:domain ml:PairwiseDistancesChunkedMethod ;
    rdfs:range xsd:float,
        xsd:int ;
    rdfs:subPropertyOf ds:hasParameter .

ml:hasParamZeroDivision a owl:DatatypeProperty ;
    rdfs:domain ml:ClassificationReportMethod,
        ml:F1ScoreMethod,
        ml:FbetaScoreMethod,
        ml:JaccardScoreMethod,
        ml:PrecisionRecallFscoreSupportMethod,
        ml:PrecisionScoreMethod,
        ml:RecallScoreMethod ;
    rdfs:range xsd:string ;
    rdfs:subPropertyOf ds:hasParameter .

ml:AdjustedRandScoreMethod a owl:Class ;
    rdfs:comment """Rand index adjusted for chance.

The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings.

The raw RI score is then "adjusted for chance" into the ARI score
using the following scheme::

    ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)

The adjusted Rand index is thus ensured to have a value close to
0.0 for random labeling independently of the number of clusters and
samples and exactly 1.0 when the clusterings are identical (up to
a permutation). The adjusted Rand index is bounded below by -0.5 for
especially discordant clusterings.

ARI is a symmetric measure::

    adjusted_rand_score(a, b) == adjusted_rand_score(b, a)

Read more in the :ref:`User Guide <adjusted_rand_score>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=int
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,), dtype=int
    Cluster labels to evaluate.

Returns
-------
ARI : float
   Similarity score between -0.5 and 1.0. Random labelings have an ARI
   close to 0.0. 1.0 stands for perfect match.

See Also
--------
adjusted_mutual_info_score : Adjusted Mutual Information.

References
----------
.. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,
  Journal of Classification 1985
  https://link.springer.com/article/10.1007%2FBF01908075

.. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie
  adjusted Rand index, Psychological Methods 2004

.. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index

.. [Chacon] :doi:`Minimum adjusted Rand index for two clusterings of a given size,
  2022, J. E. Chacón and A. I. Rastrojo <10.1007/s11634-022-00491-w>`

Examples
--------
Perfectly matching labelings have a score of 1 even

  >>> from sklearn.metrics.cluster import adjusted_rand_score
  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
  1.0
  >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized::

  >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
  0.57...

ARI is symmetric, so labelings that have pure clusters with members
coming from the same classes but unnecessary splits are penalized::

  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
  0.57...

If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the ARI is very low::

  >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
  0.0

ARI may take a negative value for especially discordant labelings that
are a worse choice than the expected value of random labels::

  >>> adjusted_rand_score([0, 0, 1, 1], [0, 1, 0, 1])
  -0.5""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:AucMethod a owl:Class ;
    rdfs:comment """Compute Area Under the Curve (AUC) using the trapezoidal rule.

This is a general function, given points on a curve.  For computing the
area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative
way to summarize a precision-recall curve, see
:func:`average_precision_score`.

Parameters
----------
x : array-like of shape (n,)
    X coordinates. These must be either monotonic increasing or monotonic
    decreasing.
y : array-like of shape (n,)
    Y coordinates.

Returns
-------
auc : float
    Area Under the Curve.

See Also
--------
roc_auc_score : Compute the area under the ROC curve.
average_precision_score : Compute average precision from prediction scores.
precision_recall_curve : Compute precision-recall pairs for different
    probability thresholds.

Examples
--------
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> pred = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
>>> metrics.auc(fpr, tpr)
0.75""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:CalinskiHarabaszScoreMethod a owl:Class ;
    rdfs:comment """Compute the Calinski and Harabasz score.

It is also known as the Variance Ratio Criterion.

The score is defined as ratio of the sum of between-cluster dispersion and
of within-cluster dispersion.

Read more in the :ref:`User Guide <calinski_harabasz_index>`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    A list of ``n_features``-dimensional data points. Each row corresponds
    to a single data point.

labels : array-like of shape (n_samples,)
    Predicted labels for each sample.

Returns
-------
score : float
    The resulting Calinski-Harabasz score.

References
----------
.. [1] `T. Calinski and J. Harabasz, 1974. "A dendrite method for cluster
   analysis". Communications in Statistics
   <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_

Examples
--------
>>> from sklearn.datasets import make_blobs
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import calinski_harabasz_score
>>> X, _ = make_blobs(random_state=0)
>>> kmeans = KMeans(n_clusters=3, random_state=0,).fit(X)
>>> calinski_harabasz_score(X, kmeans.labels_)
114.8...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:CompletenessScoreMethod a owl:Class ;
    rdfs:comment """Compute completeness metric of a cluster labeling given a ground truth.

A clustering result satisfies completeness if all the data points
that are members of a given class are elements of the same cluster.

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is not symmetric: switching ``label_true`` with ``label_pred``
will return the :func:`homogeneity_score` which will be different in
general.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Cluster labels to evaluate.

Returns
-------
completeness : float
   Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.

See Also
--------
homogeneity_score : Homogeneity metric of cluster labeling.
v_measure_score : V-Measure (NMI with arithmetic mean option).

References
----------

.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
   conditional entropy-based external cluster evaluation measure
   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_

Examples
--------

Perfect labelings are complete::

  >>> from sklearn.metrics.cluster import completeness_score
  >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Non-perfect labelings that assign all classes members to the same clusters
are still complete::

  >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))
  1.0
  >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))
  0.999...

If classes members are split across different clusters, the
assignment cannot be complete::

  >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))
  0.0
  >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))
  0.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:CoverageErrorMethod a owl:Class ;
    rdfs:comment """Coverage error measure.

Compute how far we need to go through the ranked scores to cover all
true labels. The best value is equal to the average number
of labels in ``y_true`` per sample.

Ties in ``y_scores`` are broken by giving maximal rank that would have
been assigned to all tied values.

Note: Our implementation's score is 1 greater than the one given in
Tsoumakas et al., 2010. This extends it to handle the degenerate case
in which an instance has 0 true labels.

Read more in the :ref:`User Guide <coverage_error>`.

Parameters
----------
y_true : array-like of shape (n_samples, n_labels)
    True binary labels in binary indicator format.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by "decision_function" on some classifiers).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
coverage_error : float
    The coverage error.

References
----------
.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).
       Mining multi-label data. In Data mining and knowledge discovery
       handbook (pp. 667-685). Springer US.

Examples
--------
>>> from sklearn.metrics import coverage_error
>>> y_true = [[1, 0, 0], [0, 1, 1]]
>>> y_score = [[1, 0, 0], [0, 1, 1]]
>>> coverage_error(y_true, y_score)
1.5""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:DaviesBouldinScoreMethod a owl:Class ;
    rdfs:comment """Compute the Davies-Bouldin score.

The score is defined as the average similarity measure of each cluster with
its most similar cluster, where similarity is the ratio of within-cluster
distances to between-cluster distances. Thus, clusters which are farther
apart and less dispersed will result in a better score.

The minimum score is zero, with lower values indicating better clustering.

Read more in the :ref:`User Guide <davies-bouldin_index>`.

.. versionadded:: 0.20

Parameters
----------
X : array-like of shape (n_samples, n_features)
    A list of ``n_features``-dimensional data points. Each row corresponds
    to a single data point.

labels : array-like of shape (n_samples,)
    Predicted labels for each sample.

Returns
-------
score: float
    The resulting Davies-Bouldin score.

References
----------
.. [1] Davies, David L.; Bouldin, Donald W. (1979).
   `"A Cluster Separation Measure"
   <https://ieeexplore.ieee.org/document/4766909>`__.
   IEEE Transactions on Pattern Analysis and Machine Intelligence.
   PAMI-1 (2): 224-227

Examples
--------
>>> from sklearn.metrics import davies_bouldin_score
>>> X = [[0, 1], [1, 1], [3, 4]]
>>> labels = [0, 0, 1]
>>> davies_bouldin_score(X, labels)
0.12...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:GetScorerNamesMethod a owl:Class ;
    rdfs:comment """Get the names of all available scorers.

These names can be passed to :func:`~sklearn.metrics.get_scorer` to
retrieve the scorer object.

Returns
-------
list of str
    Names of all available scorers.

Examples
--------
>>> from sklearn.metrics import get_scorer_names
>>> all_scorers = get_scorer_names()
>>> type(all_scorers)
<class 'list'>
>>> all_scorers[:3]
['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score']
>>> "roc_auc" in all_scorers
True""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:HammingLossMethod a owl:Class ;
    rdfs:comment """Compute the average Hamming loss.

The Hamming loss is the fraction of labels that are incorrectly predicted.

Read more in the :ref:`User Guide <hamming_loss>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

Returns
-------
loss : float or int
    Return the average Hamming loss between element of ``y_true`` and
    ``y_pred``.

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.
jaccard_score : Compute the Jaccard similarity coefficient score.
zero_one_loss : Compute the Zero-one classification loss. By default, the
    function will return the percentage of imperfectly predicted subsets.

Notes
-----
In multiclass classification, the Hamming loss corresponds to the Hamming
distance between ``y_true`` and ``y_pred`` which is equivalent to the
subset ``zero_one_loss`` function, when `normalize` parameter is set to
True.

In multilabel classification, the Hamming loss is different from the
subset zero-one loss. The zero-one loss considers the entire set of labels
for a given sample incorrect if it does not entirely match the true set of
labels. Hamming loss is more forgiving in that it penalizes only the
individual labels.

The Hamming loss is upperbounded by the subset zero-one loss, when
`normalize` parameter is set to True. It is always between 0 and 1,
lower being better.

References
----------
.. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:
       An Overview. International Journal of Data Warehousing & Mining,
       3(3), 1-13, July-September 2007.

.. [2] `Wikipedia entry on the Hamming distance
       <https://en.wikipedia.org/wiki/Hamming_distance>`_.

Examples
--------
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:HomogeneityScoreMethod a owl:Class ;
    rdfs:comment """Homogeneity metric of a cluster labeling given a ground truth.

A clustering result satisfies homogeneity if all of its clusters
contain only data points which are members of a single class.

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is not symmetric: switching ``label_true`` with ``label_pred``
will return the :func:`completeness_score` which will be different in
general.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Cluster labels to evaluate.

Returns
-------
homogeneity : float
   Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.

See Also
--------
completeness_score : Completeness metric of cluster labeling.
v_measure_score : V-Measure (NMI with arithmetic mean option).

References
----------

.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
   conditional entropy-based external cluster evaluation measure
   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_

Examples
--------

Perfect labelings are homogeneous::

  >>> from sklearn.metrics.cluster import homogeneity_score
  >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Non-perfect labelings that further split classes into more clusters can be
perfectly homogeneous::

  >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))
  1.000000
  >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))
  1.000000

Clusters that include samples from different classes do not make for an
homogeneous labeling::

  >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))
  0.0...
  >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))
  0.0...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:LabelRankingAveragePrecisionScoreMethod a owl:Class ;
    rdfs:comment """Compute ranking-based average precision.

Label ranking average precision (LRAP) is the average over each ground
truth label assigned to each sample, of the ratio of true vs. total
labels with lower score.

This metric is used in multilabel ranking problem, where the goal
is to give better rank to the labels associated to each sample.

The obtained score is always strictly greater than 0 and
the best value is 1.

Read more in the :ref:`User Guide <label_ranking_average_precision>`.

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)
    True binary labels in binary indicator format.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by "decision_function" on some classifiers).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.20

Returns
-------
score : float
    Ranking-based average precision score.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_average_precision_score
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_average_precision_score(y_true, y_score)
0.416...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:LabelRankingLossMethod a owl:Class ;
    rdfs:comment """Compute Ranking loss measure.

Compute the average number of label pairs that are incorrectly ordered
given y_score weighted by the size of the label set and the number of
labels not in the label set.

This is similar to the error set size, but weighted by the number of
relevant and irrelevant labels. The best performance is achieved with
a ranking loss of zero.

Read more in the :ref:`User Guide <label_ranking_loss>`.

.. versionadded:: 0.17
   A function *label_ranking_loss*

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)
    True binary labels in binary indicator format.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by "decision_function" on some classifiers).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    Average number of label pairs that are incorrectly ordered given
    y_score weighted by the size of the label set and the number of labels not
    in the label set.

References
----------
.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).
       Mining multi-label data. In Data mining and knowledge discovery
       handbook (pp. 667-685). Springer US.

Examples
--------
>>> from sklearn.metrics import label_ranking_loss
>>> y_true = [[1, 0, 0], [0, 0, 1]]
>>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]
>>> label_ranking_loss(y_true, y_score)
0.75...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MatthewsCorrcoefMethod a owl:Class ;
    rdfs:comment """Compute the Matthews correlation coefficient (MCC).

The Matthews correlation coefficient is used in machine learning as a
measure of the quality of binary and multiclass classifications. It takes
into account true and false positives and negatives and is generally
regarded as a balanced measure which can be used even if the classes are of
very different sizes. The MCC is in essence a correlation coefficient value
between -1 and +1. A coefficient of +1 represents a perfect prediction, 0
an average random prediction and -1 an inverse prediction.  The statistic
is also known as the phi coefficient. [source: Wikipedia]

Binary and multiclass labels are supported.  Only in the binary case does
this relate to information about true and false positives and negatives.
See references below.

Read more in the :ref:`User Guide <matthews_corrcoef>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

Returns
-------
mcc : float
    The Matthews correlation coefficient (+1 represents a perfect
    prediction, 0 an average random prediction and -1 and inverse
    prediction).

References
----------
.. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the
   accuracy of prediction algorithms for classification: an overview.
   <10.1093/bioinformatics/16.5.412>`

.. [2] `Wikipedia entry for the Matthews Correlation Coefficient
   <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.

.. [3] `Gorodkin, (2004). Comparing two K-category assignments by a
    K-category correlation coefficient
    <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.

.. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN
    Error Measures in MultiClass Prediction
    <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.

Examples
--------
>>> from sklearn.metrics import matthews_corrcoef
>>> y_true = [+1, +1, +1, -1]
>>> y_pred = [+1, -1, +1, +1]
>>> matthews_corrcoef(y_true, y_pred)
-0.33...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MaxErrorMethod a owl:Class ;
    rdfs:comment """The max_error metric calculates the maximum residual error.

Read more in the :ref:`User Guide <max_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

Returns
-------
max_error : float
    A positive floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import max_error
>>> y_true = [3, 2, 7, 1]
>>> y_pred = [4, 2, 7, 1]
>>> max_error(y_true, y_pred)
1""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanGammaDevianceMethod a owl:Class ;
    rdfs:comment """Mean Gamma deviance regression loss.

Gamma deviance is equivalent to the Tweedie deviance with
the power parameter `power=2`. It is invariant to scaling of
the target variable, and measures relative errors.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values. Requires y_true > 0.

y_pred : array-like of shape (n_samples,)
    Estimated target values. Requires y_pred > 0.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_gamma_deviance
>>> y_true = [2, 0.5, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_gamma_deviance(y_true, y_pred)
1.0568...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanPoissonDevianceMethod a owl:Class ;
    rdfs:comment """Mean Poisson deviance regression loss.

Poisson deviance is equivalent to the Tweedie deviance with
the power parameter `power=1`.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values. Requires y_true >= 0.

y_pred : array-like of shape (n_samples,)
    Estimated target values. Requires y_pred > 0.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_poisson_deviance
>>> y_true = [2, 0, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_poisson_deviance(y_true, y_pred)
1.4260...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PairConfusionMatrixMethod a owl:Class ;
    rdfs:comment """Pair confusion matrix arising from two clusterings [1]_.

The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix
between two clusterings by considering all pairs of samples and counting
pairs that are assigned into the same or into different clusters under
the true and predicted clusterings.

Considering a pair of samples that is clustered together a positive pair,
then as in binary classification the count of true negatives is
:math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is
:math:`C_{11}` and false positives is :math:`C_{01}`.

Read more in the :ref:`User Guide <pair_confusion_matrix>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=integral
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,), dtype=integral
    Cluster labels to evaluate.

Returns
-------
C : ndarray of shape (2, 2), dtype=np.int64
    The contingency matrix.

See Also
--------
sklearn.metrics.rand_score : Rand Score.
sklearn.metrics.adjusted_rand_score : Adjusted Rand Score.
sklearn.metrics.adjusted_mutual_info_score : Adjusted Mutual Information.

References
----------
.. [1] :doi:`Hubert, L., Arabie, P. "Comparing partitions."
       Journal of Classification 2, 193–218 (1985).
       <10.1007/BF01908075>`

Examples
--------
Perfectly matching labelings have all non-zero entries on the
diagonal regardless of actual label values:

  >>> from sklearn.metrics.cluster import pair_confusion_matrix
  >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
  array([[8, 0],
         [0, 4]]...

Labelings that assign all classes members to the same clusters
are complete but may be not always pure, hence penalized, and
have some off-diagonal non-zero entries:

  >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
  array([[8, 2],
         [0, 2]]...

Note that the matrix is not symmetric.""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:RandScoreMethod a owl:Class ;
    rdfs:comment """Rand index.

The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings [1]_ [2]_.

The raw RI score [3]_ is:

    RI = (number of agreeing pairs) / (number of pairs)

Read more in the :ref:`User Guide <rand_score>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=integral
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,), dtype=integral
    Cluster labels to evaluate.

Returns
-------
RI : float
   Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for
   perfect match.

See Also
--------
adjusted_rand_score: Adjusted Rand Score.
adjusted_mutual_info_score: Adjusted Mutual Information.

References
----------
.. [1] :doi:`Hubert, L., Arabie, P. "Comparing partitions."
   Journal of Classification 2, 193–218 (1985).
   <10.1007/BF01908075>`.

.. [2] `Wikipedia: Simple Matching Coefficient
    <https://en.wikipedia.org/wiki/Simple_matching_coefficient>`_

.. [3] `Wikipedia: Rand Index <https://en.wikipedia.org/wiki/Rand_index>`_

Examples
--------
Perfectly matching labelings have a score of 1 even

  >>> from sklearn.metrics.cluster import rand_score
  >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized:

  >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])
  0.83...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:SklearnModule a owl:Class ;
    rdfs:subClassOf ds:Module .

ml:hasPerformanceCalculationMethod a owl:ObjectProperty ;
    rdfs:domain ml:PerformanceCalculation ;
    rdfs:range ml:AccuracyScoreMethod,
        ml:AdjustedMutualInfoScoreMethod,
        ml:AdjustedRandScoreMethod,
        ml:AucMethod,
        ml:AveragePrecisionScoreMethod,
        ml:BalancedAccuracyScoreMethod,
        ml:BrierScoreLossMethod,
        ml:CalinskiHarabaszScoreMethod,
        ml:CheckScoringMethod,
        ml:ClassLikelihoodRatiosMethod,
        ml:ClassificationReportMethod,
        ml:CohenKappaScoreMethod,
        ml:CompletenessScoreMethod,
        ml:ConfusionMatrixMethod,
        ml:ConsensusScoreMethod,
        ml:CoverageErrorMethod,
        ml:D2AbsoluteErrorScoreMethod,
        ml:D2PinballScoreMethod,
        ml:D2TweedieScoreMethod,
        ml:DaviesBouldinScoreMethod,
        ml:DcgScoreMethod,
        ml:DetCurveMethod,
        ml:EuclideanDistancesMethod,
        ml:ExplainedVarianceScoreMethod,
        ml:F1ScoreMethod,
        ml:FbetaScoreMethod,
        ml:FowlkesMallowsScoreMethod,
        ml:GetScorerMethod,
        ml:GetScorerNamesMethod,
        ml:HammingLossMethod,
        ml:HingeLossMethod,
        ml:HomogeneityCompletenessVMeasureMethod,
        ml:HomogeneityScoreMethod,
        ml:JaccardScoreMethod,
        ml:LabelRankingAveragePrecisionScoreMethod,
        ml:LabelRankingLossMethod,
        ml:LogLossMethod,
        ml:MakeScorerMethod,
        ml:MatthewsCorrcoefMethod,
        ml:MaxErrorMethod,
        ml:MeanAbsoluteErrorMethod,
        ml:MeanAbsolutePercentageErrorMethod,
        ml:MeanGammaDevianceMethod,
        ml:MeanPinballLossMethod,
        ml:MeanPoissonDevianceMethod,
        ml:MeanSquaredErrorMethod,
        ml:MeanSquaredLogErrorMethod,
        ml:MeanTweedieDevianceMethod,
        ml:MedianAbsoluteErrorMethod,
        ml:MultilabelConfusionMatrixMethod,
        ml:MutualInfoScoreMethod,
        ml:NanEuclideanDistancesMethod,
        ml:NdcgScoreMethod,
        ml:NormalizedMutualInfoScoreMethod,
        ml:PairConfusionMatrixMethod,
        ml:PairwiseDistancesArgminMethod,
        ml:PairwiseDistancesArgminMinMethod,
        ml:PairwiseDistancesChunkedMethod,
        ml:PairwiseDistancesMethod,
        ml:PairwiseKernelsMethod,
        ml:PrecisionRecallCurveMethod,
        ml:PrecisionRecallFscoreSupportMethod,
        ml:PrecisionScoreMethod,
        ml:R2ScoreMethod,
        ml:RandScoreMethod,
        ml:RecallScoreMethod,
        ml:RocAucScoreMethod,
        ml:RocCurveMethod,
        ml:RootMeanSquaredErrorMethod,
        ml:RootMeanSquaredLogErrorMethod,
        ml:SilhouetteSamplesMethod,
        ml:SilhouetteScoreMethod,
        ml:TopKAccuracyScoreMethod,
        ml:VMeasureScoreMethod,
        ml:ZeroOneLossMethod ;
    rdfs:subPropertyOf ml:hasPerformanceCalculationMethod .

ml:AccuracyScoreMethod a owl:Class ;
    rdfs:comment """Accuracy classification score.

In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must *exactly* match the
corresponding set of labels in y_true.

Read more in the :ref:`User Guide <accuracy_score>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

normalize : bool, default=True
    If ``False``, return the number of correctly classified samples.
    Otherwise, return the fraction of correctly classified samples.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
score : float
    If ``normalize == True``, return the fraction of correctly
    classified samples (float), else returns the number of correctly
    classified samples (int).

    The best performance is 1 with ``normalize == True`` and the number
    of samples with ``normalize == False``.

See Also
--------
balanced_accuracy_score : Compute the balanced accuracy to deal with
    imbalanced datasets.
jaccard_score : Compute the Jaccard similarity coefficient score.
hamming_loss : Compute the average Hamming loss or Hamming distance between
    two sets of samples.
zero_one_loss : Compute the Zero-one classification loss. By default, the
    function will return the percentage of imperfectly predicted subsets.

Notes
-----
In binary classification, this function is equal to the `jaccard_score`
function.

Examples
--------
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2.0

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:AdjustedMutualInfoScoreMethod a owl:Class ;
    rdfs:comment """Adjusted Mutual Information between two clusterings.

Adjusted Mutual Information (AMI) is an adjustment of the Mutual
Information (MI) score to account for chance. It accounts for the fact that
the MI is generally higher for two clusterings with a larger number of
clusters, regardless of whether there is actually more information shared.
For two clusterings :math:`U` and :math:`V`, the AMI is given as::

    AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching :math:`U` (``label_true``)
with :math:`V` (``labels_pred``) will return the same score value. This can
be useful to measure the agreement of two independent label assignments
strategies on the same dataset when the real ground truth is not known.

Be mindful that this function is an order of magnitude slower than other
metrics, such as the Adjusted Rand Index.

Read more in the :ref:`User Guide <mutual_info_score>`.

Parameters
----------
labels_true : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets, called :math:`U` in
    the above formula.

labels_pred : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets, called :math:`V` in
    the above formula.

average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'
    How to compute the normalizer in the denominator.

    .. versionadded:: 0.20

    .. versionchanged:: 0.22
       The default value of ``average_method`` changed from 'max' to
       'arithmetic'.

Returns
-------
ami: float (upperlimited by 1.0)
   The AMI returns a value of 1 when the two partitions are identical
   (ie perfectly matched). Random partitions (independent labellings) have
   an expected AMI around 0 on average hence can be negative. The value is
   in adjusted nats (based on the natural logarithm).

See Also
--------
adjusted_rand_score : Adjusted Rand Index.
mutual_info_score : Mutual Information (not adjusted for chance).

References
----------
.. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
   Clusterings Comparison: Variants, Properties, Normalization and
   Correction for Chance, JMLR
   <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_

.. [2] `Wikipedia entry for the Adjusted Mutual Information
   <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_

Examples
--------

Perfect labelings are both homogeneous and complete, hence have
score 1.0::

  >>> from sklearn.metrics.cluster import adjusted_mutual_info_score
  >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
  ... # doctest: +SKIP
  1.0
  >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
  ... # doctest: +SKIP
  1.0

If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the AMI is null::

  >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
  ... # doctest: +SKIP
  0.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:BalancedAccuracyScoreMethod a owl:Class ;
    rdfs:comment """Compute the balanced accuracy.

The balanced accuracy in binary and multiclass classification problems to
deal with imbalanced datasets. It is defined as the average of recall
obtained on each class.

The best value is 1 and the worst value is 0 when ``adjusted=False``.

Read more in the :ref:`User Guide <balanced_accuracy_score>`.

.. versionadded:: 0.20

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

adjusted : bool, default=False
    When true, the result is adjusted for chance, so that random
    performance would score 0, while keeping perfect performance at a score
    of 1.

Returns
-------
balanced_accuracy : float
    Balanced accuracy score.

See Also
--------
average_precision_score : Compute average precision (AP) from prediction
    scores.
precision_score : Compute the precision score.
recall_score : Compute the recall score.
roc_auc_score : Compute Area Under the Receiver Operating Characteristic
    Curve (ROC AUC) from prediction scores.

Notes
-----
Some literature promotes alternative definitions of balanced accuracy. Our
definition is equivalent to :func:`accuracy_score` with class-balanced
sample weights, and shares desirable properties with the binary case.
See the :ref:`User Guide <balanced_accuracy_score>`.

References
----------
.. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).
       The balanced accuracy and its posterior distribution.
       Proceedings of the 20th International Conference on Pattern
       Recognition, 3121-24.
.. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).
       `Fundamentals of Machine Learning for Predictive Data Analytics:
       Algorithms, Worked Examples, and Case Studies
       <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.

Examples
--------
>>> from sklearn.metrics import balanced_accuracy_score
>>> y_true = [0, 1, 0, 0, 1, 0]
>>> y_pred = [0, 1, 0, 0, 0, 1]
>>> balanced_accuracy_score(y_true, y_pred)
0.625""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:BrierScoreLossMethod a owl:Class ;
    rdfs:comment """Compute the Brier score loss.

The smaller the Brier score loss, the better, hence the naming with "loss".
The Brier score measures the mean squared difference between the predicted
probability and the actual outcome. The Brier score always
takes on a value between zero and one, since this is the largest
possible difference between a predicted probability (which must be
between zero and one) and the actual outcome (which can take on values
of only 0 and 1). It can be decomposed as the sum of refinement loss and
calibration loss.

The Brier score is appropriate for binary and categorical outcomes that
can be structured as true or false, but is inappropriate for ordinal
variables which can take on three or more values (this is because the
Brier score assumes that all possible outcomes are equivalently
"distant" from one another). Which label is considered to be the positive
label is controlled via the parameter `pos_label`, which defaults to
the greater label unless `y_true` is all 0 or all -1, in which case
`pos_label` defaults to 1.

Read more in the :ref:`User Guide <brier_score_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True targets.

y_prob : array-like of shape (n_samples,)
    Probabilities of the positive class.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

pos_label : int, float, bool or str, default=None
    Label of the positive class. `pos_label` will be inferred in the
    following manner:

    * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;
    * else if `y_true` contains string, an error will be raised and
      `pos_label` should be explicitly specified;
    * otherwise, `pos_label` defaults to the greater label,
      i.e. `np.unique(y_true)[-1]`.

Returns
-------
score : float
    Brier score loss.

References
----------
.. [1] `Wikipedia entry for the Brier score
        <https://en.wikipedia.org/wiki/Brier_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import brier_score_loss
>>> y_true = np.array([0, 1, 1, 0])
>>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
>>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])
>>> brier_score_loss(y_true, y_prob)
0.037...
>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
0.037...
>>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
0.037...
>>> brier_score_loss(y_true, np.array(y_prob) > 0.5)
0.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:ClassLikelihoodRatiosMethod a owl:Class ;
    rdfs:comment """Compute binary classification positive and negative likelihood ratios.

The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`
where the sensitivity or recall is the ratio `tp / (tp + fn)` and the
specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1
- sensitivity) / specificity`. Here `tp` is the number of true positives,
`fp` the number of false positives, `tn` is the number of true negatives and
`fn` the number of false negatives. Both class likelihood ratios can be used
to obtain post-test probabilities given a pre-test probability.

`LR+` ranges from 1 to infinity. A `LR+` of 1 indicates that the probability
of predicting the positive class is the same for samples belonging to either
class; therefore, the test is useless. The greater `LR+` is, the more a
positive prediction is likely to be a true positive when compared with the
pre-test probability. A value of `LR+` lower than 1 is invalid as it would
indicate that the odds of a sample being a true positive decrease with
respect to the pre-test odds.

`LR-` ranges from 0 to 1. The closer it is to 0, the lower the probability
of a given sample to be a false negative. A `LR-` of 1 means the test is
useless because the odds of having the condition did not change after the
test. A value of `LR-` greater than 1 invalidates the classifier as it
indicates an increase in the odds of a sample belonging to the positive
class after being classified as negative. This is the case when the
classifier systematically predicts the opposite of the true label.

A typical application in medicine is to identify the positive/negative class
to the presence/absence of a disease, respectively; the classifier being a
diagnostic test; the pre-test probability of an individual having the
disease can be the prevalence of such disease (proportion of a particular
population found to be affected by a medical condition); and the post-test
probabilities would be the probability that the condition is truly present
given a positive test result.

Read more in the :ref:`User Guide <class_likelihood_ratios>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    List of labels to index the matrix. This may be used to select the
    positive and negative classes with the ordering `labels=[negative_class,
    positive_class]`. If `None` is given, those that appear at least once in
    `y_true` or `y_pred` are used in sorted order.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

raise_warning : bool, default=True
    Whether or not a case-specific warning message is raised when there is a
    zero division. Even if the error is not raised, the function will return
    nan in such cases.

Returns
-------
(positive_likelihood_ratio, negative_likelihood_ratio) : tuple
    A tuple of two float, the first containing the Positive likelihood ratio
    and the second the Negative likelihood ratio.

Warns
-----
When `false positive == 0`, the positive likelihood ratio is undefined.
When `true negative == 0`, the negative likelihood ratio is undefined.
When `true positive + false negative == 0` both ratios are undefined.
In such cases, `UserWarning` will be raised if raise_warning=True.

References
----------
.. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing
       <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import class_likelihood_ratios
>>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])
(1.5, 0.75)
>>> y_true = np.array(["non-cat", "cat", "non-cat", "cat", "non-cat"])
>>> y_pred = np.array(["cat", "cat", "non-cat", "non-cat", "non-cat"])
>>> class_likelihood_ratios(y_true, y_pred)
(1.33..., 0.66...)
>>> y_true = np.array(["non-zebra", "zebra", "non-zebra", "zebra", "non-zebra"])
>>> y_pred = np.array(["zebra", "zebra", "non-zebra", "non-zebra", "non-zebra"])
>>> class_likelihood_ratios(y_true, y_pred)
(1.5, 0.75)

To avoid ambiguities, use the notation `labels=[negative_class,
positive_class]`

>>> y_true = np.array(["non-cat", "cat", "non-cat", "cat", "non-cat"])
>>> y_pred = np.array(["cat", "cat", "non-cat", "non-cat", "non-cat"])
>>> class_likelihood_ratios(y_true, y_pred, labels=["non-cat", "cat"])
(1.5, 0.75)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:CohenKappaScoreMethod a owl:Class ;
    rdfs:comment """Compute Cohen's kappa: a statistic that measures inter-annotator agreement.

This function computes Cohen's kappa [1]_, a score that expresses the level
of agreement between two annotators on a classification problem. It is
defined as

.. math::
    \\kappa = (p_o - p_e) / (1 - p_e)

where :math:`p_o` is the empirical probability of agreement on the label
assigned to any sample (the observed agreement ratio), and :math:`p_e` is
the expected agreement when both annotators assign labels randomly.
:math:`p_e` is estimated using a per-annotator empirical prior over the
class labels [2]_.

Read more in the :ref:`User Guide <cohen_kappa>`.

Parameters
----------
y1 : array-like of shape (n_samples,)
    Labels assigned by the first annotator.

y2 : array-like of shape (n_samples,)
    Labels assigned by the second annotator. The kappa statistic is
    symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.

labels : array-like of shape (n_classes,), default=None
    List of labels to index the matrix. This may be used to select a
    subset of labels. If `None`, all labels that appear at least once in
    ``y1`` or ``y2`` are used.

weights : {'linear', 'quadratic'}, default=None
    Weighting type to calculate the score. `None` means no weighted;
    "linear" means linear weighted; "quadratic" means quadratic weighted.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
kappa : float
    The kappa statistic, which is a number between -1 and 1. The maximum
    value means complete agreement; zero or lower means chance agreement.

References
----------
.. [1] :doi:`J. Cohen (1960). "A coefficient of agreement for nominal scales".
       Educational and Psychological Measurement 20(1):37-46.
       <10.1177/001316446002000104>`
.. [2] `R. Artstein and M. Poesio (2008). "Inter-coder agreement for
       computational linguistics". Computational Linguistics 34(4):555-596
       <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.
.. [3] `Wikipedia entry for the Cohen's kappa
        <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.

Examples
--------
>>> from sklearn.metrics import cohen_kappa_score
>>> y1 = ["negative", "positive", "negative", "neutral", "positive"]
>>> y2 = ["negative", "positive", "negative", "neutral", "negative"]
>>> cohen_kappa_score(y1, y2)
0.6875""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:ConfusionMatrixMethod a owl:Class ;
    rdfs:comment """Compute confusion matrix to evaluate the accuracy of a classification.

By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`
is equal to the number of observations known to be in group :math:`i` and
predicted to be in group :math:`j`.

Thus in binary classification, the count of true negatives is
:math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is
:math:`C_{1,1}` and false positives is :math:`C_{0,1}`.

Read more in the :ref:`User Guide <confusion_matrix>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

labels : array-like of shape (n_classes), default=None
    List of labels to index the matrix. This may be used to reorder
    or select a subset of labels.
    If ``None`` is given, those that appear at least once
    in ``y_true`` or ``y_pred`` are used in sorted order.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

normalize : {'true', 'pred', 'all'}, default=None
    Normalizes confusion matrix over the true (rows), predicted (columns)
    conditions or all the population. If None, confusion matrix will not be
    normalized.

Returns
-------
C : ndarray of shape (n_classes, n_classes)
    Confusion matrix whose i-th row and j-th
    column entry indicates the number of
    samples with true label being i-th class
    and predicted label being j-th class.

See Also
--------
ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix
    given an estimator, the data, and the label.
ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix
    given the true and predicted labels.
ConfusionMatrixDisplay : Confusion Matrix visualization.

References
----------
.. [1] `Wikipedia entry for the Confusion matrix
       <https://en.wikipedia.org/wiki/Confusion_matrix>`_
       (Wikipedia and other references may use a different
       convention for axes).

Examples
--------
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

>>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
>>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> confusion_matrix(y_true, y_pred, labels=["ant", "bird", "cat"])
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

In the binary case, we can extract true positives, etc. as follows:

>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()
>>> (tn, fp, fn, tp)
(0, 2, 1, 1)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:D2AbsoluteErrorScoreMethod a owl:Class ;
    rdfs:comment """:math:`D^2` regression score function, fraction of absolute error explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical median of `y_true`
as constant prediction, disregarding the input features,
gets a :math:`D^2` score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.1

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

Returns
-------
score : float or ndarray of floats
    The :math:`D^2` score with an absolute error deviance
    or ndarray of scores if 'multioutput' is 'raw_values'.

Notes
-----
Like :math:`R^2`, :math:`D^2` score may be negative
(it need not actually be the square of a quantity D).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

 References
----------
.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. "Statistical Learning with Sparsity: The Lasso and
       Generalizations." (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_absolute_error_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> d2_absolute_error_score(y_true, y_pred)
0.764...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')
0.691...
>>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')
array([0.8125    , 0.57142857])
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> d2_absolute_error_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> d2_absolute_error_score(y_true, y_pred)
0.0
>>> y_true = [1, 2, 3]
>>> y_pred = [3, 2, 1]
>>> d2_absolute_error_score(y_true, y_pred)
-1.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:D2TweedieScoreMethod a owl:Class ;
    rdfs:comment """:math:`D^2` regression score function, fraction of Tweedie deviance explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical mean of `y_true` as
constant prediction, disregarding the input features, gets a D^2 score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.0

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

power : float, default=0
    Tweedie power parameter. Either power <= 0 or power >= 1.

    The higher `p` the less weight is given to extreme
    deviations between true and predicted targets.

    - power < 0: Extreme stable distribution. Requires: y_pred > 0.
    - power = 0 : Normal distribution, output corresponds to r2_score.
      y_true and y_pred can be any real numbers.
    - power = 1 : Poisson distribution. Requires: y_true >= 0 and
      y_pred > 0.
    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0
      and y_pred > 0.
    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.
    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0
      and y_pred > 0.
    - otherwise : Positive stable distribution. Requires: y_true > 0
      and y_pred > 0.

Returns
-------
z : float or ndarray of floats
    The D^2 score.

Notes
-----
This is not a symmetric function.

Like R^2, D^2 score may be negative (it need not actually be the square of
a quantity D).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

References
----------
.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. "Statistical Learning with Sparsity: The Lasso and
       Generalizations." (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_tweedie_score
>>> y_true = [0.5, 1, 2.5, 7]
>>> y_pred = [1, 1, 5, 3.5]
>>> d2_tweedie_score(y_true, y_pred)
0.285...
>>> d2_tweedie_score(y_true, y_pred, power=1)
0.487...
>>> d2_tweedie_score(y_true, y_pred, power=2)
0.630...
>>> d2_tweedie_score(y_true, y_true, power=2)
1.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:DetCurveMethod a owl:Class ;
    rdfs:comment """Compute error rates for different probability thresholds.

.. note::
   This metric is used for evaluation of ranking and error tradeoffs of
   a binary classification task.

Read more in the :ref:`User Guide <det_curve>`.

.. versionadded:: 0.24

Parameters
----------
y_true : ndarray of shape (n_samples,)
    True binary labels. If labels are not either {-1, 1} or {0, 1}, then
    pos_label should be explicitly given.

y_score : ndarray of shape of (n_samples,)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by "decision_function" on some classifiers).

pos_label : int, float, bool or str, default=None
    The label of the positive class.
    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},
    ``pos_label`` is set to 1, otherwise an error will be raised.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
fpr : ndarray of shape (n_thresholds,)
    False positive rate (FPR) such that element i is the false positive
    rate of predictions with score >= thresholds[i]. This is occasionally
    referred to as false acceptance probability or fall-out.

fnr : ndarray of shape (n_thresholds,)
    False negative rate (FNR) such that element i is the false negative
    rate of predictions with score >= thresholds[i]. This is occasionally
    referred to as false rejection or miss rate.

thresholds : ndarray of shape (n_thresholds,)
    Decreasing score values.

See Also
--------
DetCurveDisplay.from_estimator : Plot DET curve given an estimator and
    some data.
DetCurveDisplay.from_predictions : Plot DET curve given the true and
    predicted labels.
DetCurveDisplay : DET curve visualization.
roc_curve : Compute Receiver operating characteristic (ROC) curve.
precision_recall_curve : Compute precision-recall curve.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import det_curve
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, fnr, thresholds = det_curve(y_true, y_scores)
>>> fpr
array([0.5, 0.5, 0. ])
>>> fnr
array([0. , 0.5, 0.5])
>>> thresholds
array([0.35, 0.4 , 0.8 ])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:EuclideanDistancesMethod a owl:Class ;
    rdfs:comment """Compute the distance matrix between each pair from a vector array X and Y.

For efficiency reasons, the euclidean distance between a pair of row
vector x and y is computed as::

    dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))

This formulation has two advantages over other ways of computing distances.
First, it is computationally efficient when dealing with sparse data.
Second, if one argument varies but the other remains unchanged, then
`dot(x, x)` and/or `dot(y, y)` can be pre-computed.

However, this is not the most precise way of doing this computation,
because this equation potentially suffers from "catastrophic cancellation".
Also, the distance matrix returned by this function may not be exactly
symmetric as required by, e.g., ``scipy.spatial.distance`` functions.

Read more in the :ref:`User Guide <metrics>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
    An array where each row is a sample and each column is a feature.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None
    An array where each row is a sample and each column is a feature.
    If `None`, method uses `Y=X`.

Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None
    Pre-computed dot-products of vectors in Y (e.g.,
    ``(Y**2).sum(axis=1)``)
    May be ignored in some cases, see the note below.

squared : bool, default=False
    Return squared Euclidean distances.

X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None
    Pre-computed dot-products of vectors in X (e.g.,
    ``(X**2).sum(axis=1)``)
    May be ignored in some cases, see the note below.

Returns
-------
distances : ndarray of shape (n_samples_X, n_samples_Y)
    Returns the distances between the row vectors of `X`
    and the row vectors of `Y`.

See Also
--------
paired_distances : Distances between pairs of elements of X and Y.

Notes
-----
To achieve a better accuracy, `X_norm_squared` and `Y_norm_squared` may be
unused if they are passed as `np.float32`.

Examples
--------
>>> from sklearn.metrics.pairwise import euclidean_distances
>>> X = [[0, 1], [1, 1]]
>>> # distance between rows of X
>>> euclidean_distances(X, X)
array([[0., 1.],
       [1., 0.]])
>>> # get distance to origin
>>> euclidean_distances(X, [[0, 0]])
array([[1.        ],
       [1.41421356]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:FowlkesMallowsScoreMethod a owl:Class ;
    rdfs:comment """Measure the similarity of two clusterings of a set of points.

.. versionadded:: 0.18

The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of
the precision and recall::

    FMI = TP / sqrt((TP + FP) * (TP + FN))

Where ``TP`` is the number of **True Positive** (i.e. the number of pair of
points that belongs in the same clusters in both ``labels_true`` and
``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the
number of pair of points that belongs in the same clusters in
``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of
**False Negative** (i.e. the number of pair of points that belongs in the
same clusters in ``labels_pred`` and not in ``labels_True``).

The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.

Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=int
    A clustering of the data into disjoint subsets.

labels_pred : array-like of shape (n_samples,), dtype=int
    A clustering of the data into disjoint subsets.

sparse : bool, default=False
    Compute contingency matrix internally with sparse matrix.

Returns
-------
score : float
   The resulting Fowlkes-Mallows score.

References
----------
.. [1] `E. B. Fowkles and C. L. Mallows, 1983. "A method for comparing two
   hierarchical clusterings". Journal of the American Statistical
   Association
   <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_

.. [2] `Wikipedia entry for the Fowlkes-Mallows Index
       <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_

Examples
--------

Perfect labelings are both homogeneous and complete, hence have
score 1.0::

  >>> from sklearn.metrics.cluster import fowlkes_mallows_score
  >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
  1.0
  >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

If classes members are completely split across different clusters,
the assignment is totally random, hence the FMI is null::

  >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])
  0.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:GetScorerMethod a owl:Class ;
    rdfs:comment """Get a scorer from string.

Read more in the :ref:`User Guide <scoring_parameter>`.
:func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names
of all available scorers.

Parameters
----------
scoring : str, callable or None
    Scoring method as string. If callable it is returned as is.
    If None, returns None.

Returns
-------
scorer : callable
    The scorer.

Notes
-----
When passed a string, this function always returns a copy of the scorer
object. Calling `get_scorer` twice for the same scorer results in two
separate scorer objects.

Examples
--------
>>> import numpy as np
>>> from sklearn.dummy import DummyClassifier
>>> from sklearn.metrics import get_scorer
>>> X = np.reshape([0, 1, -1, -0.5, 2], (-1, 1))
>>> y = np.array([0, 1, 1, 0, 1])
>>> classifier = DummyClassifier(strategy="constant", constant=0).fit(X, y)
>>> accuracy = get_scorer("accuracy")
>>> accuracy(classifier, X, y)
0.4""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:HingeLossMethod a owl:Class ;
    rdfs:comment """Average hinge loss (non-regularized).

In binary class case, assuming labels in y_true are encoded with +1 and -1,
when a prediction mistake is made, ``margin = y_true * pred_decision`` is
always negative (since the signs disagree), implying ``1 - margin`` is
always greater than 1.  The cumulated hinge loss is therefore an upper
bound of the number of mistakes made by the classifier.

In multiclass case, the function expects that either all the labels are
included in y_true or an optional labels argument is provided which
contains all the labels. The multilabel margin is calculated according
to Crammer-Singer's method. As in the binary case, the cumulated hinge loss
is an upper bound of the number of mistakes made by the classifier.

Read more in the :ref:`User Guide <hinge_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True target, consisting of integers of two values. The positive label
    must be greater than the negative label.

pred_decision : array-like of shape (n_samples,) or (n_samples, n_classes)
    Predicted decisions, as output by decision_function (floats).

labels : array-like, default=None
    Contains all the labels for the problem. Used in multiclass hinge loss.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    Average hinge loss.

References
----------
.. [1] `Wikipedia entry on the Hinge loss
       <https://en.wikipedia.org/wiki/Hinge_loss>`_.

.. [2] Koby Crammer, Yoram Singer. On the Algorithmic
       Implementation of Multiclass Kernel-based Vector
       Machines. Journal of Machine Learning Research 2,
       (2001), 265-292.

.. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models
       by Robert C. Moore, John DeNero
       <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.

Examples
--------
>>> from sklearn import svm
>>> from sklearn.metrics import hinge_loss
>>> X = [[0], [1]]
>>> y = [-1, 1]
>>> est = svm.LinearSVC(dual="auto", random_state=0)
>>> est.fit(X, y)
LinearSVC(dual='auto', random_state=0)
>>> pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision
array([-2.18...,  2.36...,  0.09...])
>>> hinge_loss([-1, 1, 1], pred_decision)
0.30...

In the multiclass case:

>>> import numpy as np
>>> X = np.array([[0], [1], [2], [3]])
>>> Y = np.array([0, 1, 2, 3])
>>> labels = np.array([0, 1, 2, 3])
>>> est = svm.LinearSVC(dual="auto")
>>> est.fit(X, Y)
LinearSVC(dual='auto')
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels=labels)
0.56...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:HomogeneityCompletenessVMeasureMethod a owl:Class ;
    rdfs:comment """Compute the homogeneity and completeness and V-Measure scores at once.

Those metrics are based on normalized conditional entropy measures of
the clustering labeling to evaluate given the knowledge of a Ground
Truth class labels of the same samples.

A clustering result satisfies homogeneity if all of its clusters
contain only data points which are members of a single class.

A clustering result satisfies completeness if all the data points
that are members of a given class are elements of the same cluster.

Both scores have positive values between 0.0 and 1.0, larger values
being desirable.

Those 3 metrics are independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score values in any way.

V-Measure is furthermore symmetric: swapping ``labels_true`` and
``label_pred`` will give the same score. This does not hold for
homogeneity and completeness. V-Measure is identical to
:func:`normalized_mutual_info_score` with the arithmetic averaging
method.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Gluster labels to evaluate.

beta : float, default=1.0
    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
    If ``beta`` is greater than 1, ``completeness`` is weighted more
    strongly in the calculation. If ``beta`` is less than 1,
    ``homogeneity`` is weighted more strongly.

Returns
-------
homogeneity : float
    Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.

completeness : float
    Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.

v_measure : float
    Harmonic mean of the first two.

See Also
--------
homogeneity_score : Homogeneity metric of cluster labeling.
completeness_score : Completeness metric of cluster labeling.
v_measure_score : V-Measure (NMI with arithmetic mean option).

Examples
--------
>>> from sklearn.metrics import homogeneity_completeness_v_measure
>>> y_true, y_pred = [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 2, 2]
>>> homogeneity_completeness_v_measure(y_true, y_pred)
(0.71..., 0.77..., 0.73...)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanAbsoluteErrorMethod a owl:Class ;
    rdfs:comment """Mean absolute error regression loss.

Read more in the :ref:`User Guide <mean_absolute_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    MAE output is non-negative floating point. The best value is 0.0.

Examples
--------
>>> from sklearn.metrics import mean_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanAbsolutePercentageErrorMethod a owl:Class ;
    rdfs:comment """Mean absolute percentage error (MAPE) regression loss.

Note here that the output is not a percentage in the range [0, 100]
and a value of 100 does not mean 100% but 1e2. Furthermore, the output
can be arbitrarily high when `y_true` is small (which is specific to the
metric) or when `abs(y_true - y_pred)` is large (which is common for most
regression metrics). Read more in the
:ref:`User Guide <mean_absolute_percentage_error>`.

.. versionadded:: 0.24

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.
    If input is list then the shape must be (n_outputs,).

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute percentage error
    is returned for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    MAPE output is non-negative floating point. The best value is 0.0.
    But note that bad predictions can lead to arbitrarily large
    MAPE values, especially if some `y_true` values are very close to zero.
    Note that we return a large value instead of `inf` when `y_true` is zero.

Examples
--------
>>> from sklearn.metrics import mean_absolute_percentage_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.3273...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.5515...
>>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.6198...
>>> # the value when some element of the y_true is zero is arbitrarily high because
>>> # of the division by epsilon
>>> y_true = [1., 0., 2.4, 7.]
>>> y_pred = [1.2, 0.1, 2.4, 8.]
>>> mean_absolute_percentage_error(y_true, y_pred)
112589990684262.48""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanTweedieDevianceMethod a owl:Class ;
    rdfs:comment """Mean Tweedie deviance regression loss.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

power : float, default=0
    Tweedie power parameter. Either power <= 0 or power >= 1.

    The higher `p` the less weight is given to extreme
    deviations between true and predicted targets.

    - power < 0: Extreme stable distribution. Requires: y_pred > 0.
    - power = 0 : Normal distribution, output corresponds to
      mean_squared_error. y_true and y_pred can be any real numbers.
    - power = 1 : Poisson distribution. Requires: y_true >= 0 and
      y_pred > 0.
    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0
      and y_pred > 0.
    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.
    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0
      and y_pred > 0.
    - otherwise : Positive stable distribution. Requires: y_true > 0
      and y_pred > 0.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_tweedie_deviance
>>> y_true = [2, 0, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_tweedie_deviance(y_true, y_pred, power=1)
1.4260...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MedianAbsoluteErrorMethod a owl:Class ;
    rdfs:comment """Median absolute error regression loss.

Median absolute error output is non-negative floating point. The best value
is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values. Array-like value defines
    weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.24

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

Examples
--------
>>> from sklearn.metrics import median_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> median_absolute_error(y_true, y_pred)
0.75
>>> median_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MultilabelConfusionMatrixMethod a owl:Class ;
    rdfs:comment """Compute a confusion matrix for each class or sample.

.. versionadded:: 0.21

Compute class-wise (default) or sample-wise (samplewise=True) multilabel
confusion matrix to evaluate the accuracy of a classification, and output
confusion matrices for each class or sample.

In multilabel confusion matrix :math:`MCM`, the count of true negatives
is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,
true positives is :math:`MCM_{:,1,1}` and false positives is
:math:`MCM_{:,0,1}`.

Multiclass data will be treated as if binarized under a one-vs-rest
transformation. Returned confusion matrices will be in the order of
sorted unique labels in the union of (y_true, y_pred).

Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)
    Ground truth (correct) target values.

y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like of shape (n_classes,), default=None
    A list of classes or column indices to select some (or to force
    inclusion of classes absent from the data).

samplewise : bool, default=False
    In the multilabel case, this calculates a confusion matrix per sample.

Returns
-------
multi_confusion : ndarray of shape (n_outputs, 2, 2)
    A 2x2 confusion matrix corresponding to each output in the input.
    When calculating class-wise multi_confusion (default), then
    n_outputs = n_labels; when calculating sample-wise multi_confusion
    (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,
    the results will be returned in the order specified in ``labels``,
    otherwise the results will be returned in sorted order by default.

See Also
--------
confusion_matrix : Compute confusion matrix to evaluate the accuracy of a
    classifier.

Notes
-----
The `multilabel_confusion_matrix` calculates class-wise or sample-wise
multilabel confusion matrices, and in multiclass tasks, labels are
binarized under a one-vs-rest way; while
:func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix
for confusion between every two classes.

Examples
--------
Multilabel-indicator case:

>>> import numpy as np
>>> from sklearn.metrics import multilabel_confusion_matrix
>>> y_true = np.array([[1, 0, 1],
...                    [0, 1, 0]])
>>> y_pred = np.array([[1, 0, 0],
...                    [0, 1, 1]])
>>> multilabel_confusion_matrix(y_true, y_pred)
array([[[1, 0],
        [0, 1]],
<BLANKLINE>
       [[1, 0],
        [0, 1]],
<BLANKLINE>
       [[0, 1],
        [1, 0]]])

Multiclass case:

>>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
>>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> multilabel_confusion_matrix(y_true, y_pred,
...                             labels=["ant", "bird", "cat"])
array([[[3, 1],
        [0, 2]],
<BLANKLINE>
       [[5, 0],
        [1, 0]],
<BLANKLINE>
       [[2, 1],
        [1, 2]]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MutualInfoScoreMethod a owl:Class ;
    rdfs:comment """Mutual Information between two clusterings.

The Mutual Information is a measure of the similarity between two labels
of the same data. Where :math:`|U_i|` is the number of the samples
in cluster :math:`U_i` and :math:`|V_j|` is the number of the
samples in cluster :math:`V_j`, the Mutual Information
between clusterings :math:`U` and :math:`V` is given as:

.. math::

    MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}
    \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching :math:`U` (i.e
``label_true``) with :math:`V` (i.e. ``label_pred``) will return the
same score value. This can be useful to measure the agreement of two
independent label assignments strategies on the same dataset when the
real ground truth is not known.

Read more in the :ref:`User Guide <mutual_info_score>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=integral
    A clustering of the data into disjoint subsets, called :math:`U` in
    the above formula.

labels_pred : array-like of shape (n_samples,), dtype=integral
    A clustering of the data into disjoint subsets, called :math:`V` in
    the above formula.

contingency : {array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None
    A contingency matrix given by the
    :func:`~sklearn.metrics.cluster.contingency_matrix` function. If value
    is ``None``, it will be computed, otherwise the given value is used,
    with ``labels_true`` and ``labels_pred`` ignored.

Returns
-------
mi : float
   Mutual information, a non-negative value, measured in nats using the
   natural logarithm.

See Also
--------
adjusted_mutual_info_score : Adjusted against chance Mutual Information.
normalized_mutual_info_score : Normalized Mutual Information.

Notes
-----
The logarithm used is the natural logarithm (base-e).

Examples
--------
>>> from sklearn.metrics import mutual_info_score
>>> labels_true = [0, 1, 1, 0, 1, 0]
>>> labels_pred = [0, 1, 0, 0, 1, 1]
>>> mutual_info_score(labels_true, labels_pred)
0.056...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:NormalizedMutualInfoScoreMethod a owl:Class ;
    rdfs:comment """Normalized Mutual Information between two clusterings.

Normalized Mutual Information (NMI) is a normalization of the Mutual
Information (MI) score to scale the results between 0 (no mutual
information) and 1 (perfect correlation). In this function, mutual
information is normalized by some generalized mean of ``H(labels_true)``
and ``H(labels_pred))``, defined by the `average_method`.

This measure is not adjusted for chance. Therefore
:func:`adjusted_mutual_info_score` might be preferred.

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching ``label_true`` with
``label_pred`` will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.

Read more in the :ref:`User Guide <mutual_info_score>`.

Parameters
----------
labels_true : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets.

labels_pred : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets.

average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'
    How to compute the normalizer in the denominator.

    .. versionadded:: 0.20

    .. versionchanged:: 0.22
       The default value of ``average_method`` changed from 'geometric' to
       'arithmetic'.

Returns
-------
nmi : float
   Score between 0.0 and 1.0 in normalized nats (based on the natural
   logarithm). 1.0 stands for perfectly complete labeling.

See Also
--------
v_measure_score : V-Measure (NMI with arithmetic mean option).
adjusted_rand_score : Adjusted Rand Index.
adjusted_mutual_info_score : Adjusted Mutual Information (adjusted
    against chance).

Examples
--------

Perfect labelings are both homogeneous and complete, hence have
score 1.0::

  >>> from sklearn.metrics.cluster import normalized_mutual_info_score
  >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
  ... # doctest: +SKIP
  1.0
  >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
  ... # doctest: +SKIP
  1.0

If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the NMI is null::

  >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
  ... # doctest: +SKIP
  0.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:RootMeanSquaredErrorMethod a owl:Class ;
    rdfs:comment """Root mean squared error regression loss.

Read more in the :ref:`User Guide <mean_squared_error>`.

.. versionadded:: 1.4

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import root_mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> root_mean_squared_error(y_true, y_pred)
0.612...
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> root_mean_squared_error(y_true, y_pred)
0.822...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:RootMeanSquaredLogErrorMethod a owl:Class ;
    rdfs:comment """Root mean squared logarithmic error regression loss.

Read more in the :ref:`User Guide <mean_squared_log_error>`.

.. versionadded:: 1.4

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'

    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors when the input is of multioutput
        format.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import root_mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> root_mean_squared_log_error(y_true, y_pred)
0.199...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:SilhouetteSamplesMethod a owl:Class ;
    rdfs:comment """Compute the Silhouette Coefficient for each sample.

The Silhouette Coefficient is a measure of how well samples are clustered
with samples that are similar to themselves. Clustering models with a high
Silhouette Coefficient are said to be dense, where samples in the same
cluster are similar to each other, and well separated, where samples in
different clusters are not very similar to each other.

The Silhouette Coefficient is calculated using the mean intra-cluster
distance (``a``) and the mean nearest-cluster distance (``b``) for each
sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
b)``.
Note that Silhouette Coefficient is only defined if number of labels
is 2 ``<= n_labels <= n_samples - 1``.

This function returns the Silhouette Coefficient for each sample.

The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters.

Read more in the :ref:`User Guide <silhouette_coefficient>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             "precomputed" or (n_samples_a, n_features) otherwise
    An array of pairwise distances between samples, or a feature array. If
    a sparse matrix is provided, CSR format should be favoured avoiding
    an additional copy.

labels : array-like of shape (n_samples,)
    Label values for each sample.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by :func:`~sklearn.metrics.pairwise_distances`.
    If ``X`` is the distance array itself, use "precomputed" as the metric.
    Precomputed distance matrices must have 0 along the diagonal.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a ``scipy.spatial.distance`` metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Returns
-------
silhouette : array-like of shape (n_samples,)
    Silhouette Coefficients for each sample.

References
----------

.. [1] `Peter J. Rousseeuw (1987). "Silhouettes: a Graphical Aid to the
   Interpretation and Validation of Cluster Analysis". Computational
   and Applied Mathematics 20: 53-65.
   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_

.. [2] `Wikipedia entry on the Silhouette Coefficient
   <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_

Examples
--------
>>> from sklearn.metrics import silhouette_samples
>>> from sklearn.datasets import make_blobs
>>> from sklearn.cluster import KMeans
>>> X, y = make_blobs(n_samples=50, random_state=42)
>>> kmeans = KMeans(n_clusters=3, random_state=42)
>>> labels = kmeans.fit_predict(X)
>>> silhouette_samples(X, labels)
array([...])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:VMeasureScoreMethod a owl:Class ;
    rdfs:comment """V-measure cluster labeling given a ground truth.

This score is identical to :func:`normalized_mutual_info_score` with
the ``'arithmetic'`` option for averaging.

The V-measure is the harmonic mean between homogeneity and completeness::

    v = (1 + beta) * homogeneity * completeness
         / (beta * homogeneity + completeness)

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching ``label_true`` with
``label_pred`` will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Cluster labels to evaluate.

beta : float, default=1.0
    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
    If ``beta`` is greater than 1, ``completeness`` is weighted more
    strongly in the calculation. If ``beta`` is less than 1,
    ``homogeneity`` is weighted more strongly.

Returns
-------
v_measure : float
   Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.

See Also
--------
homogeneity_score : Homogeneity metric of cluster labeling.
completeness_score : Completeness metric of cluster labeling.
normalized_mutual_info_score : Normalized Mutual Information.

References
----------

.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
   conditional entropy-based external cluster evaluation measure
   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_

Examples
--------
Perfect labelings are both homogeneous and complete, hence have score 1.0::

  >>> from sklearn.metrics.cluster import v_measure_score
  >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])
  1.0
  >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Labelings that assign all classes members to the same clusters
are complete but not homogeneous, hence penalized::

  >>> print("%.6f" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))
  0.8...
  >>> print("%.6f" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))
  0.66...

Labelings that have pure clusters with members coming from the same
classes are homogeneous but un-necessary splits harm completeness
and thus penalize V-measure as well::

  >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))
  0.8...
  >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))
  0.66...

If classes members are completely split across different clusters,
the assignment is totally incomplete, hence the V-Measure is null::

  >>> print("%.6f" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))
  0.0...

Clusters that include samples from totally different classes totally
destroy the homogeneity of the labeling, hence::

  >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))
  0.0...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:ZeroOneLossMethod a owl:Class ;
    rdfs:comment """Zero-one classification loss.

If normalize is ``True``, return the fraction of misclassifications
(float), else it returns the number of misclassifications (int). The best
performance is 0.

Read more in the :ref:`User Guide <zero_one_loss>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

normalize : bool, default=True
    If ``False``, return the number of misclassifications.
    Otherwise, return the fraction of misclassifications.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float or int,
    If ``normalize == True``, return the fraction of misclassifications
    (float), else it returns the number of misclassifications (int).

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.
hamming_loss : Compute the average Hamming loss or Hamming distance between
    two sets of samples.
jaccard_score : Compute the Jaccard similarity coefficient score.

Notes
-----
In multilabel classification, the zero_one_loss function corresponds to
the subset zero-one loss: for each sample, the entire set of labels must be
correctly predicted, otherwise the loss for that sample is equal to one.

Examples
--------
>>> from sklearn.metrics import zero_one_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> zero_one_loss(y_true, y_pred)
0.25
>>> zero_one_loss(y_true, y_pred, normalize=False)
1.0

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:AveragePrecisionScoreMethod a owl:Class ;
    rdfs:comment """Compute average precision (AP) from prediction scores.

AP summarizes a precision-recall curve as the weighted mean of precisions
achieved at each threshold, with the increase in recall from the previous
threshold used as the weight:

.. math::
    \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n

where :math:`P_n` and :math:`R_n` are the precision and recall at the nth
threshold [1]_. This implementation is not interpolated and is different
from computing the area under the precision-recall curve with the
trapezoidal rule, which uses linear interpolation and can be too
optimistic.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
    True binary labels or binary label indicators.

y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by :term:`decision_function` on some classifiers).

average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'
    If ``None``, the scores for each class are returned. Otherwise,
    this determines the type of averaging performed on the data:

    ``'micro'``:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    ``'samples'``:
        Calculate metrics for each instance, and find their average.

    Will be ignored when ``y_true`` is binary.

pos_label : int, float, bool or str, default=1
    The label of the positive class. Only applied to binary ``y_true``.
    For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
average_precision : float
    Average precision score.

See Also
--------
roc_auc_score : Compute the area under the ROC curve.
precision_recall_curve : Compute precision-recall pairs for different
    probability thresholds.

Notes
-----
.. versionchanged:: 0.19
  Instead of linearly interpolating between operating points, precisions
  are weighted by the change in recall since the last operating point.

References
----------
.. [1] `Wikipedia entry for the Average precision
       <https://en.wikipedia.org/w/index.php?title=Information_retrieval&
       oldid=793358396#Average_precision>`_

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import average_precision_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> average_precision_score(y_true, y_scores)
0.83...
>>> y_true = np.array([0, 0, 1, 1, 2, 2])
>>> y_scores = np.array([
...     [0.7, 0.2, 0.1],
...     [0.4, 0.3, 0.3],
...     [0.1, 0.8, 0.1],
...     [0.2, 0.3, 0.5],
...     [0.4, 0.4, 0.2],
...     [0.1, 0.2, 0.7],
... ])
>>> average_precision_score(y_true, y_scores)
0.77...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:CheckScoringMethod a owl:Class ;
    rdfs:comment """Determine scorer from user options.

A TypeError will be thrown if the estimator cannot be scored.

Parameters
----------
estimator : estimator object implementing 'fit'
    The object to use to fit the data.

scoring : str or callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.
    If None, the provided estimator object's `score` method is used.

allow_none : bool, default=False
    If no scoring is specified and the estimator has no score function, we
    can either return None or raise an exception.

Returns
-------
scoring : callable
    A scorer callable object / function with signature
    ``scorer(estimator, X, y)``.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.metrics import check_scoring
>>> from sklearn.tree import DecisionTreeClassifier
>>> X, y = load_iris(return_X_y=True)
>>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)
>>> scorer = check_scoring(classifier, scoring='accuracy')
>>> scorer(classifier, X, y)
0.96...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:D2PinballScoreMethod a owl:Class ;
    rdfs:comment """:math:`D^2` regression score function, fraction of pinball loss explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical alpha-quantile of
`y_true` as constant prediction, disregarding the input features,
gets a :math:`D^2` score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.1

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

alpha : float, default=0.5
    Slope of the pinball deviance. It determines the quantile level alpha
    for which the pinball deviance and also D2 are optimal.
    The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

Returns
-------
score : float or ndarray of floats
    The :math:`D^2` score with a pinball deviance
    or ndarray of scores if `multioutput='raw_values'`.

Notes
-----
Like :math:`R^2`, :math:`D^2` score may be negative
(it need not actually be the square of a quantity D).

This metric is not well-defined for a single point and will return a NaN
value if n_samples is less than two.

 References
----------
.. [1] Eq. (7) of `Koenker, Roger; Machado, José A. F. (1999).
       "Goodness of Fit and Related Inference Processes for Quantile Regression"
       <https://doi.org/10.1080/01621459.1999.10473882>`_
.. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. "Statistical Learning with Sparsity: The Lasso and
       Generalizations." (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_pinball_score
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 3, 3]
>>> d2_pinball_score(y_true, y_pred)
0.5
>>> d2_pinball_score(y_true, y_pred, alpha=0.9)
0.772...
>>> d2_pinball_score(y_true, y_pred, alpha=0.1)
-1.045...
>>> d2_pinball_score(y_true, y_true, alpha=0.1)
1.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:ExplainedVarianceScoreMethod a owl:Class ;
    rdfs:comment """Explained variance regression score function.

Best possible score is 1.0, lower values are worse.

In the particular case when ``y_true`` is constant, the explained variance
score is not finite: it is either ``NaN`` (perfect predictions) or
``-Inf`` (imperfect predictions). To prevent such non-finite numbers to
pollute higher-level experiments such as a grid search cross-validation,
by default these cases are replaced with 1.0 (perfect predictions) or 0.0
(imperfect predictions) respectively. If ``force_finite``
is set to ``False``, this score falls back on the original :math:`R^2`
definition.

.. note::
   The Explained Variance score is similar to the
   :func:`R^2 score <r2_score>`, with the notable difference that it
   does not account for systematic offsets in the prediction. Most often
   the :func:`R^2 score <r2_score>` should be preferred.

Read more in the :ref:`User Guide <explained_variance_score>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of scores in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.

force_finite : bool, default=True
    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant
    data should be replaced with real numbers (``1.0`` if prediction is
    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting
    for hyperparameters' search procedures (e.g. grid search
    cross-validation).

    .. versionadded:: 1.1

Returns
-------
score : float or ndarray of floats
    The explained variance or ndarray if 'multioutput' is 'raw_values'.

See Also
--------
r2_score :
    Similar metric, but accounting for systematic offsets in
    prediction.

Notes
-----
This is not a symmetric function.

Examples
--------
>>> from sklearn.metrics import explained_variance_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)
0.957...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')
0.983...
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> explained_variance_score(y_true, y_pred)
1.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> explained_variance_score(y_true, y_pred)
0.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
-inf""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:LogLossMethod a owl:Class ;
    rdfs:comment """Log loss, aka logistic loss or cross-entropy loss.

This is the loss function used in (multinomial) logistic regression
and extensions of it such as neural networks, defined as the negative
log-likelihood of a logistic model that returns ``y_pred`` probabilities
for its training data ``y_true``.
The log loss is only defined for two or more labels.
For a single sample with true label :math:`y \\in \\{0,1\\}` and
a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log
loss is:

.. math::
    L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))

Read more in the :ref:`User Guide <log_loss>`.

Parameters
----------
y_true : array-like or label indicator matrix
    Ground truth (correct) labels for n_samples samples.

y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)
    Predicted probabilities, as returned by a classifier's
    predict_proba method. If ``y_pred.shape = (n_samples,)``
    the probabilities provided are assumed to be that of the
    positive class. The labels in ``y_pred`` are assumed to be
    ordered alphabetically, as done by
    :class:`~sklearn.preprocessing.LabelBinarizer`.

eps : float or "auto", default="auto"
    Log loss is undefined for p=0 or p=1, so probabilities are
    clipped to `max(eps, min(1 - eps, p))`. The default will depend on the
    data type of `y_pred` and is set to `np.finfo(y_pred.dtype).eps`.

    .. versionadded:: 1.2

    .. versionchanged:: 1.2
       The default value changed from `1e-15` to `"auto"` that is
       equivalent to `np.finfo(y_pred.dtype).eps`.

    .. deprecated:: 1.3
       `eps` is deprecated in 1.3 and will be removed in 1.5.

normalize : bool, default=True
    If true, return the mean loss per sample.
    Otherwise, return the sum of the per-sample losses.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like, default=None
    If not provided, labels will be inferred from y_true. If ``labels``
    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are
    assumed to be binary and are inferred from ``y_true``.

    .. versionadded:: 0.18

Returns
-------
loss : float
    Log loss, aka logistic loss or cross-entropy loss.

Notes
-----
The logarithm used is the natural logarithm (base-e).

References
----------
C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,
p. 209.

Examples
--------
>>> from sklearn.metrics import log_loss
>>> log_loss(["spam", "ham", "ham", "spam"],
...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])
0.21616...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanPinballLossMethod a owl:Class ;
    rdfs:comment """Pinball loss for quantile regression.

Read more in the :ref:`User Guide <pinball_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

alpha : float, slope of the pinball loss, default=0.5,
    This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,
    `alpha=0.95` is minimized by estimators of the 95th percentile.

multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    The pinball loss output is a non-negative floating point. The best
    value is 0.0.

Examples
--------
>>> from sklearn.metrics import mean_pinball_loss
>>> y_true = [1, 2, 3]
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)
0.03...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)
0.3...
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)
0.3...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)
0.03...
>>> mean_pinball_loss(y_true, y_true, alpha=0.1)
0.0
>>> mean_pinball_loss(y_true, y_true, alpha=0.9)
0.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanSquaredErrorMethod a owl:Class ;
    rdfs:comment """Mean squared error regression loss.

Read more in the :ref:`User Guide <mean_squared_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

squared : bool, default=True
    If True returns MSE value, if False returns RMSE value.

    .. deprecated:: 1.4
       `squared` is deprecated in 1.4 and will be removed in 1.6.
       Use :func:`~sklearn.metrics.root_mean_squared_error`
       instead to calculate the root mean squared error.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> mean_squared_error(y_true, y_pred)
0.708...
>>> mean_squared_error(y_true, y_pred, multioutput='raw_values')
array([0.41666667, 1.        ])
>>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.825...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MeanSquaredLogErrorMethod a owl:Class ;
    rdfs:comment """Mean squared logarithmic error regression loss.

Read more in the :ref:`User Guide <mean_squared_log_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'

    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors when the input is of multioutput
        format.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

squared : bool, default=True
    If True returns MSLE (mean squared log error) value.
    If False returns RMSLE (root mean squared log error) value.

    .. deprecated:: 1.4
       `squared` is deprecated in 1.4 and will be removed in 1.6.
       Use :func:`~sklearn.metrics.root_mean_squared_log_error`
       instead to calculate the root mean squared logarithmic error.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> mean_squared_log_error(y_true, y_pred)
0.039...
>>> y_true = [[0.5, 1], [1, 2], [7, 6]]
>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]
>>> mean_squared_log_error(y_true, y_pred)
0.044...
>>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')
array([0.00462428, 0.08377444])
>>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.060...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:NdcgScoreMethod a owl:Class ;
    rdfs:comment """Compute Normalized Discounted Cumulative Gain.

Sum the true scores ranked in the order induced by the predicted scores,
after applying a logarithmic discount. Then divide by the best possible
score (Ideal DCG, obtained for a perfect ranking) to obtain a score between
0 and 1.

This ranking metric returns a high value if true labels are ranked high by
``y_score``.

Parameters
----------
y_true : array-like of shape (n_samples, n_labels)
    True targets of multilabel classification, or true scores of entities
    to be ranked. Negative values in `y_true` may result in an output
    that is not between 0 and 1.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates, confidence values,
    or non-thresholded measure of decisions (as returned by
    "decision_function" on some classifiers).

k : int, default=None
    Only consider the highest k scores in the ranking. If `None`, use all
    outputs.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If `None`, all samples are given the same weight.

ignore_ties : bool, default=False
    Assume that there are no ties in y_score (which is likely to be the
    case if y_score is continuous) for efficiency gains.

Returns
-------
normalized_discounted_cumulative_gain : float in [0., 1.]
    The averaged NDCG scores for all samples.

See Also
--------
dcg_score : Discounted Cumulative Gain (not normalized).

References
----------
`Wikipedia entry for Discounted Cumulative Gain
<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_

Jarvelin, K., & Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.

Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013)

McSherry, F., & Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import ndcg_score
>>> # we have groud-truth relevance of some answers to a query:
>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])
>>> # we predict some scores (relevance) for the answers
>>> scores = np.asarray([[.1, .2, .3, 4, 70]])
>>> ndcg_score(true_relevance, scores)
0.69...
>>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])
>>> ndcg_score(true_relevance, scores)
0.49...
>>> # we can set k to truncate the sum; only top k answers contribute.
>>> ndcg_score(true_relevance, scores, k=4)
0.35...
>>> # the normalization takes k into account so a perfect answer
>>> # would still get 1.0
>>> ndcg_score(true_relevance, true_relevance, k=4)
1.0...
>>> # now we have some ties in our prediction
>>> scores = np.asarray([[1, 0, 0, 0, 1]])
>>> # by default ties are averaged, so here we get the average (normalized)
>>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75
>>> ndcg_score(true_relevance, scores, k=1)
0.75...
>>> # we can choose to ignore ties for faster results, but only
>>> # if we know there aren't ties in our scores, otherwise we get
>>> # wrong results:
>>> ndcg_score(true_relevance,
...           scores, k=1, ignore_ties=True)
0.5...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PrecisionRecallCurveMethod a owl:Class ;
    rdfs:comment """Compute precision-recall pairs for different probability thresholds.

Note: this implementation is restricted to the binary classification task.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The last precision and recall values are 1. and 0. respectively and do not
have a corresponding threshold. This ensures that the graph starts on the
y axis.

The first precision and recall values are precision=class balance and recall=1.0
which corresponds to a classifier that always predicts the positive class.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True binary labels. If labels are not either {-1, 1} or {0, 1}, then
    pos_label should be explicitly given.

probas_pred : array-like of shape (n_samples,)
    Target scores, can either be probability estimates of the positive
    class, or non-thresholded measure of decisions (as returned by
    `decision_function` on some classifiers).

pos_label : int, float, bool or str, default=None
    The label of the positive class.
    When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},
    ``pos_label`` is set to 1, otherwise an error will be raised.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

drop_intermediate : bool, default=False
    Whether to drop some suboptimal thresholds which would not appear
    on a plotted precision-recall curve. This is useful in order to create
    lighter precision-recall curves.

    .. versionadded:: 1.3

Returns
-------
precision : ndarray of shape (n_thresholds + 1,)
    Precision values such that element i is the precision of
    predictions with score >= thresholds[i] and the last element is 1.

recall : ndarray of shape (n_thresholds + 1,)
    Decreasing recall values such that element i is the recall of
    predictions with score >= thresholds[i] and the last element is 0.

thresholds : ndarray of shape (n_thresholds,)
    Increasing thresholds on the decision function used to compute
    precision and recall where `n_thresholds = len(np.unique(probas_pred))`.

See Also
--------
PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given
    a binary classifier.
PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve
    using predictions from a binary classifier.
average_precision_score : Compute average precision from prediction scores.
det_curve: Compute error rates for different probability thresholds.
roc_curve : Compute Receiver operating characteristic (ROC) curve.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_curve
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> precision, recall, thresholds = precision_recall_curve(
...     y_true, y_scores)
>>> precision
array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])
>>> recall
array([1. , 1. , 0.5, 0.5, 0. ])
>>> thresholds
array([0.1 , 0.35, 0.4 , 0.8 ])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:R2ScoreMethod a owl:Class ;
    rdfs:comment """:math:`R^2` (coefficient of determination) regression score function.

Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). In the general case when the true y is
non-constant, a constant model that always predicts the average y
disregarding the input features would get a :math:`R^2` score of 0.0.

In the particular case when ``y_true`` is constant, the :math:`R^2` score
is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``
(imperfect predictions). To prevent such non-finite numbers to pollute
higher-level experiments such as a grid search cross-validation, by default
these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect
predictions) respectively. You can set ``force_finite`` to ``False`` to
prevent this fix from happening.

Note: when the prediction residuals have zero mean, the :math:`R^2` score
is identical to the
:func:`Explained Variance score <explained_variance_score>`.

Read more in the :ref:`User Guide <r2_score>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'

    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.
    Default is "uniform_average".

    'raw_values' :
        Returns a full set of scores in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.

    .. versionchanged:: 0.19
        Default value of multioutput is 'uniform_average'.

force_finite : bool, default=True
    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant
    data should be replaced with real numbers (``1.0`` if prediction is
    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting
    for hyperparameters' search procedures (e.g. grid search
    cross-validation).

    .. versionadded:: 1.1

Returns
-------
z : float or ndarray of floats
    The :math:`R^2` score or ndarray of scores if 'multioutput' is
    'raw_values'.

Notes
-----
This is not a symmetric function.

Unlike most other scores, :math:`R^2` score may be negative (it need not
actually be the square of a quantity R).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

References
----------
.. [1] `Wikipedia entry on the Coefficient of determination
        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_

Examples
--------
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred,
...          multioutput='variance_weighted')
0.938...
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> r2_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> r2_score(y_true, y_pred)
0.0
>>> y_true = [1, 2, 3]
>>> y_pred = [3, 2, 1]
>>> r2_score(y_true, y_pred)
-3.0
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> r2_score(y_true, y_pred)
1.0
>>> r2_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> r2_score(y_true, y_pred)
0.0
>>> r2_score(y_true, y_pred, force_finite=False)
-inf""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:RocAucScoreMethod a owl:Class ;
    rdfs:comment """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.

Note: this implementation can be used with binary, multiclass and
multilabel classification, but some restrictions apply (see Parameters).

Read more in the :ref:`User Guide <roc_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
    True labels or binary label indicators. The binary and multiclass cases
    expect labels with shape (n_samples,) while the multilabel case expects
    binary label indicators with shape (n_samples, n_classes).

y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
    Target scores.

    * In the binary case, it corresponds to an array of shape
      `(n_samples,)`. Both probability estimates and non-thresholded
      decision values can be provided. The probability estimates correspond
      to the **probability of the class with the greater label**,
      i.e. `estimator.classes_[1]` and thus
      `estimator.predict_proba(X, y)[:, 1]`. The decision values
      corresponds to the output of `estimator.decision_function(X, y)`.
      See more information in the :ref:`User guide <roc_auc_binary>`;
    * In the multiclass case, it corresponds to an array of shape
      `(n_samples, n_classes)` of probability estimates provided by the
      `predict_proba` method. The probability estimates **must**
      sum to 1 across the possible classes. In addition, the order of the
      class scores must correspond to the order of ``labels``,
      if provided, or else to the numerical or lexicographical order of
      the labels in ``y_true``. See more information in the
      :ref:`User guide <roc_auc_multiclass>`;
    * In the multilabel case, it corresponds to an array of shape
      `(n_samples, n_classes)`. Probability estimates are provided by the
      `predict_proba` method and the non-thresholded decision values by
      the `decision_function` method. The probability estimates correspond
      to the **probability of the class with the greater label for each
      output** of the classifier. See more information in the
      :ref:`User guide <roc_auc_multilabel>`.

average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'
    If ``None``, the scores for each class are returned.
    Otherwise, this determines the type of averaging performed on the data.
    Note: multiclass ROC AUC currently only handles the 'macro' and
    'weighted' averages. For multiclass targets, `average=None` is only
    implemented for `multi_class='ovr'` and `average='micro'` is only
    implemented for `multi_class='ovr'`.

    ``'micro'``:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    ``'samples'``:
        Calculate metrics for each instance, and find their average.

    Will be ignored when ``y_true`` is binary.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

max_fpr : float > 0 and <= 1, default=None
    If not ``None``, the standardized partial AUC [2]_ over the range
    [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
    should be either equal to ``None`` or ``1.0`` as AUC ROC partial
    computation currently is not supported for multiclass.

multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
    Only used for multiclass targets. Determines the type of configuration
    to use. The default value raises an error, so either
    ``'ovr'`` or ``'ovo'`` must be passed explicitly.

    ``'ovr'``:
        Stands for One-vs-rest. Computes the AUC of each class
        against the rest [3]_ [4]_. This
        treats the multiclass case in the same way as the multilabel case.
        Sensitive to class imbalance even when ``average == 'macro'``,
        because class imbalance affects the composition of each of the
        'rest' groupings.
    ``'ovo'``:
        Stands for One-vs-one. Computes the average AUC of all
        possible pairwise combinations of classes [5]_.
        Insensitive to class imbalance when
        ``average == 'macro'``.

labels : array-like of shape (n_classes,), default=None
    Only used for multiclass targets. List of labels that index the
    classes in ``y_score``. If ``None``, the numerical or lexicographical
    order of the labels in ``y_true`` is used.

Returns
-------
auc : float
    Area Under the Curve score.

See Also
--------
average_precision_score : Area under the precision-recall curve.
roc_curve : Compute Receiver operating characteristic (ROC) curve.
RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
    (ROC) curve given an estimator and some data.
RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
    (ROC) curve given the true and predicted values.

Notes
-----
The Gini Coefficient is a summary measure of the ranking ability of binary
classifiers. It is expressed using the area under of the ROC as follows:

G = 2 * AUC - 1

Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
will ensure that random guessing will yield a score of 0 in expectation, and it is
upper bounded by 1.

References
----------
.. [1] `Wikipedia entry for the Receiver operating characteristic
        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_

.. [2] `Analyzing a portion of the ROC curve. McClish, 1989
        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_

.. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
       probability estimation trees (Section 6.2), CeDER Working Paper
       #IS-00-04, Stern School of Business, New York University.

.. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
        Recognition Letters, 27(8), 861-874.
        <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_

.. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
        Under the ROC Curve for Multiple Class Classification Problems.
        Machine Learning, 45(2), 171-186.
        <http://link.springer.com/article/10.1023/A:1010920819831>`_
.. [6] `Wikipedia entry for the Gini coefficient
        <https://en.wikipedia.org/wiki/Gini_coefficient>`_

Examples
--------
Binary case:

>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.metrics import roc_auc_score
>>> X, y = load_breast_cancer(return_X_y=True)
>>> clf = LogisticRegression(solver="liblinear", random_state=0).fit(X, y)
>>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
0.99...
>>> roc_auc_score(y, clf.decision_function(X))
0.99...

Multiclass case:

>>> from sklearn.datasets import load_iris
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(solver="liblinear").fit(X, y)
>>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
0.99...

Multilabel case:

>>> import numpy as np
>>> from sklearn.datasets import make_multilabel_classification
>>> from sklearn.multioutput import MultiOutputClassifier
>>> X, y = make_multilabel_classification(random_state=0)
>>> clf = MultiOutputClassifier(clf).fit(X, y)
>>> # get a list of n_output containing probability arrays of shape
>>> # (n_samples, n_classes)
>>> y_pred = clf.predict_proba(X)
>>> # extract the positive columns for each output
>>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
>>> roc_auc_score(y, y_pred, average=None)
array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
>>> from sklearn.linear_model import RidgeClassifierCV
>>> clf = RidgeClassifierCV().fit(X, y)
>>> roc_auc_score(y, clf.decision_function(X), average=None)
array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:RocCurveMethod a owl:Class ;
    rdfs:comment """Compute Receiver operating characteristic (ROC).

Note: this implementation is restricted to the binary classification task.

Read more in the :ref:`User Guide <roc_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True binary labels. If labels are not either {-1, 1} or {0, 1}, then
    pos_label should be explicitly given.

y_score : array-like of shape (n_samples,)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by "decision_function" on some classifiers).

pos_label : int, float, bool or str, default=None
    The label of the positive class.
    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},
    ``pos_label`` is set to 1, otherwise an error will be raised.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

drop_intermediate : bool, default=True
    Whether to drop some suboptimal thresholds which would not appear
    on a plotted ROC curve. This is useful in order to create lighter
    ROC curves.

    .. versionadded:: 0.17
       parameter *drop_intermediate*.

Returns
-------
fpr : ndarray of shape (>2,)
    Increasing false positive rates such that element i is the false
    positive rate of predictions with score >= `thresholds[i]`.

tpr : ndarray of shape (>2,)
    Increasing true positive rates such that element `i` is the true
    positive rate of predictions with score >= `thresholds[i]`.

thresholds : ndarray of shape (n_thresholds,)
    Decreasing thresholds on the decision function used to compute
    fpr and tpr. `thresholds[0]` represents no instances being predicted
    and is arbitrarily set to `np.inf`.

See Also
--------
RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
    (ROC) curve given an estimator and some data.
RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
    (ROC) curve given the true and predicted values.
det_curve: Compute error rates for different probability thresholds.
roc_auc_score : Compute the area under the ROC curve.

Notes
-----
Since the thresholds are sorted from low to high values, they
are reversed upon returning them to ensure they correspond to both ``fpr``
and ``tpr``, which are sorted in reversed order during their calculation.

An arbitrary threshold is added for the case `tpr=0` and `fpr=0` to
ensure that the curve starts at `(0, 0)`. This threshold corresponds to the
`np.inf`.

References
----------
.. [1] `Wikipedia entry for the Receiver operating characteristic
        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_

.. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition
       Letters, 2006, 27(8):861-874.

Examples
--------
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
>>> fpr
array([0. , 0. , 0.5, 0.5, 1. ])
>>> tpr
array([0. , 0.5, 0.5, 1. , 1. ])
>>> thresholds
array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:TopKAccuracyScoreMethod a owl:Class ;
    rdfs:comment """Top-k Accuracy classification score.

This metric computes the number of times where the correct label is among
the top `k` labels predicted (ranked by predicted scores). Note that the
multilabel case isn't covered here.

Read more in the :ref:`User Guide <top_k_accuracy_score>`

Parameters
----------
y_true : array-like of shape (n_samples,)
    True labels.

y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
    Target scores. These can be either probability estimates or
    non-thresholded decision values (as returned by
    :term:`decision_function` on some classifiers).
    The binary case expects scores with shape (n_samples,) while the
    multiclass case expects scores with shape (n_samples, n_classes).
    In the multiclass case, the order of the class scores must
    correspond to the order of ``labels``, if provided, or else to
    the numerical or lexicographical order of the labels in ``y_true``.
    If ``y_true`` does not contain all the labels, ``labels`` must be
    provided.

k : int, default=2
    Number of most likely outcomes considered to find the correct label.

normalize : bool, default=True
    If `True`, return the fraction of correctly classified samples.
    Otherwise, return the number of correctly classified samples.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If `None`, all samples are given the same weight.

labels : array-like of shape (n_classes,), default=None
    Multiclass only. List of labels that index the classes in ``y_score``.
    If ``None``, the numerical or lexicographical order of the labels in
    ``y_true`` is used. If ``y_true`` does not contain all the labels,
    ``labels`` must be provided.

Returns
-------
score : float
    The top-k accuracy score. The best performance is 1 with
    `normalize == True` and the number of samples with
    `normalize == False`.

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.

Notes
-----
In cases where two or more labels are assigned equal predicted scores,
the labels with the highest indices will be chosen first. This might
impact the result if the correct label falls after the threshold because
of that.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import top_k_accuracy_score
>>> y_true = np.array([0, 1, 2, 2])
>>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2
...                     [0.3, 0.4, 0.2],  # 1 is in top 2
...                     [0.2, 0.4, 0.3],  # 2 is in top 2
...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2
>>> top_k_accuracy_score(y_true, y_score, k=2)
0.75
>>> # Not normalizing gives the number of "correctly" classified samples
>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)
3""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:ClassificationReportMethod a owl:Class ;
    rdfs:comment """Build a text report showing the main classification metrics.

Read more in the :ref:`User Guide <classification_report>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like of shape (n_labels,), default=None
    Optional list of label indices to include in the report.

target_names : array-like of shape (n_labels,), default=None
    Optional display names matching the labels (same order).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

digits : int, default=2
    Number of digits for formatting output floating point values.
    When ``output_dict`` is ``True``, this will be ignored and the
    returned values will not be rounded.

output_dict : bool, default=False
    If True, return output as dict.

    .. versionadded:: 0.20

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division. If set to
    "warn", this acts as 0, but warnings are also raised.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
report : str or dict
    Text summary of the precision, recall, F1 score for each class.
    Dictionary returned if output_dict is True. Dictionary has the
    following structure::

        {'label 1': {'precision':0.5,
                     'recall':1.0,
                     'f1-score':0.67,
                     'support':1},
         'label 2': { ... },
          ...
        }

    The reported averages include macro average (averaging the unweighted
    mean per label), weighted average (averaging the support-weighted mean
    per label), and sample average (only for multilabel classification).
    Micro average (averaging the total true positives, false negatives and
    false positives) is only shown for multi-label or multi-class
    with a subset of classes, because it corresponds to accuracy
    otherwise and would be the same for all metrics.
    See also :func:`precision_recall_fscore_support` for more details
    on averages.

    Note that in binary classification, recall of the positive class
    is also known as "sensitivity"; recall of the negative class is
    "specificity".

See Also
--------
precision_recall_fscore_support: Compute precision, recall, F-measure and
    support for each class.
confusion_matrix: Compute confusion matrix to evaluate the accuracy of a
    classification.
multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.

Examples
--------
>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 2]
>>> y_pred = [0, 0, 2, 2, 1]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
              precision    recall  f1-score   support
<BLANKLINE>
     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3
<BLANKLINE>
    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
<BLANKLINE>
>>> y_pred = [1, 1, 0]
>>> y_true = [1, 1, 1]
>>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))
              precision    recall  f1-score   support
<BLANKLINE>
           1       1.00      0.67      0.80         3
           2       0.00      0.00      0.00         0
           3       0.00      0.00      0.00         0
<BLANKLINE>
   micro avg       1.00      0.67      0.80         3
   macro avg       0.33      0.22      0.27         3
weighted avg       1.00      0.67      0.80         3
<BLANKLINE>""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:ConsensusScoreMethod a owl:Class ;
    rdfs:comment """The similarity of two sets of biclusters.

Similarity between individual biclusters is computed. Then the
best matching between sets is found using the Hungarian algorithm.
The final score is the sum of similarities divided by the size of
the larger set.

Read more in the :ref:`User Guide <biclustering>`.

Parameters
----------
a : tuple (rows, columns)
    Tuple of row and column indicators for a set of biclusters.

b : tuple (rows, columns)
    Another set of biclusters like ``a``.

similarity : 'jaccard' or callable, default='jaccard'
    May be the string "jaccard" to use the Jaccard coefficient, or
    any function that takes four arguments, each of which is a 1d
    indicator vector: (a_rows, a_columns, b_rows, b_columns).

Returns
-------
consensus_score : float
   Consensus score, a non-negative value, sum of similarities
   divided by size of larger set.

References
----------

* Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis
  for bicluster acquisition
  <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.

Examples
--------
>>> from sklearn.metrics import consensus_score
>>> a = ([[True, False], [False, True]], [[False, True], [True, False]])
>>> b = ([[False, True], [True, False]], [[True, False], [False, True]])
>>> consensus_score(a, b, similarity='jaccard')
1.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:DcgScoreMethod a owl:Class ;
    rdfs:comment """Compute Discounted Cumulative Gain.

Sum the true scores ranked in the order induced by the predicted scores,
after applying a logarithmic discount.

This ranking metric yields a high value if true labels are ranked high by
``y_score``.

Usually the Normalized Discounted Cumulative Gain (NDCG, computed by
ndcg_score) is preferred.

Parameters
----------
y_true : array-like of shape (n_samples, n_labels)
    True targets of multilabel classification, or true scores of entities
    to be ranked.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates, confidence values,
    or non-thresholded measure of decisions (as returned by
    "decision_function" on some classifiers).

k : int, default=None
    Only consider the highest k scores in the ranking. If None, use all
    outputs.

log_base : float, default=2
    Base of the logarithm used for the discount. A low value means a
    sharper discount (top results are more important).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If `None`, all samples are given the same weight.

ignore_ties : bool, default=False
    Assume that there are no ties in y_score (which is likely to be the
    case if y_score is continuous) for efficiency gains.

Returns
-------
discounted_cumulative_gain : float
    The averaged sample DCG scores.

See Also
--------
ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted
    Cumulative Gain (the DCG obtained for a perfect ranking), in order to
    have a score between 0 and 1.

References
----------
`Wikipedia entry for Discounted Cumulative Gain
<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.

Jarvelin, K., & Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.

Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013).

McSherry, F., & Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import dcg_score
>>> # we have groud-truth relevance of some answers to a query:
>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])
>>> # we predict scores for the answers
>>> scores = np.asarray([[.1, .2, .3, 4, 70]])
>>> dcg_score(true_relevance, scores)
9.49...
>>> # we can set k to truncate the sum; only top k answers contribute
>>> dcg_score(true_relevance, scores, k=2)
5.63...
>>> # now we have some ties in our prediction
>>> scores = np.asarray([[1, 0, 0, 0, 1]])
>>> # by default ties are averaged, so here we get the average true
>>> # relevance of our top predictions: (10 + 5) / 2 = 7.5
>>> dcg_score(true_relevance, scores, k=1)
7.5
>>> # we can choose to ignore ties for faster results, but only
>>> # if we know there aren't ties in our scores, otherwise we get
>>> # wrong results:
>>> dcg_score(true_relevance,
...           scores, k=1, ignore_ties=True)
5.0""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:F1ScoreMethod a owl:Class ;
    rdfs:comment """Compute the F1 score, also known as balanced F-score or F-measure.

The F1 score can be interpreted as a harmonic mean of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:

.. math::
    \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}}

Where :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the
number of false negatives, and :math:`\\text{FP}` is the number of false positives.
F1 is by default
calculated as 0.0 when there are no true positives, false negatives, or
false positives.

Support beyond :term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and F1 score for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
F1 score for all `labels` are either returned or averaged depending on the
`average` parameter. Use `labels` specify the set of labels to calculate F1 score
for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division, i.e. when all
    predictions and labels are negative.

    Notes:
    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
f1_score : float or array of float, shape = [n_unique_labels]
    F1 score of the positive class in binary classification or weighted
    average of the F1 scores of each class for the multiclass task.

See Also
--------
fbeta_score : Compute the F-beta score.
precision_recall_fscore_support : Compute the precision, recall, F-score,
    and support.
jaccard_score : Compute the Jaccard similarity coefficient score.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive + false negative == 0`` (i.e. a class
is completely absent from both ``y_true`` or ``y_pred``), f-score is
undefined. In such cases, by default f-score will be set to 0.0, and
``UndefinedMetricWarning`` will be raised. This behavior can be modified by
setting the ``zero_division`` parameter.

References
----------
.. [1] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import f1_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> f1_score(y_true, y_pred, average='macro')
0.26...
>>> f1_score(y_true, y_pred, average='micro')
0.33...
>>> f1_score(y_true, y_pred, average='weighted')
0.26...
>>> f1_score(y_true, y_pred, average=None)
array([0.8, 0. , 0. ])

>>> # binary classification
>>> y_true_empty = [0, 0, 0, 0, 0, 0]
>>> y_pred_empty = [0, 0, 0, 0, 0, 0]
>>> f1_score(y_true_empty, y_pred_empty)
0.0...
>>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)
1.0...
>>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)
nan...

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> f1_score(y_true, y_pred, average=None)
array([0.66666667, 1.        , 0.66666667])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:JaccardScoreMethod a owl:Class ;
    rdfs:comment """Jaccard similarity coefficient score.

The Jaccard index [1], or Jaccard similarity coefficient, defined as
the size of the intersection divided by the size of the union of two label
sets, is used to compare set of predicted labels for a sample to the
corresponding set of labels in ``y_true``.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return the
Jaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`,
`pos_label` is ignored and scores for both classes are computed, then averaged or
both returned (when `average=None`). Similarly, for :term:`multiclass` and
:term:`multilabel` targets, scores for all `labels` are either returned or
averaged depending on the `average` parameter. Use `labels` specify the set of
labels to calculate the score for.

Read more in the :ref:`User Guide <jaccard_similarity_score>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

labels : array-like of shape (n_classes,), default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : "warn", {0.0, 1.0}, default="warn"
    Sets the value to return when there is a zero division, i.e. when there
    there are no negative values in predictions and labels. If set to
    "warn", this acts like 0, but a warning is also raised.

Returns
-------
score : float or ndarray of shape (n_unique_labels,), dtype=np.float64
    The Jaccard score. When `average` is not `None`, a single scalar is
    returned.

See Also
--------
accuracy_score : Function for calculating the accuracy score.
f1_score : Function for calculating the F1 score.
multilabel_confusion_matrix : Function for computing a confusion matrix                                  for each class or sample.

Notes
-----
:func:`jaccard_score` may be a poor metric if there are no
positives for some samples or classes. Jaccard is undefined if there are
no true or predicted labels, and our implementation will return a score
of 0 with a warning.

References
----------
.. [1] `Wikipedia entry for the Jaccard index
       <https://en.wikipedia.org/wiki/Jaccard_index>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import jaccard_score
>>> y_true = np.array([[0, 1, 1],
...                    [1, 1, 0]])
>>> y_pred = np.array([[1, 1, 1],
...                    [1, 0, 0]])

In the binary case:

>>> jaccard_score(y_true[0], y_pred[0])
0.6666...

In the 2D comparison case (e.g. image similarity):

>>> jaccard_score(y_true, y_pred, average="micro")
0.6

In the multilabel case:

>>> jaccard_score(y_true, y_pred, average='samples')
0.5833...
>>> jaccard_score(y_true, y_pred, average='macro')
0.6666...
>>> jaccard_score(y_true, y_pred, average=None)
array([0.5, 0.5, 1. ])

In the multiclass case:

>>> y_pred = [0, 2, 1, 2]
>>> y_true = [0, 1, 2, 2]
>>> jaccard_score(y_true, y_pred, average=None)
array([1. , 0. , 0.33...])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:NanEuclideanDistancesMethod a owl:Class ;
    rdfs:comment """Calculate the euclidean distances in the presence of missing values.

Compute the euclidean distance between each pair of samples in X and Y,
where Y=X is assumed if Y=None. When calculating the distance between a
pair of samples, this formulation ignores feature coordinates with a
missing value in either sample and scales up the weight of the remaining
coordinates:

    dist(x,y) = sqrt(weight * sq. distance from present coordinates)
    where,
    weight = Total # of coordinates / # of present coordinates

For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``
is:

    .. math::
        \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}

If all the coordinates are missing or if there are no common present
coordinates then NaN is returned for that pair.

Read more in the :ref:`User Guide <metrics>`.

.. versionadded:: 0.22

Parameters
----------
X : array-like of shape (n_samples_X, n_features)
    An array where each row is a sample and each column is a feature.

Y : array-like of shape (n_samples_Y, n_features), default=None
    An array where each row is a sample and each column is a feature.
    If `None`, method uses `Y=X`.

squared : bool, default=False
    Return squared Euclidean distances.

missing_values : np.nan, float or int, default=np.nan
    Representation of missing value.

copy : bool, default=True
    Make and use a deep copy of X and Y (if Y exists).

Returns
-------
distances : ndarray of shape (n_samples_X, n_samples_Y)
    Returns the distances between the row vectors of `X`
    and the row vectors of `Y`.

See Also
--------
paired_distances : Distances between pairs of elements of X and Y.

References
----------
* John K. Dixon, "Pattern Recognition with Partly Missing Data",
  IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:
  10, pp. 617 - 621, Oct. 1979.
  http://ieeexplore.ieee.org/abstract/document/4310090/

Examples
--------
>>> from sklearn.metrics.pairwise import nan_euclidean_distances
>>> nan = float("NaN")
>>> X = [[0, 1], [1, nan]]
>>> nan_euclidean_distances(X, X) # distance between rows of X
array([[0.        , 1.41421356],
       [1.41421356, 0.        ]])

>>> # get distance to origin
>>> nan_euclidean_distances(X, [[0, 0]])
array([[1.        ],
       [1.41421356]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PairwiseDistancesArgminMethod a owl:Class ;
    rdfs:comment """Compute minimum distances between one point and a set of points.

This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance).

This is mostly equivalent to calling:

    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)

but uses much less memory, and is faster for large arrays.

This function works with dense 2D arrays only.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
    Array containing points.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
    Arrays containing points.

axis : int, default=1
    Axis along which the argmin and distances are to be computed.

metric : str or callable, default="euclidean"
    Metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.

    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    Distance matrices are not supported.

    Valid values for metric are:

    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
      'manhattan']

    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
      'yule']

    See the documentation for scipy.spatial.distance for details on these
    metrics.

    .. note::
       `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.

    .. note::
       `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).

metric_kwargs : dict, default=None
    Keyword arguments to pass to specified metric function.

Returns
-------
argmin : numpy.ndarray
    Y[argmin[i], :] is the row in Y that is closest to X[i, :].

See Also
--------
pairwise_distances : Distances between every pair of samples of X and Y.
pairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also
    returns the distances.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_distances_argmin
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> pairwise_distances_argmin(X, Y)
array([0, 1])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PairwiseDistancesArgminMinMethod a owl:Class ;
    rdfs:comment """Compute minimum distances between one point and a set of points.

This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance). The minimal distances are
also returned.

This is mostly equivalent to calling:

    (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),
     pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))

but uses much less memory, and is faster for large arrays.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
    Array containing points.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
    Array containing points.

axis : int, default=1
    Axis along which the argmin and distances are to be computed.

metric : str or callable, default='euclidean'
    Metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.

    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    Distance matrices are not supported.

    Valid values for metric are:

    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
      'manhattan']

    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
      'yule']

    See the documentation for scipy.spatial.distance for details on these
    metrics.

    .. note::
       `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.

    .. note::
       `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).

metric_kwargs : dict, default=None
    Keyword arguments to pass to specified metric function.

Returns
-------
argmin : ndarray
    Y[argmin[i], :] is the row in Y that is closest to X[i, :].

distances : ndarray
    The array of minimum distances. `distances[i]` is the distance between
    the i-th row in X and the argmin[i]-th row in Y.

See Also
--------
pairwise_distances : Distances between every pair of samples of X and Y.
pairwise_distances_argmin : Same as `pairwise_distances_argmin_min` but only
    returns the argmins.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_distances_argmin_min
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> argmin, distances = pairwise_distances_argmin_min(X, Y)
>>> argmin
array([0, 1])
>>> distances
array([1., 1.])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PairwiseDistancesMethod a owl:Class ;
    rdfs:comment """Compute the distance matrix from a vector array X and optional Y.

This method takes either a vector array or a distance matrix, and returns
a distance matrix. If the input is a vector array, the distances are
computed. If the input is a distances matrix, it is returned instead.

This method provides a safe way to take a distance matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.

If Y is given (default is None), then the returned matrix is the pairwise
distance between the arrays from both X and Y.

Valid values for metric are:

- From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
  'manhattan']. These metrics support sparse matrix
  inputs.
  ['nan_euclidean'] but it does not yet support sparse matrices.

- From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',
  'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',
  'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']
  See the documentation for scipy.spatial.distance for details on these
  metrics. These metrics do not support sparse matrix inputs.

.. note::
    `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.

.. note::
    `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).

Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are
valid scipy.spatial.distance metrics), the scikit-learn implementation
will be used, which is faster and has support for sparse matrices (except
for 'cityblock'). For a verbose description of the metrics from
scikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`
function.

Read more in the :ref:`User Guide <metrics>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)
    Array of pairwise distances between samples, or a feature array.
    The shape of the array should be (n_samples_X, n_samples_X) if
    metric == "precomputed" and (n_samples_X, n_features) otherwise.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None
    An optional second feature array. Only allowed if
    metric != "precomputed".

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by scipy.spatial.distance.pdist for its metric parameter, or
    a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.
    If metric is "precomputed", X is assumed to be a distance matrix.
    Alternatively, if metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays from X as input and return a value indicating
    the distance between them.

n_jobs : int, default=None
    The number of jobs to use for the computation. This works by breaking
    down the pairwise matrix into n_jobs even slices and computing them in
    parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

force_all_finite : bool or 'allow-nan', default=True
    Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored
    for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The
    possibilities are:

    - True: Force all values of array to be finite.
    - False: accepts np.inf, np.nan, pd.NA in array.
    - 'allow-nan': accepts only np.nan and pd.NA values in array. Values
      cannot be infinite.

    .. versionadded:: 0.22
       ``force_all_finite`` accepts the string ``'allow-nan'``.

    .. versionchanged:: 0.23
       Accepts `pd.NA` and converts it into `np.nan`.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a scipy.spatial.distance metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Returns
-------
D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)
    A distance matrix D such that D_{i, j} is the distance between the
    ith and jth vectors of the given matrix X, if Y is None.
    If Y is not None, then D_{i, j} is the distance between the ith array
    from X and the jth array from Y.

See Also
--------
pairwise_distances_chunked : Performs the same calculation as this
    function, but returns a generator of chunks of the distance matrix, in
    order to limit memory usage.
sklearn.metrics.pairwise.paired_distances : Computes the distances between
    corresponding elements of two arrays.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_distances
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> pairwise_distances(X, Y, metric='sqeuclidean')
array([[1., 2.],
       [2., 1.]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PairwiseKernelsMethod a owl:Class ;
    rdfs:comment """Compute the kernel between arrays X and optional array Y.

This method takes either a vector array or a kernel matrix, and returns
a kernel matrix. If the input is a vector array, the kernels are
computed. If the input is a kernel matrix, it is returned instead.

This method provides a safe way to take a kernel matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.

If Y is given (default is None), then the returned matrix is the pairwise
kernel between the arrays from both X and Y.

Valid values for metric are:
    ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',
    'laplacian', 'sigmoid', 'cosine']

Read more in the :ref:`User Guide <metrics>`.

Parameters
----------
X : {array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)
    Array of pairwise kernels between samples, or a feature array.
    The shape of the array should be (n_samples_X, n_samples_X) if
    metric == "precomputed" and (n_samples_X, n_features) otherwise.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None
    A second feature array only if X has shape (n_samples_X, n_features).

metric : str or callable, default="linear"
    The metric to use when calculating kernel between instances in a
    feature array. If metric is a string, it must be one of the metrics
    in ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.
    If metric is "precomputed", X is assumed to be a kernel matrix.
    Alternatively, if metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two rows from X as input and return the corresponding
    kernel value as a single number. This means that callables from
    :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on
    matrices, not single samples. Use the string identifying the kernel
    instead.

filter_params : bool, default=False
    Whether to filter invalid parameters or not.

n_jobs : int, default=None
    The number of jobs to use for the computation. This works by breaking
    down the pairwise matrix into n_jobs even slices and computing them in
    parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the kernel function.

Returns
-------
K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)
    A kernel matrix K such that K_{i, j} is the kernel between the
    ith and jth vectors of the given matrix X, if Y is None.
    If Y is not None, then K_{i, j} is the kernel between the ith array
    from X and the jth array from Y.

Notes
-----
If metric is 'precomputed', Y is ignored and X is returned.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_kernels
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> pairwise_kernels(X, Y, metric='linear')
array([[0., 0.],
       [1., 2.]])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PrecisionScoreMethod a owl:Class ;
    rdfs:comment """Compute the precision.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.

The best value is 1 and the worst value is 0.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and precision for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
precision for all `labels` are either returned or averaged depending on the
`average` parameter. Use `labels` specify the set of labels to calculate precision
for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division.

    Notes:
    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)
    Precision of the positive class in binary classification or weighted
    average of the precision of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute precision, recall, F-measure and
    support for each class.
recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the
    number of true positives and ``fn`` the number of false negatives.
PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given
    an estimator and some data.
PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given
    binary class predictions.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive == 0``, precision returns 0 and
raises ``UndefinedMetricWarning``. This behavior can be
modified with ``zero_division``.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> precision_score(y_true, y_pred, average='macro')
0.22...
>>> precision_score(y_true, y_pred, average='micro')
0.33...
>>> precision_score(y_true, y_pred, average='weighted')
0.22...
>>> precision_score(y_true, y_pred, average=None)
array([0.66..., 0.        , 0.        ])
>>> y_pred = [0, 0, 0, 0, 0, 0]
>>> precision_score(y_true, y_pred, average=None)
array([0.33..., 0.        , 0.        ])
>>> precision_score(y_true, y_pred, average=None, zero_division=1)
array([0.33..., 1.        , 1.        ])
>>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)
array([0.33...,        nan,        nan])

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> precision_score(y_true, y_pred, average=None)
array([0.5, 1. , 1. ])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:RecallScoreMethod a owl:Class ;
    rdfs:comment """Compute the recall.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The best value is 1 and the worst value is 0.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
recall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and recall for both classes are computed then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
recall for all `labels` are either returned or averaged depending on the `average`
parameter. Use `labels` specify the set of labels to calculate recall for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall. Weighted recall
        is equal to accuracy.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division.

    Notes:
    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
recall : float (if average is not None) or array of float of shape              (n_unique_labels,)
    Recall of the positive class in binary classification or weighted
    average of the recall of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute precision, recall, F-measure and
    support for each class.
precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the
    number of true positives and ``fp`` the number of false positives.
balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced
    datasets.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.
PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given
    an estimator and some data.
PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given
    binary class predictions.

Notes
-----
When ``true positive + false negative == 0``, recall returns 0 and raises
``UndefinedMetricWarning``. This behavior can be modified with
``zero_division``.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import recall_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> recall_score(y_true, y_pred, average='macro')
0.33...
>>> recall_score(y_true, y_pred, average='micro')
0.33...
>>> recall_score(y_true, y_pred, average='weighted')
0.33...
>>> recall_score(y_true, y_pred, average=None)
array([1., 0., 0.])
>>> y_true = [0, 0, 0, 0, 0, 0]
>>> recall_score(y_true, y_pred, average=None)
array([0.5, 0. , 0. ])
>>> recall_score(y_true, y_pred, average=None, zero_division=1)
array([0.5, 1. , 1. ])
>>> recall_score(y_true, y_pred, average=None, zero_division=np.nan)
array([0.5, nan, nan])

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> recall_score(y_true, y_pred, average=None)
array([1. , 1. , 0.5])""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:SilhouetteScoreMethod a owl:Class ;
    rdfs:comment """Compute the mean Silhouette Coefficient of all samples.

The Silhouette Coefficient is calculated using the mean intra-cluster
distance (``a``) and the mean nearest-cluster distance (``b``) for each
sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
b)``.  To clarify, ``b`` is the distance between a sample and the nearest
cluster that the sample is not a part of.
Note that Silhouette Coefficient is only defined if number of labels
is ``2 <= n_labels <= n_samples - 1``.

This function returns the mean Silhouette Coefficient over all samples.
To obtain the values for each sample, use :func:`silhouette_samples`.

The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters. Negative values generally indicate that a sample has
been assigned to the wrong cluster, as a different cluster is more similar.

Read more in the :ref:`User Guide <silhouette_coefficient>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             "precomputed" or (n_samples_a, n_features) otherwise
    An array of pairwise distances between samples, or a feature array.

labels : array-like of shape (n_samples,)
    Predicted labels for each sample.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by :func:`~sklearn.metrics.pairwise_distances`. If ``X`` is
    the distance array itself, use ``metric="precomputed"``.

sample_size : int, default=None
    The size of the sample to use when computing the Silhouette Coefficient
    on a random subset of the data.
    If ``sample_size is None``, no sampling is used.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for selecting a subset of samples.
    Used when ``sample_size is not None``.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a scipy.spatial.distance metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Returns
-------
silhouette : float
    Mean Silhouette Coefficient for all samples.

References
----------

.. [1] `Peter J. Rousseeuw (1987). "Silhouettes: a Graphical Aid to the
   Interpretation and Validation of Cluster Analysis". Computational
   and Applied Mathematics 20: 53-65.
   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_

.. [2] `Wikipedia entry on the Silhouette Coefficient
       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_

Examples
--------
>>> from sklearn.datasets import make_blobs
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import silhouette_score
>>> X, y = make_blobs(random_state=42)
>>> kmeans = KMeans(n_clusters=2, random_state=42)
>>> silhouette_score(X, kmeans.fit_predict(X))
0.49...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:FbetaScoreMethod a owl:Class ;
    rdfs:comment """Compute the F-beta score.

The F-beta score is the weighted harmonic mean of precision and recall,
reaching its optimal value at 1 and its worst value at 0.

The `beta` parameter represents the ratio of recall importance to
precision importance. `beta > 1` gives more weight to recall, while
`beta < 1` favors precision. For example, `beta = 2` makes recall twice
as important as precision, while `beta = 0.5` does the opposite.
Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0`
only precision.

The formula for F-beta score is:

.. math::

   F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}
                    {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}

Where :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the
number of false positives, and :math:`\\text{fn}` is the number of false negatives.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is
ignored and F-beta score for both classes are computed, then averaged or both
returned (when `average=None`). Similarly, for :term:`multiclass` and
:term:`multilabel` targets, F-beta score for all `labels` are either returned or
averaged depending on the `average` parameter. Use `labels` specify the set of
labels to calculate F-beta score for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

beta : float
    Determines the weight of recall in the combined score.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division, i.e. when all
    predictions and labels are negative.

    Notes:
    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]
    F-beta score of the positive class in binary classification or weighted
    average of the F-beta score of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute the precision, recall, F-score,
    and support.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive + false negative == 0``, f-score
returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be
modified by setting ``zero_division``.

References
----------
.. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).
       Modern Information Retrieval. Addison Wesley, pp. 327-328.

.. [2] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import fbeta_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)
0.23...
>>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)
0.33...
>>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)
0.23...
>>> fbeta_score(y_true, y_pred, average=None, beta=0.5)
array([0.71..., 0.        , 0.        ])
>>> y_pred_empty = [0, 0, 0, 0, 0, 0]
>>> fbeta_score(y_true, y_pred_empty,
...             average="macro", zero_division=np.nan, beta=0.5)
0.12...""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PairwiseDistancesChunkedMethod a owl:Class ;
    rdfs:comment """Generate a distance matrix chunk by chunk with optional reduction.

In cases where not all of a pairwise distance matrix needs to be
stored at once, this is used to calculate pairwise distances in
``working_memory``-sized chunks.  If ``reduce_func`` is given, it is
run on each chunk and its return values are concatenated into lists,
arrays or sparse matrices.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)
    Array of pairwise distances between samples, or a feature array.
    The shape the array should be (n_samples_X, n_samples_X) if
    metric='precomputed' and (n_samples_X, n_features) otherwise.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None
    An optional second feature array. Only allowed if
    metric != "precomputed".

reduce_func : callable, default=None
    The function which is applied on each chunk of the distance matrix,
    reducing it to needed values.  ``reduce_func(D_chunk, start)``
    is called repeatedly, where ``D_chunk`` is a contiguous vertical
    slice of the pairwise distance matrix, starting at row ``start``.
    It should return one of: None; an array, a list, or a sparse matrix
    of length ``D_chunk.shape[0]``; or a tuple of such objects.
    Returning None is useful for in-place operations, rather than
    reductions.

    If None, pairwise_distances_chunked returns a generator of vertical
    chunks of the distance matrix.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by scipy.spatial.distance.pdist for its metric parameter,
    or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.
    If metric is "precomputed", X is assumed to be a distance matrix.
    Alternatively, if metric is a callable function, it is called on
    each pair of instances (rows) and the resulting value recorded.
    The callable should take two arrays from X as input and return a
    value indicating the distance between them.

n_jobs : int, default=None
    The number of jobs to use for the computation. This works by
    breaking down the pairwise matrix into n_jobs even slices and
    computing them in parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

working_memory : float, default=None
    The sought maximum memory for temporary distance matrix chunks.
    When None (default), the value of
    ``sklearn.get_config()['working_memory']`` is used.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a scipy.spatial.distance metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Yields
------
D_chunk : {ndarray, sparse matrix}
    A contiguous slice of distance matrix, optionally processed by
    ``reduce_func``.

Examples
--------
Without reduce_func:

>>> import numpy as np
>>> from sklearn.metrics import pairwise_distances_chunked
>>> X = np.random.RandomState(0).rand(5, 3)
>>> D_chunk = next(pairwise_distances_chunked(X))
>>> D_chunk
array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],
       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],
       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],
       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],
       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])

Retrieve all neighbors and average distance within radius r:

>>> r = .2
>>> def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r) for d in D_chunk]
...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)
...     return neigh, avg_dist
>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)
>>> neigh, avg_dist = next(gen)
>>> neigh
[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]
>>> avg_dist
array([0.039..., 0.        , 0.        , 0.039..., 0.        ])

Where r is defined per sample, we need to make use of ``start``:

>>> r = [.2, .4, .4, .3, .1]
>>> def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r[i])
...              for i, d in enumerate(D_chunk, start)]
...     return neigh
>>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))
>>> neigh
[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]

Force row-by-row generation by reducing ``working_memory``:

>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,
...                                  working_memory=0)
>>> next(gen)
[array([0, 3])]
>>> next(gen)
[array([0, 1])]""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MakeScorerMethod a owl:Class ;
    rdfs:comment """Make a scorer from a performance metric or loss function.

A scorer is a wrapper around an arbitrary metric or loss function that is called
with the signature `scorer(estimator, X, y_true, **kwargs)`.

It is accepted in all scikit-learn estimators or functions allowing a `scoring`
parameter.

The parameter `response_method` allows to specify which method of the estimator
should be used to feed the scoring/loss function.

Read more in the :ref:`User Guide <scoring>`.

Parameters
----------
score_func : callable
    Score function (or loss function) with signature
    ``score_func(y, y_pred, **kwargs)``.

response_method : {"predict_proba", "decision_function", "predict"} or             list/tuple of such str, default=None

    Specifies the response method to use get prediction from an estimator
    (i.e. :term:`predict_proba`, :term:`decision_function` or
    :term:`predict`). Possible choices are:

    - if `str`, it corresponds to the name to the method to return;
    - if a list or tuple of `str`, it provides the method names in order of
      preference. The method returned corresponds to the first method in
      the list and which is implemented by `estimator`.
    - if `None`, it is equivalent to `"predict"`.

    .. versionadded:: 1.4

greater_is_better : bool, default=True
    Whether `score_func` is a score function (default), meaning high is
    good, or a loss function, meaning low is good. In the latter case, the
    scorer object will sign-flip the outcome of the `score_func`.

needs_proba : bool, default=False
    Whether `score_func` requires `predict_proba` to get probability
    estimates out of a classifier.

    If True, for binary `y_true`, the score function is supposed to accept
    a 1D `y_pred` (i.e., probability of the positive class, shape
    `(n_samples,)`).

    .. deprecated:: 1.4
       `needs_proba` is deprecated in version 1.4 and will be removed in
       1.6. Use `response_method="predict_proba"` instead.

needs_threshold : bool, default=False
    Whether `score_func` takes a continuous decision certainty.
    This only works for binary classification using estimators that
    have either a `decision_function` or `predict_proba` method.

    If True, for binary `y_true`, the score function is supposed to accept
    a 1D `y_pred` (i.e., probability of the positive class or the decision
    function, shape `(n_samples,)`).

    For example `average_precision` or the area under the roc curve
    can not be computed using discrete predictions alone.

    .. deprecated:: 1.4
       `needs_threshold` is deprecated in version 1.4 and will be removed
       in 1.6. Use `response_method=("decision_function", "predict_proba")`
       instead to preserve the same behaviour.

**kwargs : additional arguments
    Additional parameters to be passed to `score_func`.

Returns
-------
scorer : callable
    Callable object that returns a scalar score; greater is better.

Examples
--------
>>> from sklearn.metrics import fbeta_score, make_scorer
>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
>>> ftwo_scorer
make_scorer(fbeta_score, response_method='predict', beta=2)
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
...                     scoring=ftwo_scorer)""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:PrecisionRecallFscoreSupportMethod a owl:Class ;
    rdfs:comment """Compute precision, recall, F-measure and support for each class.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label a negative sample as
positive.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The F-beta score can be interpreted as a weighted harmonic mean of
the precision and recall, where an F-beta score reaches its best
value at 1 and worst score at 0.

The F-beta score weights recall more than precision by a factor of
``beta``. ``beta == 1.0`` means recall and precision are equally important.

The support is the number of occurrences of each class in ``y_true``.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and metrics for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
metrics for all `labels` are either returned or averaged depending on the `average`
parameter. Use `labels` specify the set of labels to calculate metrics for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

beta : float, default=1.0
    The strength of recall versus precision in the F-score.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a "negative
    class". Labels not present in the data can be included and will be
    "assigned" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'binary', 'micro', 'macro', 'samples', 'weighted'},             default=None
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

warn_for : list, tuple or set, for internal use
    This determines which warnings will be made in the case that this
    function is being used to return only one of its metrics.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {"warn", 0.0, 1.0, np.nan}, default="warn"
    Sets the value to return when there is a zero division:
       - recall: when there are no positive labels
       - precision: when there are no positive predictions
       - f-score: both

    Notes:
    - If set to "warn", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
precision : float (if average is not None) or array of float, shape =        [n_unique_labels]
    Precision score.

recall : float (if average is not None) or array of float, shape =        [n_unique_labels]
    Recall score.

fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]
    F-beta score.

support : None (if average is not None) or array of int, shape =        [n_unique_labels]
    The number of occurrences of each label in ``y_true``.

Notes
-----
When ``true positive + false positive == 0``, precision is undefined.
When ``true positive + false negative == 0``, recall is undefined. When
``true positive + false negative + false positive == 0``, f-score is
undefined. In such cases, by default the metric will be set to 0, and
``UndefinedMetricWarning`` will be raised. This behavior can be modified
with ``zero_division``.

References
----------
.. [1] `Wikipedia entry for the Precision and recall
       <https://en.wikipedia.org/wiki/Precision_and_recall>`_.

.. [2] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

.. [3] `Discriminative Methods for Multi-labeled Classification Advances
       in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu
       Godbole, Sunita Sarawagi
       <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_fscore_support
>>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
>>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
>>> precision_recall_fscore_support(y_true, y_pred, average='macro')
(0.22..., 0.33..., 0.26..., None)
>>> precision_recall_fscore_support(y_true, y_pred, average='micro')
(0.33..., 0.33..., 0.33..., None)
>>> precision_recall_fscore_support(y_true, y_pred, average='weighted')
(0.22..., 0.33..., 0.26..., None)

It is possible to compute per-label precisions, recalls, F1-scores and
supports instead of averaging:

>>> precision_recall_fscore_support(y_true, y_pred, average=None,
... labels=['pig', 'dog', 'cat'])
(array([0.        , 0.        , 0.66...]),
 array([0., 0., 1.]), array([0. , 0. , 0.8]),
 array([2, 2, 2]))""" ;
    rdfs:subClassOf ds:AtomicMethod,
        ml:MetricsModule,
        ml:PerformanceCalculationMethod .

ml:MetricsModule a owl:Class ;
    rdfs:subClassOf ml:SklearnModule .

