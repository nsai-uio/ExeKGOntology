<?xml version="1.0"?>
<rdf:RDF xmlns="http://www.w3.org/2002/07/owl#"
     xml:base="http://www.w3.org/2002/07/owl"
     xmlns:owl="http://www.w3.org/2002/07/owl#"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
     xmlns:xml="http://www.w3.org/XML/1998/namespace"
     xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
     xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
    <Ontology/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Object Properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->


    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBayesianRegressionMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBayesianRegressionMethod">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
    </ObjectProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBinaryClassificationMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBinaryClassificationMethod">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
    </ObjectProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasLeastAngleRegressionMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasLeastAngleRegressionMethod">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
    </ObjectProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMulticlassClassificationMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMulticlassClassificationMethod">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
    </ObjectProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRegularizedRegressionMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRegularizedRegressionMethod">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
    </ObjectProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSimpleRegressionMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSimpleRegressionMethod">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression"/>
        <rdfs:range rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"/>
    </ObjectProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod -->


    <ObjectProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Data properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->


    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha1 -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha1">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha2 -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha2">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaInit -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaInit">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaPerTarget -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaPerTarget">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphas -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphas">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverage -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverage">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamC -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamC">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClassWeight -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClassWeight">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeScore -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeScore">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopy -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopy">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopyX -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopyX">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCriterion -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCriterion">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCs -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCs">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCv -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCv">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDual -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDual">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEarlyStopping -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEarlyStopping">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEps -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEps">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEpsilon -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEpsilon">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimator -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimator">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEta0 -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEta0">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitIntercept -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitIntercept">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPath -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPath">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGcvMode -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGcvMode">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInterceptScaling -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInterceptScaling">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsDataValid -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsDataValid">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsModelValid -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsModelValid">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamJitter -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamJitter">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratio -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratio">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratios -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratios">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda1 -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda1">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda2 -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda2">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambdaInit -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambdaInit">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRate -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRate">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLink -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLink">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLoss -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLoss">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxIter -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxIter">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNAlphas -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNAlphas">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSkips -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSkips">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSubpopulation -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSubpopulation">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrials -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrials">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultiClass -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultiClass">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNAlphas -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNAlphas">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIter -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIter">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIterNoChange -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIterNoChange">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNJobs -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNJobs">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNonzeroCoefs -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNonzeroCoefs">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSubsamples -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSubsamples">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNoiseVariance -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNoiseVariance">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNu -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNu">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPenalty -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPenalty">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositive -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositive">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPower -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPower">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerT -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerT">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrecompute -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrecompute">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantile -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantile">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRandomState -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRandomState">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRefit -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRefit">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResidualThreshold -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResidualThreshold">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoring -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoring">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSelection -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSelection">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShuffle -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShuffle">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolver -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolver">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolverOptions -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolverOptions">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopNInliers -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopNInliers">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopScore -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopScore">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCvValues -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCvValues">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThresholdLambda -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThresholdLambda">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTol -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTol">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#string"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidationFraction -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidationFraction">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#float"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVerbose -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVerbose">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#int"/>
    </DatatypeProperty>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarmStart -->


    <DatatypeProperty rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarmStart">
        <rdfs:subPropertyOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"/>
        <rdfs:domain rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"/>
        <rdfs:range rdf:resource="http://www.w3.org/2001/XMLSchema#boolean"/>
    </DatatypeProperty>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Classes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->


    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Bayesian ARD regression.

Fit the weights of a regression model, using an ARD prior. The weights of
the regression model are assumed to be in Gaussian distributions.
Also estimate the parameters lambda (precisions of the distributions of the
weights) and alpha (precision of the distribution of the noise).
The estimation is done by an iterative procedures (Evidence Maximization)

Read more in the :ref:`User Guide &lt;bayesian_regression&gt;`.

Parameters
----------
max_iter : int, default=None
    Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.

    .. versionchanged:: 1.3

tol : float, default=1e-3
    Stop the algorithm if w has converged.

alpha_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the alpha parameter.

alpha_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the alpha parameter.

lambda_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the lambda parameter.

lambda_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the lambda parameter.

compute_score : bool, default=False
    If True, compute the objective function at each step of the model.

threshold_lambda : float, default=10 000
    Threshold for removing (pruning) weights with high precision from
    the computation.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

verbose : bool, default=False
    Verbose mode when fitting the model.

n_iter : int
    Maximum number of iterations.

    .. deprecated:: 1.3
       `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
       `max_iter` instead.

Attributes
----------
coef_ : array-like of shape (n_features,)
    Coefficients of the regression model (mean of distribution)

alpha_ : float
   estimated precision of the noise.

lambda_ : array-like of shape (n_features,)
   estimated precisions of the weights.

sigma_ : array-like of shape (n_features, n_features)
    estimated variance-covariance matrix of the weights

scores_ : float
    if computed, value of the objective function (to be maximized)

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

    .. versionadded:: 1.3

intercept_ : float
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

X_offset_ : float
    If `fit_intercept=True`, offset subtracted for centering data to a
    zero mean. Set to np.zeros(n_features) otherwise.

X_scale_ : float
    Set to np.ones(n_features).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
BayesianRidge : Bayesian ridge regression.

Notes
-----
For an example, see :ref:`examples/linear_model/plot_ard.py
&lt;sphx_glr_auto_examples_linear_model_plot_ard.py&gt;`.

References
----------
D. J. C. MacKay, Bayesian nonlinear modeling for the prediction
competition, ASHRAE Transactions, 1994.

R. Salakhutdinov, Lecture notes on Statistical Machine Learning,
http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15
Their beta is our ``self.alpha_``
Their alpha is our ``self.lambda_``
ARD is a little different than the slide: only dimensions/features for
which ``self.lambda_ &lt; self.threshold_lambda`` are kept and the rest are
discarded.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.ARDRegression()
&gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
ARDRegression()
&gt;&gt;&gt; clf.predict([[1, 1]])
array([1.])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Bayesian ridge regression.

Fit a Bayesian ridge model. See the Notes section for details on this
implementation and the optimization of the regularization parameters
lambda (precision of the weights) and alpha (precision of the noise).

Read more in the :ref:`User Guide &lt;bayesian_regression&gt;`.

Parameters
----------
max_iter : int, default=None
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion. If `None`, it
    corresponds to `max_iter=300`.

    .. versionchanged:: 1.3

tol : float, default=1e-3
    Stop the algorithm if w has converged.

alpha_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the alpha parameter.

alpha_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the alpha parameter.

lambda_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the lambda parameter.

lambda_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the lambda parameter.

alpha_init : float, default=None
    Initial value for alpha (precision of the noise).
    If not set, alpha_init is 1/Var(y).

        .. versionadded:: 0.22

lambda_init : float, default=None
    Initial value for lambda (precision of the weights).
    If not set, lambda_init is 1.

        .. versionadded:: 0.22

compute_score : bool, default=False
    If True, compute the log marginal likelihood at each iteration of the
    optimization.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model.
    The intercept is not treated as a probabilistic parameter
    and thus has no associated variance. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

verbose : bool, default=False
    Verbose mode when fitting the model.

n_iter : int
    Maximum number of iterations. Should be greater than or equal to 1.

    .. deprecated:: 1.3
       `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
       `max_iter` instead.

Attributes
----------
coef_ : array-like of shape (n_features,)
    Coefficients of the regression model (mean of distribution)

intercept_ : float
    Independent term in decision function. Set to 0.0 if
    `fit_intercept = False`.

alpha_ : float
   Estimated precision of the noise.

lambda_ : float
   Estimated precision of the weights.

sigma_ : array-like of shape (n_features, n_features)
    Estimated variance-covariance matrix of the weights

scores_ : array-like of shape (n_iter_+1,)
    If computed_score is True, value of the log marginal likelihood (to be
    maximized) at each iteration of the optimization. The array starts
    with the value of the log marginal likelihood obtained for the initial
    values of alpha and lambda and ends with the value obtained for the
    estimated alpha and lambda.

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

X_offset_ : ndarray of shape (n_features,)
    If `fit_intercept=True`, offset subtracted for centering data to a
    zero mean. Set to np.zeros(n_features) otherwise.

X_scale_ : ndarray of shape (n_features,)
    Set to np.ones(n_features).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
ARDRegression : Bayesian ARD regression.

Notes
-----
There exist several strategies to perform Bayesian ridge regression. This
implementation is based on the algorithm described in Appendix A of
(Tipping, 2001) where updates of the regularization parameters are done as
suggested in (MacKay, 1992). Note that according to A New
View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these
update rules do not guarantee that the marginal likelihood is increasing
between two consecutive iterations of the optimization.

References
----------
D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,
Vol. 4, No. 3, 1992.

M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,
Journal of Machine Learning Research, Vol. 1, 2001.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.BayesianRidge()
&gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
BayesianRidge()
&gt;&gt;&gt; clf.predict([[1, 1]])
array([1.])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Elastic Net model with iterative fitting along a regularization path.

See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide &lt;elastic_net&gt;`.

Parameters
----------
l1_ratio : float or list of float, default=0.5
    Float between 0 and 1 passed to ElasticNet (scaling between
    l1 and l2 penalties). For ``l1_ratio = 0``
    the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.
    For ``0 &lt; l1_ratio &lt; 1``, the penalty is a combination of L1 and L2
    This parameter can be a list, in which case the different
    values are tested by cross-validation and the one giving the best
    prediction score is used. Note that a good choice of list of
    values for l1_ratio is often to put more values close to 1
    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,
    .9, .95, .99, 1]``.

eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path, used for each l1_ratio.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If None alphas are set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : &apos;auto&apos;, bool or array-like of shape             (n_features, n_features), default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

verbose : bool or int, default=0
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
alpha_ : float
    The amount of penalization chosen by cross validation.

l1_ratio_ : float
    The compromise between l1 and l2 penalization chosen by
    cross validation.

coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

intercept_ : float or ndarray of shape (n_targets, n_features)
    Independent term in the decision function.

mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)
    Mean square error for the test set on each fold, varying l1_ratio and
    alpha.

alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)
    The grid of alphas used for fitting, for each l1_ratio.

dual_gap_ : float
    The dual gaps at the end of the optimization for the optimal alpha.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
enet_path : Compute elastic net path with coordinate descent.
ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.

Notes
-----
In `fit`, once the best parameters `l1_ratio` and `alpha` are found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` argument of the `fit`
method should be directly passed as a Fortran-contiguous numpy array.

The parameter `l1_ratio` corresponds to alpha in the glmnet R package
while alpha corresponds to the lambda parameter in glmnet.
More specifically, the optimization objective is::

    1 / (2 * n_samples) * ||y - Xw||^2_2
    + alpha * l1_ratio * ||w||_1
    + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2

If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to::

    a * L1 + b * L2

for::

    alpha = a + b and l1_ratio = a / (a + b).

For an example, see
:ref:`examples/linear_model/plot_lasso_model_selection.py
&lt;sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py&gt;`.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import ElasticNetCV
&gt;&gt;&gt; from sklearn.datasets import make_regression

&gt;&gt;&gt; X, y = make_regression(n_features=2, random_state=0)
&gt;&gt;&gt; regr = ElasticNetCV(cv=5, random_state=0)
&gt;&gt;&gt; regr.fit(X, y)
ElasticNetCV(cv=5, random_state=0)
&gt;&gt;&gt; print(regr.alpha_)
0.199...
&gt;&gt;&gt; print(regr.intercept_)
0.398...
&gt;&gt;&gt; print(regr.predict([[0, 0]]))
[0.398...]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear regression with combined L1 and L2 priors as regularizer.

Minimizes the objective function::

        1 / (2 * n_samples) * ||y - Xw||^2_2
        + alpha * l1_ratio * ||w||_1
        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2

If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to::

        a * ||w||_1 + 0.5 * b * ||w||_2^2

where::

        alpha = a + b and l1_ratio = a / (a + b)

The parameter l1_ratio corresponds to alpha in the glmnet R package while
alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio
= 1 is the lasso penalty. Currently, l1_ratio &lt;= 0.01 is not reliable,
unless you supply your own sequence of alpha.

Read more in the :ref:`User Guide &lt;elastic_net&gt;`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the penalty terms. Defaults to 1.0.
    See the notes for the exact mathematical meaning of this
    parameter. ``alpha = 0`` is equivalent to an ordinary least square,
    solved by the :class:`LinearRegression` object. For numerical
    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
    Given this, you should use the :class:`LinearRegression` object.

l1_ratio : float, default=0.5
    The ElasticNet mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. For
    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it
    is an L1 penalty.  For ``0 &lt; l1_ratio &lt; 1``, the penalty is a
    combination of L1 and L2.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If ``False``, the
    data is assumed to be already centered.

precompute : bool or array-like of shape (n_features, n_features),                 default=False
    Whether to use a precomputed Gram matrix to speed up
    calculations. The Gram matrix can also be passed as argument.
    For sparse input this option is always ``False`` to preserve sparsity.

max_iter : int, default=1000
    The maximum number of iterations.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``, see Notes below.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)
    Sparse representation of the `coef_`.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : list of int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

dual_gap_ : float or ndarray of shape (n_targets,)
    Given param alpha, the dual gaps at the end of the optimization,
    same shape as each observation of y.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
ElasticNetCV : Elastic net model with best model selection by
    cross-validation.
SGDRegressor : Implements elastic net regression with incremental training.
SGDClassifier : Implements logistic regression with elastic net penalty
    (``SGDClassifier(loss=&quot;log_loss&quot;, penalty=&quot;elasticnet&quot;)``).

Notes
-----
To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.

The precise stopping criteria based on `tol` are the following: First, check that
that maximum coordinate update, i.e. :math:`\max_j |w_j^{new} - w_j^{old}|`
is smaller than `tol` times the maximum absolute coefficient, :math:`\max_j |w_j|`.
If so, then additionally check whether the dual gap is smaller than `tol` times
:math:`||y||_2^2 / n_{      ext{samples}}`.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import ElasticNet
&gt;&gt;&gt; from sklearn.datasets import make_regression

&gt;&gt;&gt; X, y = make_regression(n_features=2, random_state=0)
&gt;&gt;&gt; regr = ElasticNet(random_state=0)
&gt;&gt;&gt; regr.fit(X, y)
ElasticNet(random_state=0)
&gt;&gt;&gt; print(regr.coef_)
[18.83816048 64.55968825]
&gt;&gt;&gt; print(regr.intercept_)
1.451...
&gt;&gt;&gt; print(regr.predict([[0, 0]]))
[1.451...]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Generalized Linear Model with a Gamma distribution.

This regressor uses the &apos;log&apos; link function.

Read more in the :ref:`User Guide &lt;Generalized_linear_models&gt;`.

.. versionadded:: 0.23

Parameters
----------
alpha : float, default=1
    Constant that multiplies the L2 penalty term and determines the
    regularization strength. ``alpha = 0`` is equivalent to unpenalized
    GLMs. In this case, the design matrix `X` must have full column rank
    (no collinearities).
    Values of `alpha` must be in the range `[0.0, inf)`.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the linear predictor `X @ coef_ + intercept_`.

solver : {&apos;lbfgs&apos;, &apos;newton-cholesky&apos;}, default=&apos;lbfgs&apos;
    Algorithm to use in the optimization problem:

    &apos;lbfgs&apos;
        Calls scipy&apos;s L-BFGS-B optimizer.

    &apos;newton-cholesky&apos;
        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to
        iterated reweighted least squares) with an inner Cholesky based solver.
        This solver is a good choice for `n_samples` &gt;&gt; `n_features`, especially
        with one-hot encoded categorical features with rare categories. Be aware
        that the memory usage of this solver has a quadratic dependency on
        `n_features` because it explicitly computes the Hessian matrix.

        .. versionadded:: 1.2

max_iter : int, default=100
    The maximal number of iterations for the solver.
    Values must be in the range `[1, inf)`.

tol : float, default=1e-4
    Stopping criterion. For the lbfgs solver,
    the iteration will stop when ``max{|g_j|, j = 1, ..., d} &lt;= tol``
    where ``g_j`` is the j-th component of the gradient (derivative) of
    the objective function.
    Values must be in the range `(0.0, inf)`.

warm_start : bool, default=False
    If set to ``True``, reuse the solution of the previous call to ``fit``
    as initialization for `coef_` and `intercept_`.

verbose : int, default=0
    For the lbfgs solver set verbose to any positive number for verbosity.
    Values must be in the range `[0, inf)`.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the linear predictor (`X @ coef_ +
    intercept_`) in the GLM.

intercept_ : float
    Intercept (a.k.a. bias) added to linear predictor.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

n_iter_ : int
    Actual number of iterations used in the solver.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PoissonRegressor : Generalized Linear Model with a Poisson distribution.
TweedieRegressor : Generalized Linear Model with a Tweedie distribution.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.GammaRegressor()
&gt;&gt;&gt; X = [[1, 2], [2, 3], [3, 4], [4, 3]]
&gt;&gt;&gt; y = [19, 26, 33, 30]
&gt;&gt;&gt; clf.fit(X, y)
GammaRegressor()
&gt;&gt;&gt; clf.score(X, y)
0.773...
&gt;&gt;&gt; clf.coef_
array([0.072..., 0.066...])
&gt;&gt;&gt; clf.intercept_
2.896...
&gt;&gt;&gt; clf.predict([[1, 0], [2, 8]])
array([19.483..., 35.795...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment></rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment></rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>L2-regularized linear regression model that is robust to outliers.

The Huber Regressor optimizes the squared loss for the samples where
``|(y - Xw - c) / sigma| &lt; epsilon`` and the absolute loss for the samples
where ``|(y - Xw - c) / sigma| &gt; epsilon``, where the model coefficients
``w``, the intercept ``c`` and the scale ``sigma`` are parameters
to be optimized. The parameter sigma makes sure that if y is scaled up
or down by a certain factor, one does not need to rescale epsilon to
achieve the same robustness. Note that this does not take into account
the fact that the different features of X may be of different scales.

The Huber loss function has the advantage of not being heavily influenced
by the outliers while not completely ignoring their effect.

Read more in the :ref:`User Guide &lt;huber_regression&gt;`

.. versionadded:: 0.18

Parameters
----------
epsilon : float, default=1.35
    The parameter epsilon controls the number of samples that should be
    classified as outliers. The smaller the epsilon, the more robust it is
    to outliers. Epsilon must be in the range `[1, inf)`.

max_iter : int, default=100
    Maximum number of iterations that
    ``scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)`` should run for.

alpha : float, default=0.0001
    Strength of the squared L2 regularization. Note that the penalty is
    equal to ``alpha * ||w||^2``.
    Must be in the range `[0, inf)`.

warm_start : bool, default=False
    This is useful if the stored attributes of a previously used model
    has to be reused. If set to False, then the coefficients will
    be rewritten for every call to fit.
    See :term:`the Glossary &lt;warm_start&gt;`.

fit_intercept : bool, default=True
    Whether or not to fit the intercept. This can be set to False
    if the data is already centered around the origin.

tol : float, default=1e-05
    The iteration will stop when
    ``max{|proj g_i | i = 1, ..., n}`` &lt;= ``tol``
    where pg_i is the i-th component of the projected gradient.

Attributes
----------
coef_ : array, shape (n_features,)
    Features got by optimizing the L2-regularized Huber loss.

intercept_ : float
    Bias.

scale_ : float
    The value by which ``|y - Xw - c|`` is scaled down.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations that
    ``scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)`` has run for.

    .. versionchanged:: 0.20

        In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.

outliers_ : array, shape (n_samples,)
    A boolean mask which is set to True where the samples are identified
    as outliers.

See Also
--------
RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.
TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.
SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.

References
----------
.. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics
       Concomitant scale estimates, pg 172
.. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.
       https://statweb.stanford.edu/~owen/reports/hhu.pdf

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import HuberRegressor, LinearRegression
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X, y, coef = make_regression(
...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)
&gt;&gt;&gt; X[:4] = rng.uniform(10, 20, (4, 2))
&gt;&gt;&gt; y[:4] = rng.uniform(10, 20, 4)
&gt;&gt;&gt; huber = HuberRegressor().fit(X, y)
&gt;&gt;&gt; huber.score(X, y)
-7.284...
&gt;&gt;&gt; huber.predict(X[:1,])
array([806.7200...])
&gt;&gt;&gt; linear = LinearRegression().fit(X, y)
&gt;&gt;&gt; print(&quot;True coefficients:&quot;, coef)
True coefficients: [20.4923...  34.1698...]
&gt;&gt;&gt; print(&quot;Huber coefficients:&quot;, huber.coef_)
Huber coefficients: [17.7906... 31.0106...]
&gt;&gt;&gt; print(&quot;Linear Regression coefficients:&quot;, linear.coef_)
Linear Regression coefficients: [-1.9221...  7.0226...]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Cross-validated Least Angle Regression model.

See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

max_iter : int, default=500
    Maximum number of iterations to perform.

precompute : bool, &apos;auto&apos; or array-like , default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram matrix
    cannot be passed as argument since we will use only subsets of X.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

max_n_alphas : int, default=1000
    The maximum number of points on the path used to compute the
    residuals in the cross-validation.

n_jobs : int or None, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

Attributes
----------
active_ : list of length n_alphas or list of such lists
    Indices of active variables at the end of the path.
    If this is a list of lists, the outer list length is `n_targets`.

coef_ : array-like of shape (n_features,)
    parameter vector (w in the formulation formula)

intercept_ : float
    independent term in decision function

coef_path_ : array-like of shape (n_features, n_alphas)
    the varying values of the coefficients along the path

alpha_ : float
    the estimated regularization parameter alpha

alphas_ : array-like of shape (n_alphas,)
    the different values of alpha along the path

cv_alphas_ : array-like of shape (n_cv_alphas,)
    all the values of alpha along the path for the different folds

mse_path_ : array-like of shape (n_folds, n_cv_alphas)
    the mean square error on left-out for each fold along the path
    (alpha values given by ``cv_alphas``)

n_iter_ : array-like or int
    the number of iterations run by Lars with the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoLarsIC : Lasso model fit with Lars using BIC
    or AIC for model selection.
sklearn.decomposition.sparse_encode : Sparse coding.

Notes
-----
In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import LarsCV
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(n_samples=200, noise=4.0, random_state=0)
&gt;&gt;&gt; reg = LarsCV(cv=5).fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9996...
&gt;&gt;&gt; reg.alpha_
0.2961...
&gt;&gt;&gt; reg.predict(X[:1,])
array([154.3996...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Least Angle Regression model a.k.a. LAR.

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

precompute : bool, &apos;auto&apos; or array-like , default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram
    matrix can also be passed as argument.

n_nonzero_coefs : int, default=500
    Target number of non-zero coefficients. Use ``np.inf`` for no limit.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

fit_path : bool, default=True
    If True the full path is stored in the ``coef_path_`` attribute.
    If you compute the solution for a large problem or many targets,
    setting ``fit_path`` to ``False`` will lead to a speedup, especially
    with a small alpha.

jitter : float, default=None
    Upper bound on a uniform noise parameter to be added to the
    `y` values, to satisfy the model&apos;s assumption of
    one-at-a-time computations. Might help with stability.

    .. versionadded:: 0.23

random_state : int, RandomState instance or None, default=None
    Determines random number generation for jittering. Pass an int
    for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`. Ignored if `jitter` is None.

    .. versionadded:: 0.23

Attributes
----------
alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha &gt;= alpha_min``, whichever
    is smaller. If this is a list of array-like, the length of the outer
    list is `n_targets`.

active_ : list of shape (n_alphas,) or list of such lists
    Indices of active variables at the end of the path.
    If this is a list of list, the length of the outer list is `n_targets`.

coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays
    The varying values of the coefficients along the path. It is not
    present if the ``fit_path`` parameter is ``False``. If this is a list
    of array-like, the length of the outer list is `n_targets`.

coef_ : array-like of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the formulation formula).

intercept_ : float or array-like of shape (n_targets,)
    Independent term in decision function.

n_iter_ : array-like or int
    The number of iterations taken by lars_path to find the
    grid of alphas for each target.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path: Compute Least Angle Regression or Lasso
    path using LARS algorithm.
LarsCV : Cross-validated Least Angle Regression model.
sklearn.decomposition.sparse_encode : Sparse coding.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.Lars(n_nonzero_coefs=1)
&gt;&gt;&gt; reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
Lars(n_nonzero_coefs=1)
&gt;&gt;&gt; print(reg.coef_)
[ 0. -1.11...]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Lasso linear model with iterative fitting along a regularization path.

See glossary entry for :term:`cross-validation estimator`.

The best model is selected by cross-validation.

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Read more in the :ref:`User Guide &lt;lasso&gt;`.

Parameters
----------
eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If ``None`` alphas are set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : &apos;auto&apos;, bool or array-like of shape             (n_features, n_features), default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

verbose : bool or int, default=False
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

positive : bool, default=False
    If positive, restrict regression coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
alpha_ : float
    The amount of penalization chosen by cross validation.

coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

mse_path_ : ndarray of shape (n_alphas, n_folds)
    Mean square error for the test set on each fold, varying alpha.

alphas_ : ndarray of shape (n_alphas,)
    The grid of alphas used for fitting.

dual_gap_ : float or ndarray of shape (n_targets,)
    The dual gap at the end of the optimization for the optimal alpha
    (``alpha_``).

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso path using LARS
    algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : The Lasso is a linear model that estimates sparse coefficients.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoCV : Lasso linear model with iterative fitting along a regularization
    path.
LassoLarsCV : Cross-validated Lasso using the LARS algorithm.

Notes
-----
In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` argument of the `fit`
method should be directly passed as a Fortran-contiguous numpy array.

 For an example, see
 :ref:`examples/linear_model/plot_lasso_model_selection.py
 &lt;sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py&gt;`.

:class:`LassoCV` leads to different results than a hyperparameter
search using :class:`~sklearn.model_selection.GridSearchCV` with a
:class:`Lasso` model. In :class:`LassoCV`, a model for a given
penalty `alpha` is warm started using the coefficients of the
closest model (trained at the previous iteration) on the
regularization path. It tends to speed up the hyperparameter
search.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import LassoCV
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(noise=4, random_state=0)
&gt;&gt;&gt; reg = LassoCV(cv=5, random_state=0).fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9993...
&gt;&gt;&gt; reg.predict(X[:1,])
array([-78.4951...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Cross-validated Lasso, using the LARS algorithm.

See glossary entry for :term:`cross-validation estimator`.

The optimization objective for Lasso is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

max_iter : int, default=500
    Maximum number of iterations to perform.

precompute : bool or &apos;auto&apos; , default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram matrix
    cannot be passed as argument since we will use only subsets of X.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

max_n_alphas : int, default=1000
    The maximum number of points on the path used to compute the
    residuals in the cross-validation.

n_jobs : int or None, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

positive : bool, default=False
    Restrict coefficients to be &gt;= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    Under the positive restriction the model coefficients do not converge
    to the ordinary-least-squares solution for small values of alpha.
    Only coefficients up to the smallest alpha value (``alphas_[alphas_ &gt;
    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
    algorithm are typically in congruence with the solution of the
    coordinate descent Lasso estimator.
    As a consequence using LassoLarsCV only makes sense for problems where
    a sparse solution is expected and/or reached.

Attributes
----------
coef_ : array-like of shape (n_features,)
    parameter vector (w in the formulation formula)

intercept_ : float
    independent term in decision function.

coef_path_ : array-like of shape (n_features, n_alphas)
    the varying values of the coefficients along the path

alpha_ : float
    the estimated regularization parameter alpha

alphas_ : array-like of shape (n_alphas,)
    the different values of alpha along the path

cv_alphas_ : array-like of shape (n_cv_alphas,)
    all the values of alpha along the path for the different folds

mse_path_ : array-like of shape (n_folds, n_cv_alphas)
    the mean square error on left-out for each fold along the path
    (alpha values given by ``cv_alphas``)

n_iter_ : array-like or int
    the number of iterations run by Lars with the optimal alpha.

active_ : list of int
    Indices of active variables at the end of the path.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoLarsIC : Lasso model fit with Lars using BIC
    or AIC for model selection.
sklearn.decomposition.sparse_encode : Sparse coding.

Notes
-----
The object solves the same problem as the
:class:`~sklearn.linear_model.LassoCV` object. However, unlike the
:class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values
by itself. In general, because of this property, it will be more stable.
However, it is more fragile to heavily multicollinear datasets.

It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if
only a small number of features are selected compared to the total number,
for instance if there are very few samples compared to the number of
features.

In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import LassoLarsCV
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(noise=4.0, random_state=0)
&gt;&gt;&gt; reg = LassoLarsCV(cv=5).fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9993...
&gt;&gt;&gt; reg.alpha_
0.3972...
&gt;&gt;&gt; reg.predict(X[:1,])
array([-78.4831...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Lasso model fit with Lars using BIC or AIC for model selection.

The optimization objective for Lasso is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

AIC is the Akaike information criterion [2]_ and BIC is the Bayes
Information criterion [3]_. Such criteria are useful to select the value
of the regularization parameter by making a trade-off between the
goodness of fit and the complexity of the model. A good model should
explain well the data while being simple.

Read more in the :ref:`User Guide &lt;lasso_lars_ic&gt;`.

Parameters
----------
criterion : {&apos;aic&apos;, &apos;bic&apos;}, default=&apos;aic&apos;
    The type of criterion to use.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

precompute : bool, &apos;auto&apos; or array-like, default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=500
    Maximum number of iterations to perform. Can be used for
    early stopping.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

positive : bool, default=False
    Restrict coefficients to be &gt;= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    Under the positive restriction the model coefficients do not converge
    to the ordinary-least-squares solution for small values of alpha.
    Only coefficients up to the smallest alpha value (``alphas_[alphas_ &gt;
    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
    algorithm are typically in congruence with the solution of the
    coordinate descent Lasso estimator.
    As a consequence using LassoLarsIC only makes sense for problems where
    a sparse solution is expected and/or reached.

noise_variance : float, default=None
    The estimated noise variance of the data. If `None`, an unbiased
    estimate is computed by an OLS model. However, it is only possible
    in the case where `n_samples &gt; n_features + fit_intercept`.

    .. versionadded:: 1.1

Attributes
----------
coef_ : array-like of shape (n_features,)
    parameter vector (w in the formulation formula)

intercept_ : float
    independent term in decision function.

alpha_ : float
    the alpha parameter chosen by the information criterion

alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha &gt;= alpha_min``, whichever
    is smaller. If a list, it will be of length `n_targets`.

n_iter_ : int
    number of iterations run by lars_path to find the grid of
    alphas.

criterion_ : array-like of shape (n_alphas,)
    The value of the information criteria (&apos;aic&apos;, &apos;bic&apos;) across all
    alphas. The alpha which has the smallest information criterion is
    chosen, as specified in [1]_.

noise_variance_ : float
    The estimated noise variance from the data used to compute the
    criterion.

    .. versionadded:: 1.1

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.
sklearn.decomposition.sparse_encode : Sparse coding.

Notes
-----
The number of degrees of freedom is computed as in [1]_.

To have more details regarding the mathematical formulation of the
AIC and BIC criteria, please refer to :ref:`User Guide &lt;lasso_lars_ic&gt;`.

References
----------
.. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.
        &quot;On the degrees of freedom of the lasso.&quot;
        The Annals of Statistics 35.5 (2007): 2173-2192.
        &lt;0712.0881&gt;`

.. [2] `Wikipedia entry on the Akaike information criterion
        &lt;https://en.wikipedia.org/wiki/Akaike_information_criterion&gt;`_

.. [3] `Wikipedia entry on the Bayesian information criterion
        &lt;https://en.wikipedia.org/wiki/Bayesian_information_criterion&gt;`_

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.LassoLarsIC(criterion=&apos;bic&apos;)
&gt;&gt;&gt; X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]
&gt;&gt;&gt; y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]
&gt;&gt;&gt; reg.fit(X, y)
LassoLarsIC(criterion=&apos;bic&apos;)
&gt;&gt;&gt; print(reg.coef_)
[ 0.  -1.11...]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Lasso model fit with Least Angle Regression a.k.a. Lars.

It is a Linear Model trained with an L1 prior as regularizer.

The optimization objective for Lasso is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the penalty term. Defaults to 1.0.
    ``alpha = 0`` is equivalent to an ordinary least square, solved
    by :class:`LinearRegression`. For numerical reasons, using
    ``alpha = 0`` with the LassoLars object is not advised and you
    should prefer the LinearRegression object.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

precompute : bool, &apos;auto&apos; or array-like, default=&apos;auto&apos;
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``&apos;auto&apos;`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=500
    Maximum number of iterations to perform.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

fit_path : bool, default=True
    If ``True`` the full path is stored in the ``coef_path_`` attribute.
    If you compute the solution for a large problem or many targets,
    setting ``fit_path`` to ``False`` will lead to a speedup, especially
    with a small alpha.

positive : bool, default=False
    Restrict coefficients to be &gt;= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    Under the positive restriction the model coefficients will not converge
    to the ordinary-least-squares solution for small values of alpha.
    Only coefficients up to the smallest alpha value (``alphas_[alphas_ &gt;
    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
    algorithm are typically in congruence with the solution of the
    coordinate descent Lasso estimator.

jitter : float, default=None
    Upper bound on a uniform noise parameter to be added to the
    `y` values, to satisfy the model&apos;s assumption of
    one-at-a-time computations. Might help with stability.

    .. versionadded:: 0.23

random_state : int, RandomState instance or None, default=None
    Determines random number generation for jittering. Pass an int
    for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`. Ignored if `jitter` is None.

    .. versionadded:: 0.23

Attributes
----------
alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha &gt;= alpha_min``, whichever
    is smaller. If this is a list of array-like, the length of the outer
    list is `n_targets`.

active_ : list of length n_alphas or list of such lists
    Indices of active variables at the end of the path.
    If this is a list of list, the length of the outer list is `n_targets`.

coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays
    If a list is passed it&apos;s expected to be one of n_targets such arrays.
    The varying values of the coefficients along the path. It is not
    present if the ``fit_path`` parameter is ``False``. If this is a list
    of array-like, the length of the outer list is `n_targets`.

coef_ : array-like of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the formulation formula).

intercept_ : float or array-like of shape (n_targets,)
    Independent term in decision function.

n_iter_ : array-like or int
    The number of iterations taken by lars_path to find the
    grid of alphas for each target.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.
LassoLarsIC : Lasso model fit with Lars using BIC
    or AIC for model selection.
sklearn.decomposition.sparse_encode : Sparse coding.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.LassoLars(alpha=0.01)
&gt;&gt;&gt; reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])
LassoLars(alpha=0.01)
&gt;&gt;&gt; print(reg.coef_)
[ 0.         -0.955...]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear Model trained with L1 prior as regularizer (aka the Lasso).

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Technically the Lasso model is optimizing the same objective function as
the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).

Read more in the :ref:`User Guide &lt;lasso&gt;`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the L1 term, controlling regularization
    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.

    When `alpha = 0`, the objective is equivalent to ordinary least
    squares, solved by the :class:`LinearRegression` object. For numerical
    reasons, using `alpha = 0` with the `Lasso` object is not advised.
    Instead, you should use the :class:`LinearRegression` object.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : bool or array-like of shape (n_features, n_features),                 default=False
    Whether to use a precomputed Gram matrix to speed up
    calculations. The Gram matrix can also be passed as argument.
    For sparse input this option is always ``False`` to preserve sparsity.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``, see Notes below.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

dual_gap_ : float or ndarray of shape (n_targets,)
    Given param alpha, the dual gaps at the end of the optimization,
    same shape as each observation of y.

sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)
    Readonly property derived from ``coef_``.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : int or list of int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Regularization path using LARS.
lasso_path : Regularization path using Lasso.
LassoLars : Lasso Path along the regularization parameter using LARS algorithm.
LassoCV : Lasso alpha parameter by cross-validation.
LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.
sklearn.decomposition.sparse_encode : Sparse coding array estimator.

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.

Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to `1 / (2C)` in other linear
models such as :class:`~sklearn.linear_model.LogisticRegression` or
:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.

The precise stopping criteria based on `tol` are the following: First, check that
that maximum coordinate update, i.e. :math:`\max_j |w_j^{new} - w_j^{old}|`
is smaller than `tol` times the maximum absolute coefficient, :math:`\max_j |w_j|`.
If so, then additionally check whether the dual gap is smaller than `tol` times
:math:`||y||_2^2 / n_{\text{samples}}`.

The target can be a 2-dimensional array, resulting in the optimization of the
following objective::

    (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11

where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.
It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which
instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise
sparsity in the coefficients.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.Lasso(alpha=0.1)
&gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
Lasso(alpha=0.1)
&gt;&gt;&gt; print(clf.coef_)
[0.85 0.  ]
&gt;&gt;&gt; print(clf.intercept_)
0.15...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"/>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Ordinary least squares Linear Regression.

LinearRegression fits a linear model with coefficients w = (w1, ..., wp)
to minimize the residual sum of squares between the observed targets in
the dataset, and the targets predicted by the linear approximation.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

n_jobs : int, default=None
    The number of jobs to use for the computation. This will only provide
    speedup in case of sufficiently large problems, that is if firstly
    `n_targets &gt; 1` and secondly `X` is sparse or if `positive` is set
    to `True`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary &lt;n_jobs&gt;` for more details.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive. This
    option is only supported for dense arrays.

    .. versionadded:: 0.24

Attributes
----------
coef_ : array of shape (n_features, ) or (n_targets, n_features)
    Estimated coefficients for the linear regression problem.
    If multiple targets are passed during the fit (y 2D), this
    is a 2D array of shape (n_targets, n_features), while if only
    one target is passed, this is a 1D array of length n_features.

rank_ : int
    Rank of matrix `X`. Only available when `X` is dense.

singular_ : array of shape (min(X, y),)
    Singular values of `X`. Only available when `X` is dense.

intercept_ : float or array of shape (n_targets,)
    Independent term in the linear model. Set to 0.0 if
    `fit_intercept = False`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression addresses some of the
    problems of Ordinary Least Squares by imposing a penalty on the
    size of the coefficients with l2 regularization.
Lasso : The Lasso is a linear model that estimates
    sparse coefficients with l1 regularization.
ElasticNet : Elastic-Net is a linear regression
    model trained with both l1 and l2 -norm regularization of the
    coefficients.

Notes
-----
From the implementation point of view, this is just plain Ordinary
Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares
(scipy.optimize.nnls) wrapped as a predictor object.

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import LinearRegression
&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
&gt;&gt;&gt; # y = 1 * x_0 + 2 * x_1 + 3
&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3
&gt;&gt;&gt; reg = LinearRegression().fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
1.0
&gt;&gt;&gt; reg.coef_
array([1., 2.])
&gt;&gt;&gt; reg.intercept_
3.0...
&gt;&gt;&gt; reg.predict(np.array([[3, 5]]))
array([16.])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment></rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Logistic Regression CV (aka logit, MaxEnt) classifier.

See glossary entry for :term:`cross-validation estimator`.

This class implements logistic regression using liblinear, newton-cg, sag
of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
regularization with primal formulation. The liblinear solver supports both
L1 and L2 regularization, with a dual formulation only for the L2 penalty.
Elastic-Net penalty is only supported by the saga solver.

For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter
is selected by the cross-validator
:class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed
using the :term:`cv` parameter. The &apos;newton-cg&apos;, &apos;sag&apos;, &apos;saga&apos; and &apos;lbfgs&apos;
solvers can warm-start the coefficients (see :term:`Glossary&lt;warm_start&gt;`).

Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.

Parameters
----------
Cs : int or list of floats, default=10
    Each of the values in Cs describes the inverse of regularization
    strength. If Cs is as an int, then a grid of Cs values are chosen
    in a logarithmic scale between 1e-4 and 1e4.
    Like in support vector machines, smaller values specify stronger
    regularization.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the decision function.

cv : int or cross-validation generator, default=None
    The default cross-validation generator used is Stratified K-Folds.
    If an integer is provided, then it is the number of folds used.
    See the module :mod:`sklearn.model_selection` module for the
    list of possible cross-validation objects.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

dual : bool, default=False
    Dual (constrained) or primal (regularized, see also
    :ref:`this equation &lt;regularized-logistic-loss&gt;`) formulation. Dual formulation
    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
    n_samples &gt; n_features.

penalty : {&apos;l1&apos;, &apos;l2&apos;, &apos;elasticnet&apos;}, default=&apos;l2&apos;
    Specify the norm of the penalty:

    - `&apos;l2&apos;`: add a L2 penalty term (used by default);
    - `&apos;l1&apos;`: add a L1 penalty term;
    - `&apos;elasticnet&apos;`: both L1 and L2 penalty terms are added.

    .. warning::
       Some penalties may not work with some solvers. See the parameter
       `solver` below, to know the compatibility between the penalty and
       solver.

scoring : str or callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``. For a list of scoring functions
    that can be used, look at :mod:`sklearn.metrics`. The
    default scoring option used is &apos;accuracy&apos;.

solver : {&apos;lbfgs&apos;, &apos;liblinear&apos;, &apos;newton-cg&apos;, &apos;newton-cholesky&apos;, &apos;sag&apos;, &apos;saga&apos;},             default=&apos;lbfgs&apos;

    Algorithm to use in the optimization problem. Default is &apos;lbfgs&apos;.
    To choose a solver, you might want to consider the following aspects:

        - For small datasets, &apos;liblinear&apos; is a good choice, whereas &apos;sag&apos;
          and &apos;saga&apos; are faster for large ones;
        - For multiclass problems, only &apos;newton-cg&apos;, &apos;sag&apos;, &apos;saga&apos; and
          &apos;lbfgs&apos; handle multinomial loss;
        - &apos;liblinear&apos; might be slower in :class:`LogisticRegressionCV`
          because it does not handle warm-starting. &apos;liblinear&apos; is
          limited to one-versus-rest schemes.
        - &apos;newton-cholesky&apos; is a good choice for `n_samples` &gt;&gt; `n_features`,
          especially with one-hot encoded categorical features with rare
          categories. Note that it is limited to binary classification and the
          one-versus-rest reduction for multiclass classification. Be aware that
          the memory usage of this solver has a quadratic dependency on
          `n_features` because it explicitly computes the Hessian matrix.

    .. warning::
       The choice of the algorithm depends on the penalty chosen.
       Supported penalties by solver:

       - &apos;lbfgs&apos;           -   [&apos;l2&apos;]
       - &apos;liblinear&apos;       -   [&apos;l1&apos;, &apos;l2&apos;]
       - &apos;newton-cg&apos;       -   [&apos;l2&apos;]
       - &apos;newton-cholesky&apos; -   [&apos;l2&apos;]
       - &apos;sag&apos;             -   [&apos;l2&apos;]
       - &apos;saga&apos;            -   [&apos;elasticnet&apos;, &apos;l1&apos;, &apos;l2&apos;]

    .. note::
       &apos;sag&apos; and &apos;saga&apos; fast convergence is only guaranteed on features
       with approximately the same scale. You can preprocess the data with
       a scaler from :mod:`sklearn.preprocessing`.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.
    .. versionadded:: 1.2
       newton-cholesky solver.

tol : float, default=1e-4
    Tolerance for stopping criteria.

max_iter : int, default=100
    Maximum number of iterations of the optimization algorithm.

class_weight : dict or &apos;balanced&apos;, default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

    .. versionadded:: 0.17
       class_weight == &apos;balanced&apos;

n_jobs : int, default=None
    Number of CPU cores used during the cross-validation loop.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

verbose : int, default=0
    For the &apos;liblinear&apos;, &apos;sag&apos; and &apos;lbfgs&apos; solvers set verbose to any
    positive number for verbosity.

refit : bool, default=True
    If set to True, the scores are averaged across all folds, and the
    coefs and the C that corresponds to the best score is taken, and a
    final refit is done using these parameters.
    Otherwise the coefs, intercepts and C that correspond to the
    best scores across folds are averaged.

intercept_scaling : float, default=1
    Useful only when the solver &apos;liblinear&apos; is used
    and self.fit_intercept is set to True. In this case, x becomes
    [x, self.intercept_scaling],
    i.e. a &quot;synthetic&quot; feature with constant value equal to
    intercept_scaling is appended to the instance vector.
    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.

    Note! the synthetic feature weight is subject to l1/l2 regularization
    as all other features.
    To lessen the effect of regularization on synthetic feature weight
    (and therefore on the intercept) intercept_scaling has to be increased.

multi_class : {&apos;auto, &apos;ovr&apos;, &apos;multinomial&apos;}, default=&apos;auto&apos;
    If the option chosen is &apos;ovr&apos;, then a binary problem is fit for each
    label. For &apos;multinomial&apos; the loss minimised is the multinomial loss fit
    across the entire probability distribution, *even when the data is
    binary*. &apos;multinomial&apos; is unavailable when solver=&apos;liblinear&apos;.
    &apos;auto&apos; selects &apos;ovr&apos; if the data is binary, or if solver=&apos;liblinear&apos;,
    and otherwise selects &apos;multinomial&apos;.

    .. versionadded:: 0.18
       Stochastic Average Gradient descent solver for &apos;multinomial&apos; case.
    .. versionchanged:: 0.22
        Default changed from &apos;ovr&apos; to &apos;auto&apos; in 0.22.

random_state : int, RandomState instance, default=None
    Used when `solver=&apos;sag&apos;`, &apos;saga&apos; or &apos;liblinear&apos; to shuffle the data.
    Note that this only applies to the solver and not the cross-validation
    generator. See :term:`Glossary &lt;random_state&gt;` for details.

l1_ratios : list of float, default=None
    The list of Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``.
    Only used if ``penalty=&apos;elasticnet&apos;``. A value of 0 is equivalent to
    using ``penalty=&apos;l2&apos;``, while 1 is equivalent to using
    ``penalty=&apos;l1&apos;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a combination
    of L1 and L2.

Attributes
----------
classes_ : ndarray of shape (n_classes, )
    A list of class labels known to the classifier.

coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.

    `coef_` is of shape (1, n_features) when the given problem
    is binary.

intercept_ : ndarray of shape (1,) or (n_classes,)
    Intercept (a.k.a. bias) added to the decision function.

    If `fit_intercept` is set to False, the intercept is set to zero.
    `intercept_` is of shape(1,) when the problem is binary.

Cs_ : ndarray of shape (n_cs)
    Array of C i.e. inverse of regularization parameter values used
    for cross-validation.

l1_ratios_ : ndarray of shape (n_l1_ratios)
    Array of l1_ratios used for cross-validation. If no l1_ratio is used
    (i.e. penalty is not &apos;elasticnet&apos;), this is set to ``[None]``

coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)
    dict with classes as the keys, and the path of coefficients obtained
    during cross-validating across each fold and then across each Cs
    after doing an OvR for the corresponding class as values.
    If the &apos;multi_class&apos; option is set to &apos;multinomial&apos;, then
    the coefs_paths are the coefficients corresponding to each class.
    Each dict value has shape ``(n_folds, n_cs, n_features)`` or
    ``(n_folds, n_cs, n_features + 1)`` depending on whether the
    intercept is fit or not. If ``penalty=&apos;elasticnet&apos;``, the shape is
    ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or
    ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.

scores_ : dict
    dict with classes as the keys, and the values as the
    grid of scores obtained during cross-validating each fold, after doing
    an OvR for the corresponding class. If the &apos;multi_class&apos; option
    given is &apos;multinomial&apos; then the same scores are repeated across
    all classes, since this is the multinomial class. Each dict value
    has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if
    ``penalty=&apos;elasticnet&apos;``.

C_ : ndarray of shape (n_classes,) or (n_classes - 1,)
    Array of C that maps to the best scores across every class. If refit is
    set to False, then for each class, the best C is the average of the
    C&apos;s that correspond to the best scores for each fold.
    `C_` is of shape(n_classes,) when the problem is binary.

l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)
    Array of l1_ratio that maps to the best scores across every class. If
    refit is set to False, then for each class, the best l1_ratio is the
    average of the l1_ratio&apos;s that correspond to the best scores for each
    fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.

n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)
    Actual number of iterations for all classes, folds and Cs.
    In the binary or multinomial cases, the first dimension is equal to 1.
    If ``penalty=&apos;elasticnet&apos;``, the shape is ``(n_classes, n_folds,
    n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
LogisticRegression : Logistic regression without tuning the
    hyperparameter `C`.

Examples
--------
&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegressionCV
&gt;&gt;&gt; X, y = load_iris(return_X_y=True)
&gt;&gt;&gt; clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)
&gt;&gt;&gt; clf.predict(X[:2, :])
array([0, 0])
&gt;&gt;&gt; clf.predict_proba(X[:2, :]).shape
(2, 3)
&gt;&gt;&gt; clf.score(X, y)
0.98...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Logistic Regression (aka logit, MaxEnt) classifier.

In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the &apos;multi_class&apos; option is set to &apos;ovr&apos;, and uses the
cross-entropy loss if the &apos;multi_class&apos; option is set to &apos;multinomial&apos;.
(Currently the &apos;multinomial&apos; option is supported only by the &apos;lbfgs&apos;,
&apos;sag&apos;, &apos;saga&apos; and &apos;newton-cg&apos; solvers.)

This class implements regularized logistic regression using the
&apos;liblinear&apos; library, &apos;newton-cg&apos;, &apos;sag&apos;, &apos;saga&apos; and &apos;lbfgs&apos; solvers. **Note
that regularization is applied by default**. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).

The &apos;newton-cg&apos;, &apos;sag&apos;, and &apos;lbfgs&apos; solvers support only L2 regularization
with primal formulation, or no regularization. The &apos;liblinear&apos; solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
&apos;saga&apos; solver.

Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.

Parameters
----------
penalty : {&apos;l1&apos;, &apos;l2&apos;, &apos;elasticnet&apos;, None}, default=&apos;l2&apos;
    Specify the norm of the penalty:

    - `None`: no penalty is added;
    - `&apos;l2&apos;`: add a L2 penalty term and it is the default choice;
    - `&apos;l1&apos;`: add a L1 penalty term;
    - `&apos;elasticnet&apos;`: both L1 and L2 penalty terms are added.

    .. warning::
       Some penalties may not work with some solvers. See the parameter
       `solver` below, to know the compatibility between the penalty and
       solver.

    .. versionadded:: 0.19
       l1 penalty with SAGA solver (allowing &apos;multinomial&apos; + L1)

dual : bool, default=False
    Dual (constrained) or primal (regularized, see also
    :ref:`this equation &lt;regularized-logistic-loss&gt;`) formulation. Dual formulation
    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
    n_samples &gt; n_features.

tol : float, default=1e-4
    Tolerance for stopping criteria.

C : float, default=1.0
    Inverse of regularization strength; must be a positive float.
    Like in support vector machines, smaller values specify stronger
    regularization.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the decision function.

intercept_scaling : float, default=1
    Useful only when the solver &apos;liblinear&apos; is used
    and self.fit_intercept is set to True. In this case, x becomes
    [x, self.intercept_scaling],
    i.e. a &quot;synthetic&quot; feature with constant value equal to
    intercept_scaling is appended to the instance vector.
    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.

    Note! the synthetic feature weight is subject to l1/l2 regularization
    as all other features.
    To lessen the effect of regularization on synthetic feature weight
    (and therefore on the intercept) intercept_scaling has to be increased.

class_weight : dict or &apos;balanced&apos;, default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

    .. versionadded:: 0.17
       *class_weight=&apos;balanced&apos;*

random_state : int, RandomState instance, default=None
    Used when ``solver`` == &apos;sag&apos;, &apos;saga&apos; or &apos;liblinear&apos; to shuffle the
    data. See :term:`Glossary &lt;random_state&gt;` for details.

solver : {&apos;lbfgs&apos;, &apos;liblinear&apos;, &apos;newton-cg&apos;, &apos;newton-cholesky&apos;, &apos;sag&apos;, &apos;saga&apos;},             default=&apos;lbfgs&apos;

    Algorithm to use in the optimization problem. Default is &apos;lbfgs&apos;.
    To choose a solver, you might want to consider the following aspects:

        - For small datasets, &apos;liblinear&apos; is a good choice, whereas &apos;sag&apos;
          and &apos;saga&apos; are faster for large ones;
        - For multiclass problems, only &apos;newton-cg&apos;, &apos;sag&apos;, &apos;saga&apos; and
          &apos;lbfgs&apos; handle multinomial loss;
        - &apos;liblinear&apos; is limited to one-versus-rest schemes.
        - &apos;newton-cholesky&apos; is a good choice for `n_samples` &gt;&gt; `n_features`,
          especially with one-hot encoded categorical features with rare
          categories. Note that it is limited to binary classification and the
          one-versus-rest reduction for multiclass classification. Be aware that
          the memory usage of this solver has a quadratic dependency on
          `n_features` because it explicitly computes the Hessian matrix.

    .. warning::
       The choice of the algorithm depends on the penalty chosen.
       Supported penalties by solver:

       - &apos;lbfgs&apos;           -   [&apos;l2&apos;, None]
       - &apos;liblinear&apos;       -   [&apos;l1&apos;, &apos;l2&apos;]
       - &apos;newton-cg&apos;       -   [&apos;l2&apos;, None]
       - &apos;newton-cholesky&apos; -   [&apos;l2&apos;, None]
       - &apos;sag&apos;             -   [&apos;l2&apos;, None]
       - &apos;saga&apos;            -   [&apos;elasticnet&apos;, &apos;l1&apos;, &apos;l2&apos;, None]

    .. note::
       &apos;sag&apos; and &apos;saga&apos; fast convergence is only guaranteed on features
       with approximately the same scale. You can preprocess the data with
       a scaler from :mod:`sklearn.preprocessing`.

    .. seealso::
       Refer to the User Guide for more information regarding
       :class:`LogisticRegression` and more specifically the
       :ref:`Table &lt;Logistic_regression&gt;`
       summarizing solver/penalty supports.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.
    .. versionchanged:: 0.22
        The default solver changed from &apos;liblinear&apos; to &apos;lbfgs&apos; in 0.22.
    .. versionadded:: 1.2
       newton-cholesky solver.

max_iter : int, default=100
    Maximum number of iterations taken for the solvers to converge.

multi_class : {&apos;auto&apos;, &apos;ovr&apos;, &apos;multinomial&apos;}, default=&apos;auto&apos;
    If the option chosen is &apos;ovr&apos;, then a binary problem is fit for each
    label. For &apos;multinomial&apos; the loss minimised is the multinomial loss fit
    across the entire probability distribution, *even when the data is
    binary*. &apos;multinomial&apos; is unavailable when solver=&apos;liblinear&apos;.
    &apos;auto&apos; selects &apos;ovr&apos; if the data is binary, or if solver=&apos;liblinear&apos;,
    and otherwise selects &apos;multinomial&apos;.

    .. versionadded:: 0.18
       Stochastic Average Gradient descent solver for &apos;multinomial&apos; case.
    .. versionchanged:: 0.22
        Default changed from &apos;ovr&apos; to &apos;auto&apos; in 0.22.

verbose : int, default=0
    For the liblinear and lbfgs solvers set verbose to any positive
    number for verbosity.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    Useless for liblinear solver. See :term:`the Glossary &lt;warm_start&gt;`.

    .. versionadded:: 0.17
       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.

n_jobs : int, default=None
    Number of CPU cores used when parallelizing over classes if
    multi_class=&apos;ovr&apos;&quot;. This parameter is ignored when the ``solver`` is
    set to &apos;liblinear&apos; regardless of whether &apos;multi_class&apos; is specified or
    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors.
    See :term:`Glossary &lt;n_jobs&gt;` for more details.

l1_ratio : float, default=None
    The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only
    used if ``penalty=&apos;elasticnet&apos;``. Setting ``l1_ratio=0`` is equivalent
    to using ``penalty=&apos;l2&apos;``, while setting ``l1_ratio=1`` is equivalent
    to using ``penalty=&apos;l1&apos;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a
    combination of L1 and L2.

Attributes
----------

classes_ : ndarray of shape (n_classes, )
    A list of class labels known to the classifier.

coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.

    `coef_` is of shape (1, n_features) when the given problem is binary.
    In particular, when `multi_class=&apos;multinomial&apos;`, `coef_` corresponds
    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).

intercept_ : ndarray of shape (1,) or (n_classes,)
    Intercept (a.k.a. bias) added to the decision function.

    If `fit_intercept` is set to False, the intercept is set to zero.
    `intercept_` is of shape (1,) when the given problem is binary.
    In particular, when `multi_class=&apos;multinomial&apos;`, `intercept_`
    corresponds to outcome 1 (True) and `-intercept_` corresponds to
    outcome 0 (False).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : ndarray of shape (n_classes,) or (1, )
    Actual number of iterations for all classes. If binary or multinomial,
    it returns only 1 element. For liblinear solver, only the maximum
    number of iteration across all classes is given.

    .. versionchanged:: 0.20

        In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.

See Also
--------
SGDClassifier : Incrementally trained logistic regression (when given
    the parameter ``loss=&quot;log_loss&quot;``).
LogisticRegressionCV : Logistic regression with built-in cross validation.

Notes
-----
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.

Predict output may not match that of standalone liblinear in certain
cases. See :ref:`differences from liblinear &lt;liblinear_differences&gt;`
in the narrative documentation.

References
----------

L-BFGS-B -- Software for Large-scale Bound-constrained Optimization
    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html

LIBLINEAR -- A Library for Large Linear Classification
    https://www.csie.ntu.edu.tw/~cjlin/liblinear/

SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach
    Minimizing Finite Sums with the Stochastic Average Gradient
    https://hal.inria.fr/hal-00860051/document

SAGA -- Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).
        :arxiv:`&quot;SAGA: A Fast Incremental Gradient Method With Support
        for Non-Strongly Convex Composite Objectives&quot; &lt;1407.0202&gt;`

Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent
    methods for logistic regression and maximum entropy models.
    Machine Learning 85(1-2):41-75.
    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf

Examples
--------
&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
&gt;&gt;&gt; X, y = load_iris(return_X_y=True)
&gt;&gt;&gt; clf = LogisticRegression(random_state=0).fit(X, y)
&gt;&gt;&gt; clf.predict(X[:2, :])
array([0, 0])
&gt;&gt;&gt; clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
&gt;&gt;&gt; clf.score(X, y)
0.97...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment></rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Multi-task L1/L2 ElasticNet with built-in cross-validation.

See glossary entry for :term:`cross-validation estimator`.

The optimization objective for MultiTaskElasticNet is::

    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2
    + alpha * l1_ratio * ||W||_21
    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2

Where::

    ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}

i.e. the sum of norm of each row.

Read more in the :ref:`User Guide &lt;multi_task_elastic_net&gt;`.

.. versionadded:: 0.15

Parameters
----------
l1_ratio : float or list of float, default=0.5
    The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
    is an L2 penalty.
    For ``0 &lt; l1_ratio &lt; 1``, the penalty is a combination of L1/L2 and L2.
    This parameter can be a list, in which case the different
    values are tested by cross-validation and the one giving the best
    prediction score is used. Note that a good choice of list of
    values for l1_ratio is often to put more values close to 1
    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,
    .9, .95, .99, 1]``.

eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If not provided, set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

verbose : bool or int, default=0
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation. Note that this is
    used only if multiple values for l1_ratio are given.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula).
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

alpha_ : float
    The amount of penalization chosen by cross validation.

mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)
    Mean square error for the test set on each fold, varying alpha.

alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)
    The grid of alphas used for fitting, for each l1_ratio.

l1_ratio_ : float
    Best l1_ratio obtained by cross-validation.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

dual_gap_ : float
    The dual gap at the end of the optimization for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.
ElasticNetCV : Elastic net model with best model selection by
    cross-validation.
MultiTaskLassoCV : Multi-task Lasso model trained with L1 norm
    as regularizer and built-in cross-validation.

Notes
-----
The algorithm used to fit the model is coordinate descent.

In `fit`, once the best parameters `l1_ratio` and `alpha` are found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` and `y` arguments of the
`fit` method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.MultiTaskElasticNetCV(cv=3)
&gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]],
...         [[0, 0], [1, 1], [2, 2]])
MultiTaskElasticNetCV(cv=3)
&gt;&gt;&gt; print(clf.coef_)
[[0.52875032 0.46958558]
 [0.52875032 0.46958558]]
&gt;&gt;&gt; print(clf.intercept_)
[0.00166409 0.00166409]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.

The optimization objective for MultiTaskElasticNet is::

    (1 / (2 * n_samples)) * ||Y - XW||_Fro^2
    + alpha * l1_ratio * ||W||_21
    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2

Where::

    ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)

i.e. the sum of norms of each row.

Read more in the :ref:`User Guide &lt;multi_task_elastic_net&gt;`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the L1/L2 term. Defaults to 1.0.

l1_ratio : float, default=0.5
    The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
    is an L2 penalty.
    For ``0 &lt; l1_ratio &lt; 1``, the penalty is a combination of L1/L2 and L2.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula). If a 1D y is
    passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

dual_gap_ : float
    The dual gaps at the end of the optimization.

eps_ : float
    The tolerance scaled scaled by the variance of the target `y`.

sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)
    Sparse representation of the `coef_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in
    cross-validation.
ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.
MultiTaskLasso : Multi-task Lasso model trained with L1/L2
    mixed-norm as regularizer.

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.MultiTaskElasticNet(alpha=0.1)
&gt;&gt;&gt; clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
MultiTaskElasticNet(alpha=0.1)
&gt;&gt;&gt; print(clf.coef_)
[[0.45663524 0.45612256]
 [0.45663524 0.45612256]]
&gt;&gt;&gt; print(clf.intercept_)
[0.0872422 0.0872422]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.

See glossary entry for :term:`cross-validation estimator`.

The optimization objective for MultiTaskLasso is::

    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21

Where::

    ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}

i.e. the sum of norm of each row.

Read more in the :ref:`User Guide &lt;multi_task_lasso&gt;`.

.. versionadded:: 0.15

Parameters
----------
eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If not provided, set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

verbose : bool or int, default=False
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation. Note that this is
    used only if multiple values for l1_ratio are given.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula).
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

alpha_ : float
    The amount of penalization chosen by cross validation.

mse_path_ : ndarray of shape (n_alphas, n_folds)
    Mean square error for the test set on each fold, varying alpha.

alphas_ : ndarray of shape (n_alphas,)
    The grid of alphas used for fitting.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

dual_gap_ : float
    The dual gap at the end of the optimization for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2
    mixed-norm as regularizer.
ElasticNetCV : Elastic net model with best model selection by
    cross-validation.
MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in
    cross-validation.

Notes
-----
The algorithm used to fit the model is coordinate descent.

In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` and `y` arguments of the
`fit` method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import MultiTaskLassoCV
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; from sklearn.metrics import r2_score
&gt;&gt;&gt; X, y = make_regression(n_targets=2, noise=4, random_state=0)
&gt;&gt;&gt; reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)
&gt;&gt;&gt; r2_score(y, reg.predict(X))
0.9994...
&gt;&gt;&gt; reg.alpha_
0.5713...
&gt;&gt;&gt; reg.predict(X[:1,])
array([[153.7971...,  94.9015...]])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21

Where::

    ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}

i.e. the sum of norm of each row.

Read more in the :ref:`User Guide &lt;multi_task_lasso&gt;`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the L1/L2 term. Defaults to 1.0.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == &apos;random&apos;.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

selection : {&apos;cyclic&apos;, &apos;random&apos;}, default=&apos;cyclic&apos;
    If set to &apos;random&apos;, a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to &apos;random&apos;) often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula).
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

dual_gap_ : ndarray of shape (n_alphas,)
    The dual gaps at the end of the optimization for each alpha.

eps_ : float
    The tolerance scaled scaled by the variance of the target `y`.

sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)
    Sparse representation of the `coef_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).
MultiTaskLassoCV: Multi-task L1 regularized linear model with built-in
    cross-validation.
MultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.MultiTaskLasso(alpha=0.1)
&gt;&gt;&gt; clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])
MultiTaskLasso(alpha=0.1)
&gt;&gt;&gt; print(clf.coef_)
[[0.         0.60809415]
[0.         0.94592424]]
&gt;&gt;&gt; print(clf.intercept_)
[-0.41888636 -0.87382323]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Cross-validated Orthogonal Matching Pursuit model (OMP).

See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide &lt;omp&gt;`.

Parameters
----------
copy : bool, default=True
    Whether the design matrix X must be copied by the algorithm. A false
    value is only helpful if X is already Fortran-ordered, otherwise a
    copy is made anyway.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

max_iter : int, default=None
    Maximum numbers of iterations to perform, therefore maximum features
    to include. 10% of ``n_features`` but at least 5 if available.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

verbose : bool or int, default=False
    Sets the verbosity amount.

Attributes
----------
intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the problem formulation).

n_nonzero_coefs_ : int
    Estimated number of non-zero coefficients giving the best mean squared
    error over the cross-validation folds.

n_iter_ : int or array-like
    Number of active features across every target for the model refit with
    the best hyperparameters got by cross-validating across all folds.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.
orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit
    problems using only the Gram matrix X.T * X and the product X.T * y.
lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).
LarsCV : Cross-validated Least Angle Regression model.
LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.
sklearn.decomposition.sparse_encode : Generic sparse coding.
    Each column of the result is the solution to a Lasso problem.

Notes
-----
In `fit`, once the optimal number of non-zero coefficients is found through
cross-validation, the model is fit again using the entire training set.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import OrthogonalMatchingPursuitCV
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(n_features=100, n_informative=10,
...                        noise=4, random_state=0)
&gt;&gt;&gt; reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9991...
&gt;&gt;&gt; reg.n_nonzero_coefs_
10
&gt;&gt;&gt; reg.predict(X[:1,])
array([-78.3854...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Orthogonal Matching Pursuit model (OMP).

Read more in the :ref:`User Guide &lt;omp&gt;`.

Parameters
----------
n_nonzero_coefs : int, default=None
    Desired number of non-zero entries in the solution. If None (by
    default) this value is set to 10% of n_features.

tol : float, default=None
    Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : &apos;auto&apos; or bool, default=&apos;auto&apos;
    Whether to use a precomputed Gram and Xy matrix to speed up
    calculations. Improves performance when :term:`n_targets` or
    :term:`n_samples` is very large. Note that if you already have such
    matrices, you can pass them directly to the fit method.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the formula).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : int or array-like
    Number of active features across every target.

n_nonzero_coefs_ : int
    The number of non-zero coefficients in the solution. If
    `n_nonzero_coefs` is None and `tol` is None this value is either set
    to 10% of `n_features` or 1, whichever is greater.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.
orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit
    problems using only the Gram matrix X.T * X and the product X.T * y.
lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
sklearn.decomposition.sparse_encode : Generic sparse coding.
    Each column of the result is the solution to a Lasso problem.
OrthogonalMatchingPursuitCV : Cross-validated
    Orthogonal Matching Pursuit model (OMP).

Notes
-----
Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)

This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import OrthogonalMatchingPursuit
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(noise=4, random_state=0)
&gt;&gt;&gt; reg = OrthogonalMatchingPursuit().fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9991...
&gt;&gt;&gt; reg.predict(X[:1,])
array([-78.3854...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Passive Aggressive Classifier.

Read more in the :ref:`User Guide &lt;passive_aggressive&gt;`.

Parameters
----------
C : float, default=1.0
    Maximum step size (regularization). Defaults to 1.0.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss &gt; previous_loss - tol).

    .. versionadded:: 0.19

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set aside
    a stratified fraction of training data as validation and terminate
    training when validation score is not improving by at least `tol` for
    `n_iter_no_change` consecutive epochs.

    .. versionadded:: 0.20

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

    .. versionadded:: 0.20

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before early stopping.

    .. versionadded:: 0.20

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.

loss : str, default=&quot;hinge&quot;
    The loss function to be used:
    hinge: equivalent to PA-I in the reference paper.
    squared_hinge: equivalent to PA-II in the reference paper.

n_jobs : int or None, default=None
    The number of CPUs to use to do the OVA (One Versus All, for
    multi-class problems) computation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

random_state : int, RandomState instance, default=None
    Used to shuffle the training data, when ``shuffle`` is set to
    ``True``. Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary &lt;random_state&gt;`.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.

class_weight : dict, {class_label: weight} or &quot;balanced&quot; or None,             default=None
    Preset for the class_weight fit parameter.

    Weights associated with classes. If not given, all classes
    are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    .. versionadded:: 0.17
       parameter *class_weight* to automatically weight samples.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights and stores the
    result in the ``coef_`` attribute. If set to an int greater than 1,
    averaging will begin once the total number of samples seen reaches
    average. So average=10 will begin averaging after seeing 10 samples.

    .. versionadded:: 0.19
       parameter *average* to use weights averaging in SGD.

Attributes
----------
coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.
    For multiclass fits, it is the maximum over every binary fit.

classes_ : ndarray of shape (n_classes,)
    The unique classes labels.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

loss_function_ : callable
    Loss function used by the algorithm.

See Also
--------
SGDClassifier : Incrementally trained logistic regression.
Perceptron : Linear perceptron classifier.

References
----------
Online Passive-Aggressive Algorithms
&lt;http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import PassiveAggressiveClassifier
&gt;&gt;&gt; from sklearn.datasets import make_classification
&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)
&gt;&gt;&gt; clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,
... tol=1e-3)
&gt;&gt;&gt; clf.fit(X, y)
PassiveAggressiveClassifier(random_state=0)
&gt;&gt;&gt; print(clf.coef_)
[[0.26642044 0.45070924 0.67251877 0.64185414]]
&gt;&gt;&gt; print(clf.intercept_)
[1.84127814]
&gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))
[1]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Passive Aggressive Regressor.

Read more in the :ref:`User Guide &lt;passive_aggressive&gt;`.

Parameters
----------

C : float, default=1.0
    Maximum step size (regularization). Defaults to 1.0.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered. Defaults to True.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss &gt; previous_loss - tol).

    .. versionadded:: 0.19

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation.
    score is not improving. If set to True, it will automatically set aside
    a fraction of training data as validation and terminate
    training when validation score is not improving by at least tol for
    n_iter_no_change consecutive epochs.

    .. versionadded:: 0.20

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

    .. versionadded:: 0.20

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before early stopping.

    .. versionadded:: 0.20

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.

loss : str, default=&quot;epsilon_insensitive&quot;
    The loss function to be used:
    epsilon_insensitive: equivalent to PA-I in the reference paper.
    squared_epsilon_insensitive: equivalent to PA-II in the reference
    paper.

epsilon : float, default=0.1
    If the difference between the current prediction and the correct label
    is below this threshold, the model is not updated.

random_state : int, RandomState instance, default=None
    Used to shuffle the training data, when ``shuffle`` is set to
    ``True``. Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary &lt;random_state&gt;`.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights and stores the
    result in the ``coef_`` attribute. If set to an int greater than 1,
    averaging will begin once the total number of samples seen reaches
    average. So average=10 will begin averaging after seeing 10 samples.

    .. versionadded:: 0.19
       parameter *average* to use weights averaging in SGD.

Attributes
----------
coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]
    Weights assigned to the features.

intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

See Also
--------
SGDRegressor : Linear model fitted by minimizing a regularized
    empirical loss with SGD.

References
----------
Online Passive-Aggressive Algorithms
&lt;http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import PassiveAggressiveRegressor
&gt;&gt;&gt; from sklearn.datasets import make_regression

&gt;&gt;&gt; X, y = make_regression(n_features=4, random_state=0)
&gt;&gt;&gt; regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,
... tol=1e-3)
&gt;&gt;&gt; regr.fit(X, y)
PassiveAggressiveRegressor(max_iter=100, random_state=0)
&gt;&gt;&gt; print(regr.coef_)
[20.48736655 34.18818427 67.59122734 87.94731329]
&gt;&gt;&gt; print(regr.intercept_)
[-0.02306214]
&gt;&gt;&gt; print(regr.predict([[0, 0, 0, 0]]))
[-0.02306214]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear perceptron classifier.

The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier`
by fixing the `loss` and `learning_rate` parameters as::

    SGDClassifier(loss=&quot;perceptron&quot;, learning_rate=&quot;constant&quot;)

Other available parameters are described below and are forwarded to
:class:`~sklearn.linear_model.SGDClassifier`.

Read more in the :ref:`User Guide &lt;perceptron&gt;`.

Parameters
----------

penalty : {&apos;l2&apos;,&apos;l1&apos;,&apos;elasticnet&apos;}, default=None
    The penalty (aka regularization term) to be used.

alpha : float, default=0.0001
    Constant that multiplies the regularization term if regularization is
    used.

l1_ratio : float, default=0.15
    The Elastic Net mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`.
    `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.
    Only used if `penalty=&apos;elasticnet&apos;`.

    .. versionadded:: 0.24

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`partial_fit` method.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss &gt; previous_loss - tol).

    .. versionadded:: 0.19

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.

eta0 : float, default=1
    Constant by which the updates are multiplied.

n_jobs : int, default=None
    The number of CPUs to use to do the OVA (One Versus All, for
    multi-class problems) computation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

random_state : int, RandomState instance or None, default=0
    Used to shuffle the training data, when ``shuffle`` is set to
    ``True``. Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary &lt;random_state&gt;`.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set aside
    a stratified fraction of training data as validation and terminate
    training when validation score is not improving by at least `tol` for
    `n_iter_no_change` consecutive epochs.

    .. versionadded:: 0.20

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

    .. versionadded:: 0.20

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before early stopping.

    .. versionadded:: 0.20

class_weight : dict, {class_label: weight} or &quot;balanced&quot;, default=None
    Preset for the class_weight fit parameter.

    Weights associated with classes. If not given, all classes
    are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution. See
    :term:`the Glossary &lt;warm_start&gt;`.

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    The unique classes labels.

coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

loss_function_ : concrete LossFunction
    The function that determines the loss, or difference between the
    output of the algorithm and the target values.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.
    For multiclass fits, it is the maximum over every binary fit.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

See Also
--------
sklearn.linear_model.SGDClassifier : Linear classifiers
    (SVM, logistic regression, etc.) with SGD training.

Notes
-----
``Perceptron`` is a classification algorithm which shares the same
underlying implementation with ``SGDClassifier``. In fact,
``Perceptron()`` is equivalent to `SGDClassifier(loss=&quot;perceptron&quot;,
eta0=1, learning_rate=&quot;constant&quot;, penalty=None)`.

References
----------
https://en.wikipedia.org/wiki/Perceptron and references therein.

Examples
--------
&gt;&gt;&gt; from sklearn.datasets import load_digits
&gt;&gt;&gt; from sklearn.linear_model import Perceptron
&gt;&gt;&gt; X, y = load_digits(return_X_y=True)
&gt;&gt;&gt; clf = Perceptron(tol=1e-3, random_state=0)
&gt;&gt;&gt; clf.fit(X, y)
Perceptron()
&gt;&gt;&gt; clf.score(X, y)
0.939...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Generalized Linear Model with a Poisson distribution.

This regressor uses the &apos;log&apos; link function.

Read more in the :ref:`User Guide &lt;Generalized_linear_models&gt;`.

.. versionadded:: 0.23

Parameters
----------
alpha : float, default=1
    Constant that multiplies the L2 penalty term and determines the
    regularization strength. ``alpha = 0`` is equivalent to unpenalized
    GLMs. In this case, the design matrix `X` must have full column rank
    (no collinearities).
    Values of `alpha` must be in the range `[0.0, inf)`.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the linear predictor (`X @ coef + intercept`).

solver : {&apos;lbfgs&apos;, &apos;newton-cholesky&apos;}, default=&apos;lbfgs&apos;
    Algorithm to use in the optimization problem:

    &apos;lbfgs&apos;
        Calls scipy&apos;s L-BFGS-B optimizer.

    &apos;newton-cholesky&apos;
        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to
        iterated reweighted least squares) with an inner Cholesky based solver.
        This solver is a good choice for `n_samples` &gt;&gt; `n_features`, especially
        with one-hot encoded categorical features with rare categories. Be aware
        that the memory usage of this solver has a quadratic dependency on
        `n_features` because it explicitly computes the Hessian matrix.

        .. versionadded:: 1.2

max_iter : int, default=100
    The maximal number of iterations for the solver.
    Values must be in the range `[1, inf)`.

tol : float, default=1e-4
    Stopping criterion. For the lbfgs solver,
    the iteration will stop when ``max{|g_j|, j = 1, ..., d} &lt;= tol``
    where ``g_j`` is the j-th component of the gradient (derivative) of
    the objective function.
    Values must be in the range `(0.0, inf)`.

warm_start : bool, default=False
    If set to ``True``, reuse the solution of the previous call to ``fit``
    as initialization for ``coef_`` and ``intercept_`` .

verbose : int, default=0
    For the lbfgs solver set verbose to any positive number for verbosity.
    Values must be in the range `[0, inf)`.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the linear predictor (`X @ coef_ +
    intercept_`) in the GLM.

intercept_ : float
    Intercept (a.k.a. bias) added to linear predictor.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Actual number of iterations used in the solver.

See Also
--------
TweedieRegressor : Generalized Linear Model with a Tweedie distribution.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.PoissonRegressor()
&gt;&gt;&gt; X = [[1, 2], [2, 3], [3, 4], [4, 3]]
&gt;&gt;&gt; y = [12, 17, 22, 21]
&gt;&gt;&gt; clf.fit(X, y)
PoissonRegressor()
&gt;&gt;&gt; clf.score(X, y)
0.990...
&gt;&gt;&gt; clf.coef_
array([0.121..., 0.158...])
&gt;&gt;&gt; clf.intercept_
2.088...
&gt;&gt;&gt; clf.predict([[1, 1], [3, 4]])
array([10.676..., 21.875...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear regression model that predicts conditional quantiles.

The linear :class:`QuantileRegressor` optimizes the pinball loss for a
desired `quantile` and is robust to outliers.

This model uses an L1 regularization like
:class:`~sklearn.linear_model.Lasso`.

Read more in the :ref:`User Guide &lt;quantile_regression&gt;`.

.. versionadded:: 1.0

Parameters
----------
quantile : float, default=0.5
    The quantile that the model tries to predict. It must be strictly
    between 0 and 1. If 0.5 (default), the model predicts the 50%
    quantile, i.e. the median.

alpha : float, default=1.0
    Regularization constant that multiplies the L1 penalty term.

fit_intercept : bool, default=True
    Whether or not to fit the intercept.

solver : {&apos;highs-ds&apos;, &apos;highs-ipm&apos;, &apos;highs&apos;, &apos;interior-point&apos;,             &apos;revised simplex&apos;}, default=&apos;highs&apos;
    Method used by :func:`scipy.optimize.linprog` to solve the linear
    programming formulation.

    From `scipy&gt;=1.6.0`, it is recommended to use the highs methods because
    they are the fastest ones. Solvers &quot;highs-ds&quot;, &quot;highs-ipm&quot; and &quot;highs&quot;
    support sparse input data and, in fact, always convert to sparse csc.

    From `scipy&gt;=1.11.0`, &quot;interior-point&quot; is not available anymore.

    .. versionchanged:: 1.4
       The default of `solver` changed to `&quot;highs&quot;` in version 1.4.

solver_options : dict, default=None
    Additional parameters passed to :func:`scipy.optimize.linprog` as
    options. If `None` and if `solver=&apos;interior-point&apos;`, then
    `{&quot;lstsq&quot;: True}` is passed to :func:`scipy.optimize.linprog` for the
    sake of stability.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the features.

intercept_ : float
    The intercept of the model, aka bias term.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations performed by the solver.

See Also
--------
Lasso : The Lasso is a linear model that estimates sparse coefficients
    with l1 regularization.
HuberRegressor : Linear regression model that is robust to outliers.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import QuantileRegressor
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; n_samples, n_features = 10, 2
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; y = rng.randn(n_samples)
&gt;&gt;&gt; X = rng.randn(n_samples, n_features)
&gt;&gt;&gt; # the two following lines are optional in practice
&gt;&gt;&gt; from sklearn.utils.fixes import sp_version, parse_version
&gt;&gt;&gt; solver = &quot;highs&quot; if sp_version &gt;= parse_version(&quot;1.6.0&quot;) else &quot;interior-point&quot;
&gt;&gt;&gt; reg = QuantileRegressor(quantile=0.8, solver=solver).fit(X, y)
&gt;&gt;&gt; np.mean(y &lt;= reg.predict(X))
0.8</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>RANSAC (RANdom SAmple Consensus) algorithm.

RANSAC is an iterative algorithm for the robust estimation of parameters
from a subset of inliers from the complete data set.

Read more in the :ref:`User Guide &lt;ransac_regression&gt;`.

Parameters
----------
estimator : object, default=None
    Base estimator object which implements the following methods:

     * `fit(X, y)`: Fit model to given training data and target values.
     * `score(X, y)`: Returns the mean accuracy on the given test data,
       which is used for the stop criterion defined by `stop_score`.
       Additionally, the score is used to decide which of two equally
       large consensus sets is chosen as the better one.
     * `predict(X)`: Returns predicted values using the linear model,
       which is used to compute residual error using loss function.

    If `estimator` is None, then
    :class:`~sklearn.linear_model.LinearRegression` is used for
    target values of dtype float.

    Note that the current implementation only supports regression
    estimators.

min_samples : int (&gt;= 1) or float ([0, 1]), default=None
    Minimum number of samples chosen randomly from original data. Treated
    as an absolute number of samples for `min_samples &gt;= 1`, treated as a
    relative number `ceil(min_samples * X.shape[0])` for
    `min_samples &lt; 1`. This is typically chosen as the minimal number of
    samples necessary to estimate the given `estimator`. By default a
    :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and
    `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly
    dependent upon the model, so if a `estimator` other than
    :class:`~sklearn.linear_model.LinearRegression` is used, the user must
    provide a value.

residual_threshold : float, default=None
    Maximum residual for a data sample to be classified as an inlier.
    By default the threshold is chosen as the MAD (median absolute
    deviation) of the target values `y`. Points whose residuals are
    strictly equal to the threshold are considered as inliers.

is_data_valid : callable, default=None
    This function is called with the randomly selected data before the
    model is fitted to it: `is_data_valid(X, y)`. If its return value is
    False the current randomly chosen sub-sample is skipped.

is_model_valid : callable, default=None
    This function is called with the estimated model and the randomly
    selected data: `is_model_valid(model, X, y)`. If its return value is
    False the current randomly chosen sub-sample is skipped.
    Rejecting samples with this function is computationally costlier than
    with `is_data_valid`. `is_model_valid` should therefore only be used if
    the estimated model is needed for making the rejection decision.

max_trials : int, default=100
    Maximum number of iterations for random sample selection.

max_skips : int, default=np.inf
    Maximum number of iterations that can be skipped due to finding zero
    inliers or invalid data defined by ``is_data_valid`` or invalid models
    defined by ``is_model_valid``.

    .. versionadded:: 0.19

stop_n_inliers : int, default=np.inf
    Stop iteration if at least this number of inliers are found.

stop_score : float, default=np.inf
    Stop iteration if score is greater equal than this threshold.

stop_probability : float in range [0, 1], default=0.99
    RANSAC iteration stops if at least one outlier-free set of the training
    data is sampled in RANSAC. This requires to generate at least N
    samples (iterations)::

        N &gt;= log(1 - probability) / log(1 - e**m)

    where the probability (confidence) is typically set to high value such
    as 0.99 (the default) and e is the current fraction of inliers w.r.t.
    the total number of samples.

loss : str, callable, default=&apos;absolute_error&apos;
    String inputs, &apos;absolute_error&apos; and &apos;squared_error&apos; are supported which
    find the absolute error and squared error per sample respectively.

    If ``loss`` is a callable, then it should be a function that takes
    two arrays as inputs, the true and predicted value and returns a 1-D
    array with the i-th value of the array corresponding to the loss
    on ``X[i]``.

    If the loss on a sample is greater than the ``residual_threshold``,
    then this sample is classified as an outlier.

    .. versionadded:: 0.18

random_state : int, RandomState instance, default=None
    The generator used to initialize the centers.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

Attributes
----------
estimator_ : object
    Best fitted model (copy of the `estimator` object).

n_trials_ : int
    Number of random selection trials until one of the stop criteria is
    met. It is always ``&lt;= max_trials``.

inlier_mask_ : bool array of shape [n_samples]
    Boolean mask of inliers classified as ``True``.

n_skips_no_inliers_ : int
    Number of iterations skipped due to finding zero inliers.

    .. versionadded:: 0.19

n_skips_invalid_data_ : int
    Number of iterations skipped due to invalid data defined by
    ``is_data_valid``.

    .. versionadded:: 0.19

n_skips_invalid_model_ : int
    Number of iterations skipped due to an invalid model defined by
    ``is_model_valid``.

    .. versionadded:: 0.19

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
HuberRegressor : Linear regression model that is robust to outliers.
TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.
SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.

References
----------
.. [1] https://en.wikipedia.org/wiki/RANSAC
.. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf
.. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import RANSACRegressor
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(
...     n_samples=200, n_features=2, noise=4.0, random_state=0)
&gt;&gt;&gt; reg = RANSACRegressor(random_state=0).fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9885...
&gt;&gt;&gt; reg.predict(X[:1,])
array([-31.9417...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Ridge regression with built-in cross-validation.

See glossary entry for :term:`cross-validation estimator`.

By default, it performs efficient Leave-One-Out Cross-Validation.

Read more in the :ref:`User Guide &lt;ridge_regression&gt;`.

Parameters
----------
alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)
    Array of alpha values to try.
    Regularization strength; must be a positive float. Regularization
    improves the conditioning of the problem and reduces the variance of
    the estimates. Larger values specify stronger regularization.
    Alpha corresponds to ``1 / (2C)`` in other linear models such as
    :class:`~sklearn.linear_model.LogisticRegression` or
    :class:`~sklearn.svm.LinearSVC`.
    If using Leave-One-Out cross-validation, alphas must be positive.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

scoring : str, callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.
    If None, the negative mean squared error if cv is &apos;auto&apos; or None
    (i.e. when using leave-one-out cross-validation), and r2 score
    otherwise.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the efficient Leave-One-Out cross-validation
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if ``y`` is binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used, else,
    :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

gcv_mode : {&apos;auto&apos;, &apos;svd&apos;, &apos;eigen&apos;}, default=&apos;auto&apos;
    Flag indicating which strategy to use when performing
    Leave-One-Out Cross-Validation. Options are::

        &apos;auto&apos; : use &apos;svd&apos; if n_samples &gt; n_features, otherwise use &apos;eigen&apos;
        &apos;svd&apos; : force use of singular value decomposition of X when X is
            dense, eigenvalue decomposition of X^T.X when X is sparse.
        &apos;eigen&apos; : force computation via eigendecomposition of X.X^T

    The &apos;auto&apos; mode is the default and is intended to pick the cheaper
    option of the two depending on the shape of the training data.

store_cv_values : bool, default=False
    Flag indicating if the cross-validation values corresponding to
    each alpha should be stored in the ``cv_values_`` attribute (see
    below). This flag is only compatible with ``cv=None`` (i.e. using
    Leave-One-Out Cross-Validation).

alpha_per_target : bool, default=False
    Flag indicating whether to optimize the alpha value (picked from the
    `alphas` parameter list) for each target separately (for multi-output
    settings: multiple prediction targets). When set to `True`, after
    fitting, the `alpha_` attribute will contain a value for each target.
    When set to `False`, a single alpha is used for all targets.

    .. versionadded:: 0.24

Attributes
----------
cv_values_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional
    Cross-validation values for each alpha (only available if
    ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been
    called, this attribute will contain the mean squared errors if
    `scoring is None` otherwise it will contain standardized per point
    prediction values.

coef_ : ndarray of shape (n_features) or (n_targets, n_features)
    Weight vector(s).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

alpha_ : float or ndarray of shape (n_targets,)
    Estimated regularization parameter, or, if ``alpha_per_target=True``,
    the estimated regularization parameter for each target.

best_score_ : float or ndarray of shape (n_targets,)
    Score of base estimator with best alpha, or, if
    ``alpha_per_target=True``, a score for each target.

    .. versionadded:: 0.23

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression.
RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.
RidgeClassifierCV : Ridge classifier with built-in cross validation.

Examples
--------
&gt;&gt;&gt; from sklearn.datasets import load_diabetes
&gt;&gt;&gt; from sklearn.linear_model import RidgeCV
&gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)
&gt;&gt;&gt; clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
&gt;&gt;&gt; clf.score(X, y)
0.5166...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Ridge classifier with built-in cross-validation.

See glossary entry for :term:`cross-validation estimator`.

By default, it performs Leave-One-Out Cross-Validation. Currently,
only the n_features &gt; n_samples case is handled efficiently.

Read more in the :ref:`User Guide &lt;ridge_regression&gt;`.

Parameters
----------
alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)
    Array of alpha values to try.
    Regularization strength; must be a positive float. Regularization
    improves the conditioning of the problem and reduces the variance of
    the estimates. Larger values specify stronger regularization.
    Alpha corresponds to ``1 / (2C)`` in other linear models such as
    :class:`~sklearn.linear_model.LogisticRegression` or
    :class:`~sklearn.svm.LinearSVC`.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

scoring : str, callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the efficient Leave-One-Out cross-validation
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
    cross-validation strategies that can be used here.

class_weight : dict or &apos;balanced&apos;, default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

store_cv_values : bool, default=False
    Flag indicating if the cross-validation values corresponding to
    each alpha should be stored in the ``cv_values_`` attribute (see
    below). This flag is only compatible with ``cv=None`` (i.e. using
    Leave-One-Out Cross-Validation).

Attributes
----------
cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional
    Cross-validation values for each alpha (only if ``store_cv_values=True`` and
    ``cv=None``). After ``fit()`` has been called, this attribute will
    contain the mean squared errors if `scoring is None` otherwise it
    will contain standardized per point prediction values.

coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)
    Coefficient of the features in the decision function.

    ``coef_`` is of shape (1, n_features) when the given problem is binary.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

alpha_ : float
    Estimated regularization parameter.

best_score_ : float
    Score of base estimator with best alpha.

    .. versionadded:: 0.23

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression.
RidgeClassifier : Ridge classifier.
RidgeCV : Ridge regression with built-in cross validation.

Notes
-----
For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.

Examples
--------
&gt;&gt;&gt; from sklearn.datasets import load_breast_cancer
&gt;&gt;&gt; from sklearn.linear_model import RidgeClassifierCV
&gt;&gt;&gt; X, y = load_breast_cancer(return_X_y=True)
&gt;&gt;&gt; clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
&gt;&gt;&gt; clf.score(X, y)
0.9630...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Classifier using Ridge regression.

This classifier first converts the target values into ``{-1, 1}`` and
then treats the problem as a regression task (multi-output regression in
the multiclass case).

Read more in the :ref:`User Guide &lt;ridge_regression&gt;`.

Parameters
----------
alpha : float, default=1.0
    Regularization strength; must be a positive float. Regularization
    improves the conditioning of the problem and reduces the variance of
    the estimates. Larger values specify stronger regularization.
    Alpha corresponds to ``1 / (2C)`` in other linear models such as
    :class:`~sklearn.linear_model.LogisticRegression` or
    :class:`~sklearn.svm.LinearSVC`.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set to false, no
    intercept will be used in calculations (e.g. data is expected to be
    already centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

max_iter : int, default=None
    Maximum number of iterations for conjugate gradient solver.
    The default value is determined by scipy.sparse.linalg.

tol : float, default=1e-4
    The precision of the solution (`coef_`) is determined by `tol` which
    specifies a different convergence criterion for each solver:

    - &apos;svd&apos;: `tol` has no impact.

    - &apos;cholesky&apos;: `tol` has no impact.

    - &apos;sparse_cg&apos;: norm of residuals smaller than `tol`.

    - &apos;lsqr&apos;: `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,
      which control the norm of the residual vector in terms of the norms of
      matrix and coefficients.

    - &apos;sag&apos; and &apos;saga&apos;: relative change of coef smaller than `tol`.

    - &apos;lbfgs&apos;: maximum of the absolute (projected) gradient=max|residuals|
      smaller than `tol`.

    .. versionchanged:: 1.2
       Default value changed from 1e-3 to 1e-4 for consistency with other linear
       models.

class_weight : dict or &apos;balanced&apos;, default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

solver : {&apos;auto&apos;, &apos;svd&apos;, &apos;cholesky&apos;, &apos;lsqr&apos;, &apos;sparse_cg&apos;,             &apos;sag&apos;, &apos;saga&apos;, &apos;lbfgs&apos;}, default=&apos;auto&apos;
    Solver to use in the computational routines:

    - &apos;auto&apos; chooses the solver automatically based on the type of data.

    - &apos;svd&apos; uses a Singular Value Decomposition of X to compute the Ridge
      coefficients. It is the most stable solver, in particular more stable
      for singular matrices than &apos;cholesky&apos; at the cost of being slower.

    - &apos;cholesky&apos; uses the standard scipy.linalg.solve function to
      obtain a closed-form solution.

    - &apos;sparse_cg&apos; uses the conjugate gradient solver as found in
      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
      more appropriate than &apos;cholesky&apos; for large-scale data
      (possibility to set `tol` and `max_iter`).

    - &apos;lsqr&apos; uses the dedicated regularized least-squares routine
      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
      procedure.

    - &apos;sag&apos; uses a Stochastic Average Gradient descent, and &apos;saga&apos; uses
      its unbiased and more flexible version named SAGA. Both methods
      use an iterative procedure, and are often faster than other solvers
      when both n_samples and n_features are large. Note that &apos;sag&apos; and
      &apos;saga&apos; fast convergence is only guaranteed on features with
      approximately the same scale. You can preprocess the data with a
      scaler from sklearn.preprocessing.

      .. versionadded:: 0.17
         Stochastic Average Gradient descent solver.
      .. versionadded:: 0.19
         SAGA solver.

    - &apos;lbfgs&apos; uses L-BFGS-B algorithm implemented in
      `scipy.optimize.minimize`. It can be used only when `positive`
      is True.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.
    Only &apos;lbfgs&apos; solver is supported in this case.

random_state : int, RandomState instance, default=None
    Used when ``solver`` == &apos;sag&apos; or &apos;saga&apos; to shuffle the data.
    See :term:`Glossary &lt;random_state&gt;` for details.

Attributes
----------
coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.

    ``coef_`` is of shape (1, n_features) when the given problem is binary.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

n_iter_ : None or ndarray of shape (n_targets,)
    Actual number of iterations for each target. Available only for
    sag and lsqr solvers. Other solvers will return None.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression.
RidgeClassifierCV :  Ridge classifier with built-in cross validation.

Notes
-----
For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.

Examples
--------
&gt;&gt;&gt; from sklearn.datasets import load_breast_cancer
&gt;&gt;&gt; from sklearn.linear_model import RidgeClassifier
&gt;&gt;&gt; X, y = load_breast_cancer(return_X_y=True)
&gt;&gt;&gt; clf = RidgeClassifier().fit(X, y)
&gt;&gt;&gt; clf.score(X, y)
0.9595...</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear least squares with l2 regularization.

Minimizes the objective function::

||y - Xw||^2_2 + alpha * ||w||^2_2

This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape (n_samples, n_targets)).

Read more in the :ref:`User Guide &lt;ridge_regression&gt;`.

Parameters
----------
alpha : {float, ndarray of shape (n_targets,)}, default=1.0
    Constant that multiplies the L2 term, controlling regularization
    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.

    When `alpha = 0`, the objective is equivalent to ordinary least
    squares, solved by the :class:`LinearRegression` object. For numerical
    reasons, using `alpha = 0` with the `Ridge` object is not advised.
    Instead, you should use the :class:`LinearRegression` object.

    If an array is passed, penalties are assumed to be specific to the
    targets. Hence they must correspond in number.

fit_intercept : bool, default=True
    Whether to fit the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. ``X`` and ``y`` are expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

max_iter : int, default=None
    Maximum number of iterations for conjugate gradient solver.
    For &apos;sparse_cg&apos; and &apos;lsqr&apos; solvers, the default value is determined
    by scipy.sparse.linalg. For &apos;sag&apos; solver, the default value is 1000.
    For &apos;lbfgs&apos; solver, the default value is 15000.

tol : float, default=1e-4
    The precision of the solution (`coef_`) is determined by `tol` which
    specifies a different convergence criterion for each solver:

    - &apos;svd&apos;: `tol` has no impact.

    - &apos;cholesky&apos;: `tol` has no impact.

    - &apos;sparse_cg&apos;: norm of residuals smaller than `tol`.

    - &apos;lsqr&apos;: `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,
      which control the norm of the residual vector in terms of the norms of
      matrix and coefficients.

    - &apos;sag&apos; and &apos;saga&apos;: relative change of coef smaller than `tol`.

    - &apos;lbfgs&apos;: maximum of the absolute (projected) gradient=max|residuals|
      smaller than `tol`.

    .. versionchanged:: 1.2
       Default value changed from 1e-3 to 1e-4 for consistency with other linear
       models.

solver : {&apos;auto&apos;, &apos;svd&apos;, &apos;cholesky&apos;, &apos;lsqr&apos;, &apos;sparse_cg&apos;,             &apos;sag&apos;, &apos;saga&apos;, &apos;lbfgs&apos;}, default=&apos;auto&apos;
    Solver to use in the computational routines:

    - &apos;auto&apos; chooses the solver automatically based on the type of data.

    - &apos;svd&apos; uses a Singular Value Decomposition of X to compute the Ridge
      coefficients. It is the most stable solver, in particular more stable
      for singular matrices than &apos;cholesky&apos; at the cost of being slower.

    - &apos;cholesky&apos; uses the standard scipy.linalg.solve function to
      obtain a closed-form solution.

    - &apos;sparse_cg&apos; uses the conjugate gradient solver as found in
      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
      more appropriate than &apos;cholesky&apos; for large-scale data
      (possibility to set `tol` and `max_iter`).

    - &apos;lsqr&apos; uses the dedicated regularized least-squares routine
      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
      procedure.

    - &apos;sag&apos; uses a Stochastic Average Gradient descent, and &apos;saga&apos; uses
      its improved, unbiased version named SAGA. Both methods also use an
      iterative procedure, and are often faster than other solvers when
      both n_samples and n_features are large. Note that &apos;sag&apos; and
      &apos;saga&apos; fast convergence is only guaranteed on features with
      approximately the same scale. You can preprocess the data with a
      scaler from sklearn.preprocessing.

    - &apos;lbfgs&apos; uses L-BFGS-B algorithm implemented in
      `scipy.optimize.minimize`. It can be used only when `positive`
      is True.

    All solvers except &apos;svd&apos; support both dense and sparse data. However, only
    &apos;lsqr&apos;, &apos;sag&apos;, &apos;sparse_cg&apos;, and &apos;lbfgs&apos; support sparse input when
    `fit_intercept` is True.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.
    Only &apos;lbfgs&apos; solver is supported in this case.

random_state : int, RandomState instance, default=None
    Used when ``solver`` == &apos;sag&apos; or &apos;saga&apos; to shuffle the data.
    See :term:`Glossary &lt;random_state&gt;` for details.

    .. versionadded:: 0.17
       `random_state` to support Stochastic Average Gradient.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Weight vector(s).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

n_iter_ : None or ndarray of shape (n_targets,)
    Actual number of iterations for each target. Available only for
    sag and lsqr solvers. Other solvers will return None.

    .. versionadded:: 0.17

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
RidgeClassifier : Ridge classifier.
RidgeCV : Ridge regression with built-in cross validation.
:class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression
    combines ridge regression with the kernel trick.

Notes
-----
Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to ``1 / (2C)`` in other linear
models such as :class:`~sklearn.linear_model.LogisticRegression` or
:class:`~sklearn.svm.LinearSVC`.

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import Ridge
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; n_samples, n_features = 10, 5
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; y = rng.randn(n_samples)
&gt;&gt;&gt; X = rng.randn(n_samples, n_features)
&gt;&gt;&gt; clf = Ridge(alpha=1.0)
&gt;&gt;&gt; clf.fit(X, y)
Ridge()</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear classifiers (SVM, logistic regression, etc.) with SGD training.

This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning via the `partial_fit` method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.

This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).

The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.

Read more in the :ref:`User Guide &lt;sgd&gt;`.

Parameters
----------
loss : {&apos;hinge&apos;, &apos;log_loss&apos;, &apos;modified_huber&apos;, &apos;squared_hinge&apos;,        &apos;perceptron&apos;, &apos;squared_error&apos;, &apos;huber&apos;, &apos;epsilon_insensitive&apos;,        &apos;squared_epsilon_insensitive&apos;}, default=&apos;hinge&apos;
    The loss function to be used.

    - &apos;hinge&apos; gives a linear SVM.
    - &apos;log_loss&apos; gives logistic regression, a probabilistic classifier.
    - &apos;modified_huber&apos; is another smooth loss that brings tolerance to
      outliers as well as probability estimates.
    - &apos;squared_hinge&apos; is like hinge but is quadratically penalized.
    - &apos;perceptron&apos; is the linear loss used by the perceptron algorithm.
    - The other losses, &apos;squared_error&apos;, &apos;huber&apos;, &apos;epsilon_insensitive&apos; and
      &apos;squared_epsilon_insensitive&apos; are designed for regression but can be useful
      in classification as well; see
      :class:`~sklearn.linear_model.SGDRegressor` for a description.

    More details about the losses formulas can be found in the
    :ref:`User Guide &lt;sgd_mathematical_formulation&gt;`.

penalty : {&apos;l2&apos;, &apos;l1&apos;, &apos;elasticnet&apos;, None}, default=&apos;l2&apos;
    The penalty (aka regularization term) to be used. Defaults to &apos;l2&apos;
    which is the standard regularizer for linear SVM models. &apos;l1&apos; and
    &apos;elasticnet&apos; might bring sparsity to the model (feature selection)
    not achievable with &apos;l2&apos;. No penalty is added when set to `None`.

alpha : float, default=0.0001
    Constant that multiplies the regularization term. The higher the
    value, the stronger the regularization. Also used to compute the
    learning rate when `learning_rate` is set to &apos;optimal&apos;.
    Values must be in the range `[0.0, inf)`.

l1_ratio : float, default=0.15
    The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
    Only used if `penalty` is &apos;elasticnet&apos;.
    Values must be in the range `[0.0, 1.0]`.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`partial_fit` method.
    Values must be in the range `[1, inf)`.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, training will stop
    when (loss &gt; best_loss - tol) for ``n_iter_no_change`` consecutive
    epochs.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.19

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.
    Values must be in the range `[0, inf)`.

epsilon : float, default=0.1
    Epsilon in the epsilon-insensitive loss functions; only if `loss` is
    &apos;huber&apos;, &apos;epsilon_insensitive&apos;, or &apos;squared_epsilon_insensitive&apos;.
    For &apos;huber&apos;, determines the threshold at which it becomes less
    important to get the prediction exactly right.
    For epsilon-insensitive, any differences between the current prediction
    and the correct label are ignored if they are less than this threshold.
    Values must be in the range `[0.0, inf)`.

n_jobs : int, default=None
    The number of CPUs to use to do the OVA (One Versus All, for
    multi-class problems) computation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

random_state : int, RandomState instance, default=None
    Used for shuffling the data, when ``shuffle`` is set to ``True``.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.
    Integer values must be in the range `[0, 2**32 - 1]`.

learning_rate : str, default=&apos;optimal&apos;
    The learning rate schedule:

    - &apos;constant&apos;: `eta = eta0`
    - &apos;optimal&apos;: `eta = 1.0 / (alpha * (t + t0))`
      where `t0` is chosen by a heuristic proposed by Leon Bottou.
    - &apos;invscaling&apos;: `eta = eta0 / pow(t, power_t)`
    - &apos;adaptive&apos;: `eta = eta0`, as long as the training keeps decreasing.
      Each time n_iter_no_change consecutive epochs fail to decrease the
      training loss by tol or fail to increase validation score by tol if
      `early_stopping` is `True`, the current learning rate is divided by 5.

        .. versionadded:: 0.20
            Added &apos;adaptive&apos; option

eta0 : float, default=0.0
    The initial learning rate for the &apos;constant&apos;, &apos;invscaling&apos; or
    &apos;adaptive&apos; schedules. The default value is 0.0 as eta0 is not used by
    the default schedule &apos;optimal&apos;.
    Values must be in the range `[0.0, inf)`.

power_t : float, default=0.5
    The exponent for inverse scaling learning rate.
    Values must be in the range `(-inf, inf)`.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to `True`, it will automatically set aside
    a stratified fraction of training data as validation and terminate
    training when validation score returned by the `score` method is not
    improving by at least tol for n_iter_no_change consecutive epochs.

    .. versionadded:: 0.20
        Added &apos;early_stopping&apos; option

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if `early_stopping` is True.
    Values must be in the range `(0.0, 1.0)`.

    .. versionadded:: 0.20
        Added &apos;validation_fraction&apos; option

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before stopping
    fitting.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Integer values must be in the range `[1, max_iter)`.

    .. versionadded:: 0.20
        Added &apos;n_iter_no_change&apos; option

class_weight : dict, {class_label: weight} or &quot;balanced&quot;, default=None
    Preset for the class_weight fit parameter.

    Weights associated with classes. If not given, all classes
    are supposed to have weight one.

    The &quot;balanced&quot; mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.
    If a dynamic learning rate is used, the learning rate is adapted
    depending on the number of samples already seen. Calling ``fit`` resets
    this counter, while ``partial_fit`` will result in increasing the
    existing counter.

average : bool or int, default=False
    When set to `True`, computes the averaged SGD weights across all
    updates and stores the result in the ``coef_`` attribute. If set to
    an int greater than 1, averaging will begin once the total number of
    samples seen reaches `average`. So ``average=10`` will begin
    averaging after seeing 10 samples.
    Integer values must be in the range `[1, n_samples]`.

Attributes
----------
coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

n_iter_ : int
    The actual number of iterations before reaching the stopping criterion.
    For multiclass fits, it is the maximum over every binary fit.

loss_function_ : concrete ``LossFunction``

    .. deprecated:: 1.4
        Attribute `loss_function_` was deprecated in version 1.4 and will be
        removed in 1.6.

classes_ : array of shape (n_classes,)

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.svm.LinearSVC : Linear support vector classification.
LogisticRegression : Logistic regression.
Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to
    ``SGDClassifier(loss=&quot;perceptron&quot;, eta0=1, learning_rate=&quot;constant&quot;,
    penalty=None)``.

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier
&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler
&gt;&gt;&gt; from sklearn.pipeline import make_pipeline
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
&gt;&gt;&gt; Y = np.array([1, 1, 2, 2])
&gt;&gt;&gt; # Always scale the input. The most convenient way is to use a pipeline.
&gt;&gt;&gt; clf = make_pipeline(StandardScaler(),
...                     SGDClassifier(max_iter=1000, tol=1e-3))
&gt;&gt;&gt; clf.fit(X, Y)
Pipeline(steps=[(&apos;standardscaler&apos;, StandardScaler()),
                (&apos;sgdclassifier&apos;, SGDClassifier())])
&gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))
[1]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Solves linear One-Class SVM using Stochastic Gradient Descent.

This implementation is meant to be used with a kernel approximation
technique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results
similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by
default.

Read more in the :ref:`User Guide &lt;sgd_online_one_class_svm&gt;`.

.. versionadded:: 1.0

Parameters
----------
nu : float, default=0.5
    The nu parameter of the One Class SVM: an upper bound on the
    fraction of training errors and a lower bound of the fraction of
    support vectors. Should be in the interval (0, 1]. By default 0.5
    will be taken.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. Defaults to True.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    `partial_fit`. Defaults to 1000.
    Values must be in the range `[1, inf)`.

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss &gt; previous_loss - tol). Defaults to 1e-3.
    Values must be in the range `[0.0, inf)`.

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.
    Defaults to True.

verbose : int, default=0
    The verbosity level.

random_state : int, RandomState instance or None, default=None
    The seed of the pseudo random number generator to use when shuffling
    the data.  If int, random_state is the seed used by the random number
    generator; If RandomState instance, random_state is the random number
    generator; If None, the random number generator is the RandomState
    instance used by `np.random`.

learning_rate : {&apos;constant&apos;, &apos;optimal&apos;, &apos;invscaling&apos;, &apos;adaptive&apos;}, default=&apos;optimal&apos;
    The learning rate schedule to use with `fit`. (If using `partial_fit`,
    learning rate must be controlled directly).

    - &apos;constant&apos;: `eta = eta0`
    - &apos;optimal&apos;: `eta = 1.0 / (alpha * (t + t0))`
      where t0 is chosen by a heuristic proposed by Leon Bottou.
    - &apos;invscaling&apos;: `eta = eta0 / pow(t, power_t)`
    - &apos;adaptive&apos;: eta = eta0, as long as the training keeps decreasing.
      Each time n_iter_no_change consecutive epochs fail to decrease the
      training loss by tol or fail to increase validation score by tol if
      early_stopping is True, the current learning rate is divided by 5.

eta0 : float, default=0.0
    The initial learning rate for the &apos;constant&apos;, &apos;invscaling&apos; or
    &apos;adaptive&apos; schedules. The default value is 0.0 as eta0 is not used by
    the default schedule &apos;optimal&apos;.
    Values must be in the range `[0.0, inf)`.

power_t : float, default=0.5
    The exponent for inverse scaling learning rate.
    Values must be in the range `(-inf, inf)`.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.
    If a dynamic learning rate is used, the learning rate is adapted
    depending on the number of samples already seen. Calling ``fit`` resets
    this counter, while ``partial_fit``  will result in increasing the
    existing counter.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights and stores the
    result in the ``coef_`` attribute. If set to an int greater than 1,
    averaging will begin once the total number of samples seen reaches
    average. So ``average=10`` will begin averaging after seeing 10
    samples.

Attributes
----------
coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features.

offset_ : ndarray of shape (1,)
    Offset used to define the decision function from the raw scores.
    We have the relation: decision_function = score_samples - offset.

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

loss_function_ : concrete ``LossFunction``

    .. deprecated:: 1.4
        ``loss_function_`` was deprecated in version 1.4 and will be removed in
        1.6.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.

Notes
-----
This estimator has a linear complexity in the number of training samples
and is thus better suited than the `sklearn.svm.OneClassSVM`
implementation for datasets with a large number of training samples (say
&gt; 10,000).

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
&gt;&gt;&gt; clf = linear_model.SGDOneClassSVM(random_state=42)
&gt;&gt;&gt; clf.fit(X)
SGDOneClassSVM(random_state=42)

&gt;&gt;&gt; print(clf.predict([[4, 4]]))
[1]</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Linear model fitted by minimizing a regularized empirical loss with SGD.

SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).

The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.

This implementation works with data represented as dense numpy arrays of
floating point values for the features.

Read more in the :ref:`User Guide &lt;sgd&gt;`.

Parameters
----------
loss : str, default=&apos;squared_error&apos;
    The loss function to be used. The possible values are &apos;squared_error&apos;,
    &apos;huber&apos;, &apos;epsilon_insensitive&apos;, or &apos;squared_epsilon_insensitive&apos;

    The &apos;squared_error&apos; refers to the ordinary least squares fit.
    &apos;huber&apos; modifies &apos;squared_error&apos; to focus less on getting outliers
    correct by switching from squared to linear loss past a distance of
    epsilon. &apos;epsilon_insensitive&apos; ignores errors less than epsilon and is
    linear past that; this is the loss function used in SVR.
    &apos;squared_epsilon_insensitive&apos; is the same but becomes squared loss past
    a tolerance of epsilon.

    More details about the losses formulas can be found in the
    :ref:`User Guide &lt;sgd_mathematical_formulation&gt;`.

penalty : {&apos;l2&apos;, &apos;l1&apos;, &apos;elasticnet&apos;, None}, default=&apos;l2&apos;
    The penalty (aka regularization term) to be used. Defaults to &apos;l2&apos;
    which is the standard regularizer for linear SVM models. &apos;l1&apos; and
    &apos;elasticnet&apos; might bring sparsity to the model (feature selection)
    not achievable with &apos;l2&apos;. No penalty is added when set to `None`.

alpha : float, default=0.0001
    Constant that multiplies the regularization term. The higher the
    value, the stronger the regularization. Also used to compute the
    learning rate when `learning_rate` is set to &apos;optimal&apos;.
    Values must be in the range `[0.0, inf)`.

l1_ratio : float, default=0.15
    The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
    Only used if `penalty` is &apos;elasticnet&apos;.
    Values must be in the range `[0.0, 1.0]`.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`partial_fit` method.
    Values must be in the range `[1, inf)`.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, training will stop
    when (loss &gt; best_loss - tol) for ``n_iter_no_change`` consecutive
    epochs.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.19

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.
    Values must be in the range `[0, inf)`.

epsilon : float, default=0.1
    Epsilon in the epsilon-insensitive loss functions; only if `loss` is
    &apos;huber&apos;, &apos;epsilon_insensitive&apos;, or &apos;squared_epsilon_insensitive&apos;.
    For &apos;huber&apos;, determines the threshold at which it becomes less
    important to get the prediction exactly right.
    For epsilon-insensitive, any differences between the current prediction
    and the correct label are ignored if they are less than this threshold.
    Values must be in the range `[0.0, inf)`.

random_state : int, RandomState instance, default=None
    Used for shuffling the data, when ``shuffle`` is set to ``True``.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

learning_rate : str, default=&apos;invscaling&apos;
    The learning rate schedule:

    - &apos;constant&apos;: `eta = eta0`
    - &apos;optimal&apos;: `eta = 1.0 / (alpha * (t + t0))`
      where t0 is chosen by a heuristic proposed by Leon Bottou.
    - &apos;invscaling&apos;: `eta = eta0 / pow(t, power_t)`
    - &apos;adaptive&apos;: eta = eta0, as long as the training keeps decreasing.
      Each time n_iter_no_change consecutive epochs fail to decrease the
      training loss by tol or fail to increase validation score by tol if
      early_stopping is True, the current learning rate is divided by 5.

        .. versionadded:: 0.20
            Added &apos;adaptive&apos; option

eta0 : float, default=0.01
    The initial learning rate for the &apos;constant&apos;, &apos;invscaling&apos; or
    &apos;adaptive&apos; schedules. The default value is 0.01.
    Values must be in the range `[0.0, inf)`.

power_t : float, default=0.25
    The exponent for inverse scaling learning rate.
    Values must be in the range `(-inf, inf)`.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set aside
    a fraction of training data as validation and terminate
    training when validation score returned by the `score` method is not
    improving by at least `tol` for `n_iter_no_change` consecutive
    epochs.

    .. versionadded:: 0.20
        Added &apos;early_stopping&apos; option

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if `early_stopping` is True.
    Values must be in the range `(0.0, 1.0)`.

    .. versionadded:: 0.20
        Added &apos;validation_fraction&apos; option

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before stopping
    fitting.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Integer values must be in the range `[1, max_iter)`.

    .. versionadded:: 0.20
        Added &apos;n_iter_no_change&apos; option

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary &lt;warm_start&gt;`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.
    If a dynamic learning rate is used, the learning rate is adapted
    depending on the number of samples already seen. Calling ``fit`` resets
    this counter, while ``partial_fit``  will result in increasing the
    existing counter.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights across all
    updates and stores the result in the ``coef_`` attribute. If set to
    an int greater than 1, averaging will begin once the total number of
    samples seen reaches `average`. So ``average=10`` will begin
    averaging after seeing 10 samples.

Attributes
----------
coef_ : ndarray of shape (n_features,)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,)
    The intercept term.

n_iter_ : int
    The actual number of iterations before reaching the stopping criterion.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
HuberRegressor : Linear regression model that is robust to outliers.
Lars : Least Angle Regression model.
Lasso : Linear Model trained with L1 prior as regularizer.
RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.
Ridge : Linear least squares with l2 regularization.
sklearn.svm.SVR : Epsilon-Support Vector Regression.
TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.linear_model import SGDRegressor
&gt;&gt;&gt; from sklearn.pipeline import make_pipeline
&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler
&gt;&gt;&gt; n_samples, n_features = 10, 5
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; y = rng.randn(n_samples)
&gt;&gt;&gt; X = rng.randn(n_samples, n_features)
&gt;&gt;&gt; # Always scale the input. The most convenient way is to use a pipeline.
&gt;&gt;&gt; reg = make_pipeline(StandardScaler(),
...                     SGDRegressor(max_iter=1000, tol=1e-3))
&gt;&gt;&gt; reg.fit(X, y)
Pipeline(steps=[(&apos;standardscaler&apos;, StandardScaler()),
                (&apos;sgdregressor&apos;, SGDRegressor())])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module"/>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment></rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Theil-Sen Estimator: robust multivariate regression model.

The algorithm calculates least square solutions on subsets with size
n_subsamples of the samples in X. Any value of n_subsamples between the
number of features and samples leads to an estimator with a compromise
between robustness and efficiency. Since the number of least square
solutions is &quot;n_samples choose n_subsamples&quot;, it can be extremely large
and can therefore be limited with max_subpopulation. If this limit is
reached, the subsets are chosen randomly. In a final step, the spatial
median (or L1 median) is calculated of all least square solutions.

Read more in the :ref:`User Guide &lt;theil_sen_regression&gt;`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

max_subpopulation : int, default=1e4
    Instead of computing with a set of cardinality &apos;n choose k&apos;, where n is
    the number of samples and k is the number of subsamples (at least
    number of features), consider only a stochastic subpopulation of a
    given maximal size if &apos;n choose k&apos; is larger than max_subpopulation.
    For other than small problem sizes this parameter will determine
    memory usage and runtime if n_subsamples is not changed. Note that the
    data type should be int but floats such as 1e4 can be accepted too.

n_subsamples : int, default=None
    Number of samples to calculate the parameters. This is at least the
    number of features (plus 1 if fit_intercept=True) and the number of
    samples as a maximum. A lower number leads to a higher breakdown
    point and a low efficiency while a high number leads to a low
    breakdown point and a high efficiency. If None, take the
    minimum number of subsamples leading to maximal robustness.
    If n_subsamples is set to n_samples, Theil-Sen is identical to least
    squares.

max_iter : int, default=300
    Maximum number of iterations for the calculation of spatial median.

tol : float, default=1e-3
    Tolerance when calculating spatial median.

random_state : int, RandomState instance or None, default=None
    A random number generator instance to define the state of the random
    permutations generator. Pass an int for reproducible output across
    multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.

verbose : bool, default=False
    Verbose mode when fitting the model.

Attributes
----------
coef_ : ndarray of shape (n_features,)
    Coefficients of the regression model (median of distribution).

intercept_ : float
    Estimated intercept of regression model.

breakdown_ : float
    Approximated breakdown point.

n_iter_ : int
    Number of iterations needed for the spatial median.

n_subpopulation_ : int
    Number of combinations taken into account from &apos;n choose k&apos;, where n is
    the number of samples and k is the number of subsamples.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
HuberRegressor : Linear regression model that is robust to outliers.
RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.
SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.

References
----------
- Theil-Sen Estimators in a Multiple Linear Regression Model, 2009
  Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang
  http://home.olemiss.edu/~xdang/papers/MTSE.pdf

Examples
--------
&gt;&gt;&gt; from sklearn.linear_model import TheilSenRegressor
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; X, y = make_regression(
...     n_samples=200, n_features=2, noise=4.0, random_state=0)
&gt;&gt;&gt; reg = TheilSenRegressor(random_state=0).fit(X, y)
&gt;&gt;&gt; reg.score(X, y)
0.9884...
&gt;&gt;&gt; reg.predict(X[:1,])
array([-31.5871...])</rdfs:comment>
    </Class>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
    


    <!-- https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod -->


    <Class rdf:about="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod">
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"/>
        <rdfs:subClassOf rdf:resource="https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"/>
        <rdfs:comment>Generalized Linear Model with a Tweedie distribution.

This estimator can be used to model different GLMs depending on the
``power`` parameter, which determines the underlying distribution.

Read more in the :ref:`User Guide &lt;Generalized_linear_models&gt;`.

.. versionadded:: 0.23

Parameters
----------
power : float, default=0
        The power determines the underlying target distribution according
        to the following table:

        +-------+------------------------+
        | Power | Distribution           |
        +=======+========================+
        | 0     | Normal                 |
        +-------+------------------------+
        | 1     | Poisson                |
        +-------+------------------------+
        | (1,2) | Compound Poisson Gamma |
        +-------+------------------------+
        | 2     | Gamma                  |
        +-------+------------------------+
        | 3     | Inverse Gaussian       |
        +-------+------------------------+

        For ``0 &lt; power &lt; 1``, no distribution exists.

alpha : float, default=1
    Constant that multiplies the L2 penalty term and determines the
    regularization strength. ``alpha = 0`` is equivalent to unpenalized
    GLMs. In this case, the design matrix `X` must have full column rank
    (no collinearities).
    Values of `alpha` must be in the range `[0.0, inf)`.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the linear predictor (`X @ coef + intercept`).

link : {&apos;auto&apos;, &apos;identity&apos;, &apos;log&apos;}, default=&apos;auto&apos;
    The link function of the GLM, i.e. mapping from linear predictor
    `X @ coeff + intercept` to prediction `y_pred`. Option &apos;auto&apos; sets
    the link depending on the chosen `power` parameter as follows:

    - &apos;identity&apos; for ``power &lt;= 0``, e.g. for the Normal distribution
    - &apos;log&apos; for ``power &gt; 0``, e.g. for Poisson, Gamma and Inverse Gaussian
      distributions

solver : {&apos;lbfgs&apos;, &apos;newton-cholesky&apos;}, default=&apos;lbfgs&apos;
    Algorithm to use in the optimization problem:

    &apos;lbfgs&apos;
        Calls scipy&apos;s L-BFGS-B optimizer.

    &apos;newton-cholesky&apos;
        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to
        iterated reweighted least squares) with an inner Cholesky based solver.
        This solver is a good choice for `n_samples` &gt;&gt; `n_features`, especially
        with one-hot encoded categorical features with rare categories. Be aware
        that the memory usage of this solver has a quadratic dependency on
        `n_features` because it explicitly computes the Hessian matrix.

        .. versionadded:: 1.2

max_iter : int, default=100
    The maximal number of iterations for the solver.
    Values must be in the range `[1, inf)`.

tol : float, default=1e-4
    Stopping criterion. For the lbfgs solver,
    the iteration will stop when ``max{|g_j|, j = 1, ..., d} &lt;= tol``
    where ``g_j`` is the j-th component of the gradient (derivative) of
    the objective function.
    Values must be in the range `(0.0, inf)`.

warm_start : bool, default=False
    If set to ``True``, reuse the solution of the previous call to ``fit``
    as initialization for ``coef_`` and ``intercept_`` .

verbose : int, default=0
    For the lbfgs solver set verbose to any positive number for verbosity.
    Values must be in the range `[0, inf)`.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the linear predictor (`X @ coef_ +
    intercept_`) in the GLM.

intercept_ : float
    Intercept (a.k.a. bias) added to linear predictor.

n_iter_ : int
    Actual number of iterations used in the solver.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PoissonRegressor : Generalized Linear Model with a Poisson distribution.
GammaRegressor : Generalized Linear Model with a Gamma distribution.

Examples
--------
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; clf = linear_model.TweedieRegressor()
&gt;&gt;&gt; X = [[1, 2], [2, 3], [3, 4], [4, 3]]
&gt;&gt;&gt; y = [2, 3.5, 5, 5.5]
&gt;&gt;&gt; clf.fit(X, y)
TweedieRegressor()
&gt;&gt;&gt; clf.score(X, y)
0.839...
&gt;&gt;&gt; clf.coef_
array([0.599..., 0.299...])
&gt;&gt;&gt; clf.intercept_
1.600...
&gt;&gt;&gt; clf.predict([[1, 1], [3, 4]])
array([2.500..., 4.599...])</rdfs:comment>
    </Class>
</rdf:RDF>



<!-- Generated by the OWL API (version 5.1.18) https://github.com/owlcs/owlapi/ -->


