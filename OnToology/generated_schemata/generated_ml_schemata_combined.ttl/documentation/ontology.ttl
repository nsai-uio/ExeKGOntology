@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@base <http://www.w3.org/2002/07/owl#> .

[ rdf:type owl:Ontology
 ] .

#################################################################
#    Object Properties
#################################################################

###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBayesianRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBayesianRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression> ;
                                                                                                                 rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBinaryClassificationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBinaryClassificationMethod> rdf:type owl:ObjectProperty ;
                                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification> ;
                                                                                                                   rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBoostingRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBoostingRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BoostingRegression> ;
                                                                                                                 rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasClusteringMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasClusteringMethod> rdf:type owl:ObjectProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Clustering> ;
                                                                                                         rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BallTreeMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KDTreeMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataProcessingMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataProcessingMethod> rdf:type owl:ObjectProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataProcessing> ;
                                                                                                             rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelCentererMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelEncoderMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataSplittingMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataSplittingMethod> rdf:type owl:ObjectProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataSplittingMethod> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplitting> ;
                                                                                                            rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneGroupOutMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneOutMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PredefinedSplitMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDecisionTreeRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDecisionTreeRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegression> ;
                                                                                                                     rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDecompositionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDecompositionMethod> rdf:type owl:ObjectProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Decomposition> ;
                                                                                                            rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasFeatureSelectionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasFeatureSelectionMethod> rdf:type owl:ObjectProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelection> ;
                                                                                                               rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Chi2Method> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FClassifMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasInstanceBasedRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasInstanceBasedRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#InstanceBasedRegression> ;
                                                                                                                      rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasLeastAngleRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasLeastAngleRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression> ;
                                                                                                                   rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasModelSelectionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasModelSelectionMethod> rdf:type owl:ObjectProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelection> ;
                                                                                                             rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMulticlassClassificationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMulticlassClassificationMethod> rdf:type owl:ObjectProperty ;
                                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification> ;
                                                                                                                       rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMultilabelClassificationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMultilabelClassificationMethod> rdf:type owl:ObjectProperty ;
                                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelClassification> ;
                                                                                                                       rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPerformanceCalculationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPerformanceCalculationMethod> rdf:type owl:ObjectProperty ;
                                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPerformanceCalculationMethod> ;
                                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculation> ;
                                                                                                                     rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedRandScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AucMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CalinskiHarabaszScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CompletenessScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CoverageErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DaviesBouldinScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerNamesMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HammingLossMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingAveragePrecisionScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingLossMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MatthewsCorrcoefMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanGammaDevianceMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPoissonDevianceMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairConfusionMatrixMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod> rdf:type owl:ObjectProperty .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRandomForestRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRandomForestRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegression> ;
                                                                                                                     rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRegularizedRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRegularizedRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression> ;
                                                                                                                    rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSimpleRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSimpleRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression> ;
                                                                                                               rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSupportVectorRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSupportVectorRegressionMethod> rdf:type owl:ObjectProperty ;
                                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> ;
                                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SupportVectorRegression> ;
                                                                                                                      rdfs:range <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod> rdf:type owl:ObjectProperty .


#################################################################
#    Data properties
#################################################################

###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> rdf:type owl:DatatypeProperty .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamA
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamA> rdf:type owl:DatatypeProperty ;
                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod> ;
                                                                                               rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAcceptSparse
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAcceptSparse> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamActivation
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamActivation> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAddIndicator
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAddIndicator> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAdjusted
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAdjusted> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod> ;
                                                                                                      rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAffinity
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAffinity> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAggressiveElimination
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAggressiveElimination> rdf:type owl:DatatypeProperty ;
                                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ;
                                                                                                                   rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlgorithm
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlgorithm> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAllowNone
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAllowNone> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAllowSingleCluster
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAllowSingleCluster> rdf:type owl:DatatypeProperty ;
                                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                                rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int ,
                                                                                                              xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha1
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha1> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha2
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha2> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaH
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaH> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaInit> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaPerTarget
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaPerTarget> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ;
                                                                                                            rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaW
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaW> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphas
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphas> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlternateSign
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlternateSign> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAssignLabels
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAssignLabels> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAtol
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAtol> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ;
                                                                                                  rdfs:range xsd:float ,
                                                                                                             xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverage
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverage> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                     rdfs:range xsd:boolean ,
                                                                                                                xsd:int ,
                                                                                                                xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverageMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverageMethod> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod> ;
                                                                                                           rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAxis
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAxis> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod> ;
                                                                                                  rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamB
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamB> rdf:type owl:DatatypeProperty ;
                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod> ;
                                                                                               rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBandwidth
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBandwidth> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int ,
                                                                                                                  xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBatchSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBatchSize> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ;
                                                                                                       rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod> ;
                                                                                                  rdfs:range xsd:float ,
                                                                                                             xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta1
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta1> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta2
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta2> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBetaLoss
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBetaLoss> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int ,
                                                                                                                 xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBinSeeding
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBinSeeding> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBinarize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBinarize> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int ,
                                                                                                                 xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBisectingStrategy
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBisectingStrategy> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ;
                                                                                                               rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBootstrap
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBootstrap> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBootstrapFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBootstrapFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ;
                                                                                                               rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBranchingFactor
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBranchingFactor> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> ;
                                                                                                             rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBreadthFirst
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBreadthFirst> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBreakTies
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBreakTies> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamC
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamC> rdf:type owl:DatatypeProperty ;
                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ;
                                                                                               rdfs:range xsd:float ,
                                                                                                          xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCacheSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCacheSize> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCallback
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCallback> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCategoricalFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCategoricalFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ;
                                                                                                                 rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCategories
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCategories> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCcpAlpha
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCcpAlpha> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCenter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCenter> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod> ;
                                                                                                    rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCheckInverse
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCheckInverse> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClassWeight
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClassWeight> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClip
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClip> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod> ;
                                                                                                  rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterAll
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterAll> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterMethod> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ;
                                                                                                           rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterSelectionEpsilon
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterSelectionEpsilon> rdf:type owl:DatatypeProperty ;
                                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                                     rdfs:range xsd:float ,
                                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterSelectionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterSelectionMethod> rdf:type owl:DatatypeProperty ;
                                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCodeInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCodeInit> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCodeSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCodeSize> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCoef0
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCoef0> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeDistances
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeDistances> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ;
                                                                                                              rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeFullTree
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeFullTree> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ;
                                                                                                             rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeLabels
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeLabels> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeScore
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeScore> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamConnectivity
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamConnectivity> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamContamination
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamContamination> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamContingency
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamContingency> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamConvergenceIter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamConvergenceIter> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ;
                                                                                                             rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopy
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopy> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod> ;
                                                                                                  rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopyX
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopyX> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ;
                                                                                                   rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCriterion
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCriterion> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCs> rdf:type owl:DatatypeProperty ;
                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ;
                                                                                                rdfs:range xsd:int ,
                                                                                                           xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCv
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCv> rdf:type owl:DatatypeProperty ;
                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> ;
                                                                                                rdfs:range xsd:int ,
                                                                                                           xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDamping
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDamping> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDecisionFunctionShape
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDecisionFunctionShape> rdf:type owl:DatatypeProperty ;
                                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ;
                                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDegree
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDegree> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int ,
                                                                                                               xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDictInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDictInit> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDictionary
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDictionary> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDigits
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDigits> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod> ;
                                                                                                    rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDirection
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDirection> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDiscreteFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDiscreteFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod> ;
                                                                                                              rdfs:range xsd:boolean ,
                                                                                                                         xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDistanceThreshold
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDistanceThreshold> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ;
                                                                                                               rdfs:range xsd:float ,
                                                                                                                          xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDocTopicPrior
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDocTopicPrior> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDrop
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDrop> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ;
                                                                                                  rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDropIntermediate
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDropIntermediate> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod> ;
                                                                                                              rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDtype
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDtype> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDual
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDual> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ;
                                                                                                  rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEarlyStopping
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEarlyStopping> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEigenSolver
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEigenSolver> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEigenTol
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEigenTol> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEncode
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEncode> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEncodedMissingValue
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEncodedMissingValue> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ;
                                                                                                                 rdfs:range xsd:int ,
                                                                                                                            xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEps
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEps> rdf:type owl:DatatypeProperty ;
                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ;
                                                                                                 rdfs:range xsd:float ,
                                                                                                            xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEpsilon
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEpsilon> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamErrorOnNew
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamErrorOnNew> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamErrorScore
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamErrorScore> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimator
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimator> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimators
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimators> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEta0
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEta0> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                  rdfs:range xsd:float ,
                                                                                                             xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEvaluateEvery
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEvaluateEvery> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                           rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamExtrapolation
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamExtrapolation> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                           rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFactor
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFactor> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureNameCombiner
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureNameCombiner> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ;
                                                                                                                 rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureNamesOut
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureNamesOut> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                             rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureRange
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureRange> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFillValue
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFillValue> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFilterParams
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFilterParams> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFinalEstimator
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFinalEstimator> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ;
                                                                                                            rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitAlgorithm
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitAlgorithm> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitIntercept
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitIntercept> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitInverseTransform
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitInverseTransform> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ;
                                                                                                                 rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPath
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPath> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ;
                                                                                                     rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPrior
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPrior> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod> ;
                                                                                                      rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFlattenTransform
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFlattenTransform> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> ;
                                                                                                              rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceAllFinite
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceAllFinite> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod> ;
                                                                                                            rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceAlpha
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceAlpha> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceFinite
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceFinite> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod> ;
                                                                                                         rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForgetFactor
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForgetFactor> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ;
                                                                                                          rdfs:range xsd:float ,
                                                                                                                     xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFreshRestarts
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFreshRestarts> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFreshRestartsMaxIter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFreshRestartsMaxIter> rdf:type owl:DatatypeProperty ;
                                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ;
                                                                                                                  rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFun
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFun> rdf:type owl:DatatypeProperty ;
                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ;
                                                                                                 rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFunArgs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFunArgs> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFunc
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFunc> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                  rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGamma
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGamma> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int ,
                                                                                                              xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGap
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGap> rdf:type owl:DatatypeProperty ;
                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod> ;
                                                                                                 rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGcvMode
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGcvMode> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGreaterIsBetter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGreaterIsBetter> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> ;
                                                                                                             rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamHandleUnknown
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamHandleUnknown> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ;
                                                                                                           rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIgnoreImplicitZeros
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIgnoreImplicitZeros> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ;
                                                                                                                 rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIgnoreTies
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIgnoreTies> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamImportanceGetter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamImportanceGetter> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ;
                                                                                                              rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIncludeBias
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIncludeBias> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                         rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInit> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ;
                                                                                                  rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInitSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInitSize> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ;
                                                                                                      rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInputType
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInputType> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInteractionCst
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInteractionCst> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ;
                                                                                                            rdfs:range xsd:int ,
                                                                                                                       xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInteractionOnly
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInteractionOnly> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod> ;
                                                                                                             rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInterceptScaling
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInterceptScaling> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ;
                                                                                                              rdfs:range xsd:float ,
                                                                                                                         xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInvKwArgs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInvKwArgs> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInverseFunc
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInverseFunc> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsDataValid
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsDataValid> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsModelValid
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsModelValid> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIteratedPower
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIteratedPower> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ;
                                                                                                           rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamJitter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamJitter> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamK
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamK> rdf:type owl:DatatypeProperty ;
                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod> ;
                                                                                               rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKeepEmptyFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKeepEmptyFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ;
                                                                                                               rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKernel
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKernel> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKernelParams
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKernelParams> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKnots
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKnots> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKwArgs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKwArgs> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratio
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratio> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratios
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratios> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL2Regularization
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL2Regularization> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ;
                                                                                                              rdfs:range xsd:float ,
                                                                                                                         xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda1
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda1> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda2
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda2> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambdaInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambdaInit> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ;
                                                                                                        rdfs:range xsd:float ,
                                                                                                                   xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLeafSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLeafSize> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ;
                                                                                                      rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningDecay
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningDecay> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningMethod> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                            rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningOffset
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningOffset> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                            rdfs:range xsd:float ,
                                                                                                                       xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRate
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRate> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                          rdfs:range xsd:float ,
                                                                                                                     xsd:int ,
                                                                                                                     xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRateInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRateInit> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                              rdfs:range xsd:float ,
                                                                                                                         xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLink
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLink> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                  rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLinkage
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLinkage> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLogBase
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLogBase> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLoss
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLoss> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                  rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxBins
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxBins> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ;
                                                                                                     rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxCategories
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxCategories> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ;
                                                                                                           rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxClusterSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxClusterSize> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                            rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxDepth
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxDepth> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                      rdfs:range xsd:int ,
                                                                                                                 xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxDocUpdateIter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxDocUpdateIter> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                              rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxEps
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxEps> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ;
                                                                                                         rdfs:range xsd:float ,
                                                                                                                    xsd:int ,
                                                                                                                    xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxFun
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxFun> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                    rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxIter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxIter> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                     rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxLeafNodes
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxLeafNodes> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                          rdfs:range xsd:int ,
                                                                                                                     xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNAlphas
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNAlphas> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNoImprovement
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNoImprovement> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ;
                                                                                                              rdfs:range xsd:int ,
                                                                                                                         xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxResources
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxResources> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ;
                                                                                                          rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSamples
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSamples> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ;
                                                                                                        rdfs:range xsd:float ,
                                                                                                                   xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSkips
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSkips> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                      rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSubpopulation
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSubpopulation> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ;
                                                                                                              rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrainSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrainSize> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod> ;
                                                                                                          rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrials
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrials> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                       rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMeanChangeTol
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMeanChangeTol> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMemory
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMemory> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMethod> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetric
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetric> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetricKwargs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetricKwargs> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetricParams
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetricParams> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinBinFreq
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinBinFreq> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinCategories
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinCategories> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod> ;
                                                                                                           rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinClusterSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinClusterSize> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                            rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinFeaturesToSelect
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinFeaturesToSelect> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ;
                                                                                                                 rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinFrequency
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinFrequency> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ;
                                                                                                          rdfs:range xsd:float ,
                                                                                                                     xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinImpurityDecrease
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinImpurityDecrease> rdf:type owl:DatatypeProperty ;
                                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                                 rdfs:range xsd:float ,
                                                                                                                            xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinResources
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinResources> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ;
                                                                                                          rdfs:range xsd:int ,
                                                                                                                     xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamples
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamples> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamplesLeaf
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamplesLeaf> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                            rdfs:range xsd:float ,
                                                                                                                       xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamplesSplit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamplesSplit> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                             rdfs:range xsd:float ,
                                                                                                                        xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinWeightFractionLeaf
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinWeightFractionLeaf> rdf:type owl:DatatypeProperty ;
                                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                                   rdfs:range xsd:float ,
                                                                                                                              xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMiniBatch
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMiniBatch> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMissingValues
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMissingValues> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int ,
                                                                                                                      xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMode
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMode> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ;
                                                                                                  rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMomentum
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMomentum> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMonotonicCst
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMonotonicCst> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultiClass
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultiClass> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultioutput
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultioutput> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNAlphas
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNAlphas> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ;
                                                                                                     rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNBest
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNBest> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ;
                                                                                                   rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNBins
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNBins> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ;
                                                                                                   rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNCandidates
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNCandidates> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ;
                                                                                                         rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNClusters
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNClusters> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ;
                                                                                                       rdfs:range xsd:int ,
                                                                                                                  xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNComponents
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNComponents> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ;
                                                                                                         rdfs:range xsd:float ,
                                                                                                                    xsd:int ,
                                                                                                                    xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNEstimators
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNEstimators> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ;
                                                                                                         rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNFeatures
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNFeatures> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod> ;
                                                                                                       rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNFeaturesToSelect
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNFeaturesToSelect> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ;
                                                                                                               rdfs:range xsd:float ,
                                                                                                                          xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNGroups
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNGroups> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod> ;
                                                                                                     rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNInit> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ;
                                                                                                   rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIter> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ;
                                                                                                   rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIterNoChange
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIterNoChange> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                           rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNJobs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNJobs> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod> ;
                                                                                                   rdfs:range xsd:int ,
                                                                                                              xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNKnots
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNKnots> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                    rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNeighbors
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNeighbors> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNonzeroCoefs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNonzeroCoefs> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod> ;
                                                                                                           rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNOversamples
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNOversamples> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ;
                                                                                                          rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNQuantiles
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNQuantiles> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNRepeats
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNRepeats> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod> ;
                                                                                                      rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSplits
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSplits> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod> ;
                                                                                                     rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSubsamples
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSubsamples> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ;
                                                                                                         rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSvdVecs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSvdVecs> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ;
                                                                                                      rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNeedsProba
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNeedsProba> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNeedsThreshold
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNeedsThreshold> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> ;
                                                                                                            rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNegLabel
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNegLabel> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod> ;
                                                                                                      rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNesterovsMomentum
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNesterovsMomentum> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ;
                                                                                                               rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNoiseVariance
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNoiseVariance> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNorm
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNorm> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod> ;
                                                                                                  rdfs:range xsd:boolean ,
                                                                                                             xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNormOrder
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNormOrder> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ;
                                                                                                       rdfs:range xsd:int ,
                                                                                                                  xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNormalize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNormalize> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod> ;
                                                                                                       rdfs:range xsd:boolean ,
                                                                                                                  xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNovelty
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNovelty> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ;
                                                                                                     rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNu
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNu> rdf:type owl:DatatypeProperty ;
                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ;
                                                                                                rdfs:range xsd:float ,
                                                                                                           xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOobScore
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOobScore> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ;
                                                                                                      rdfs:range xsd:boolean ,
                                                                                                                 xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOrder
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOrder> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutlierLabel
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutlierLabel> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutputDict
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutputDict> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutputDistribution
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutputDistribution> rdf:type owl:DatatypeProperty ;
                                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ;
                                                                                                                rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamP
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamP> rdf:type owl:DatatypeProperty ;
                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ;
                                                                                               rdfs:range xsd:float ,
                                                                                                          xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParam
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParam> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamDistributions
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamDistributions> rdf:type owl:DatatypeProperty ;
                                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ;
                                                                                                                rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamGrid
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamGrid> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamName
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamName> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPassthrough
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPassthrough> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ;
                                                                                                         rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPenalty
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPenalty> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPercentile
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPercentile> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPerpTol
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPerpTol> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                     rdfs:range xsd:float ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPoolingFunc
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPoolingFunc> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPosLabel
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPosLabel> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod> ;
                                                                                                      rdfs:range xsd:boolean ,
                                                                                                                 xsd:float ,
                                                                                                                 xsd:int ,
                                                                                                                 xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositive
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositive> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ;
                                                                                                      rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositiveCode
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositiveCode> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositiveDict
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositiveDict> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPower
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPower> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                   rdfs:range xsd:float ,
                                                                                                              xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerIterationNormalizer
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerIterationNormalizer> rdf:type owl:DatatypeProperty ;
                                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ;
                                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerT
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerT> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPreDispatch
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPreDispatch> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ;
                                                                                                         rdfs:range xsd:int ,
                                                                                                                    xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrecompute
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrecompute> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod> ;
                                                                                                        rdfs:range xsd:boolean ,
                                                                                                                   xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPredDecision
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPredDecision> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPredecessorCorrection
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPredecessorCorrection> rdf:type owl:DatatypeProperty ;
                                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> ;
                                                                                                                   rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPreference
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPreference> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ;
                                                                                                        rdfs:range xsd:float ,
                                                                                                                   xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrefit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrefit> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ;
                                                                                                    rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamProbability
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamProbability> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ;
                                                                                                         rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantile
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantile> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantileRange
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantileRange> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> ;
                                                                                                           rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRadius
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRadius> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRaiseWarning
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRaiseWarning> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRandomState
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRandomState> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ;
                                                                                                         rdfs:range xsd:int ,
                                                                                                                    xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReassignmentRatio
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReassignmentRatio> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ;
                                                                                                               rdfs:range xsd:float ,
                                                                                                                          xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReduceFunc
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReduceFunc> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRefit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRefit> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ;
                                                                                                   rdfs:range xsd:boolean ,
                                                                                                              xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRemoveZeroEig
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRemoveZeroEig> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResidualThreshold
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResidualThreshold> rdf:type owl:DatatypeProperty ;
                                                                                                               rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                               rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                               rdfs:range xsd:float ,
                                                                                                                          xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResource
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResource> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResponseMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResponseMethod> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> ;
                                                                                                            rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReturnTrainScore
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReturnTrainScore> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ;
                                                                                                              rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRidgeAlpha
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRidgeAlpha> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ;
                                                                                                        rdfs:range xsd:float ,
                                                                                                                   xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRotation
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRotation> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRtol
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRtol> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> ;
                                                                                                  rdfs:range xsd:float ,
                                                                                                             xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSampleSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSampleSize> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod> ;
                                                                                                        rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSamplewise
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSamplewise> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod> ;
                                                                                                        rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoreFunc
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoreFunc> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoreName
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoreName> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoring
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoring> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSeeds
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSeeds> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSelection
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSelection> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSeparator
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSeparator> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShrinkThreshold
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShrinkThreshold> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod> ;
                                                                                                             rdfs:range xsd:float ,
                                                                                                                        xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShrinking
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShrinking> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShuffle
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShuffle> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> ;
                                                                                                     rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSimilarity
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSimilarity> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSmooth
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSmooth> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> ;
                                                                                                    rdfs:range xsd:float ,
                                                                                                               xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolver
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolver> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolverOptions
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolverOptions> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> ;
                                                                                                           rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSort
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSort> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod> ;
                                                                                                  rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSparse
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSparse> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod> ;
                                                                                                    rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSparseOutput
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSparseOutput> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSplitSign
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSplitSign> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSplitter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSplitter> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSquared
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSquared> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod> ;
                                                                                                     rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStackMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStackMethod> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStandardize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStandardize> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod> ;
                                                                                                         rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStep
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStep> rdf:type owl:DatatypeProperty ;
                                                                                                  rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                  rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> ;
                                                                                                  rdfs:range xsd:float ,
                                                                                                             xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopNInliers
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopNInliers> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                          rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopScore
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopScore> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCenters
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCenters> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCvValues
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCvValues> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStrategy
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStrategy> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStratify
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStratify> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> ;
                                                                                                      rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSubsample
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSubsample> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int ,
                                                                                                                  xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSvdMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSvdMethod> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSvdSolver
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSvdSolver> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ;
                                                                                                       rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTargetType
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTargetType> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTestScores
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTestScores> rdf:type owl:DatatypeProperty ;
                                                                                                        rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                        rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod> ;
                                                                                                        rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTestSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTestSize> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> ;
                                                                                                      rdfs:range xsd:float ,
                                                                                                                 xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThreshold
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThreshold> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int ,
                                                                                                                  xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThresholdLambda
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThresholdLambda> rdf:type owl:DatatypeProperty ;
                                                                                                             rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                             rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ;
                                                                                                             rdfs:range xsd:float ,
                                                                                                                        xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTol
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTol> rdf:type owl:DatatypeProperty ;
                                                                                                 rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                 rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> ,
                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                 rdfs:range xsd:float ,
                                                                                                            xsd:int ,
                                                                                                            xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTopicWordPrior
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTopicWordPrior> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                            rdfs:range xsd:float ,
                                                                                                                       xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTotalSamples
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTotalSamples> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ;
                                                                                                          rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTrainScores
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTrainScores> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod> ;
                                                                                                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTrainSize
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTrainSize> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> ;
                                                                                                       rdfs:range xsd:float ,
                                                                                                                  xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformAlgorithm
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformAlgorithm> rdf:type owl:DatatypeProperty ;
                                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                                rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformAlpha
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformAlpha> rdf:type owl:DatatypeProperty ;
                                                                                                            rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                            rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                            rdfs:range xsd:float ,
                                                                                                                       xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformMaxIter
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformMaxIter> rdf:type owl:DatatypeProperty ;
                                                                                                              rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                              rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                              rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformNNonzeroCoefs
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformNNonzeroCoefs> rdf:type owl:DatatypeProperty ;
                                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> ;
                                                                                                                    rdfs:range xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUInit> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUnitVariance
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUnitVariance> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> ;
                                                                                                          rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUnknownValue
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUnknownValue> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> ;
                                                                                                          rdfs:range xsd:int ,
                                                                                                                     xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVInit> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidate
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidate> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> ;
                                                                                                      rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidationFraction
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidationFraction> rdf:type owl:DatatypeProperty ;
                                                                                                                rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                                rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ;
                                                                                                                rdfs:range xsd:float ,
                                                                                                                           xsd:int ,
                                                                                                                           xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVarSmoothing
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVarSmoothing> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod> ;
                                                                                                          rdfs:range xsd:float ,
                                                                                                                     xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVerbose
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVerbose> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod> ;
                                                                                                     rdfs:range xsd:boolean ,
                                                                                                                xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVoting
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVoting> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> ;
                                                                                                    rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWInit
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWInit> rdf:type owl:DatatypeProperty ;
                                                                                                   rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                   rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ;
                                                                                                   rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarmStart
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarmStart> rdf:type owl:DatatypeProperty ;
                                                                                                       rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                       rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> ;
                                                                                                       rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarnFor
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarnFor> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWeights
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWeights> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> ;
                                                                                                     rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWhiten
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWhiten> rdf:type owl:DatatypeProperty ;
                                                                                                    rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                    rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> ;
                                                                                                    rdfs:range xsd:boolean ,
                                                                                                               xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWhitenSolver
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWhitenSolver> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> ;
                                                                                                          rdfs:range xsd:string .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithCentering
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithCentering> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> ;
                                                                                                           rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithMean
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithMean> rdf:type owl:DatatypeProperty ;
                                                                                                      rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                      rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod> ;
                                                                                                      rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithScaling
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithScaling> rdf:type owl:DatatypeProperty ;
                                                                                                         rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                         rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> ;
                                                                                                         rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithStd
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithStd> rdf:type owl:DatatypeProperty ;
                                                                                                     rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                     rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod> ;
                                                                                                     rdfs:range xsd:boolean .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWorkingMemory
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWorkingMemory> rdf:type owl:DatatypeProperty ;
                                                                                                           rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                           rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod> ;
                                                                                                           rdfs:range xsd:float ,
                                                                                                                      xsd:int .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamZeroDivision
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamZeroDivision> rdf:type owl:DatatypeProperty ;
                                                                                                          rdfs:subPropertyOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter> ;
                                                                                                          rdfs:domain <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod> ;
                                                                                                          rdfs:range xsd:string .


#################################################################
#    Classes
#################################################################

###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Bayesian ARD regression.

Fit the weights of a regression model, using an ARD prior. The weights of
the regression model are assumed to be in Gaussian distributions.
Also estimate the parameters lambda (precisions of the distributions of the
weights) and alpha (precision of the distribution of the noise).
The estimation is done by an iterative procedures (Evidence Maximization)

Read more in the :ref:`User Guide <bayesian_regression>`.

Parameters
----------
max_iter : int, default=None
    Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.

    .. versionchanged:: 1.3

tol : float, default=1e-3
    Stop the algorithm if w has converged.

alpha_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the alpha parameter.

alpha_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the alpha parameter.

lambda_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the lambda parameter.

lambda_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the lambda parameter.

compute_score : bool, default=False
    If True, compute the objective function at each step of the model.

threshold_lambda : float, default=10 000
    Threshold for removing (pruning) weights with high precision from
    the computation.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

verbose : bool, default=False
    Verbose mode when fitting the model.

n_iter : int
    Maximum number of iterations.

    .. deprecated:: 1.3
       `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
       `max_iter` instead.

Attributes
----------
coef_ : array-like of shape (n_features,)
    Coefficients of the regression model (mean of distribution)

alpha_ : float
   estimated precision of the noise.

lambda_ : array-like of shape (n_features,)
   estimated precisions of the weights.

sigma_ : array-like of shape (n_features, n_features)
    estimated variance-covariance matrix of the weights

scores_ : float
    if computed, value of the objective function (to be maximized)

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

    .. versionadded:: 1.3

intercept_ : float
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

X_offset_ : float
    If `fit_intercept=True`, offset subtracted for centering data to a
    zero mean. Set to np.zeros(n_features) otherwise.

X_scale_ : float
    Set to np.ones(n_features).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
BayesianRidge : Bayesian ridge regression.

Notes
-----
For an example, see :ref:`examples/linear_model/plot_ard.py
<sphx_glr_auto_examples_linear_model_plot_ard.py>`.

References
----------
D. J. C. MacKay, Bayesian nonlinear modeling for the prediction
competition, ASHRAE Transactions, 1994.

R. Salakhutdinov, Lecture notes on Statistical Machine Learning,
http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15
Their beta is our ``self.alpha_``
Their alpha is our ``self.lambda_``
ARD is a little different than the slide: only dimensions/features for
which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are
discarded.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.ARDRegression()
>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
ARDRegression()
>>> clf.predict([[1, 1]])
array([1.])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                         rdfs:comment """Accuracy classification score.

In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must *exactly* match the
corresponding set of labels in y_true.

Read more in the :ref:`User Guide <accuracy_score>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

normalize : bool, default=True
    If ``False``, return the number of correctly classified samples.
    Otherwise, return the fraction of correctly classified samples.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
score : float
    If ``normalize == True``, return the fraction of correctly
    classified samples (float), else returns the number of correctly
    classified samples (int).

    The best performance is 1 with ``normalize == True`` and the number
    of samples with ``normalize == False``.

See Also
--------
balanced_accuracy_score : Compute the balanced accuracy to deal with
    imbalanced datasets.
jaccard_score : Compute the Jaccard similarity coefficient score.
hamming_loss : Compute the average Hamming loss or Hamming distance between
    two sets of samples.
zero_one_loss : Compute the Zero-one classification loss. By default, the
    function will return the percentage of imperfectly predicted subsets.

Notes
-----
In binary classification, this function is equal to the `jaccard_score`
function.

Examples
--------
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2.0

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """An AdaBoost classifier.

An AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.

This class implements the algorithm based on [2]_.

Read more in the :ref:`User Guide <adaboost>`.

.. versionadded:: 0.14

Parameters
----------
estimator : object, default=None
    The base estimator from which the boosted ensemble is built.
    Support for sample weighting is required, as well as proper
    ``classes_`` and ``n_classes_`` attributes. If ``None``, then
    the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`
    initialized with `max_depth=1`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=50
    The maximum number of estimators at which boosting is terminated.
    In case of perfect fit, the learning procedure is stopped early.
    Values must be in the range `[1, inf)`.

learning_rate : float, default=1.0
    Weight applied to each classifier at each boosting iteration. A higher
    learning rate increases the contribution of each classifier. There is
    a trade-off between the `learning_rate` and `n_estimators` parameters.
    Values must be in the range `(0.0, inf)`.

algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'
    If 'SAMME.R' then use the SAMME.R real boosting algorithm.
    ``estimator`` must support calculation of class probabilities.
    If 'SAMME' then use the SAMME discrete boosting algorithm.
    The SAMME.R algorithm typically converges faster than SAMME,
    achieving a lower test error with fewer boosting iterations.

    .. deprecated:: 1.4
        `\"SAMME.R\"` is deprecated and will be removed in version 1.6.
        '\"SAMME\"' will become the default.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given at each `estimator` at each
    boosting iteration.
    Thus, it is only used when `estimator` exposes a `random_state`.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of classifiers
    The collection of fitted sub-estimators.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_classes_ : int
    The number of classes.

estimator_weights_ : ndarray of floats
    Weights for each estimator in the boosted ensemble.

estimator_errors_ : ndarray of floats
    Classification error for each estimator in the boosted
    ensemble.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances if supported by the
    ``estimator`` (when based on decision trees).

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
AdaBoostRegressor : An AdaBoost regressor that begins by fitting a
    regressor on the original dataset and then fits additional copies of
    the regressor on the same dataset but where the weights of instances
    are adjusted according to the error of the current prediction.

GradientBoostingClassifier : GB builds an additive model in a forward
    stage-wise fashion. Regression trees are fit on the negative gradient
    of the binomial or multinomial deviance loss function. Binary
    classification is a special case where only a single regression tree is
    induced.

sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning
    method used for classification.
    Creates a model that predicts the value of a target variable by
    learning simple decision rules inferred from the data features.

References
----------
.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of
       on-Line Learning and an Application to Boosting\", 1995.

.. [2] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"
       Statistics and its Interface 2.3 (2009): 349-360.
       <10.4310/SII.2009.v2.n3.a8>`

Examples
--------
>>> from sklearn.ensemble import AdaBoostClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)
>>> clf.fit(X, y)
AdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)
>>> clf.predict([[0, 0, 0, 0]])
array([1])
>>> clf.score(X, y)
0.96...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                             rdfs:comment """An AdaBoost regressor.

An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.

This class implements the algorithm known as AdaBoost.R2 [2].

Read more in the :ref:`User Guide <adaboost>`.

.. versionadded:: 0.14

Parameters
----------
estimator : object, default=None
    The base estimator from which the boosted ensemble is built.
    If ``None``, then the base estimator is
    :class:`~sklearn.tree.DecisionTreeRegressor` initialized with
    `max_depth=3`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=50
    The maximum number of estimators at which boosting is terminated.
    In case of perfect fit, the learning procedure is stopped early.
    Values must be in the range `[1, inf)`.

learning_rate : float, default=1.0
    Weight applied to each regressor at each boosting iteration. A higher
    learning rate increases the contribution of each regressor. There is
    a trade-off between the `learning_rate` and `n_estimators` parameters.
    Values must be in the range `(0.0, inf)`.

loss : {'linear', 'square', 'exponential'}, default='linear'
    The loss function to use when updating the weights after each
    boosting iteration.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given at each `estimator` at each
    boosting iteration.
    Thus, it is only used when `estimator` exposes a `random_state`.
    In addition, it controls the bootstrap of the weights used to train the
    `estimator` at each boosting iteration.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of regressors
    The collection of fitted sub-estimators.

estimator_weights_ : ndarray of floats
    Weights for each estimator in the boosted ensemble.

estimator_errors_ : ndarray of floats
    Regression error for each estimator in the boosted ensemble.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances if supported by the
    ``estimator`` (when based on decision trees).

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
AdaBoostClassifier : An AdaBoost classifier.
GradientBoostingRegressor : Gradient Boosting Classification Tree.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.

References
----------
.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of
       on-Line Learning and an Application to Boosting\", 1995.

.. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.

Examples
--------
>>> from sklearn.ensemble import AdaBoostRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
>>> regr.fit(X, y)
AdaBoostRegressor(n_estimators=100, random_state=0)
>>> regr.predict([[0, 0, 0, 0]])
array([4.7972...])
>>> regr.score(X, y)
0.9771...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod> rdf:type owl:Class ;
                                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                   rdfs:comment """Adjusted Mutual Information between two clusterings.

Adjusted Mutual Information (AMI) is an adjustment of the Mutual
Information (MI) score to account for chance. It accounts for the fact that
the MI is generally higher for two clusterings with a larger number of
clusters, regardless of whether there is actually more information shared.
For two clusterings :math:`U` and :math:`V`, the AMI is given as::

    AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching :math:`U` (``label_true``)
with :math:`V` (``labels_pred``) will return the same score value. This can
be useful to measure the agreement of two independent label assignments
strategies on the same dataset when the real ground truth is not known.

Be mindful that this function is an order of magnitude slower than other
metrics, such as the Adjusted Rand Index.

Read more in the :ref:`User Guide <mutual_info_score>`.

Parameters
----------
labels_true : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets, called :math:`U` in
    the above formula.

labels_pred : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets, called :math:`V` in
    the above formula.

average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'
    How to compute the normalizer in the denominator.

    .. versionadded:: 0.20

    .. versionchanged:: 0.22
       The default value of ``average_method`` changed from 'max' to
       'arithmetic'.

Returns
-------
ami: float (upperlimited by 1.0)
   The AMI returns a value of 1 when the two partitions are identical
   (ie perfectly matched). Random partitions (independent labellings) have
   an expected AMI around 0 on average hence can be negative. The value is
   in adjusted nats (based on the natural logarithm).

See Also
--------
adjusted_rand_score : Adjusted Rand Index.
mutual_info_score : Mutual Information (not adjusted for chance).

References
----------
.. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
   Clusterings Comparison: Variants, Properties, Normalization and
   Correction for Chance, JMLR
   <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_

.. [2] `Wikipedia entry for the Adjusted Mutual Information
   <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_

Examples
--------

Perfect labelings are both homogeneous and complete, hence have
score 1.0::

  >>> from sklearn.metrics.cluster import adjusted_mutual_info_score
  >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
  ... # doctest: +SKIP
  1.0
  >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
  ... # doctest: +SKIP
  1.0

If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the AMI is null::

  >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
  ... # doctest: +SKIP
  0.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedRandScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedRandScoreMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Rand index adjusted for chance.

The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings.

The raw RI score is then \"adjusted for chance\" into the ARI score
using the following scheme::

    ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)

The adjusted Rand index is thus ensured to have a value close to
0.0 for random labeling independently of the number of clusters and
samples and exactly 1.0 when the clusterings are identical (up to
a permutation). The adjusted Rand index is bounded below by -0.5 for
especially discordant clusterings.

ARI is a symmetric measure::

    adjusted_rand_score(a, b) == adjusted_rand_score(b, a)

Read more in the :ref:`User Guide <adjusted_rand_score>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=int
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,), dtype=int
    Cluster labels to evaluate.

Returns
-------
ARI : float
   Similarity score between -0.5 and 1.0. Random labelings have an ARI
   close to 0.0. 1.0 stands for perfect match.

See Also
--------
adjusted_mutual_info_score : Adjusted Mutual Information.

References
----------
.. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,
  Journal of Classification 1985
  https://link.springer.com/article/10.1007%2FBF01908075

.. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie
  adjusted Rand index, Psychological Methods 2004

.. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index

.. [Chacon] :doi:`Minimum adjusted Rand index for two clusterings of a given size,
  2022, J. E. Chacn and A. I. Rastrojo <10.1007/s11634-022-00491-w>`

Examples
--------
Perfectly matching labelings have a score of 1 even

  >>> from sklearn.metrics.cluster import adjusted_rand_score
  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
  1.0
  >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized::

  >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
  0.57...

ARI is symmetric, so labelings that have pure clusters with members
coming from the same classes but unnecessary splits are penalized::

  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
  0.57...

If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the ARI is very low::

  >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
  0.0

ARI may take a negative value for especially discordant labelings that
are a worse choice than the expected value of random labels::

  >>> adjusted_rand_score([0, 0, 1, 1], [0, 1, 0, 1])
  -0.5""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                               rdfs:comment """Perform Affinity Propagation Clustering of data.

Read more in the :ref:`User Guide <affinity_propagation>`.

Parameters
----------
damping : float, default=0.5
    Damping factor in the range `[0.5, 1.0)` is the extent to
    which the current value is maintained relative to
    incoming values (weighted 1 - damping). This in order
    to avoid numerical oscillations when updating these
    values (messages).

max_iter : int, default=200
    Maximum number of iterations.

convergence_iter : int, default=15
    Number of iterations with no change in the number
    of estimated clusters that stops the convergence.

copy : bool, default=True
    Make a copy of input data.

preference : array-like of shape (n_samples,) or float, default=None
    Preferences for each point - points with larger values of
    preferences are more likely to be chosen as exemplars. The number
    of exemplars, ie of clusters, is influenced by the input
    preferences value. If the preferences are not passed as arguments,
    they will be set to the median of the input similarities.

affinity : {'euclidean', 'precomputed'}, default='euclidean'
    Which affinity to use. At the moment 'precomputed' and
    ``euclidean`` are supported. 'euclidean' uses the
    negative squared euclidean distance between points.

verbose : bool, default=False
    Whether to be verbose.

random_state : int, RandomState instance or None, default=None
    Pseudo-random number generator to control the starting state.
    Use an int for reproducible results across function calls.
    See the :term:`Glossary <random_state>`.

    .. versionadded:: 0.23
        this parameter was previously hardcoded as 0.

Attributes
----------
cluster_centers_indices_ : ndarray of shape (n_clusters,)
    Indices of cluster centers.

cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Cluster centers (if affinity != ``precomputed``).

labels_ : ndarray of shape (n_samples,)
    Labels of each point.

affinity_matrix_ : ndarray of shape (n_samples, n_samples)
    Stores the affinity matrix used in ``fit``.

n_iter_ : int
    Number of iterations taken to converge.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
AgglomerativeClustering : Recursively merges the pair of
    clusters that minimally increases a given linkage distance.
FeatureAgglomeration : Similar to AgglomerativeClustering,
    but recursively merges features instead of samples.
KMeans : K-Means clustering.
MiniBatchKMeans : Mini-Batch K-Means clustering.
MeanShift : Mean shift clustering using a flat kernel.
SpectralClustering : Apply clustering to a projection
    of the normalized Laplacian.

Notes
-----
For an example, see :ref:`examples/cluster/plot_affinity_propagation.py
<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.

The algorithmic complexity of affinity propagation is quadratic
in the number of points.

When the algorithm does not converge, it will still return a arrays of
``cluster_center_indices`` and labels if there are any exemplars/clusters,
however they may be degenerate and should be used with caution.

When ``fit`` does not converge, ``cluster_centers_`` is still populated
however it may be degenerate. In such a case, proceed with caution.
If ``fit`` does not converge and fails to produce any ``cluster_centers_``
then ``predict`` will label every sample as ``-1``.

When all training samples have equal similarities and equal preferences,
the assignment of cluster centers and labels depends on the preference.
If the preference is smaller than the similarities, ``fit`` will result in
a single cluster center and label ``0`` for every sample. Otherwise, every
training sample becomes its own cluster center and is assigned a unique
label.

References
----------

Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages
Between Data Points\", Science Feb. 2007

Examples
--------
>>> from sklearn.cluster import AffinityPropagation
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 4], [4, 0]])
>>> clustering = AffinityPropagation(random_state=5).fit(X)
>>> clustering
AffinityPropagation(random_state=5)
>>> clustering.labels_
array([0, 0, 0, 1, 1, 1])
>>> clustering.predict([[0, 0], [4, 4]])
array([0, 1])
>>> clustering.cluster_centers_
array([[1, 2],
       [4, 2]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod> rdf:type owl:Class ;
                                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                   rdfs:comment """Agglomerative Clustering.

Recursively merges pair of clusters of sample data; uses linkage distance.

Read more in the :ref:`User Guide <hierarchical_clustering>`.

Parameters
----------
n_clusters : int or None, default=2
    The number of clusters to find. It must be ``None`` if
    ``distance_threshold`` is not ``None``.

metric : str or callable, default=\"euclidean\"
    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",
    \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only
    \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed
    as input for the fit method.

    .. versionadded:: 1.2

    .. deprecated:: 1.4
       `metric=None` is deprecated in 1.4 and will be removed in 1.6.
       Let `metric` be the default value (i.e. `\"euclidean\"`) instead.

memory : str or object with the joblib.Memory interface, default=None
    Used to cache the output of the computation of the tree.
    By default, no caching is done. If a string is given, it is the
    path to the caching directory.

connectivity : array-like or callable, default=None
    Connectivity matrix. Defines for each sample the neighboring
    samples following a given structure of the data.
    This can be a connectivity matrix itself or a callable that transforms
    the data into a connectivity matrix, such as derived from
    `kneighbors_graph`. Default is ``None``, i.e, the
    hierarchical clustering algorithm is unstructured.

compute_full_tree : 'auto' or bool, default='auto'
    Stop early the construction of the tree at ``n_clusters``. This is
    useful to decrease computation time if the number of clusters is not
    small compared to the number of samples. This option is useful only
    when specifying a connectivity matrix. Note also that when varying the
    number of clusters and using caching, it may be advantageous to compute
    the full tree. It must be ``True`` if ``distance_threshold`` is not
    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent
    to `True` when `distance_threshold` is not `None` or that `n_clusters`
    is inferior to the maximum between 100 or `0.02 * n_samples`.
    Otherwise, \"auto\" is equivalent to `False`.

linkage : {'ward', 'complete', 'average', 'single'}, default='ward'
    Which linkage criterion to use. The linkage criterion determines which
    distance to use between sets of observation. The algorithm will merge
    the pairs of cluster that minimize this criterion.

    - 'ward' minimizes the variance of the clusters being merged.
    - 'average' uses the average of the distances of each observation of
      the two sets.
    - 'complete' or 'maximum' linkage uses the maximum distances between
      all observations of the two sets.
    - 'single' uses the minimum of the distances between all observations
      of the two sets.

    .. versionadded:: 0.20
        Added the 'single' option

distance_threshold : float, default=None
    The linkage distance threshold at or above which clusters will not be
    merged. If not ``None``, ``n_clusters`` must be ``None`` and
    ``compute_full_tree`` must be ``True``.

    .. versionadded:: 0.21

compute_distances : bool, default=False
    Computes distances between clusters even if `distance_threshold` is not
    used. This can be used to make dendrogram visualization, but introduces
    a computational and memory overhead.

    .. versionadded:: 0.24

Attributes
----------
n_clusters_ : int
    The number of clusters found by the algorithm. If
    ``distance_threshold=None``, it will be equal to the given
    ``n_clusters``.

labels_ : ndarray of shape (n_samples)
    Cluster labels for each point.

n_leaves_ : int
    Number of leaves in the hierarchical tree.

n_connected_components_ : int
    The estimated number of connected components in the graph.

    .. versionadded:: 0.21
        ``n_connected_components_`` was added to replace ``n_components_``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

children_ : array-like of shape (n_samples-1, 2)
    The children of each non-leaf node. Values less than `n_samples`
    correspond to leaves of the tree which are the original samples.
    A node `i` greater than or equal to `n_samples` is a non-leaf
    node and has children `children_[i - n_samples]`. Alternatively
    at the i-th iteration, children[i][0] and children[i][1]
    are merged to form node `n_samples + i`.

distances_ : array-like of shape (n_nodes-1,)
    Distances between nodes in the corresponding place in `children_`.
    Only computed if `distance_threshold` is used or `compute_distances`
    is set to `True`.

See Also
--------
FeatureAgglomeration : Agglomerative clustering but for features instead of
    samples.
ward_tree : Hierarchical clustering with ward linkage.

Examples
--------
>>> from sklearn.cluster import AgglomerativeClustering
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 4], [4, 0]])
>>> clustering = AgglomerativeClustering().fit(X)
>>> clustering
AgglomerativeClustering()
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AucMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AucMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                               rdfs:comment """Compute Area Under the Curve (AUC) using the trapezoidal rule.

This is a general function, given points on a curve.  For computing the
area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative
way to summarize a precision-recall curve, see
:func:`average_precision_score`.

Parameters
----------
x : array-like of shape (n,)
    X coordinates. These must be either monotonic increasing or monotonic
    decreasing.
y : array-like of shape (n,)
    Y coordinates.

Returns
-------
auc : float
    Area Under the Curve.

See Also
--------
roc_auc_score : Compute the area under the ROC curve.
average_precision_score : Compute average precision from prediction scores.
precision_recall_curve : Compute precision-recall pairs for different
    probability thresholds.

Examples
--------
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> pred = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
>>> metrics.auc(fpr, tpr)
0.75""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                 rdfs:comment """Compute average precision (AP) from prediction scores.

AP summarizes a precision-recall curve as the weighted mean of precisions
achieved at each threshold, with the increase in recall from the previous
threshold used as the weight:

.. math::
    \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n

where :math:`P_n` and :math:`R_n` are the precision and recall at the nth
threshold [1]_. This implementation is not interpolated and is different
from computing the area under the precision-recall curve with the
trapezoidal rule, which uses linear interpolation and can be too
optimistic.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
    True binary labels or binary label indicators.

y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by :term:`decision_function` on some classifiers).

average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'
    If ``None``, the scores for each class are returned. Otherwise,
    this determines the type of averaging performed on the data:

    ``'micro'``:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    ``'samples'``:
        Calculate metrics for each instance, and find their average.

    Will be ignored when ``y_true`` is binary.

pos_label : int, float, bool or str, default=1
    The label of the positive class. Only applied to binary ``y_true``.
    For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
average_precision : float
    Average precision score.

See Also
--------
roc_auc_score : Compute the area under the ROC curve.
precision_recall_curve : Compute precision-recall pairs for different
    probability thresholds.

Notes
-----
.. versionchanged:: 0.19
  Instead of linearly interpolating between operating points, precisions
  are weighted by the change in recall since the last operating point.

References
----------
.. [1] `Wikipedia entry for the Average precision
       <https://en.wikipedia.org/w/index.php?title=Information_retrieval&
       oldid=793358396#Average_precision>`_

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import average_precision_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> average_precision_score(y_true, y_scores)
0.83...
>>> y_true = np.array([0, 0, 1, 1, 2, 2])
>>> y_scores = np.array([
...     [0.7, 0.2, 0.1],
...     [0.4, 0.3, 0.3],
...     [0.1, 0.8, 0.1],
...     [0.2, 0.3, 0.5],
...     [0.4, 0.4, 0.2],
...     [0.1, 0.2, 0.7],
... ])
>>> average_precision_score(y_true, y_scores)
0.77...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                             rdfs:comment """A Bagging classifier.

A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.

This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting [1]_. If samples are drawn with
replacement, then the method is known as Bagging [2]_. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces [3]_. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches [4]_.

Read more in the :ref:`User Guide <bagging>`.

.. versionadded:: 0.15

Parameters
----------
estimator : object, default=None
    The base estimator to fit on random subsets of the dataset.
    If None, then the base estimator is a
    :class:`~sklearn.tree.DecisionTreeClassifier`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=10
    The number of base estimators in the ensemble.

max_samples : int or float, default=1.0
    The number of samples to draw from X to train each base estimator (with
    replacement by default, see `bootstrap` for more details).

    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples.

max_features : int or float, default=1.0
    The number of features to draw from X to train each base estimator (
    without replacement by default, see `bootstrap_features` for more
    details).

    - If int, then draw `max_features` features.
    - If float, then draw `max(1, int(max_features * n_features_in_))` features.

bootstrap : bool, default=True
    Whether samples are drawn with replacement. If False, sampling
    without replacement is performed.

bootstrap_features : bool, default=False
    Whether features are drawn with replacement.

oob_score : bool, default=False
    Whether to use out-of-bag samples to estimate
    the generalization error. Only available if bootstrap=True.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit
    a whole new ensemble. See :term:`the Glossary <warm_start>`.

    .. versionadded:: 0.17
       *warm_start* constructor parameter.

n_jobs : int, default=None
    The number of jobs to run in parallel for both :meth:`fit` and
    :meth:`predict`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the random resampling of the original dataset
    (sample wise and feature wise).
    If the base estimator accepts a `random_state` attribute, a different
    seed is generated for each instance in the ensemble.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

estimators_ : list of estimators
    The collection of fitted base estimators.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

estimators_features_ : list of arrays
    The subset of drawn features for each base estimator.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_classes_ : int or list
    The number of classes.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_decision_function_ : ndarray of shape (n_samples, n_classes)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_decision_function_` might contain NaN. This attribute exists
    only when ``oob_score`` is True.

See Also
--------
BaggingRegressor : A Bagging regressor.

References
----------

.. [1] L. Breiman, \"Pasting small votes for classification in large
       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.

.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,
       1996.

.. [3] T. Ho, \"The random subspace method for constructing decision
       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
       1998.

.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine
       Learning and Knowledge Discovery in Databases, 346-361, 2012.

Examples
--------
>>> from sklearn.svm import SVC
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=100, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = BaggingClassifier(estimator=SVC(),
...                         n_estimators=10, random_state=0).fit(X, y)
>>> clf.predict([[0, 0, 0, 0]])
array([1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """A Bagging regressor.

A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.

This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting [1]_. If samples are drawn with
replacement, then the method is known as Bagging [2]_. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces [3]_. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches [4]_.

Read more in the :ref:`User Guide <bagging>`.

.. versionadded:: 0.15

Parameters
----------
estimator : object, default=None
    The base estimator to fit on random subsets of the dataset.
    If None, then the base estimator is a
    :class:`~sklearn.tree.DecisionTreeRegressor`.

    .. versionadded:: 1.2
       `base_estimator` was renamed to `estimator`.

n_estimators : int, default=10
    The number of base estimators in the ensemble.

max_samples : int or float, default=1.0
    The number of samples to draw from X to train each base estimator (with
    replacement by default, see `bootstrap` for more details).

    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples.

max_features : int or float, default=1.0
    The number of features to draw from X to train each base estimator (
    without replacement by default, see `bootstrap_features` for more
    details).

    - If int, then draw `max_features` features.
    - If float, then draw `max(1, int(max_features * n_features_in_))` features.

bootstrap : bool, default=True
    Whether samples are drawn with replacement. If False, sampling
    without replacement is performed.

bootstrap_features : bool, default=False
    Whether features are drawn with replacement.

oob_score : bool, default=False
    Whether to use out-of-bag samples to estimate
    the generalization error. Only available if bootstrap=True.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit
    a whole new ensemble. See :term:`the Glossary <warm_start>`.

n_jobs : int, default=None
    The number of jobs to run in parallel for both :meth:`fit` and
    :meth:`predict`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the random resampling of the original dataset
    (sample wise and feature wise).
    If the base estimator accepts a `random_state` attribute, a different
    seed is generated for each instance in the ensemble.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

Attributes
----------
estimator_ : estimator
    The base estimator from which the ensemble is grown.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

estimators_ : list of estimators
    The collection of fitted sub-estimators.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

estimators_features_ : list of arrays
    The subset of drawn features for each base estimator.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_prediction_ : ndarray of shape (n_samples,)
    Prediction computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_prediction_` might contain NaN. This attribute exists only
    when ``oob_score`` is True.

See Also
--------
BaggingClassifier : A Bagging classifier.

References
----------

.. [1] L. Breiman, \"Pasting small votes for classification in large
       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.

.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,
       1996.

.. [3] T. Ho, \"The random subspace method for constructing decision
       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
       1998.

.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine
       Learning and Knowledge Discovery in Databases, 346-361, 2012.

Examples
--------
>>> from sklearn.svm import SVR
>>> from sklearn.ensemble import BaggingRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_samples=100, n_features=4,
...                        n_informative=2, n_targets=1,
...                        random_state=0, shuffle=False)
>>> regr = BaggingRegressor(estimator=SVR(),
...                         n_estimators=10, random_state=0).fit(X, y)
>>> regr.predict([[0, 0, 0, 0]])
array([-2.8720...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                 rdfs:comment """Compute the balanced accuracy.

The balanced accuracy in binary and multiclass classification problems to
deal with imbalanced datasets. It is defined as the average of recall
obtained on each class.

The best value is 1 and the worst value is 0 when ``adjusted=False``.

Read more in the :ref:`User Guide <balanced_accuracy_score>`.

.. versionadded:: 0.20

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

adjusted : bool, default=False
    When true, the result is adjusted for chance, so that random
    performance would score 0, while keeping perfect performance at a score
    of 1.

Returns
-------
balanced_accuracy : float
    Balanced accuracy score.

See Also
--------
average_precision_score : Compute average precision (AP) from prediction
    scores.
precision_score : Compute the precision score.
recall_score : Compute the recall score.
roc_auc_score : Compute Area Under the Receiver Operating Characteristic
    Curve (ROC AUC) from prediction scores.

Notes
-----
Some literature promotes alternative definitions of balanced accuracy. Our
definition is equivalent to :func:`accuracy_score` with class-balanced
sample weights, and shares desirable properties with the binary case.
See the :ref:`User Guide <balanced_accuracy_score>`.

References
----------
.. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).
       The balanced accuracy and its posterior distribution.
       Proceedings of the 20th International Conference on Pattern
       Recognition, 3121-24.
.. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).
       `Fundamentals of Machine Learning for Predictive Data Analytics:
       Algorithms, Worked Examples, and Case Studies
       <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.

Examples
--------
>>> from sklearn.metrics import balanced_accuracy_score
>>> y_true = [0, 1, 0, 0, 1, 0]
>>> y_pred = [0, 1, 0, 0, 0, 1]
>>> balanced_accuracy_score(y_true, y_pred)
0.625""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BallTreeMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BallTreeMethod> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                    rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Bayesian ridge regression.

Fit a Bayesian ridge model. See the Notes section for details on this
implementation and the optimization of the regularization parameters
lambda (precision of the weights) and alpha (precision of the noise).

Read more in the :ref:`User Guide <bayesian_regression>`.

Parameters
----------
max_iter : int, default=None
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion. If `None`, it
    corresponds to `max_iter=300`.

    .. versionchanged:: 1.3

tol : float, default=1e-3
    Stop the algorithm if w has converged.

alpha_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the alpha parameter.

alpha_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the alpha parameter.

lambda_1 : float, default=1e-6
    Hyper-parameter : shape parameter for the Gamma distribution prior
    over the lambda parameter.

lambda_2 : float, default=1e-6
    Hyper-parameter : inverse scale parameter (rate parameter) for the
    Gamma distribution prior over the lambda parameter.

alpha_init : float, default=None
    Initial value for alpha (precision of the noise).
    If not set, alpha_init is 1/Var(y).

        .. versionadded:: 0.22

lambda_init : float, default=None
    Initial value for lambda (precision of the weights).
    If not set, lambda_init is 1.

        .. versionadded:: 0.22

compute_score : bool, default=False
    If True, compute the log marginal likelihood at each iteration of the
    optimization.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model.
    The intercept is not treated as a probabilistic parameter
    and thus has no associated variance. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

verbose : bool, default=False
    Verbose mode when fitting the model.

n_iter : int
    Maximum number of iterations. Should be greater than or equal to 1.

    .. deprecated:: 1.3
       `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
       `max_iter` instead.

Attributes
----------
coef_ : array-like of shape (n_features,)
    Coefficients of the regression model (mean of distribution)

intercept_ : float
    Independent term in decision function. Set to 0.0 if
    `fit_intercept = False`.

alpha_ : float
   Estimated precision of the noise.

lambda_ : float
   Estimated precision of the weights.

sigma_ : array-like of shape (n_features, n_features)
    Estimated variance-covariance matrix of the weights

scores_ : array-like of shape (n_iter_+1,)
    If computed_score is True, value of the log marginal likelihood (to be
    maximized) at each iteration of the optimization. The array starts
    with the value of the log marginal likelihood obtained for the initial
    values of alpha and lambda and ends with the value obtained for the
    estimated alpha and lambda.

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

X_offset_ : ndarray of shape (n_features,)
    If `fit_intercept=True`, offset subtracted for centering data to a
    zero mean. Set to np.zeros(n_features) otherwise.

X_scale_ : ndarray of shape (n_features,)
    Set to np.ones(n_features).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
ARDRegression : Bayesian ARD regression.

Notes
-----
There exist several strategies to perform Bayesian ridge regression. This
implementation is based on the algorithm described in Appendix A of
(Tipping, 2001) where updates of the regularization parameters are done as
suggested in (MacKay, 1992). Note that according to A New
View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these
update rules do not guarantee that the marginal likelihood is increasing
between two consecutive iterations of the optimization.

References
----------
D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,
Vol. 4, No. 3, 1992.

M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,
Journal of Machine Learning Research, Vol. 1, 2001.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.BayesianRidge()
>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
BayesianRidge()
>>> clf.predict([[1, 1]])
array([1.])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                       rdfs:comment """Naive Bayes classifier for multivariate Bernoulli models.

Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.

Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.

Parameters
----------
alpha : float or array-like of shape (n_features,), default=1.0
    Additive (Laplace/Lidstone) smoothing parameter
    (set alpha=0 and force_alpha=True, for no smoothing).

force_alpha : bool, default=True
    If False and alpha is less than 1e-10, it will set alpha to
    1e-10. If True, alpha will remain unchanged. This may cause
    numerical errors if alpha is too close to 0.

    .. versionadded:: 1.2
    .. versionchanged:: 1.4
       The default value of `force_alpha` changed to `True`.

binarize : float or None, default=0.0
    Threshold for binarizing (mapping to booleans) of sample features.
    If None, input is presumed to already consist of binary vectors.

fit_prior : bool, default=True
    Whether to learn class prior probabilities or not.
    If false, a uniform prior will be used.

class_prior : array-like of shape (n_classes,), default=None
    Prior probabilities of the classes. If specified, the priors are not
    adjusted according to the data.

Attributes
----------
class_count_ : ndarray of shape (n_classes,)
    Number of samples encountered for each class during fitting. This
    value is weighted by the sample weight when provided.

class_log_prior_ : ndarray of shape (n_classes,)
    Log probability of each class (smoothed).

classes_ : ndarray of shape (n_classes,)
    Class labels known to the classifier

feature_count_ : ndarray of shape (n_classes, n_features)
    Number of samples encountered for each (class, feature)
    during fitting. This value is weighted by the sample weight when
    provided.

feature_log_prob_ : ndarray of shape (n_classes, n_features)
    Empirical log probability of features given a class, P(x_i|y).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
CategoricalNB : Naive Bayes classifier for categorical features.
ComplementNB : The Complement Naive Bayes classifier
    described in Rennie et al. (2003).
GaussianNB : Gaussian Naive Bayes (GaussianNB).
MultinomialNB : Naive Bayes classifier for multinomial models.

References
----------
C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html

A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41-48.

V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).

Examples
--------
>>> import numpy as np
>>> rng = np.random.RandomState(1)
>>> X = rng.randint(5, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB()
>>> print(clf.predict(X[2:3]))
[3]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                        rdfs:comment """Bernoulli Restricted Boltzmann Machine (RBM).

A Restricted Boltzmann Machine with binary visible units and
binary hidden units. Parameters are estimated using Stochastic Maximum
Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)
[2].

The time complexity of this implementation is ``O(d ** 2)`` assuming
d ~ n_features ~ n_components.

Read more in the :ref:`User Guide <rbm>`.

Parameters
----------
n_components : int, default=256
    Number of binary hidden units.

learning_rate : float, default=0.1
    The learning rate for weight updates. It is *highly* recommended
    to tune this hyper-parameter. Reasonable values are in the
    10**[0., -3.] range.

batch_size : int, default=10
    Number of examples per minibatch.

n_iter : int, default=10
    Number of iterations/sweeps over the training dataset to perform
    during training.

verbose : int, default=0
    The verbosity level. The default, zero, means silent mode. Range
    of values is [0, inf].

random_state : int, RandomState instance or None, default=None
    Determines random number generation for:

    - Gibbs sampling from visible and hidden layers.

    - Initializing components, sampling from layers during fit.

    - Corrupting the data when scoring samples.

    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
intercept_hidden_ : array-like of shape (n_components,)
    Biases of the hidden units.

intercept_visible_ : array-like of shape (n_features,)
    Biases of the visible units.

components_ : array-like of shape (n_components, n_features)
    Weight matrix, where `n_features` is the number of
    visible units and `n_components` is the number of hidden units.

h_samples_ : array-like of shape (batch_size, n_components)
    Hidden Activation sampled from the model distribution,
    where `batch_size` is the number of examples per minibatch and
    `n_components` is the number of hidden units.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.neural_network.MLPRegressor : Multi-layer Perceptron regressor.
sklearn.neural_network.MLPClassifier : Multi-layer Perceptron classifier.
sklearn.decomposition.PCA : An unsupervised linear dimensionality
    reduction model.

References
----------

[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for
    deep belief nets. Neural Computation 18, pp 1527-1554.
    https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf

[2] Tieleman, T. Training Restricted Boltzmann Machines using
    Approximations to the Likelihood Gradient. International Conference
    on Machine Learning (ICML) 2008

Examples
--------

>>> import numpy as np
>>> from sklearn.neural_network import BernoulliRBM
>>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
>>> model = BernoulliRBM(n_components=2)
>>> model.fit(X)
BernoulliRBM(n_components=2)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                     rdfs:comment """Binarize data (set feature values to 0 or 1) according to a threshold.

Values greater than the threshold map to 1, while values less than
or equal to the threshold map to 0. With the default threshold of 0,
only positive values map to 1.

Binarization is a common operation on text count data where the
analyst can decide to only consider the presence or absence of a
feature rather than a quantified number of occurrences for instance.

It can also be used as a pre-processing step for estimators that
consider boolean random variables (e.g. modelled using the Bernoulli
distribution in a Bayesian setting).

Read more in the :ref:`User Guide <preprocessing_binarization>`.

Parameters
----------
threshold : float, default=0.0
    Feature values below or equal to this are replaced by 0, above it by 1.
    Threshold may not be less than 0 for operations on sparse matrices.

copy : bool, default=True
    Set to False to perform inplace binarization and avoid a copy (if
    the input is already a numpy array or a scipy.sparse CSR matrix).

Attributes
----------
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
binarize : Equivalent function without the estimator API.
KBinsDiscretizer : Bin continuous data into intervals.
OneHotEncoder : Encode categorical features as a one-hot numeric array.

Notes
-----
If the input is a sparse matrix, only the non-zero values are subject
to update by the :class:`Binarizer` class.

This estimator is :term:`stateless` and does not need to be fitted.
However, we recommend to call :meth:`fit_transform` instead of
:meth:`transform`, as parameter validation is only performed in
:meth:`fit`.

Examples
--------
>>> from sklearn.preprocessing import Binarizer
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> transformer = Binarizer().fit(X)  # fit does nothing.
>>> transformer
Binarizer()
>>> transformer.transform(X)
array([[1., 0., 1.],
       [1., 0., 0.],
       [0., 1., 0.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment """Implements the BIRCH clustering algorithm.

It is a memory-efficient, online-learning algorithm provided as an
alternative to :class:`MiniBatchKMeans`. It constructs a tree
data structure with the cluster centroids being read off the leaf.
These can be either the final cluster centroids or can be provided as input
to another clustering algorithm such as :class:`AgglomerativeClustering`.

Read more in the :ref:`User Guide <birch>`.

.. versionadded:: 0.16

Parameters
----------
threshold : float, default=0.5
    The radius of the subcluster obtained by merging a new sample and the
    closest subcluster should be lesser than the threshold. Otherwise a new
    subcluster is started. Setting this value to be very low promotes
    splitting and vice-versa.

branching_factor : int, default=50
    Maximum number of CF subclusters in each node. If a new samples enters
    such that the number of subclusters exceed the branching_factor then
    that node is split into two nodes with the subclusters redistributed
    in each. The parent subcluster of that node is removed and two new
    subclusters are added as parents of the 2 split nodes.

n_clusters : int, instance of sklearn.cluster model or None, default=3
    Number of clusters after the final clustering step, which treats the
    subclusters from the leaves as new samples.

    - `None` : the final clustering step is not performed and the
      subclusters are returned as they are.

    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model
      is fit treating the subclusters as new samples and the initial data
      is mapped to the label of the closest subcluster.

    - `int` : the model fit is :class:`AgglomerativeClustering` with
      `n_clusters` set to be equal to the int.

compute_labels : bool, default=True
    Whether or not to compute labels for each fit.

copy : bool, default=True
    Whether or not to make a copy of the given data. If set to False,
    the initial data will be overwritten.

Attributes
----------
root_ : _CFNode
    Root of the CFTree.

dummy_leaf_ : _CFNode
    Start pointer to all the leaves.

subcluster_centers_ : ndarray
    Centroids of all subclusters read directly from the leaves.

subcluster_labels_ : ndarray
    Labels assigned to the centroids of the subclusters after
    they are clustered globally.

labels_ : ndarray of shape (n_samples,)
    Array of labels assigned to the input data.
    if partial_fit is used instead of fit, they are assigned to the
    last batch of data.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MiniBatchKMeans : Alternative implementation that does incremental updates
    of the centers' positions using mini-batches.

Notes
-----
The tree data structure consists of nodes with each node consisting of
a number of subclusters. The maximum number of subclusters in a node
is determined by the branching factor. Each subcluster maintains a
linear sum, squared sum and the number of samples in that subcluster.
In addition, each subcluster can also have a node as its child, if the
subcluster is not a member of a leaf node.

For a new point entering the root, it is merged with the subcluster closest
to it and the linear sum, squared sum and the number of samples of that
subcluster are updated. This is done recursively till the properties of
the leaf node are updated.

References
----------
* Tian Zhang, Raghu Ramakrishnan, Maron Livny
  BIRCH: An efficient data clustering method for large databases.
  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf

* Roberto Perdisci
  JBirch - Java implementation of BIRCH clustering algorithm
  https://code.google.com/archive/p/jbirch

Examples
--------
>>> from sklearn.cluster import Birch
>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]
>>> brc = Birch(n_clusters=None)
>>> brc.fit(X)
Birch(n_clusters=None)
>>> brc.predict(X)
array([0, 0, 0, 1, 1, 1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """Bisecting K-Means clustering.

Read more in the :ref:`User Guide <bisect_k_means>`.

.. versionadded:: 1.1

Parameters
----------
n_clusters : int, default=8
    The number of clusters to form as well as the number of
    centroids to generate.

init : {'k-means++', 'random'} or callable, default='random'
    Method for initialization:

    'k-means++' : selects initial cluster centers for k-mean
    clustering in a smart way to speed up convergence. See section
    Notes in k_init for more details.

    'random': choose `n_clusters` observations (rows) at random from data
    for the initial centroids.

    If a callable is passed, it should take arguments X, n_clusters and a
    random state and return an initialization.

n_init : int, default=1
    Number of time the inner k-means algorithm will be run with different
    centroid seeds in each bisection.
    That will result producing for each bisection best output of n_init
    consecutive runs in terms of inertia.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization
    in inner K-Means. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

max_iter : int, default=300
    Maximum number of iterations of the inner k-means algorithm at each
    bisection.

verbose : int, default=0
    Verbosity mode.

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations  to declare
    convergence. Used in inner k-means algorithm at each bisection to pick
    best possible clusters.

copy_x : bool, default=True
    When pre-computing distances it is more numerically accurate to center
    the data first. If copy_x is True (default), then the original data is
    not modified. If False, the original data is modified, and put back
    before the function returns, but small numerical differences may be
    introduced by subtracting and then adding the data mean. Note that if
    the original data is not C-contiguous, a copy will be made even if
    copy_x is False. If the original data is sparse, but not in CSR format,
    a copy will be made even if copy_x is False.

algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"
    Inner K-means algorithm used in bisection.
    The classical EM-style algorithm is `\"lloyd\"`.
    The `\"elkan\"` variation can be more efficient on some datasets with
    well-defined clusters, by using the triangle inequality. However it's
    more memory intensive due to the allocation of an extra array of shape
    `(n_samples, n_clusters)`.

bisecting_strategy : {\"biggest_inertia\", \"largest_cluster\"},            default=\"biggest_inertia\"
    Defines how bisection should be performed:

     - \"biggest_inertia\" means that BisectingKMeans will always check
        all calculated cluster for cluster with biggest SSE
        (Sum of squared errors) and bisect it. This approach concentrates on
        precision, but may be costly in terms of execution time (especially for
        larger amount of data points).

     - \"largest_cluster\" - BisectingKMeans will always split cluster with
        largest amount of points assigned to it from all clusters
        previously calculated. That should work faster than picking by SSE
        ('biggest_inertia') and may produce similar results in most cases.

Attributes
----------
cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers. If the algorithm stops before fully
    converging (see ``tol`` and ``max_iter``), these will not be
    consistent with ``labels_``.

labels_ : ndarray of shape (n_samples,)
    Labels of each point.

inertia_ : float
    Sum of squared distances of samples to their closest cluster center,
    weighted by the sample weights if provided.

n_features_in_ : int
    Number of features seen during :term:`fit`.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

See Also
--------
KMeans : Original implementation of K-Means algorithm.

Notes
-----
It might be inefficient when n_cluster is less than 3, due to unnecessary
calculations for that case.

Examples
--------
>>> from sklearn.cluster import BisectingKMeans
>>> import numpy as np
>>> X = np.array([[1, 1], [10, 1], [3, 1],
...               [10, 0], [2, 1], [10, 2],
...               [10, 8], [10, 9], [10, 10]])
>>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)
>>> bisect_means.labels_
array([0, 2, 0, 2, 0, 2, 1, 1, 1], dtype=int32)
>>> bisect_means.predict([[0, 0], [12, 3]])
array([0, 2], dtype=int32)
>>> bisect_means.cluster_centers_
array([[ 2., 1.],
       [10., 9.],
       [10., 1.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BoostingRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BoostingRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                          rdfs:comment """Compute the Brier score loss.

The smaller the Brier score loss, the better, hence the naming with \"loss\".
The Brier score measures the mean squared difference between the predicted
probability and the actual outcome. The Brier score always
takes on a value between zero and one, since this is the largest
possible difference between a predicted probability (which must be
between zero and one) and the actual outcome (which can take on values
of only 0 and 1). It can be decomposed as the sum of refinement loss and
calibration loss.

The Brier score is appropriate for binary and categorical outcomes that
can be structured as true or false, but is inappropriate for ordinal
variables which can take on three or more values (this is because the
Brier score assumes that all possible outcomes are equivalently
\"distant\" from one another). Which label is considered to be the positive
label is controlled via the parameter `pos_label`, which defaults to
the greater label unless `y_true` is all 0 or all -1, in which case
`pos_label` defaults to 1.

Read more in the :ref:`User Guide <brier_score_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True targets.

y_prob : array-like of shape (n_samples,)
    Probabilities of the positive class.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

pos_label : int, float, bool or str, default=None
    Label of the positive class. `pos_label` will be inferred in the
    following manner:

    * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;
    * else if `y_true` contains string, an error will be raised and
      `pos_label` should be explicitly specified;
    * otherwise, `pos_label` defaults to the greater label,
      i.e. `np.unique(y_true)[-1]`.

Returns
-------
score : float
    Brier score loss.

References
----------
.. [1] `Wikipedia entry for the Brier score
        <https://en.wikipedia.org/wiki/Brier_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import brier_score_loss
>>> y_true = np.array([0, 1, 1, 0])
>>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])
>>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])
>>> brier_score_loss(y_true, y_prob)
0.037...
>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
0.037...
>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")
0.037...
>>> brier_score_loss(y_true, np.array(y_prob) > 0.5)
0.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CalinskiHarabaszScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CalinskiHarabaszScoreMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                 rdfs:comment """Compute the Calinski and Harabasz score.

It is also known as the Variance Ratio Criterion.

The score is defined as ratio of the sum of between-cluster dispersion and
of within-cluster dispersion.

Read more in the :ref:`User Guide <calinski_harabasz_index>`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    A list of ``n_features``-dimensional data points. Each row corresponds
    to a single data point.

labels : array-like of shape (n_samples,)
    Predicted labels for each sample.

Returns
-------
score : float
    The resulting Calinski-Harabasz score.

References
----------
.. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster
   analysis\". Communications in Statistics
   <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_

Examples
--------
>>> from sklearn.datasets import make_blobs
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import calinski_harabasz_score
>>> X, _ = make_blobs(random_state=0)
>>> kmeans = KMeans(n_clusters=3, random_state=0,).fit(X)
>>> calinski_harabasz_score(X, kmeans.labels_)
114.8...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Naive Bayes classifier for categorical features.

The categorical Naive Bayes classifier is suitable for classification with
discrete features that are categorically distributed. The categories of
each feature are drawn from a categorical distribution.

Read more in the :ref:`User Guide <categorical_naive_bayes>`.

Parameters
----------
alpha : float, default=1.0
    Additive (Laplace/Lidstone) smoothing parameter
    (set alpha=0 and force_alpha=True, for no smoothing).

force_alpha : bool, default=True
    If False and alpha is less than 1e-10, it will set alpha to
    1e-10. If True, alpha will remain unchanged. This may cause
    numerical errors if alpha is too close to 0.

    .. versionadded:: 1.2
    .. versionchanged:: 1.4
       The default value of `force_alpha` changed to `True`.

fit_prior : bool, default=True
    Whether to learn class prior probabilities or not.
    If false, a uniform prior will be used.

class_prior : array-like of shape (n_classes,), default=None
    Prior probabilities of the classes. If specified, the priors are not
    adjusted according to the data.

min_categories : int or array-like of shape (n_features,), default=None
    Minimum number of categories per feature.

    - integer: Sets the minimum number of categories per feature to
      `n_categories` for each features.
    - array-like: shape (n_features,) where `n_categories[i]` holds the
      minimum number of categories for the ith column of the input.
    - None (default): Determines the number of categories automatically
      from the training data.

    .. versionadded:: 0.24

Attributes
----------
category_count_ : list of arrays of shape (n_features,)
    Holds arrays of shape (n_classes, n_categories of respective feature)
    for each feature. Each array provides the number of samples
    encountered for each class and category of the specific feature.

class_count_ : ndarray of shape (n_classes,)
    Number of samples encountered for each class during fitting. This
    value is weighted by the sample weight when provided.

class_log_prior_ : ndarray of shape (n_classes,)
    Smoothed empirical log probability for each class.

classes_ : ndarray of shape (n_classes,)
    Class labels known to the classifier

feature_log_prob_ : list of arrays of shape (n_features,)
    Holds arrays of shape (n_classes, n_categories of respective feature)
    for each feature. Each array provides the empirical log probability
    of categories given the respective feature and class, ``P(x_i|y)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_categories_ : ndarray of shape (n_features,), dtype=np.int64
    Number of categories for each feature. This value is
    inferred from the data or set by the minimum number of categories.

    .. versionadded:: 0.24

See Also
--------
BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
ComplementNB : Complement Naive Bayes classifier.
GaussianNB : Gaussian Naive Bayes.
MultinomialNB : Naive Bayes classifier for multinomial models.

Examples
--------
>>> import numpy as np
>>> rng = np.random.RandomState(1)
>>> X = rng.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import CategoricalNB
>>> clf = CategoricalNB()
>>> clf.fit(X, y)
CategoricalNB()
>>> print(clf.predict(X[2:3]))
[3]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                        rdfs:comment """Determine scorer from user options.

A TypeError will be thrown if the estimator cannot be scored.

Parameters
----------
estimator : estimator object implementing 'fit'
    The object to use to fit the data.

scoring : str or callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.
    If None, the provided estimator object's `score` method is used.

allow_none : bool, default=False
    If no scoring is specified and the estimator has no score function, we
    can either return None or raise an exception.

Returns
-------
scoring : callable
    A scorer callable object / function with signature
    ``scorer(estimator, X, y)``.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.metrics import check_scoring
>>> from sklearn.tree import DecisionTreeClassifier
>>> X, y = load_iris(return_X_y=True)
>>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)
>>> scorer = check_scoring(classifier, scoring='accuracy')
>>> scorer(classifier, X, y)
0.96...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Chi2Method
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Chi2Method> rdf:type owl:Class ;
                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                rdfs:comment """Compute chi-squared stats between each non-negative feature and class.

This score can be used to select the `n_features` features with the
highest values for the test chi-squared statistic from X, which must
contain only **non-negative features** such as booleans or frequencies
(e.g., term counts in document classification), relative to the classes.

Recall that the chi-square test measures dependence between stochastic
variables, so using this function \"weeds out\" the features that are the
most likely to be independent of class and therefore irrelevant for
classification.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Sample vectors.

y : array-like of shape (n_samples,)
    Target vector (class labels).

Returns
-------
chi2 : ndarray of shape (n_features,)
    Chi2 statistics for each feature.

p_values : ndarray of shape (n_features,)
    P-values for each feature.

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
f_regression : F-value between label/feature for regression tasks.

Notes
-----
Complexity of this algorithm is O(n_classes * n_features).

Examples
--------
>>> import numpy as np
>>> from sklearn.feature_selection import chi2
>>> X = np.array([[1, 1, 3],
...               [0, 1, 5],
...               [5, 4, 1],
...               [6, 6, 2],
...               [1, 4, 0],
...               [0, 0, 0]])
>>> y = np.array([1, 1, 0, 0, 2, 2])
>>> chi2_stats, p_values = chi2(X, y)
>>> chi2_stats
array([15.3...,  6.5       ,  8.9...])
>>> p_values
array([0.0004..., 0.0387..., 0.0116... ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                 rdfs:comment """Compute binary classification positive and negative likelihood ratios.

The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`
where the sensitivity or recall is the ratio `tp / (tp + fn)` and the
specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1
- sensitivity) / specificity`. Here `tp` is the number of true positives,
`fp` the number of false positives, `tn` is the number of true negatives and
`fn` the number of false negatives. Both class likelihood ratios can be used
to obtain post-test probabilities given a pre-test probability.

`LR+` ranges from 1 to infinity. A `LR+` of 1 indicates that the probability
of predicting the positive class is the same for samples belonging to either
class; therefore, the test is useless. The greater `LR+` is, the more a
positive prediction is likely to be a true positive when compared with the
pre-test probability. A value of `LR+` lower than 1 is invalid as it would
indicate that the odds of a sample being a true positive decrease with
respect to the pre-test odds.

`LR-` ranges from 0 to 1. The closer it is to 0, the lower the probability
of a given sample to be a false negative. A `LR-` of 1 means the test is
useless because the odds of having the condition did not change after the
test. A value of `LR-` greater than 1 invalidates the classifier as it
indicates an increase in the odds of a sample belonging to the positive
class after being classified as negative. This is the case when the
classifier systematically predicts the opposite of the true label.

A typical application in medicine is to identify the positive/negative class
to the presence/absence of a disease, respectively; the classifier being a
diagnostic test; the pre-test probability of an individual having the
disease can be the prevalence of such disease (proportion of a particular
population found to be affected by a medical condition); and the post-test
probabilities would be the probability that the condition is truly present
given a positive test result.

Read more in the :ref:`User Guide <class_likelihood_ratios>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    List of labels to index the matrix. This may be used to select the
    positive and negative classes with the ordering `labels=[negative_class,
    positive_class]`. If `None` is given, those that appear at least once in
    `y_true` or `y_pred` are used in sorted order.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

raise_warning : bool, default=True
    Whether or not a case-specific warning message is raised when there is a
    zero division. Even if the error is not raised, the function will return
    nan in such cases.

Returns
-------
(positive_likelihood_ratio, negative_likelihood_ratio) : tuple
    A tuple of two float, the first containing the Positive likelihood ratio
    and the second the Negative likelihood ratio.

Warns
-----
When `false positive == 0`, the positive likelihood ratio is undefined.
When `true negative == 0`, the negative likelihood ratio is undefined.
When `true positive + false negative == 0` both ratios are undefined.
In such cases, `UserWarning` will be raised if raise_warning=True.

References
----------
.. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing
       <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import class_likelihood_ratios
>>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])
(1.5, 0.75)
>>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])
>>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])
>>> class_likelihood_ratios(y_true, y_pred)
(1.33..., 0.66...)
>>> y_true = np.array([\"non-zebra\", \"zebra\", \"non-zebra\", \"zebra\", \"non-zebra\"])
>>> y_pred = np.array([\"zebra\", \"zebra\", \"non-zebra\", \"non-zebra\", \"non-zebra\"])
>>> class_likelihood_ratios(y_true, y_pred)
(1.5, 0.75)

To avoid ambiguities, use the notation `labels=[negative_class,
positive_class]`

>>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])
>>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])
>>> class_likelihood_ratios(y_true, y_pred, labels=[\"non-cat\", \"cat\"])
(1.5, 0.75)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                rdfs:comment """Build a text report showing the main classification metrics.

Read more in the :ref:`User Guide <classification_report>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like of shape (n_labels,), default=None
    Optional list of label indices to include in the report.

target_names : array-like of shape (n_labels,), default=None
    Optional display names matching the labels (same order).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

digits : int, default=2
    Number of digits for formatting output floating point values.
    When ``output_dict`` is ``True``, this will be ignored and the
    returned values will not be rounded.

output_dict : bool, default=False
    If True, return output as dict.

    .. versionadded:: 0.20

zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"
    Sets the value to return when there is a zero division. If set to
    \"warn\", this acts as 0, but warnings are also raised.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
report : str or dict
    Text summary of the precision, recall, F1 score for each class.
    Dictionary returned if output_dict is True. Dictionary has the
    following structure::

        {'label 1': {'precision':0.5,
                     'recall':1.0,
                     'f1-score':0.67,
                     'support':1},
         'label 2': { ... },
          ...
        }

    The reported averages include macro average (averaging the unweighted
    mean per label), weighted average (averaging the support-weighted mean
    per label), and sample average (only for multilabel classification).
    Micro average (averaging the total true positives, false negatives and
    false positives) is only shown for multi-label or multi-class
    with a subset of classes, because it corresponds to accuracy
    otherwise and would be the same for all metrics.
    See also :func:`precision_recall_fscore_support` for more details
    on averages.

    Note that in binary classification, recall of the positive class
    is also known as \"sensitivity\"; recall of the negative class is
    \"specificity\".

See Also
--------
precision_recall_fscore_support: Compute precision, recall, F-measure and
    support for each class.
confusion_matrix: Compute confusion matrix to evaluate the accuracy of a
    classification.
multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.

Examples
--------
>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 2]
>>> y_pred = [0, 0, 2, 2, 1]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
              precision    recall  f1-score   support
<BLANKLINE>
     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3
<BLANKLINE>
    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
<BLANKLINE>
>>> y_pred = [1, 1, 0]
>>> y_true = [1, 1, 1]
>>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))
              precision    recall  f1-score   support
<BLANKLINE>
           1       1.00      0.67      0.80         3
           2       0.00      0.00      0.00         0
           3       0.00      0.00      0.00         0
<BLANKLINE>
   micro avg       1.00      0.67      0.80         3
   macro avg       0.33      0.22      0.27         3
weighted avg       1.00      0.67      0.80         3
<BLANKLINE>""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Clustering
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Clustering> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                           rdfs:comment """Compute Cohen's kappa: a statistic that measures inter-annotator agreement.

This function computes Cohen's kappa [1]_, a score that expresses the level
of agreement between two annotators on a classification problem. It is
defined as

.. math::
    \\kappa = (p_o - p_e) / (1 - p_e)

where :math:`p_o` is the empirical probability of agreement on the label
assigned to any sample (the observed agreement ratio), and :math:`p_e` is
the expected agreement when both annotators assign labels randomly.
:math:`p_e` is estimated using a per-annotator empirical prior over the
class labels [2]_.

Read more in the :ref:`User Guide <cohen_kappa>`.

Parameters
----------
y1 : array-like of shape (n_samples,)
    Labels assigned by the first annotator.

y2 : array-like of shape (n_samples,)
    Labels assigned by the second annotator. The kappa statistic is
    symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.

labels : array-like of shape (n_classes,), default=None
    List of labels to index the matrix. This may be used to select a
    subset of labels. If `None`, all labels that appear at least once in
    ``y1`` or ``y2`` are used.

weights : {'linear', 'quadratic'}, default=None
    Weighting type to calculate the score. `None` means no weighted;
    \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
kappa : float
    The kappa statistic, which is a number between -1 and 1. The maximum
    value means complete agreement; zero or lower means chance agreement.

References
----------
.. [1] :doi:`J. Cohen (1960). \"A coefficient of agreement for nominal scales\".
       Educational and Psychological Measurement 20(1):37-46.
       <10.1177/001316446002000104>`
.. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for
       computational linguistics\". Computational Linguistics 34(4):555-596
       <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.
.. [3] `Wikipedia entry for the Cohen's kappa
        <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.

Examples
--------
>>> from sklearn.metrics import cohen_kappa_score
>>> y1 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"positive\"]
>>> y2 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"negative\"]
>>> cohen_kappa_score(y1, y2)
0.6875""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                        rdfs:comment """The Complement Naive Bayes classifier described in Rennie et al. (2003).

The Complement Naive Bayes classifier was designed to correct the \"severe
assumptions\" made by the standard Multinomial Naive Bayes classifier. It is
particularly suited for imbalanced data sets.

Read more in the :ref:`User Guide <complement_naive_bayes>`.

.. versionadded:: 0.20

Parameters
----------
alpha : float or array-like of shape (n_features,), default=1.0
    Additive (Laplace/Lidstone) smoothing parameter
    (set alpha=0 and force_alpha=True, for no smoothing).

force_alpha : bool, default=True
    If False and alpha is less than 1e-10, it will set alpha to
    1e-10. If True, alpha will remain unchanged. This may cause
    numerical errors if alpha is too close to 0.

    .. versionadded:: 1.2
    .. versionchanged:: 1.4
       The default value of `force_alpha` changed to `True`.

fit_prior : bool, default=True
    Only used in edge case with a single class in the training set.

class_prior : array-like of shape (n_classes,), default=None
    Prior probabilities of the classes. Not used.

norm : bool, default=False
    Whether or not a second normalization of the weights is performed. The
    default behavior mirrors the implementations found in Mahout and Weka,
    which do not follow the full algorithm described in Table 9 of the
    paper.

Attributes
----------
class_count_ : ndarray of shape (n_classes,)
    Number of samples encountered for each class during fitting. This
    value is weighted by the sample weight when provided.

class_log_prior_ : ndarray of shape (n_classes,)
    Smoothed empirical log probability for each class. Only used in edge
    case with a single class in the training set.

classes_ : ndarray of shape (n_classes,)
    Class labels known to the classifier

feature_all_ : ndarray of shape (n_features,)
    Number of samples encountered for each feature during fitting. This
    value is weighted by the sample weight when provided.

feature_count_ : ndarray of shape (n_classes, n_features)
    Number of samples encountered for each (class, feature) during fitting.
    This value is weighted by the sample weight when provided.

feature_log_prob_ : ndarray of shape (n_classes, n_features)
    Empirical weights for class complements.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
CategoricalNB : Naive Bayes classifier for categorical features.
GaussianNB : Gaussian Naive Bayes.
MultinomialNB : Naive Bayes classifier for multinomial models.

References
----------
Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).
Tackling the poor assumptions of naive bayes text classifiers. In ICML
(Vol. 3, pp. 616-623).
https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf

Examples
--------
>>> import numpy as np
>>> rng = np.random.RandomState(1)
>>> X = rng.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import ComplementNB
>>> clf = ComplementNB()
>>> clf.fit(X, y)
ComplementNB()
>>> print(clf.predict(X[2:3]))
[3]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CompletenessScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CompletenessScoreMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Compute completeness metric of a cluster labeling given a ground truth.

A clustering result satisfies completeness if all the data points
that are members of a given class are elements of the same cluster.

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is not symmetric: switching ``label_true`` with ``label_pred``
will return the :func:`homogeneity_score` which will be different in
general.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Cluster labels to evaluate.

Returns
-------
completeness : float
   Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.

See Also
--------
homogeneity_score : Homogeneity metric of cluster labeling.
v_measure_score : V-Measure (NMI with arithmetic mean option).

References
----------

.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
   conditional entropy-based external cluster evaluation measure
   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_

Examples
--------

Perfect labelings are complete::

  >>> from sklearn.metrics.cluster import completeness_score
  >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Non-perfect labelings that assign all classes members to the same clusters
are still complete::

  >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))
  1.0
  >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))
  0.999...

If classes members are split across different clusters, the
assignment cannot be complete::

  >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))
  0.0
  >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))
  0.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                           rdfs:comment """Compute confusion matrix to evaluate the accuracy of a classification.

By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`
is equal to the number of observations known to be in group :math:`i` and
predicted to be in group :math:`j`.

Thus in binary classification, the count of true negatives is
:math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is
:math:`C_{1,1}` and false positives is :math:`C_{0,1}`.

Read more in the :ref:`User Guide <confusion_matrix>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

labels : array-like of shape (n_classes), default=None
    List of labels to index the matrix. This may be used to reorder
    or select a subset of labels.
    If ``None`` is given, those that appear at least once
    in ``y_true`` or ``y_pred`` are used in sorted order.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

normalize : {'true', 'pred', 'all'}, default=None
    Normalizes confusion matrix over the true (rows), predicted (columns)
    conditions or all the population. If None, confusion matrix will not be
    normalized.

Returns
-------
C : ndarray of shape (n_classes, n_classes)
    Confusion matrix whose i-th row and j-th
    column entry indicates the number of
    samples with true label being i-th class
    and predicted label being j-th class.

See Also
--------
ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix
    given an estimator, the data, and the label.
ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix
    given the true and predicted labels.
ConfusionMatrixDisplay : Confusion Matrix visualization.

References
----------
.. [1] `Wikipedia entry for the Confusion matrix
       <https://en.wikipedia.org/wiki/Confusion_matrix>`_
       (Wikipedia and other references may use a different
       convention for axes).

Examples
--------
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]
>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]
>>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

In the binary case, we can extract true positives, etc. as follows:

>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()
>>> (tn, fp, fn, tp)
(0, 2, 1, 1)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                          rdfs:comment """The similarity of two sets of biclusters.

Similarity between individual biclusters is computed. Then the
best matching between sets is found using the Hungarian algorithm.
The final score is the sum of similarities divided by the size of
the larger set.

Read more in the :ref:`User Guide <biclustering>`.

Parameters
----------
a : tuple (rows, columns)
    Tuple of row and column indicators for a set of biclusters.

b : tuple (rows, columns)
    Another set of biclusters like ``a``.

similarity : 'jaccard' or callable, default='jaccard'
    May be the string \"jaccard\" to use the Jaccard coefficient, or
    any function that takes four arguments, each of which is a 1d
    indicator vector: (a_rows, a_columns, b_rows, b_columns).

Returns
-------
consensus_score : float
   Consensus score, a non-negative value, sum of similarities
   divided by size of larger set.

References
----------

* Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis
  for bicluster acquisition
  <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.

Examples
--------
>>> from sklearn.metrics import consensus_score
>>> a = ([[True, False], [False, True]], [[False, True], [True, False]])
>>> b = ([[False, True], [True, False]], [[True, False], [False, True]])
>>> consensus_score(a, b, similarity='jaccard')
1.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CoverageErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CoverageErrorMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                         rdfs:comment """Coverage error measure.

Compute how far we need to go through the ranked scores to cover all
true labels. The best value is equal to the average number
of labels in ``y_true`` per sample.

Ties in ``y_scores`` are broken by giving maximal rank that would have
been assigned to all tied values.

Note: Our implementation's score is 1 greater than the one given in
Tsoumakas et al., 2010. This extends it to handle the degenerate case
in which an instance has 0 true labels.

Read more in the :ref:`User Guide <coverage_error>`.

Parameters
----------
y_true : array-like of shape (n_samples, n_labels)
    True binary labels in binary indicator format.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by \"decision_function\" on some classifiers).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
coverage_error : float
    The coverage error.

References
----------
.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).
       Mining multi-label data. In Data mining and knowledge discovery
       handbook (pp. 667-685). Springer US.

Examples
--------
>>> from sklearn.metrics import coverage_error
>>> y_true = [[1, 0, 0], [0, 1, 1]]
>>> y_score = [[1, 0, 0], [0, 1, 1]]
>>> coverage_error(y_true, y_score)
1.5""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                rdfs:comment """:math:`D^2` regression score function, fraction of absolute error explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical median of `y_true`
as constant prediction, disregarding the input features,
gets a :math:`D^2` score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.1

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

Returns
-------
score : float or ndarray of floats
    The :math:`D^2` score with an absolute error deviance
    or ndarray of scores if 'multioutput' is 'raw_values'.

Notes
-----
Like :math:`R^2`, :math:`D^2` score may be negative
(it need not actually be the square of a quantity D).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

 References
----------
.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. \"Statistical Learning with Sparsity: The Lasso and
       Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_absolute_error_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> d2_absolute_error_score(y_true, y_pred)
0.764...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')
0.691...
>>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')
array([0.8125    , 0.57142857])
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> d2_absolute_error_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> d2_absolute_error_score(y_true, y_pred)
0.0
>>> y_true = [1, 2, 3]
>>> y_pred = [3, 2, 1]
>>> d2_absolute_error_score(y_true, y_pred)
-1.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                          rdfs:comment """:math:`D^2` regression score function, fraction of pinball loss explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical alpha-quantile of
`y_true` as constant prediction, disregarding the input features,
gets a :math:`D^2` score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.1

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

alpha : float, default=0.5
    Slope of the pinball deviance. It determines the quantile level alpha
    for which the pinball deviance and also D2 are optimal.
    The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

Returns
-------
score : float or ndarray of floats
    The :math:`D^2` score with a pinball deviance
    or ndarray of scores if `multioutput='raw_values'`.

Notes
-----
Like :math:`R^2`, :math:`D^2` score may be negative
(it need not actually be the square of a quantity D).

This metric is not well-defined for a single point and will return a NaN
value if n_samples is less than two.

 References
----------
.. [1] Eq. (7) of `Koenker, Roger; Machado, Jos A. F. (1999).
       \"Goodness of Fit and Related Inference Processes for Quantile Regression\"
       <https://doi.org/10.1080/01621459.1999.10473882>`_
.. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. \"Statistical Learning with Sparsity: The Lasso and
       Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_pinball_score
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 3, 3]
>>> d2_pinball_score(y_true, y_pred)
0.5
>>> d2_pinball_score(y_true, y_pred, alpha=0.9)
0.772...
>>> d2_pinball_score(y_true, y_pred, alpha=0.1)
-1.045...
>>> d2_pinball_score(y_true, y_true, alpha=0.1)
1.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                          rdfs:comment """:math:`D^2` regression score function, fraction of Tweedie deviance explained.

Best possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A model that always uses the empirical mean of `y_true` as
constant prediction, disregarding the input features, gets a D^2 score of 0.0.

Read more in the :ref:`User Guide <d2_score>`.

.. versionadded:: 1.0

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

power : float, default=0
    Tweedie power parameter. Either power <= 0 or power >= 1.

    The higher `p` the less weight is given to extreme
    deviations between true and predicted targets.

    - power < 0: Extreme stable distribution. Requires: y_pred > 0.
    - power = 0 : Normal distribution, output corresponds to r2_score.
      y_true and y_pred can be any real numbers.
    - power = 1 : Poisson distribution. Requires: y_true >= 0 and
      y_pred > 0.
    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0
      and y_pred > 0.
    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.
    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0
      and y_pred > 0.
    - otherwise : Positive stable distribution. Requires: y_true > 0
      and y_pred > 0.

Returns
-------
z : float or ndarray of floats
    The D^2 score.

Notes
-----
This is not a symmetric function.

Like R^2, D^2 score may be negative (it need not actually be the square of
a quantity D).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

References
----------
.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.
       Wainwright. \"Statistical Learning with Sparsity: The Lasso and
       Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/

Examples
--------
>>> from sklearn.metrics import d2_tweedie_score
>>> y_true = [0.5, 1, 2.5, 7]
>>> y_pred = [1, 1, 5, 3.5]
>>> d2_tweedie_score(y_true, y_pred)
0.285...
>>> d2_tweedie_score(y_true, y_pred, power=1)
0.487...
>>> d2_tweedie_score(y_true, y_pred, power=2)
0.630...
>>> d2_tweedie_score(y_true, y_true, power=2)
1.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod> rdf:type owl:Class ;
                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                  rdfs:comment """Perform DBSCAN clustering from vector array or distance matrix.

DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them.
Good for data which contains clusters of similar density.

The worst case memory complexity of DBSCAN is :math:`O({n}^2)`, which can
occur when the `eps` param is large and `min_samples` is low.

Read more in the :ref:`User Guide <dbscan>`.

Parameters
----------
eps : float, default=0.5
    The maximum distance between two samples for one to be considered
    as in the neighborhood of the other. This is not a maximum bound
    on the distances of points within a cluster. This is the most
    important DBSCAN parameter to choose appropriately for your data set
    and distance function.

min_samples : int, default=5
    The number of samples (or total weight) in a neighborhood for a point to
    be considered as a core point. This includes the point itself. If
    `min_samples` is set to a higher value, DBSCAN will find denser clusters,
    whereas if it is set to a lower value, the found clusters will be more
    sparse.

metric : str, or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string or callable, it must be one of
    the options allowed by :func:`sklearn.metrics.pairwise_distances` for
    its metric parameter.
    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors for DBSCAN.

    .. versionadded:: 0.17
       metric *precomputed* to accept precomputed sparse matrix.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

    .. versionadded:: 0.19

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    The algorithm to be used by the NearestNeighbors module
    to compute pointwise distances and find nearest neighbors.
    See NearestNeighbors module documentation for details.

leaf_size : int, default=30
    Leaf size passed to BallTree or cKDTree. This can affect the speed
    of the construction and query, as well as the memory required
    to store the tree. The optimal value depends
    on the nature of the problem.

p : float, default=None
    The power of the Minkowski metric to be used to calculate distance
    between points. If None, then ``p=2`` (equivalent to the Euclidean
    distance).

n_jobs : int, default=None
    The number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
core_sample_indices_ : ndarray of shape (n_core_samples,)
    Indices of core samples.

components_ : ndarray of shape (n_core_samples, n_features)
    Copy of each core sample found by training.

labels_ : ndarray of shape (n_samples)
    Cluster labels for each point in the dataset given to fit().
    Noisy samples are given the label -1.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
OPTICS : A similar clustering at multiple values of eps. Our implementation
    is optimized for memory usage.

Notes
-----
For an example, see :ref:`examples/cluster/plot_dbscan.py
<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.

This implementation bulk-computes all neighborhood queries, which increases
the memory complexity to O(n.d) where d is the average number of neighbors,
while original DBSCAN had memory complexity O(n). It may attract a higher
memory complexity when querying these nearest neighborhoods, depending
on the ``algorithm``.

One way to avoid the query complexity is to pre-compute sparse
neighborhoods in chunks using
:func:`NearestNeighbors.radius_neighbors_graph
<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
``mode='distance'``, then using ``metric='precomputed'`` here.

Another way to reduce memory and computation time is to remove
(near-)duplicate points and use ``sample_weight`` instead.

:class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory
usage.

References
----------
Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based
Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"
<https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.
In: Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996

Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
:doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"
<10.1145/3068335>`
ACM Transactions on Database Systems (TODS), 42(3), 19.

Examples
--------
>>> from sklearn.cluster import DBSCAN
>>> import numpy as np
>>> X = np.array([[1, 2], [2, 2], [2, 3],
...               [8, 7], [8, 8], [25, 80]])
>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
>>> clustering.labels_
array([ 0,  0,  0,  1,  1, -1])
>>> clustering
DBSCAN(eps=3, min_samples=2)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataProcessing
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataProcessing> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplitting
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplitting> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DaviesBouldinScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DaviesBouldinScoreMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                              rdfs:comment """Compute the Davies-Bouldin score.

The score is defined as the average similarity measure of each cluster with
its most similar cluster, where similarity is the ratio of within-cluster
distances to between-cluster distances. Thus, clusters which are farther
apart and less dispersed will result in a better score.

The minimum score is zero, with lower values indicating better clustering.

Read more in the :ref:`User Guide <davies-bouldin_index>`.

.. versionadded:: 0.20

Parameters
----------
X : array-like of shape (n_samples, n_features)
    A list of ``n_features``-dimensional data points. Each row corresponds
    to a single data point.

labels : array-like of shape (n_samples,)
    Predicted labels for each sample.

Returns
-------
score: float
    The resulting Davies-Bouldin score.

References
----------
.. [1] Davies, David L.; Bouldin, Donald W. (1979).
   `\"A Cluster Separation Measure\"
   <https://ieeexplore.ieee.org/document/4766909>`__.
   IEEE Transactions on Pattern Analysis and Machine Intelligence.
   PAMI-1 (2): 224-227

Examples
--------
>>> from sklearn.metrics import davies_bouldin_score
>>> X = [[0, 1], [1, 1], [3, 4]]
>>> labels = [0, 0, 1]
>>> davies_bouldin_score(X, labels)
0.12...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                    rdfs:comment """Compute Discounted Cumulative Gain.

Sum the true scores ranked in the order induced by the predicted scores,
after applying a logarithmic discount.

This ranking metric yields a high value if true labels are ranked high by
``y_score``.

Usually the Normalized Discounted Cumulative Gain (NDCG, computed by
ndcg_score) is preferred.

Parameters
----------
y_true : array-like of shape (n_samples, n_labels)
    True targets of multilabel classification, or true scores of entities
    to be ranked.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates, confidence values,
    or non-thresholded measure of decisions (as returned by
    \"decision_function\" on some classifiers).

k : int, default=None
    Only consider the highest k scores in the ranking. If None, use all
    outputs.

log_base : float, default=2
    Base of the logarithm used for the discount. A low value means a
    sharper discount (top results are more important).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If `None`, all samples are given the same weight.

ignore_ties : bool, default=False
    Assume that there are no ties in y_score (which is likely to be the
    case if y_score is continuous) for efficiency gains.

Returns
-------
discounted_cumulative_gain : float
    The averaged sample DCG scores.

See Also
--------
ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted
    Cumulative Gain (the DCG obtained for a perfect ranking), in order to
    have a score between 0 and 1.

References
----------
`Wikipedia entry for Discounted Cumulative Gain
<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.

Jarvelin, K., & Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.

Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013).

McSherry, F., & Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import dcg_score
>>> # we have groud-truth relevance of some answers to a query:
>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])
>>> # we predict scores for the answers
>>> scores = np.asarray([[.1, .2, .3, 4, 70]])
>>> dcg_score(true_relevance, scores)
9.49...
>>> # we can set k to truncate the sum; only top k answers contribute
>>> dcg_score(true_relevance, scores, k=2)
5.63...
>>> # now we have some ties in our prediction
>>> scores = np.asarray([[1, 0, 0, 0, 1]])
>>> # by default ties are averaged, so here we get the average true
>>> # relevance of our top predictions: (10 + 5) / 2 = 7.5
>>> dcg_score(true_relevance, scores, k=1)
7.5
>>> # we can choose to ignore ties for faster results, but only
>>> # if we know there aren't ties in our scores, otherwise we get
>>> # wrong results:
>>> dcg_score(true_relevance,
...           scores, k=1, ignore_ties=True)
5.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod> rdf:type owl:Class ;
                                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule> ;
                                                                                                                  rdfs:comment """A decision tree classifier.

Read more in the :ref:`User Guide <tree>`.

Parameters
----------
criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"
    The function to measure the quality of a split. Supported criteria are
    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the
    Shannon information gain, see :ref:`tree_mathematical_formulation`.

splitter : {\"best\", \"random\"}, default=\"best\"
    The strategy used to choose the split at each node. Supported
    strategies are \"best\" to choose the best split and \"random\" to choose
    the best random split.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : int, float or {\"sqrt\", \"log2\"}, default=None
    The number of features to consider when looking for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a fraction and
          `max(1, int(max_features * n_features_in_))` features are considered at
          each split.
        - If \"sqrt\", then `max_features=sqrt(n_features)`.
        - If \"log2\", then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of the estimator. The features are always
    randomly permuted at each split, even if ``splitter`` is set to
    ``\"best\"``. When ``max_features < n_features``, the algorithm will
    select ``max_features`` at random at each split before finding the best
    split among them. But the best found split may vary across different
    runs, even if ``max_features=n_features``. That is the case, if the
    improvement of the criterion is identical for several splits and one
    split has to be selected at random. To obtain a deterministic behaviour
    during fitting, ``random_state`` has to be fixed to an integer.
    See :term:`Glossary <random_state>` for details.

max_leaf_nodes : int, default=None
    Grow a tree with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

class_weight : dict, list of dict or \"balanced\", default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If None, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonic increase
      - 0: no constraint
      - -1: monotonic decrease

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multiclass classifications (i.e. when `n_classes > 2`),
      - multioutput classifications (i.e. when `n_outputs_ > 1`),
      - classifications trained on data with missing values.

    The constraints hold over the probability of the positive class.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
classes_ : ndarray of shape (n_classes,) or list of ndarray
    The classes labels (single output problem),
    or a list of arrays of class labels (multi-output problem).

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance [4]_.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

max_features_ : int
    The inferred value of max_features.

n_classes_ : int or list of int
    The number of classes (for single output problems),
    or a list containing the number of classes for each
    output (for multi-output problems).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

tree_ : Tree instance
    The underlying Tree object. Please refer to
    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
    for basic usage of these attributes.

See Also
--------
DecisionTreeRegressor : A decision tree regressor.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

The :meth:`predict` method operates using the :func:`numpy.argmax`
function on the outputs of :meth:`predict_proba`. This means that in
case the highest predicted probabilities are tied, the classifier will
predict the tied class with the lowest index in :term:`classes_`.

References
----------

.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning

.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification
       and Regression Trees\", Wadsworth, Belmont, CA, 1984.

.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical
       Learning\", Springer, 2009.

.. [4] L. Breiman, and A. Cutler, \"Random Forests\",
       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.tree import DecisionTreeClassifier
>>> clf = DecisionTreeClassifier(random_state=0)
>>> iris = load_iris()
>>> cross_val_score(clf, iris.data, iris.target, cv=10)
...                             # doctest: +SKIP
...
array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,
        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule> ;
                                                                                                                 rdfs:comment """A decision tree regressor.

Read more in the :ref:`User Guide <tree>`.

Parameters
----------
criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\",             \"poisson\"}, default=\"squared_error\"
    The function to measure the quality of a split. Supported criteria
    are \"squared_error\" for the mean squared error, which is equal to
    variance reduction as feature selection criterion and minimizes the L2
    loss using the mean of each terminal node, \"friedman_mse\", which uses
    mean squared error with Friedman's improvement score for potential
    splits, \"absolute_error\" for the mean absolute error, which minimizes
    the L1 loss using the median of each terminal node, and \"poisson\" which
    uses reduction in Poisson deviance to find splits.

    .. versionadded:: 0.18
       Mean Absolute Error (MAE) criterion.

    .. versionadded:: 0.24
        Poisson deviance criterion.

splitter : {\"best\", \"random\"}, default=\"best\"
    The strategy used to choose the split at each node. Supported
    strategies are \"best\" to choose the best split and \"random\" to choose
    the best random split.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : int, float or {\"sqrt\", \"log2\"}, default=None
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of the estimator. The features are always
    randomly permuted at each split, even if ``splitter`` is set to
    ``\"best\"``. When ``max_features < n_features``, the algorithm will
    select ``max_features`` at random at each split before finding the best
    split among them. But the best found split may vary across different
    runs, even if ``max_features=n_features``. That is the case, if the
    improvement of the criterion is identical for several splits and one
    split has to be selected at random. To obtain a deterministic behaviour
    during fitting, ``random_state`` has to be fixed to an integer.
    See :term:`Glossary <random_state>` for details.

max_leaf_nodes : int, default=None
    Grow a tree with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonic increase
      - 0: no constraint
      - -1: monotonic decrease

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multioutput regressions (i.e. when `n_outputs_ > 1`),
      - regressions trained on data with missing values.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
feature_importances_ : ndarray of shape (n_features,)
    The feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the
    (normalized) total reduction of the criterion brought
    by that feature. It is also known as the Gini importance [4]_.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

max_features_ : int
    The inferred value of max_features.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

tree_ : Tree instance
    The underlying Tree object. Please refer to
    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
    for basic usage of these attributes.

See Also
--------
DecisionTreeClassifier : A decision tree classifier.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------

.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning

.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification
       and Regression Trees\", Wadsworth, Belmont, CA, 1984.

.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical
       Learning\", Springer, 2009.

.. [4] L. Breiman, and A. Cutler, \"Random Forests\",
       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.tree import DecisionTreeRegressor
>>> X, y = load_diabetes(return_X_y=True)
>>> regressor = DecisionTreeRegressor(random_state=0)
>>> cross_val_score(regressor, X, y, cv=10)
...                    # doctest: +SKIP
...
array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,
       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Decomposition
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Decomposition> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                    rdfs:comment """Compute error rates for different probability thresholds.

.. note::
   This metric is used for evaluation of ranking and error tradeoffs of
   a binary classification task.

Read more in the :ref:`User Guide <det_curve>`.

.. versionadded:: 0.24

Parameters
----------
y_true : ndarray of shape (n_samples,)
    True binary labels. If labels are not either {-1, 1} or {0, 1}, then
    pos_label should be explicitly given.

y_score : ndarray of shape of (n_samples,)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by \"decision_function\" on some classifiers).

pos_label : int, float, bool or str, default=None
    The label of the positive class.
    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},
    ``pos_label`` is set to 1, otherwise an error will be raised.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
fpr : ndarray of shape (n_thresholds,)
    False positive rate (FPR) such that element i is the false positive
    rate of predictions with score >= thresholds[i]. This is occasionally
    referred to as false acceptance probability or fall-out.

fnr : ndarray of shape (n_thresholds,)
    False negative rate (FNR) such that element i is the false negative
    rate of predictions with score >= thresholds[i]. This is occasionally
    referred to as false rejection or miss rate.

thresholds : ndarray of shape (n_thresholds,)
    Decreasing score values.

See Also
--------
DetCurveDisplay.from_estimator : Plot DET curve given an estimator and
    some data.
DetCurveDisplay.from_predictions : Plot DET curve given the true and
    predicted labels.
DetCurveDisplay : DET curve visualization.
roc_curve : Compute Receiver operating characteristic (ROC) curve.
precision_recall_curve : Compute precision-recall curve.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import det_curve
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, fnr, thresholds = det_curve(y_true, y_scores)
>>> fpr
array([0.5, 0.5, 0. ])
>>> fnr
array([0. , 0.5, 0.5])
>>> thresholds
array([0.35, 0.4 , 0.8 ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                          rdfs:comment """Transforms lists of feature-value mappings to vectors.

This transformer turns lists of mappings (dict-like objects) of feature
names to feature values into Numpy arrays or scipy.sparse matrices for use
with scikit-learn estimators.

When feature values are strings, this transformer will do a binary one-hot
(aka one-of-K) coding: one boolean-valued feature is constructed for each
of the possible string values that the feature can take on. For instance,
a feature \"f\" that can take on the values \"ham\" and \"spam\" will become two
features in the output, one signifying \"f=ham\", the other \"f=spam\".

If a feature value is a sequence or set of strings, this transformer
will iterate over the values and will count the occurrences of each string
value.

However, note that this transformer will only do a binary one-hot encoding
when feature values are of type string. If categorical features are
represented as numeric values such as int or iterables of strings, the
DictVectorizer can be followed by
:class:`~sklearn.preprocessing.OneHotEncoder` to complete
binary one-hot encoding.

Features that do not occur in a sample (mapping) will have a zero value
in the resulting array/matrix.

For an efficiency comparison of the different feature extractors, see
:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.

Read more in the :ref:`User Guide <dict_feature_extraction>`.

Parameters
----------
dtype : dtype, default=np.float64
    The type of feature values. Passed to Numpy array/scipy.sparse matrix
    constructors as the dtype argument.
separator : str, default=\"=\"
    Separator string used when constructing new features for one-hot
    coding.
sparse : bool, default=True
    Whether transform should produce scipy.sparse matrices.
sort : bool, default=True
    Whether ``feature_names_`` and ``vocabulary_`` should be
    sorted when fitting.

Attributes
----------
vocabulary_ : dict
    A dictionary mapping feature names to feature indices.

feature_names_ : list
    A list of length n_features containing the feature names (e.g., \"f=ham\"
    and \"f=spam\").

See Also
--------
FeatureHasher : Performs vectorization using only a hash function.
sklearn.preprocessing.OrdinalEncoder : Handles nominal/categorical
    features encoded as columns of arbitrary data types.

Examples
--------
>>> from sklearn.feature_extraction import DictVectorizer
>>> v = DictVectorizer(sparse=False)
>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
>>> X = v.fit_transform(D)
>>> X
array([[2., 0., 1.],
       [0., 1., 3.]])
>>> v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},
...                            {'baz': 1.0, 'foo': 3.0}]
True
>>> v.transform({'foo': 4, 'unseen_feature': 3})
array([[0., 0., 4.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                              rdfs:comment """Dictionary learning.

Finds a dictionary (a set of atoms) that performs well at sparsely
encoding the fitted data.

Solves the optimization problem::

    (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1
                (U,V)
                with || V_k ||_2 <= 1 for all  0 <= k < n_components

||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for
the entry-wise matrix norm which is the sum of the absolute values
of all the entries in the matrix.

Read more in the :ref:`User Guide <DictionaryLearning>`.

Parameters
----------
n_components : int, default=None
    Number of dictionary elements to extract. If None, then ``n_components``
    is set to ``n_features``.

alpha : float, default=1.0
    Sparsity controlling parameter.

max_iter : int, default=1000
    Maximum number of iterations to perform.

tol : float, default=1e-8
    Tolerance for numerical error.

fit_algorithm : {'lars', 'cd'}, default='lars'
    * `'lars'`: uses the least angle regression method to solve the lasso
      problem (:func:`~sklearn.linear_model.lars_path`);
    * `'cd'`: uses the coordinate descent method to compute the
      Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be
      faster if the estimated components are sparse.

    .. versionadded:: 0.17
       *cd* coordinate descent method to improve speed.

transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'
    Algorithm used to transform the data:

    - `'lars'`: uses the least angle regression method
      (:func:`~sklearn.linear_model.lars_path`);
    - `'lasso_lars'`: uses Lars to compute the Lasso solution.
    - `'lasso_cd'`: uses the coordinate descent method to compute the
      Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`
      will be faster if the estimated components are sparse.
    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse
      solution.
    - `'threshold'`: squashes to zero all coefficients less than alpha from
      the projection ``dictionary * X'``.

    .. versionadded:: 0.17
       *lasso_cd* coordinate descent method to improve speed.

transform_n_nonzero_coefs : int, default=None
    Number of nonzero coefficients to target in each column of the
    solution. This is only used by `algorithm='lars'` and
    `algorithm='omp'`. If `None`, then
    `transform_n_nonzero_coefs=int(n_features / 10)`.

transform_alpha : float, default=None
    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the
    penalty applied to the L1 norm.
    If `algorithm='threshold'`, `alpha` is the absolute value of the
    threshold below which coefficients will be squashed to zero.
    If `None`, defaults to `alpha`.

    .. versionchanged:: 1.2
        When None, default value changed from 1.0 to `alpha`.

n_jobs : int or None, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

code_init : ndarray of shape (n_samples, n_components), default=None
    Initial value for the code, for warm restart. Only used if `code_init`
    and `dict_init` are not None.

dict_init : ndarray of shape (n_components, n_features), default=None
    Initial values for the dictionary, for warm restart. Only used if
    `code_init` and `dict_init` are not None.

callback : callable, default=None
    Callable that gets invoked every five iterations.

    .. versionadded:: 1.3

verbose : bool, default=False
    To control the verbosity of the procedure.

split_sign : bool, default=False
    Whether to split the sparse feature vector into the concatenation of
    its negative part and its positive part. This can improve the
    performance of downstream classifiers.

random_state : int, RandomState instance or None, default=None
    Used for initializing the dictionary when ``dict_init`` is not
    specified, randomly shuffling the data when ``shuffle`` is set to
    ``True``, and updating the dictionary. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

positive_code : bool, default=False
    Whether to enforce positivity when finding the code.

    .. versionadded:: 0.20

positive_dict : bool, default=False
    Whether to enforce positivity when finding the dictionary.

    .. versionadded:: 0.20

transform_max_iter : int, default=1000
    Maximum number of iterations to perform if `algorithm='lasso_cd'` or
    `'lasso_lars'`.

    .. versionadded:: 0.22

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    dictionary atoms extracted from the data

error_ : array
    vector of errors at each iteration

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations run.

See Also
--------
MiniBatchDictionaryLearning: A faster, less accurate, version of the
    dictionary learning algorithm.
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.
SparseCoder : Find a sparse representation of data from a fixed,
    precomputed dictionary.
SparsePCA : Sparse Principal Components Analysis.

References
----------

J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_sparse_coded_signal
>>> from sklearn.decomposition import DictionaryLearning
>>> X, dictionary, code = make_sparse_coded_signal(
...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,
...     random_state=42,
... )
>>> dict_learner = DictionaryLearning(
...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,
...     random_state=42,
... )
>>> X_transformed = dict_learner.fit(X).transform(X)

We can check the level of sparsity of `X_transformed`:

>>> np.mean(X_transformed == 0)
0.52...

We can compare the average squared euclidean norm of the reconstruction
error of the sparse coded signal relative to the squared euclidean norm of
the original signal:

>>> X_hat = X_transformed @ dict_learner.components_
>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))
0.05...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                        rdfs:comment """Elastic Net model with iterative fitting along a regularization path.

See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide <elastic_net>`.

Parameters
----------
l1_ratio : float or list of float, default=0.5
    Float between 0 and 1 passed to ElasticNet (scaling between
    l1 and l2 penalties). For ``l1_ratio = 0``
    the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.
    For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2
    This parameter can be a list, in which case the different
    values are tested by cross-validation and the one giving the best
    prediction score is used. Note that a good choice of list of
    values for l1_ratio is often to put more values close to 1
    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,
    .9, .95, .99, 1]``.

eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path, used for each l1_ratio.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If None alphas are set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

verbose : bool or int, default=0
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
alpha_ : float
    The amount of penalization chosen by cross validation.

l1_ratio_ : float
    The compromise between l1 and l2 penalization chosen by
    cross validation.

coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

intercept_ : float or ndarray of shape (n_targets, n_features)
    Independent term in the decision function.

mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)
    Mean square error for the test set on each fold, varying l1_ratio and
    alpha.

alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)
    The grid of alphas used for fitting, for each l1_ratio.

dual_gap_ : float
    The dual gaps at the end of the optimization for the optimal alpha.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
enet_path : Compute elastic net path with coordinate descent.
ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.

Notes
-----
In `fit`, once the best parameters `l1_ratio` and `alpha` are found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` argument of the `fit`
method should be directly passed as a Fortran-contiguous numpy array.

The parameter `l1_ratio` corresponds to alpha in the glmnet R package
while alpha corresponds to the lambda parameter in glmnet.
More specifically, the optimization objective is::

    1 / (2 * n_samples) * ||y - Xw||^2_2
    + alpha * l1_ratio * ||w||_1
    + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2

If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to::

    a * L1 + b * L2

for::

    alpha = a + b and l1_ratio = a / (a + b).

For an example, see
:ref:`examples/linear_model/plot_lasso_model_selection.py
<sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.

Examples
--------
>>> from sklearn.linear_model import ElasticNetCV
>>> from sklearn.datasets import make_regression

>>> X, y = make_regression(n_features=2, random_state=0)
>>> regr = ElasticNetCV(cv=5, random_state=0)
>>> regr.fit(X, y)
ElasticNetCV(cv=5, random_state=0)
>>> print(regr.alpha_)
0.199...
>>> print(regr.intercept_)
0.398...
>>> print(regr.predict([[0, 0]]))
[0.398...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                      rdfs:comment """Linear regression with combined L1 and L2 priors as regularizer.

Minimizes the objective function::

        1 / (2 * n_samples) * ||y - Xw||^2_2
        + alpha * l1_ratio * ||w||_1
        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2

If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to::

        a * ||w||_1 + 0.5 * b * ||w||_2^2

where::

        alpha = a + b and l1_ratio = a / (a + b)

The parameter l1_ratio corresponds to alpha in the glmnet R package while
alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio
= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,
unless you supply your own sequence of alpha.

Read more in the :ref:`User Guide <elastic_net>`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the penalty terms. Defaults to 1.0.
    See the notes for the exact mathematical meaning of this
    parameter. ``alpha = 0`` is equivalent to an ordinary least square,
    solved by the :class:`LinearRegression` object. For numerical
    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
    Given this, you should use the :class:`LinearRegression` object.

l1_ratio : float, default=0.5
    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For
    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it
    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a
    combination of L1 and L2.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If ``False``, the
    data is assumed to be already centered.

precompute : bool or array-like of shape (n_features, n_features),                 default=False
    Whether to use a precomputed Gram matrix to speed up
    calculations. The Gram matrix can also be passed as argument.
    For sparse input this option is always ``False`` to preserve sparsity.

max_iter : int, default=1000
    The maximum number of iterations.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``, see Notes below.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)
    Sparse representation of the `coef_`.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : list of int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

dual_gap_ : float or ndarray of shape (n_targets,)
    Given param alpha, the dual gaps at the end of the optimization,
    same shape as each observation of y.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
ElasticNetCV : Elastic net model with best model selection by
    cross-validation.
SGDRegressor : Implements elastic net regression with incremental training.
SGDClassifier : Implements logistic regression with elastic net penalty
    (``SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\")``).

Notes
-----
To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.

The precise stopping criteria based on `tol` are the following: First, check that
that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`
is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.
If so, then additionally check whether the dual gap is smaller than `tol` times
:math:`||y||_2^2 / n_{      ext{samples}}`.

Examples
--------
>>> from sklearn.linear_model import ElasticNet
>>> from sklearn.datasets import make_regression

>>> X, y = make_regression(n_features=2, random_state=0)
>>> regr = ElasticNet(random_state=0)
>>> regr.fit(X, y)
ElasticNet(random_state=0)
>>> print(regr.coef_)
[18.83816048 64.55968825]
>>> print(regr.intercept_)
1.451...
>>> print(regr.predict([[0, 0]]))
[1.451...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                              rdfs:comment """Compute the distance matrix between each pair from a vector array X and Y.

For efficiency reasons, the euclidean distance between a pair of row
vector x and y is computed as::

    dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))

This formulation has two advantages over other ways of computing distances.
First, it is computationally efficient when dealing with sparse data.
Second, if one argument varies but the other remains unchanged, then
`dot(x, x)` and/or `dot(y, y)` can be pre-computed.

However, this is not the most precise way of doing this computation,
because this equation potentially suffers from \"catastrophic cancellation\".
Also, the distance matrix returned by this function may not be exactly
symmetric as required by, e.g., ``scipy.spatial.distance`` functions.

Read more in the :ref:`User Guide <metrics>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
    An array where each row is a sample and each column is a feature.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None
    An array where each row is a sample and each column is a feature.
    If `None`, method uses `Y=X`.

Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None
    Pre-computed dot-products of vectors in Y (e.g.,
    ``(Y**2).sum(axis=1)``)
    May be ignored in some cases, see the note below.

squared : bool, default=False
    Return squared Euclidean distances.

X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None
    Pre-computed dot-products of vectors in X (e.g.,
    ``(X**2).sum(axis=1)``)
    May be ignored in some cases, see the note below.

Returns
-------
distances : ndarray of shape (n_samples_X, n_samples_Y)
    Returns the distances between the row vectors of `X`
    and the row vectors of `Y`.

See Also
--------
paired_distances : Distances between pairs of elements of X and Y.

Notes
-----
To achieve a better accuracy, `X_norm_squared`and `Y_norm_squared` may be
unused if they are passed as `np.float32`.

Examples
--------
>>> from sklearn.metrics.pairwise import euclidean_distances
>>> X = [[0, 1], [1, 1]]
>>> # distance between rows of X
>>> euclidean_distances(X, X)
array([[0., 1.],
       [1., 0.]])
>>> # get distance to origin
>>> euclidean_distances(X, [[0, 0]])
array([[1.        ],
       [1.41421356]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod> rdf:type owl:Class ;
                                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                  rdfs:comment """Explained variance regression score function.

Best possible score is 1.0, lower values are worse.

In the particular case when ``y_true`` is constant, the explained variance
score is not finite: it is either ``NaN`` (perfect predictions) or
``-Inf`` (imperfect predictions). To prevent such non-finite numbers to
pollute higher-level experiments such as a grid search cross-validation,
by default these cases are replaced with 1.0 (perfect predictions) or 0.0
(imperfect predictions) respectively. If ``force_finite``
is set to ``False``, this score falls back on the original :math:`R^2`
definition.

.. note::
   The Explained Variance score is similar to the
   :func:`R^2 score <r2_score>`, with the notable difference that it
   does not account for systematic offsets in the prediction. Most often
   the :func:`R^2 score <r2_score>` should be preferred.

Read more in the :ref:`User Guide <explained_variance_score>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.

    'raw_values' :
        Returns a full set of scores in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.

force_finite : bool, default=True
    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant
    data should be replaced with real numbers (``1.0`` if prediction is
    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting
    for hyperparameters' search procedures (e.g. grid search
    cross-validation).

    .. versionadded:: 1.1

Returns
-------
score : float or ndarray of floats
    The explained variance or ndarray if 'multioutput' is 'raw_values'.

See Also
--------
r2_score :
    Similar metric, but accounting for systematic offsets in
    prediction.

Notes
-----
This is not a symmetric function.

Examples
--------
>>> from sklearn.metrics import explained_variance_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)
0.957...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')
0.983...
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> explained_variance_score(y_true, y_pred)
1.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> explained_variance_score(y_true, y_pred)
0.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
-inf""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule> ;
                                                                                                               rdfs:comment """An extremely randomized tree classifier.

Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the `max_features` randomly
selected features and the best split among those is chosen. When
`max_features` is set 1, this amounts to building a totally random
decision tree.

Warning: Extra-trees should only be used within ensemble methods.

Read more in the :ref:`User Guide <tree>`.

Parameters
----------
criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"
    The function to measure the quality of a split. Supported criteria are
    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the
    Shannon information gain, see :ref:`tree_mathematical_formulation`.

splitter : {\"random\", \"best\"}, default=\"random\"
    The strategy used to choose the split at each node. Supported
    strategies are \"best\" to choose the best split and \"random\" to choose
    the best random split.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : int, float, {\"sqrt\", \"log2\"} or None, default=\"sqrt\"
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at
      each split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

random_state : int, RandomState instance or None, default=None
    Used to pick randomly the `max_features` used at each split.
    See :term:`Glossary <random_state>` for details.

max_leaf_nodes : int, default=None
    Grow a tree with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

class_weight : dict, list of dict or \"balanced\", default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If None, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonic increase
      - 0: no constraint
      - -1: monotonic decrease

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multiclass classifications (i.e. when `n_classes > 2`),
      - multioutput classifications (i.e. when `n_outputs_ > 1`),
      - classifications trained on data with missing values.

    The constraints hold over the probability of the positive class.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
classes_ : ndarray of shape (n_classes,) or list of ndarray
    The classes labels (single output problem),
    or a list of arrays of class labels (multi-output problem).

max_features_ : int
    The inferred value of max_features.

n_classes_ : int or list of int
    The number of classes (for single output problems),
    or a list containing the number of classes for each
    output (for multi-output problems).

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

tree_ : Tree instance
    The underlying Tree object. Please refer to
    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
    for basic usage of these attributes.

See Also
--------
ExtraTreeRegressor : An extremely randomized tree regressor.
sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.
sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.
sklearn.ensemble.RandomForestClassifier : A random forest classifier.
sklearn.ensemble.RandomForestRegressor : A random forest regressor.
sklearn.ensemble.RandomTreesEmbedding : An ensemble of
    totally random trees.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------

.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",
       Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.tree import ExtraTreeClassifier
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(
...    X, y, random_state=0)
>>> extra_tree = ExtraTreeClassifier(random_state=0)
>>> cls = BaggingClassifier(extra_tree, random_state=0).fit(
...    X_train, y_train)
>>> cls.score(X_test, y_test)
0.8947...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule> ;
                                                                                                              rdfs:comment """An extremely randomized tree regressor.

Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the `max_features` randomly
selected features and the best split among those is chosen. When
`max_features` is set 1, this amounts to building a totally random
decision tree.

Warning: Extra-trees should only be used within ensemble methods.

Read more in the :ref:`User Guide <tree>`.

Parameters
----------
criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"},             default=\"squared_error\"
    The function to measure the quality of a split. Supported criteria
    are \"squared_error\" for the mean squared error, which is equal to
    variance reduction as feature selection criterion and minimizes the L2
    loss using the mean of each terminal node, \"friedman_mse\", which uses
    mean squared error with Friedman's improvement score for potential
    splits, \"absolute_error\" for the mean absolute error, which minimizes
    the L1 loss using the median of each terminal node, and \"poisson\" which
    uses reduction in Poisson deviance to find splits.

    .. versionadded:: 0.18
       Mean Absolute Error (MAE) criterion.

    .. versionadded:: 0.24
        Poisson deviance criterion.

splitter : {\"random\", \"best\"}, default=\"random\"
    The strategy used to choose the split at each node. Supported
    strategies are \"best\" to choose the best split and \"random\" to choose
    the best random split.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : int, float, {\"sqrt\", \"log2\"} or None, default=1.0
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `\"auto\"` to `1.0`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

random_state : int, RandomState instance or None, default=None
    Used to pick randomly the `max_features` used at each split.
    See :term:`Glossary <random_state>` for details.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

max_leaf_nodes : int, default=None
    Grow a tree with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonic increase
      - 0: no constraint
      - -1: monotonic decrease

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multioutput regressions (i.e. when `n_outputs_ > 1`),
      - regressions trained on data with missing values.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
max_features_ : int
    The inferred value of max_features.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

feature_importances_ : ndarray of shape (n_features,)
    Return impurity-based feature importances (the higher, the more
    important the feature).

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

tree_ : Tree instance
    The underlying Tree object. Please refer to
    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
    for basic usage of these attributes.

See Also
--------
ExtraTreeClassifier : An extremely randomized tree classifier.
sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.
sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------

.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",
       Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.ensemble import BaggingRegressor
>>> from sklearn.tree import ExtraTreeRegressor
>>> X, y = load_diabetes(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> extra_tree = ExtraTreeRegressor(random_state=0)
>>> reg = BaggingRegressor(extra_tree, random_state=0).fit(
...     X_train, y_train)
>>> reg.score(X_test, y_test)
0.33...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                rdfs:comment """An extra-trees classifier.

This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"
    The function to measure the quality of a split. Supported criteria are
    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the
    Shannon information gain, see :ref:`tree_mathematical_formulation`.
    Note: This parameter is tree-specific.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=False
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.accuracy_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls 3 sources of randomness:

    - the bootstrapping of the samples used when building trees
      (if ``bootstrap=True``)
    - the sampling of the features to consider when looking for the best
      split at each node (if ``max_features < n_features``)
    - the draw of the splits for each of the `max_features`

    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    The \"balanced_subsample\" mode is the same as \"balanced\" except that
    weights are computed based on the bootstrap sample for every tree
    grown.

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonically increasing
      - 0: no constraint
      - -1: monotonically decreasing

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multiclass classifications (i.e. when `n_classes > 2`),
      - multioutput classifications (i.e. when `n_outputs_ > 1`),
      - classifications trained on data with missing values.

    The constraints hold over the probability of the positive class.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeClassifier`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeClassifier
    The collection of fitted sub-estimators.

classes_ : ndarray of shape (n_classes,) or a list of such arrays
    The classes labels (single output problem), or a list of arrays of
    class labels (multi-output problem).

n_classes_ : int or list
    The number of classes (single output problem), or a list containing the
    number of classes for each output (multi-output problem).

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_decision_function_` might contain NaN. This attribute exists
    only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
ExtraTreesRegressor : An extra-trees regressor with random splits.
RandomForestClassifier : A random forest classifier with optimal splits.
RandomForestRegressor : Ensemble regressor using trees with optimal splits.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------
.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized
       trees\", Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_features=4, random_state=0)
>>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)
>>> clf.fit(X, y)
ExtraTreesClassifier(random_state=0)
>>> clf.predict([[0, 0, 0, 0]])
array([1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                               rdfs:comment """An extra-trees regressor.

This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"
    The function to measure the quality of a split. Supported criteria
    are \"squared_error\" for the mean squared error, which is equal to
    variance reduction as feature selection criterion and minimizes the L2
    loss using the mean of each terminal node, \"friedman_mse\", which uses
    mean squared error with Friedman's improvement score for potential
    splits, \"absolute_error\" for the mean absolute error, which minimizes
    the L1 loss using the median of each terminal node, and \"poisson\" which
    uses reduction in Poisson deviance to find splits.
    Training using \"absolute_error\" is significantly slower
    than when using \"squared_error\".

    .. versionadded:: 0.18
       Mean Absolute Error (MAE) criterion.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None or 1.0, then `max_features=n_features`.

    .. note::
        The default of 1.0 is equivalent to bagged trees and more
        randomness can be achieved by setting smaller values, e.g. 0.3.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `\"auto\"` to 1.0.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=False
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.r2_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls 3 sources of randomness:

    - the bootstrapping of the samples used when building trees
      (if ``bootstrap=True``)
    - the sampling of the features to consider when looking for the best
      split at each node (if ``max_features < n_features``)
    - the draw of the splits for each of the `max_features`

    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max_samples * X.shape[0]` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonically increasing
      - 0: no constraint
      - -1: monotonically decreasing

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multioutput regressions (i.e. when `n_outputs_ > 1`),
      - regressions trained on data with missing values.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
    Prediction computed with out-of-bag estimate on the training set.
    This attribute exists only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
ExtraTreesClassifier : An extra-trees classifier with random splits.
RandomForestClassifier : A random forest classifier with optimal splits.
RandomForestRegressor : Ensemble regressor using trees with optimal splits.

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

References
----------
.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",
       Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.ensemble import ExtraTreesRegressor
>>> X, y = load_diabetes(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(
...    X_train, y_train)
>>> reg.score(X_test, y_test)
0.2727...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                   rdfs:comment """Compute the F1 score, also known as balanced F-score or F-measure.

The F1 score can be interpreted as a harmonic mean of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:

.. math::
    \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}}

Where :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the
number of false negatives, and :math:`\\text{FP}` is the number of false positives.
F1 is by default
calculated as 0.0 when there are no true positives, false negatives, or
false positives.

Support beyond :term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and F1 score for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
F1 score for all `labels` are either returned or averaged depending on the
`average` parameter. Use `labels` specify the set of labels to calculate F1 score
for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a \"negative
    class\". Labels not present in the data can be included and will be
    \"assigned\" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"
    Sets the value to return when there is a zero division, i.e. when all
    predictions and labels are negative.

    Notes:
    - If set to \"warn\", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
f1_score : float or array of float, shape = [n_unique_labels]
    F1 score of the positive class in binary classification or weighted
    average of the F1 scores of each class for the multiclass task.

See Also
--------
fbeta_score : Compute the F-beta score.
precision_recall_fscore_support : Compute the precision, recall, F-score,
    and support.
jaccard_score : Compute the Jaccard similarity coefficient score.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive + false negative == 0`` (i.e. a class
is completely absent from both ``y_true`` or ``y_pred``), f-score is
undefined. In such cases, by default f-score will be set to 0.0, and
``UndefinedMetricWarning`` will be raised. This behavior can be modified by
setting the ``zero_division`` parameter.

References
----------
.. [1] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import f1_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> f1_score(y_true, y_pred, average='macro')
0.26...
>>> f1_score(y_true, y_pred, average='micro')
0.33...
>>> f1_score(y_true, y_pred, average='weighted')
0.26...
>>> f1_score(y_true, y_pred, average=None)
array([0.8, 0. , 0. ])

>>> # binary classification
>>> y_true_empty = [0, 0, 0, 0, 0, 0]
>>> y_pred_empty = [0, 0, 0, 0, 0, 0]
>>> f1_score(y_true_empty, y_pred_empty)
0.0...
>>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)
1.0...
>>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)
nan...

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> f1_score(y_true, y_pred, average=None)
array([0.66666667, 1.        , 0.66666667])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FClassifMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FClassifMethod> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                    rdfs:comment """Compute the ANOVA F-value for the provided sample.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The set of regressors that will be tested sequentially.

y : array-like of shape (n_samples,)
    The target vector.

Returns
-------
f_statistic : ndarray of shape (n_features,)
    F-statistic for each feature.

p_values : ndarray of shape (n_features,)
    P-values associated with the F-statistic.

See Also
--------
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.

Examples
--------
>>> from sklearn.datasets import make_classification
>>> from sklearn.feature_selection import f_classif
>>> X, y = make_classification(
...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,
...     shuffle=False, random_state=42
... )
>>> f_statistic, p_values = f_classif(X, y)
>>> f_statistic
array([2.2...e+02, 7.0...e-01, 1.6...e+00, 9.3...e-01,
       5.4...e+00, 3.2...e-01, 4.7...e-02, 5.7...e-01,
       7.5...e-01, 8.9...e-02])
>>> p_values
array([7.1...e-27, 4.0...e-01, 1.9...e-01, 3.3...e-01,
       2.2...e-02, 5.7...e-01, 8.2...e-01, 4.5...e-01,
       3.8...e-01, 7.6...e-01])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                       rdfs:comment """Univariate linear regression tests returning F-statistic and p-values.

Quick linear model for testing the effect of a single regressor,
sequentially for many regressors.

This is done in 2 steps:

1. The cross correlation between each regressor and the target is computed
   using :func:`r_regression` as::

       E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))

2. It is converted to an F score and then to a p-value.

:func:`f_regression` is derived from :func:`r_regression` and will rank
features in the same order if all the features are positively correlated
with the target.

Note however that contrary to :func:`f_regression`, :func:`r_regression`
values lie in [-1, 1] and can thus be negative. :func:`f_regression` is
therefore recommended as a feature selection criterion to identify
potentially predictive feature for a downstream classifier, irrespective of
the sign of the association with the target variable.

Furthermore :func:`f_regression` returns p-values while
:func:`r_regression` does not.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data matrix.

y : array-like of shape (n_samples,)
    The target vector.

center : bool, default=True
    Whether or not to center the data matrix `X` and the target vector `y`.
    By default, `X` and `y` will be centered.

force_finite : bool, default=True
    Whether or not to force the F-statistics and associated p-values to
    be finite. There are two cases where the F-statistic is expected to not
    be finite:

    - when the target `y` or some features in `X` are constant. In this
      case, the Pearson's R correlation is not defined leading to obtain
      `np.nan` values in the F-statistic and p-value. When
      `force_finite=True`, the F-statistic is set to `0.0` and the
      associated p-value is set to `1.0`.
    - when a feature in `X` is perfectly correlated (or
      anti-correlated) with the target `y`. In this case, the F-statistic
      is expected to be `np.inf`. When `force_finite=True`, the F-statistic
      is set to `np.finfo(dtype).max` and the associated p-value is set to
      `0.0`.

    .. versionadded:: 1.1

Returns
-------
f_statistic : ndarray of shape (n_features,)
    F-statistic for each feature.

p_values : ndarray of shape (n_features,)
    P-values associated with the F-statistic.

See Also
--------
r_regression: Pearson's R between label/feature for regression tasks.
f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
SelectPercentile: Select features based on percentile of the highest
    scores.

Examples
--------
>>> from sklearn.datasets import make_regression
>>> from sklearn.feature_selection import f_regression
>>> X, y = make_regression(
...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
... )
>>> f_statistic, p_values = f_regression(X, y)
>>> f_statistic
array([1.2...+00, 2.6...+13, 2.6...+00])
>>> p_values
array([2.7..., 1.5..., 1.0...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                          rdfs:comment """Factor Analysis (FA).

A simple linear generative model with Gaussian latent variables.

The observations are assumed to be caused by a linear transformation of
lower dimensional latent factors and added Gaussian noise.
Without loss of generality the factors are distributed according to a
Gaussian with zero mean and unit covariance. The noise is also zero mean
and has an arbitrary diagonal covariance matrix.

If we would restrict the model further, by assuming that the Gaussian
noise is even isotropic (all diagonal entries are the same) we would obtain
:class:`PCA`.

FactorAnalysis performs a maximum likelihood estimate of the so-called
`loading` matrix, the transformation of the latent variables to the
observed ones, using SVD based approach.

Read more in the :ref:`User Guide <FA>`.

.. versionadded:: 0.13

Parameters
----------
n_components : int, default=None
    Dimensionality of latent space, the number of components
    of ``X`` that are obtained after ``transform``.
    If None, n_components is set to the number of features.

tol : float, default=1e-2
    Stopping tolerance for log-likelihood increase.

copy : bool, default=True
    Whether to make a copy of X. If ``False``, the input X gets overwritten
    during fitting.

max_iter : int, default=1000
    Maximum number of iterations.

noise_variance_init : array-like of shape (n_features,), default=None
    The initial guess of the noise variance for each feature.
    If None, it defaults to np.ones(n_features).

svd_method : {'lapack', 'randomized'}, default='randomized'
    Which SVD method to use. If 'lapack' use standard SVD from
    scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.
    Defaults to 'randomized'. For most applications 'randomized' will
    be sufficiently precise while providing significant speed gains.
    Accuracy can also be improved by setting higher values for
    `iterated_power`. If this is not sufficient, for maximum precision
    you should choose 'lapack'.

iterated_power : int, default=3
    Number of iterations for the power method. 3 by default. Only used
    if ``svd_method`` equals 'randomized'.

rotation : {'varimax', 'quartimax'}, default=None
    If not None, apply the indicated rotation. Currently, varimax and
    quartimax are implemented. See
    `\"The varimax criterion for analytic rotation in factor analysis\"
    <https://link.springer.com/article/10.1007%2FBF02289233>`_
    H. F. Kaiser, 1958.

    .. versionadded:: 0.24

random_state : int or RandomState instance, default=0
    Only used when ``svd_method`` equals 'randomized'. Pass an int for
    reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Components with maximum variance.

loglike_ : list of shape (n_iterations,)
    The log likelihood at each iteration.

noise_variance_ : ndarray of shape (n_features,)
    The estimated noise variance for each feature.

n_iter_ : int
    Number of iterations run.

mean_ : ndarray of shape (n_features,)
    Per-feature empirical mean, estimated from the training set.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PCA: Principal component analysis is also a latent linear variable model
    which however assumes equal noise variance for each feature.
    This extra assumption makes probabilistic PCA faster as it can be
    computed in closed form.
FastICA: Independent component analysis, a latent variable model with
    non-Gaussian latent variables.

References
----------
- David Barber, Bayesian Reasoning and Machine Learning,
  Algorithm 21.1.

- Christopher M. Bishop: Pattern Recognition and Machine Learning,
  Chapter 12.2.4.

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.decomposition import FactorAnalysis
>>> X, _ = load_digits(return_X_y=True)
>>> transformer = FactorAnalysis(n_components=7, random_state=0)
>>> X_transformed = transformer.fit_transform(X)
>>> X_transformed.shape
(1797, 7)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                   rdfs:comment """FastICA: a fast algorithm for Independent Component Analysis.

The implementation is based on [1]_.

Read more in the :ref:`User Guide <ICA>`.

Parameters
----------
n_components : int, default=None
    Number of components to use. If None is passed, all are used.

algorithm : {'parallel', 'deflation'}, default='parallel'
    Specify which algorithm to use for FastICA.

whiten : str or bool, default='unit-variance'
    Specify the whitening strategy to use.

    - If 'arbitrary-variance', a whitening with variance
      arbitrary is used.
    - If 'unit-variance', the whitening matrix is rescaled to ensure that
      each recovered source has unit variance.
    - If False, the data is already considered to be whitened, and no
      whitening is performed.

    .. versionchanged:: 1.3
        The default value of `whiten` changed to 'unit-variance' in 1.3.

fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'
    The functional form of the G function used in the
    approximation to neg-entropy. Could be either 'logcosh', 'exp',
    or 'cube'.
    You can also provide your own function. It should return a tuple
    containing the value of the function, and of its derivative, in the
    point. The derivative should be averaged along its last dimension.
    Example::

        def my_g(x):
            return x ** 3, (3 * x ** 2).mean(axis=-1)

fun_args : dict, default=None
    Arguments to send to the functional form.
    If empty or None and if fun='logcosh', fun_args will take value
    {'alpha' : 1.0}.

max_iter : int, default=200
    Maximum number of iterations during fit.

tol : float, default=1e-4
    A positive scalar giving the tolerance at which the
    un-mixing matrix is considered to have converged.

w_init : array-like of shape (n_components, n_components), default=None
    Initial un-mixing array. If `w_init=None`, then an array of values
    drawn from a normal distribution is used.

whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"
    The solver to use for whitening.

    - \"svd\" is more stable numerically if the problem is degenerate, and
      often faster when `n_samples <= n_features`.

    - \"eigh\" is generally more memory efficient when
      `n_samples >= n_features`, and can be faster when
      `n_samples >= 50 * n_features`.

    .. versionadded:: 1.2

random_state : int, RandomState instance or None, default=None
    Used to initialize ``w_init`` when not specified, with a
    normal distribution. Pass an int, for reproducible results
    across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    The linear operator to apply to the data to get the independent
    sources. This is equal to the unmixing matrix when ``whiten`` is
    False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when
    ``whiten`` is True.

mixing_ : ndarray of shape (n_features, n_components)
    The pseudo-inverse of ``components_``. It is the linear operator
    that maps independent sources to the data.

mean_ : ndarray of shape(n_features,)
    The mean over features. Only set if `self.whiten` is True.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    If the algorithm is \"deflation\", n_iter is the
    maximum number of iterations run across all components. Else
    they are just the number of iterations taken to converge.

whitening_ : ndarray of shape (n_components, n_features)
    Only set if whiten is 'True'. This is the pre-whitening matrix
    that projects data onto the first `n_components` principal components.

See Also
--------
PCA : Principal component analysis (PCA).
IncrementalPCA : Incremental principal components analysis (IPCA).
KernelPCA : Kernel Principal component analysis (KPCA).
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.
SparsePCA : Sparse Principal Components Analysis (SparsePCA).

References
----------
.. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:
       Algorithms and Applications, Neural Networks, 13(4-5), 2000,
       pp. 411-430.

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.decomposition import FastICA
>>> X, _ = load_digits(return_X_y=True)
>>> transformer = FastICA(n_components=7,
...         random_state=0,
...         whiten='unit-variance')
>>> X_transformed = transformer.fit_transform(X)
>>> X_transformed.shape
(1797, 7)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                      rdfs:comment """Compute the F-beta score.

The F-beta score is the weighted harmonic mean of precision and recall,
reaching its optimal value at 1 and its worst value at 0.

The `beta` parameter represents the ratio of recall importance to
precision importance. `beta > 1` gives more weight to recall, while
`beta < 1` favors precision. For example, `beta = 2` makes recall twice
as important as precision, while `beta = 0.5` does the opposite.
Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0`
only precision.

The formula for F-beta score is:

.. math::

   F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}
                    {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}

Where :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the
number of false positives, and :math:`\\text{fn}` is the number of false negatives.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is
ignored and F-beta score for both classes are computed, then averaged or both
returned (when `average=None`). Similarly, for :term:`multiclass` and
:term:`multilabel` targets, F-beta score for all `labels` are either returned or
averaged depending on the `average` parameter. Use `labels` specify the set of
labels to calculate F-beta score for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

beta : float
    Determines the weight of recall in the combined score.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a \"negative
    class\". Labels not present in the data can be included and will be
    \"assigned\" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"
    Sets the value to return when there is a zero division, i.e. when all
    predictions and labels are negative.

    Notes:
    - If set to \"warn\", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]
    F-beta score of the positive class in binary classification or weighted
    average of the F-beta score of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute the precision, recall, F-score,
    and support.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive + false negative == 0``, f-score
returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be
modified by setting ``zero_division``.

References
----------
.. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).
       Modern Information Retrieval. Addison Wesley, pp. 327-328.

.. [2] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import fbeta_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)
0.23...
>>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)
0.33...
>>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)
0.23...
>>> fbeta_score(y_true, y_pred, average=None, beta=0.5)
array([0.71..., 0.        , 0.        ])
>>> y_pred_empty = [0, 0, 0, 0, 0, 0]
>>> fbeta_score(y_true, y_pred_empty,
...             average=\"macro\", zero_division=np.nan, beta=0.5)
0.12...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                rdfs:comment """Agglomerate features.

Recursively merges pair of clusters of features.

Read more in the :ref:`User Guide <hierarchical_clustering>`.

Parameters
----------
n_clusters : int or None, default=2
    The number of clusters to find. It must be ``None`` if
    ``distance_threshold`` is not ``None``.

metric : str or callable, default=\"euclidean\"
    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",
    \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only
    \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed
    as input for the fit method.

    .. versionadded:: 1.2

    .. deprecated:: 1.4
       `metric=None` is deprecated in 1.4 and will be removed in 1.6.
       Let `metric` be the default value (i.e. `\"euclidean\"`) instead.

memory : str or object with the joblib.Memory interface, default=None
    Used to cache the output of the computation of the tree.
    By default, no caching is done. If a string is given, it is the
    path to the caching directory.

connectivity : array-like or callable, default=None
    Connectivity matrix. Defines for each feature the neighboring
    features following a given structure of the data.
    This can be a connectivity matrix itself or a callable that transforms
    the data into a connectivity matrix, such as derived from
    `kneighbors_graph`. Default is `None`, i.e, the
    hierarchical clustering algorithm is unstructured.

compute_full_tree : 'auto' or bool, default='auto'
    Stop early the construction of the tree at `n_clusters`. This is useful
    to decrease computation time if the number of clusters is not small
    compared to the number of features. This option is useful only when
    specifying a connectivity matrix. Note also that when varying the
    number of clusters and using caching, it may be advantageous to compute
    the full tree. It must be ``True`` if ``distance_threshold`` is not
    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent
    to `True` when `distance_threshold` is not `None` or that `n_clusters`
    is inferior to the maximum between 100 or `0.02 * n_samples`.
    Otherwise, \"auto\" is equivalent to `False`.

linkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"
    Which linkage criterion to use. The linkage criterion determines which
    distance to use between sets of features. The algorithm will merge
    the pairs of cluster that minimize this criterion.

    - \"ward\" minimizes the variance of the clusters being merged.
    - \"complete\" or maximum linkage uses the maximum distances between
      all features of the two sets.
    - \"average\" uses the average of the distances of each feature of
      the two sets.
    - \"single\" uses the minimum of the distances between all features
      of the two sets.

pooling_func : callable, default=np.mean
    This combines the values of agglomerated features into a single
    value, and should accept an array of shape [M, N] and the keyword
    argument `axis=1`, and reduce it to an array of size [M].

distance_threshold : float, default=None
    The linkage distance threshold at or above which clusters will not be
    merged. If not ``None``, ``n_clusters`` must be ``None`` and
    ``compute_full_tree`` must be ``True``.

    .. versionadded:: 0.21

compute_distances : bool, default=False
    Computes distances between clusters even if `distance_threshold` is not
    used. This can be used to make dendrogram visualization, but introduces
    a computational and memory overhead.

    .. versionadded:: 0.24

Attributes
----------
n_clusters_ : int
    The number of clusters found by the algorithm. If
    ``distance_threshold=None``, it will be equal to the given
    ``n_clusters``.

labels_ : array-like of (n_features,)
    Cluster labels for each feature.

n_leaves_ : int
    Number of leaves in the hierarchical tree.

n_connected_components_ : int
    The estimated number of connected components in the graph.

    .. versionadded:: 0.21
        ``n_connected_components_`` was added to replace ``n_components_``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

children_ : array-like of shape (n_nodes-1, 2)
    The children of each non-leaf node. Values less than `n_features`
    correspond to leaves of the tree which are the original samples.
    A node `i` greater than or equal to `n_features` is a non-leaf
    node and has children `children_[i - n_features]`. Alternatively
    at the i-th iteration, children[i][0] and children[i][1]
    are merged to form node `n_features + i`.

distances_ : array-like of shape (n_nodes-1,)
    Distances between nodes in the corresponding place in `children_`.
    Only computed if `distance_threshold` is used or `compute_distances`
    is set to `True`.

See Also
--------
AgglomerativeClustering : Agglomerative clustering samples instead of
    features.
ward_tree : Hierarchical clustering with ward linkage.

Examples
--------
>>> import numpy as np
>>> from sklearn import datasets, cluster
>>> digits = datasets.load_digits()
>>> images = digits.images
>>> X = np.reshape(images, (len(images), -1))
>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)
>>> agglo.fit(X)
FeatureAgglomeration(n_clusters=32)
>>> X_reduced = agglo.transform(X)
>>> X_reduced.shape
(1797, 32)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                         rdfs:comment """Implements feature hashing, aka the hashing trick.

This class turns sequences of symbolic feature names (strings) into
scipy.sparse matrices, using a hash function to compute the matrix column
corresponding to a name. The hash function employed is the signed 32-bit
version of Murmurhash3.

Feature names of type byte string are used as-is. Unicode strings are
converted to UTF-8 first, but no Unicode normalization is done.
Feature values must be (finite) numbers.

This class is a low-memory alternative to DictVectorizer and
CountVectorizer, intended for large-scale (online) learning and situations
where memory is tight, e.g. when running prediction code on embedded
devices.

For an efficiency comparison of the different feature extractors, see
:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.

Read more in the :ref:`User Guide <feature_hashing>`.

.. versionadded:: 0.13

Parameters
----------
n_features : int, default=2**20
    The number of features (columns) in the output matrices. Small numbers
    of features are likely to cause hash collisions, but large numbers
    will cause larger coefficient dimensions in linear learners.
input_type : str, default='dict'
    Choose a string from {'dict', 'pair', 'string'}.
    Either \"dict\" (the default) to accept dictionaries over
    (feature_name, value); \"pair\" to accept pairs of (feature_name, value);
    or \"string\" to accept single strings.
    feature_name should be a string, while value should be a number.
    In the case of \"string\", a value of 1 is implied.
    The feature_name is hashed to find the appropriate column for the
    feature. The value's sign might be flipped in the output (but see
    non_negative, below).
dtype : numpy dtype, default=np.float64
    The type of feature values. Passed to scipy.sparse matrix constructors
    as the dtype argument. Do not set this to bool, np.boolean or any
    unsigned integer type.
alternate_sign : bool, default=True
    When True, an alternating sign is added to the features as to
    approximately conserve the inner product in the hashed space even for
    small n_features. This approach is similar to sparse random projection.

    .. versionchanged:: 0.19
        ``alternate_sign`` replaces the now deprecated ``non_negative``
        parameter.

See Also
--------
DictVectorizer : Vectorizes string-valued features using a hash table.
sklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.

Notes
-----
This estimator is :term:`stateless` and does not need to be fitted.
However, we recommend to call :meth:`fit_transform` instead of
:meth:`transform`, as parameter validation is only performed in
:meth:`fit`.

Examples
--------
>>> from sklearn.feature_extraction import FeatureHasher
>>> h = FeatureHasher(n_features=10)
>>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]
>>> f = h.transform(D)
>>> f.toarray()
array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],
       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])

With `input_type=\"string\"`, the input must be an iterable over iterables of
strings:

>>> h = FeatureHasher(n_features=8, input_type=\"string\")
>>> raw_X = [[\"dog\", \"cat\", \"snake\"], [\"snake\", \"dog\"], [\"cat\", \"bird\"]]
>>> f = h.transform(raw_X)
>>> f.toarray()
array([[ 0.,  0.,  0., -1.,  0., -1.,  0.,  1.],
       [ 0.,  0.,  0., -1.,  0., -1.,  0.,  0.],
       [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  1.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelection
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelection> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                               rdfs:comment """Measure the similarity of two clusterings of a set of points.

.. versionadded:: 0.18

The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of
the precision and recall::

    FMI = TP / sqrt((TP + FP) * (TP + FN))

Where ``TP`` is the number of **True Positive** (i.e. the number of pair of
points that belongs in the same clusters in both ``labels_true`` and
``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the
number of pair of points that belongs in the same clusters in
``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of
**False Negative** (i.e. the number of pair of points that belongs in the
same clusters in ``labels_pred`` and not in ``labels_True``).

The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.

Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=int
    A clustering of the data into disjoint subsets.

labels_pred : array-like of shape (n_samples,), dtype=int
    A clustering of the data into disjoint subsets.

sparse : bool, default=False
    Compute contingency matrix internally with sparse matrix.

Returns
-------
score : float
   The resulting Fowlkes-Mallows score.

References
----------
.. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two
   hierarchical clusterings\". Journal of the American Statistical
   Association
   <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_

.. [2] `Wikipedia entry for the Fowlkes-Mallows Index
       <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_

Examples
--------

Perfect labelings are both homogeneous and complete, hence have
score 1.0::

  >>> from sklearn.metrics.cluster import fowlkes_mallows_score
  >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
  1.0
  >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

If classes members are completely split across different clusters,
the assignment is totally random, hence the FMI is null::

  >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])
  0.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                               rdfs:comment """Constructs a transformer from an arbitrary callable.

A FunctionTransformer forwards its X (and optionally y) arguments to a
user-defined function or function object and returns the result of this
function. This is useful for stateless transformations such as taking the
log of frequencies, doing custom scaling, etc.

Note: If a lambda is used as the function, then the resulting
transformer will not be pickleable.

.. versionadded:: 0.17

Read more in the :ref:`User Guide <function_transformer>`.

Parameters
----------
func : callable, default=None
    The callable to use for the transformation. This will be passed
    the same arguments as transform, with args and kwargs forwarded.
    If func is None, then func will be the identity function.

inverse_func : callable, default=None
    The callable to use for the inverse transformation. This will be
    passed the same arguments as inverse transform, with args and
    kwargs forwarded. If inverse_func is None, then inverse_func
    will be the identity function.

validate : bool, default=False
    Indicate that the input X array should be checked before calling
    ``func``. The possibilities are:

    - If False, there is no input validation.
    - If True, then X will be converted to a 2-dimensional NumPy array or
      sparse matrix. If the conversion is not possible an exception is
      raised.

    .. versionchanged:: 0.22
       The default of ``validate`` changed from True to False.

accept_sparse : bool, default=False
    Indicate that func accepts a sparse matrix as input. If validate is
    False, this has no effect. Otherwise, if accept_sparse is false,
    sparse matrix inputs will cause an exception to be raised.

check_inverse : bool, default=True
   Whether to check that or ``func`` followed by ``inverse_func`` leads to
   the original inputs. It can be used for a sanity check, raising a
   warning when the condition is not fulfilled.

   .. versionadded:: 0.20

feature_names_out : callable, 'one-to-one' or None, default=None
    Determines the list of feature names that will be returned by the
    `get_feature_names_out` method. If it is 'one-to-one', then the output
    feature names will be equal to the input feature names. If it is a
    callable, then it must take two positional arguments: this
    `FunctionTransformer` (`self`) and an array-like of input feature names
    (`input_features`). It must return an array-like of output feature
    names. The `get_feature_names_out` method is only defined if
    `feature_names_out` is not None.

    See ``get_feature_names_out`` for more details.

    .. versionadded:: 1.1

kw_args : dict, default=None
    Dictionary of additional keyword arguments to pass to func.

    .. versionadded:: 0.18

inv_kw_args : dict, default=None
    Dictionary of additional keyword arguments to pass to inverse_func.

    .. versionadded:: 0.18

Attributes
----------
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X` has feature
    names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MaxAbsScaler : Scale each feature by its maximum absolute value.
StandardScaler : Standardize features by removing the mean and
    scaling to unit variance.
LabelBinarizer : Binarize labels in a one-vs-all fashion.
MultiLabelBinarizer : Transform between iterable of iterables
    and a multilabel format.

Notes
-----
If `func` returns an output with a `columns` attribute, then the columns is enforced
to be consistent with the output of `get_feature_names_out`.

Examples
--------
>>> import numpy as np
>>> from sklearn.preprocessing import FunctionTransformer
>>> transformer = FunctionTransformer(np.log1p)
>>> X = np.array([[0, 1], [2, 3]])
>>> transformer.transform(X)
array([[0.       , 0.6931...],
       [1.0986..., 1.3862...]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                          rdfs:comment """Generalized Linear Model with a Gamma distribution.

This regressor uses the 'log' link function.

Read more in the :ref:`User Guide <Generalized_linear_models>`.

.. versionadded:: 0.23

Parameters
----------
alpha : float, default=1
    Constant that multiplies the L2 penalty term and determines the
    regularization strength. ``alpha = 0`` is equivalent to unpenalized
    GLMs. In this case, the design matrix `X` must have full column rank
    (no collinearities).
    Values of `alpha` must be in the range `[0.0, inf)`.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the linear predictor `X @ coef_ + intercept_`.

solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'
    Algorithm to use in the optimization problem:

    'lbfgs'
        Calls scipy's L-BFGS-B optimizer.

    'newton-cholesky'
        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to
        iterated reweighted least squares) with an inner Cholesky based solver.
        This solver is a good choice for `n_samples` >> `n_features`, especially
        with one-hot encoded categorical features with rare categories. Be aware
        that the memory usage of this solver has a quadratic dependency on
        `n_features` because it explicitly computes the Hessian matrix.

        .. versionadded:: 1.2

max_iter : int, default=100
    The maximal number of iterations for the solver.
    Values must be in the range `[1, inf)`.

tol : float, default=1e-4
    Stopping criterion. For the lbfgs solver,
    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``
    where ``g_j`` is the j-th component of the gradient (derivative) of
    the objective function.
    Values must be in the range `(0.0, inf)`.

warm_start : bool, default=False
    If set to ``True``, reuse the solution of the previous call to ``fit``
    as initialization for `coef_` and `intercept_`.

verbose : int, default=0
    For the lbfgs solver set verbose to any positive number for verbosity.
    Values must be in the range `[0, inf)`.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the linear predictor (`X @ coef_ +
    intercept_`) in the GLM.

intercept_ : float
    Intercept (a.k.a. bias) added to linear predictor.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

n_iter_ : int
    Actual number of iterations used in the solver.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PoissonRegressor : Generalized Linear Model with a Poisson distribution.
TweedieRegressor : Generalized Linear Model with a Tweedie distribution.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.GammaRegressor()
>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]
>>> y = [19, 26, 33, 30]
>>> clf.fit(X, y)
GammaRegressor()
>>> clf.score(X, y)
0.773...
>>> clf.coef_
array([0.072..., 0.066...])
>>> clf.intercept_
2.896...
>>> clf.predict([[1, 0], [2, 8]])
array([19.483..., 35.795...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                      rdfs:comment """Gaussian Naive Bayes (GaussianNB).

Can perform online updates to model parameters via :meth:`partial_fit`.
For details on algorithm used to update feature means and variance online,
see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:

    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf

Read more in the :ref:`User Guide <gaussian_naive_bayes>`.

Parameters
----------
priors : array-like of shape (n_classes,), default=None
    Prior probabilities of the classes. If specified, the priors are not
    adjusted according to the data.

var_smoothing : float, default=1e-9
    Portion of the largest variance of all features that is added to
    variances for calculation stability.

    .. versionadded:: 0.20

Attributes
----------
class_count_ : ndarray of shape (n_classes,)
    number of training samples observed in each class.

class_prior_ : ndarray of shape (n_classes,)
    probability of each class.

classes_ : ndarray of shape (n_classes,)
    class labels known to the classifier.

epsilon_ : float
    absolute additive value to variances.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

var_ : ndarray of shape (n_classes, n_features)
    Variance of each feature per class.

    .. versionadded:: 1.0

theta_ : ndarray of shape (n_classes, n_features)
    mean of each feature per class.

See Also
--------
BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
CategoricalNB : Naive Bayes classifier for categorical features.
ComplementNB : Complement Naive Bayes classifier.
MultinomialNB : Naive Bayes classifier for multinomial models.

Examples
--------
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> Y = np.array([1, 1, 1, 2, 2, 2])
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(X, Y)
GaussianNB()
>>> print(clf.predict([[-0.8, -1]]))
[1]
>>> clf_pf = GaussianNB()
>>> clf_pf.partial_fit(X, Y, np.unique(Y))
GaussianNB()
>>> print(clf_pf.predict([[-0.8, -1]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod> rdf:type owl:Class ;
                                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                   rdfs:comment """Univariate feature selector with configurable strategy.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
score_func : callable, default=f_classif
    Function taking two arrays X and y, and returning a pair of arrays
    (scores, pvalues). For modes 'percentile' or 'kbest' it can return
    a single array scores.

mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'
    Feature selection mode. Note that the `'percentile'` and `'kbest'`
    modes are supporting unsupervised feature selection (when `y` is `None`).

param : \"all\", float or int, default=1e-5
    Parameter of the corresponding mode.

Attributes
----------
scores_ : array-like of shape (n_features,)
    Scores of features.

pvalues_ : array-like of shape (n_features,)
    p-values of feature scores, None if `score_func` returned scores only.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
mutual_info_classif : Mutual information for a discrete target.
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.
mutual_info_regression : Mutual information for a continuous target.
SelectPercentile : Select features based on percentile of the highest
    scores.
SelectKBest : Select features based on the k highest scores.
SelectFpr : Select features based on a false positive rate test.
SelectFdr : Select features based on an estimated false discovery rate.
SelectFwe : Select features based on family-wise error rate.

Examples
--------
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.feature_selection import GenericUnivariateSelect, chi2
>>> X, y = load_breast_cancer(return_X_y=True)
>>> X.shape
(569, 30)
>>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)
>>> X_new = transformer.fit_transform(X, y)
>>> X_new.shape
(569, 20)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                     rdfs:comment """Get a scorer from string.

Read more in the :ref:`User Guide <scoring_parameter>`.
:func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names
of all available scorers.

Parameters
----------
scoring : str, callable or None
    Scoring method as string. If callable it is returned as is.
    If None, returns None.

Returns
-------
scorer : callable
    The scorer.

Notes
-----
When passed a string, this function always returns a copy of the scorer
object. Calling `get_scorer` twice for the same scorer results in two
separate scorer objects.

Examples
--------
>>> import numpy as np
>>> from sklearn.dummy import DummyClassifier
>>> from sklearn.metrics import get_scorer
>>> X = np.reshape([0, 1, -1, -0.5, 2], (-1, 1))
>>> y = np.array([0, 1, 1, 0, 1])
>>> classifier = DummyClassifier(strategy=\"constant\", constant=0).fit(X, y)
>>> accuracy = get_scorer(\"accuracy\")
>>> accuracy(classifier, X, y)
0.4""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerNamesMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerNamesMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                          rdfs:comment """Get the names of all available scorers.

These names can be passed to :func:`~sklearn.metrics.get_scorer` to
retrieve the scorer object.

Returns
-------
list of str
    Names of all available scorers.

Examples
--------
>>> from sklearn.metrics import get_scorer_names
>>> all_scorers = get_scorer_names()
>>> type(all_scorers)
<class 'list'>
>>> all_scorers[:3]
['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score']
>>> \"roc_auc\" in all_scorers
True""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod> rdf:type owl:Class ;
                                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                      rdfs:comment """Gradient Boosting for classification.

This algorithm builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage ``n_classes_`` regression trees are fit on the negative gradient
of the loss function, e.g. binary or multiclass log loss. Binary
classification is a special case where only a single regression tree is
induced.

:class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster
variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).

Read more in the :ref:`User Guide <gradient_boosting>`.

Parameters
----------
loss : {'log_loss', 'exponential'}, default='log_loss'
    The loss function to be optimized. 'log_loss' refers to binomial and
    multinomial deviance, the same as used in logistic regression.
    It is a good choice for classification with probabilistic outputs.
    For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.

learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by `learning_rate`.
    There is a trade-off between learning_rate and n_estimators.
    Values must be in the range `[0.0, inf)`.

n_estimators : int, default=100
    The number of boosting stages to perform. Gradient boosting
    is fairly robust to over-fitting so a large number usually
    results in better performance.
    Values must be in the range `[1, inf)`.

subsample : float, default=1.0
    The fraction of samples to be used for fitting the individual base
    learners. If smaller than 1.0 this results in Stochastic Gradient
    Boosting. `subsample` interacts with the parameter `n_estimators`.
    Choosing `subsample < 1.0` leads to a reduction of variance
    and an increase in bias.
    Values must be in the range `(0.0, 1.0]`.

criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'
    The function to measure the quality of a split. Supported criteria are
    'friedman_mse' for the mean squared error with improvement score by
    Friedman, 'squared_error' for mean squared error. The default value of
    'friedman_mse' is generally the best as it can provide a better
    approximation in some cases.

    .. versionadded:: 0.18

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, values must be in the range `[2, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`
      will be `ceil(min_samples_split * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`
      will be `ceil(min_samples_leaf * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.
    Values must be in the range `[0.0, 0.5]`.

max_depth : int or None, default=3
    Maximum depth of the individual regression estimators. The maximum
    depth limits the number of nodes in the tree. Tune this parameter
    for best performance; the best value depends on the interaction
    of the input variables. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.
    If int, values must be in the range `[1, inf)`.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.
    Values must be in the range `[0.0, inf)`.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

init : estimator or 'zero', default=None
    An estimator object that is used to compute the initial predictions.
    ``init`` has to provide :term:`fit` and :term:`predict_proba`. If
    'zero', the initial raw predictions are set to zero. By default, a
    ``DummyEstimator`` predicting the classes priors is used.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given to each Tree estimator at each
    boosting iteration.
    In addition, it controls the random permutation of the features at
    each split (see Notes for more details).
    It also controls the random splitting of the training data to obtain a
    validation set if `n_iter_no_change` is not None.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

max_features : {'sqrt', 'log2'}, int or float, default=None
    The number of features to consider when looking for the best split:

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and the features
      considered at each split will be `max(1, int(max_features * n_features_in_))`.
    - If 'sqrt', then `max_features=sqrt(n_features)`.
    - If 'log2', then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    Choosing `max_features < n_features` leads to a reduction of variance
    and an increase in bias.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

verbose : int, default=0
    Enable verbose output. If 1 then it prints progress and performance
    once in a while (the more trees the lower the frequency). If greater
    than 1 then it prints progress and performance for every tree.
    Values must be in the range `[0, inf)`.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    Values must be in the range `[2, inf)`.
    If `None`, then unlimited number of leaf nodes.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just erase the
    previous solution. See :term:`the Glossary <warm_start>`.

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Values must be in the range `(0.0, 1.0)`.
    Only used if ``n_iter_no_change`` is set to an integer.

    .. versionadded:: 0.20

n_iter_no_change : int, default=None
    ``n_iter_no_change`` is used to decide if early stopping will be used
    to terminate training when validation score is not improving. By
    default it is set to None to disable early stopping. If set to a
    number, it will set aside ``validation_fraction`` size of the training
    data as validation and terminate training when validation score is not
    improving in all of the previous ``n_iter_no_change`` numbers of
    iterations. The split is stratified.
    Values must be in the range `[1, inf)`.
    See
    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.

    .. versionadded:: 0.20

tol : float, default=1e-4
    Tolerance for the early stopping. When the loss is not improving
    by at least tol for ``n_iter_no_change`` iterations (if set to a
    number), the training stops.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.20

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed.
    Values must be in the range `[0.0, inf)`.
    See :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

Attributes
----------
n_estimators_ : int
    The number of estimators as selected by early stopping (if
    ``n_iter_no_change`` is specified). Otherwise it is set to
    ``n_estimators``.

    .. versionadded:: 0.20

n_trees_per_iteration_ : int
    The number of trees that are built at each iteration. For binary classifiers,
    this is always 1.

    .. versionadded:: 1.4.0

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

oob_improvement_ : ndarray of shape (n_estimators,)
    The improvement in loss on the out-of-bag samples
    relative to the previous iteration.
    ``oob_improvement_[0]`` is the improvement in
    loss of the first stage over the ``init`` estimator.
    Only available if ``subsample < 1.0``.

oob_scores_ : ndarray of shape (n_estimators,)
    The full history of the loss values on the out-of-bag
    samples. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

oob_score_ : float
    The last value of the loss on the out-of-bag samples. It is
    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

train_score_ : ndarray of shape (n_estimators,)
    The i-th score ``train_score_[i]`` is the loss of the
    model at iteration ``i`` on the in-bag sample.
    If ``subsample == 1`` this is the loss on the training data.

init_ : estimator
    The estimator that provides the initial predictions. Set via the ``init``
    argument.

estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``n_trees_per_iteration_``)
    The collection of fitted sub-estimators. ``n_trees_per_iteration_`` is 1 for
    binary classification, otherwise ``n_classes``.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_classes_ : int
    The number of classes.

max_features_ : int
    The inferred value of max_features.

See Also
--------
HistGradientBoostingClassifier : Histogram-based Gradient Boosting
    Classification Tree.
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
RandomForestClassifier : A meta-estimator that fits a number of decision
    tree classifiers on various sub-samples of the dataset and uses
    averaging to improve the predictive accuracy and control over-fitting.
AdaBoostClassifier : A meta-estimator that begins by fitting a classifier
    on the original dataset and then fits additional copies of the
    classifier on the same dataset where the weights of incorrectly
    classified instances are adjusted such that subsequent classifiers
    focus more on difficult cases.

Notes
-----
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
``max_features=n_features``, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
``random_state`` has to be fixed.

References
----------
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

J. Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.

Examples
--------
The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.

>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]

>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                     rdfs:comment """Gradient Boosting for regression.

This estimator builds an additive model in a forward stage-wise fashion; it
allows for the optimization of arbitrary differentiable loss functions. In
each stage a regression tree is fit on the negative gradient of the given
loss function.

:class:`sklearn.ensemble.HistGradientBoostingRegressor` is a much faster
variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).

Read more in the :ref:`User Guide <gradient_boosting>`.

Parameters
----------
loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'
    Loss function to be optimized. 'squared_error' refers to the squared
    error for regression. 'absolute_error' refers to the absolute error of
    regression and is a robust loss function. 'huber' is a
    combination of the two. 'quantile' allows quantile regression (use
    `alpha` to specify the quantile).

learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by `learning_rate`.
    There is a trade-off between learning_rate and n_estimators.
    Values must be in the range `[0.0, inf)`.

n_estimators : int, default=100
    The number of boosting stages to perform. Gradient boosting
    is fairly robust to over-fitting so a large number usually
    results in better performance.
    Values must be in the range `[1, inf)`.

subsample : float, default=1.0
    The fraction of samples to be used for fitting the individual base
    learners. If smaller than 1.0 this results in Stochastic Gradient
    Boosting. `subsample` interacts with the parameter `n_estimators`.
    Choosing `subsample < 1.0` leads to a reduction of variance
    and an increase in bias.
    Values must be in the range `(0.0, 1.0]`.

criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'
    The function to measure the quality of a split. Supported criteria are
    \"friedman_mse\" for the mean squared error with improvement score by
    Friedman, \"squared_error\" for mean squared error. The default value of
    \"friedman_mse\" is generally the best as it can provide a better
    approximation in some cases.

    .. versionadded:: 0.18

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, values must be in the range `[2, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`
      will be `ceil(min_samples_split * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`
      will be `ceil(min_samples_leaf * n_samples)`.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.
    Values must be in the range `[0.0, 0.5]`.

max_depth : int or None, default=3
    Maximum depth of the individual regression estimators. The maximum
    depth limits the number of nodes in the tree. Tune this parameter
    for best performance; the best value depends on the interaction
    of the input variables. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.
    If int, values must be in the range `[1, inf)`.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.
    Values must be in the range `[0.0, inf)`.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

init : estimator or 'zero', default=None
    An estimator object that is used to compute the initial predictions.
    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the
    initial raw predictions are set to zero. By default a
    ``DummyEstimator`` is used, predicting either the average target value
    (for loss='squared_error'), or a quantile for the other losses.

random_state : int, RandomState instance or None, default=None
    Controls the random seed given to each Tree estimator at each
    boosting iteration.
    In addition, it controls the random permutation of the features at
    each split (see Notes for more details).
    It also controls the random splitting of the training data to obtain a
    validation set if `n_iter_no_change` is not None.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

max_features : {'sqrt', 'log2'}, int or float, default=None
    The number of features to consider when looking for the best split:

    - If int, values must be in the range `[1, inf)`.
    - If float, values must be in the range `(0.0, 1.0]` and the features
      considered at each split will be `max(1, int(max_features * n_features_in_))`.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    Choosing `max_features < n_features` leads to a reduction of variance
    and an increase in bias.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

alpha : float, default=0.9
    The alpha-quantile of the huber loss function and the quantile
    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
    Values must be in the range `(0.0, 1.0)`.

verbose : int, default=0
    Enable verbose output. If 1 then it prints progress and performance
    once in a while (the more trees the lower the frequency). If greater
    than 1 then it prints progress and performance for every tree.
    Values must be in the range `[0, inf)`.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    Values must be in the range `[2, inf)`.
    If None, then unlimited number of leaf nodes.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just erase the
    previous solution. See :term:`the Glossary <warm_start>`.

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Values must be in the range `(0.0, 1.0)`.
    Only used if ``n_iter_no_change`` is set to an integer.

    .. versionadded:: 0.20

n_iter_no_change : int, default=None
    ``n_iter_no_change`` is used to decide if early stopping will be used
    to terminate training when validation score is not improving. By
    default it is set to None to disable early stopping. If set to a
    number, it will set aside ``validation_fraction`` size of the training
    data as validation and terminate training when validation score is not
    improving in all of the previous ``n_iter_no_change`` numbers of
    iterations.
    Values must be in the range `[1, inf)`.
    See
    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.

    .. versionadded:: 0.20

tol : float, default=1e-4
    Tolerance for the early stopping. When the loss is not improving
    by at least tol for ``n_iter_no_change`` iterations (if set to a
    number), the training stops.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.20

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed.
    Values must be in the range `[0.0, inf)`.
    See :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

Attributes
----------
n_estimators_ : int
    The number of estimators as selected by early stopping (if
    ``n_iter_no_change`` is specified). Otherwise it is set to
    ``n_estimators``.

n_trees_per_iteration_ : int
    The number of trees that are built at each iteration. For regressors, this is
    always 1.

    .. versionadded:: 1.4.0

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

oob_improvement_ : ndarray of shape (n_estimators,)
    The improvement in loss on the out-of-bag samples
    relative to the previous iteration.
    ``oob_improvement_[0]`` is the improvement in
    loss of the first stage over the ``init`` estimator.
    Only available if ``subsample < 1.0``.

oob_scores_ : ndarray of shape (n_estimators,)
    The full history of the loss values on the out-of-bag
    samples. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

oob_score_ : float
    The last value of the loss on the out-of-bag samples. It is
    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.

    .. versionadded:: 1.3

train_score_ : ndarray of shape (n_estimators,)
    The i-th score ``train_score_[i]`` is the loss of the
    model at iteration ``i`` on the in-bag sample.
    If ``subsample == 1`` this is the loss on the training data.

init_ : estimator
    The estimator that provides the initial predictions. Set via the ``init``
    argument.

estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)
    The collection of fitted sub-estimators.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

max_features_ : int
    The inferred value of max_features.

See Also
--------
HistGradientBoostingRegressor : Histogram-based Gradient Boosting
    Classification Tree.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
sklearn.ensemble.RandomForestRegressor : A random forest regressor.

Notes
-----
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
``max_features=n_features``, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
``random_state`` has to be fixed.

References
----------
J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.

J. Friedman, Stochastic Gradient Boosting, 1999

T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.

Examples
--------
>>> from sklearn.datasets import make_regression
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_regression(random_state=0)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
>>> reg = GradientBoostingRegressor(random_state=0)
>>> reg.fit(X_train, y_train)
GradientBoostingRegressor(random_state=0)
>>> reg.predict(X_test[1:2])
array([-61...])
>>> reg.score(X_test, y_test)
0.4...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                        rdfs:comment """Exhaustive search over specified parameter values for an estimator.

Important members are fit, predict.

GridSearchCV implements a \"fit\" and a \"score\" method.
It also implements \"score_samples\", \"predict\", \"predict_proba\",
\"decision_function\", \"transform\" and \"inverse_transform\" if they are
implemented in the estimator used.

The parameters of the estimator used to apply these methods are optimized
by cross-validated grid-search over a parameter grid.

Read more in the :ref:`User Guide <grid_search>`.

Parameters
----------
estimator : estimator object
    This is assumed to implement the scikit-learn estimator interface.
    Either estimator needs to provide a ``score`` function,
    or ``scoring`` must be passed.

param_grid : dict or list of dictionaries
    Dictionary with parameters names (`str`) as keys and lists of
    parameter settings to try as values, or a list of such
    dictionaries, in which case the grids spanned by each dictionary
    in the list are explored. This enables searching over any sequence
    of parameter settings.

scoring : str, callable, list, tuple or dict, default=None
    Strategy to evaluate the performance of the cross-validated model on
    the test set.

    If `scoring` represents a single score, one can use:

    - a single string (see :ref:`scoring_parameter`);
    - a callable (see :ref:`scoring`) that returns a single value.

    If `scoring` represents multiple scores, one can use:

    - a list or tuple of unique strings;
    - a callable returning a dictionary where the keys are the metric
      names and the values are the metric scores;
    - a dictionary with metric names as keys and callables a values.

    See :ref:`multimetric_grid_search` for an example.

n_jobs : int, default=None
    Number of jobs to run in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionchanged:: v0.20
       `n_jobs` default changed from 1 to None

refit : bool, str, or callable, default=True
    Refit an estimator using the best found parameters on the whole
    dataset.

    For multiple metric evaluation, this needs to be a `str` denoting the
    scorer that would be used to find the best parameters for refitting
    the estimator at the end.

    Where there are considerations other than maximum score in
    choosing a best estimator, ``refit`` can be set to a function which
    returns the selected ``best_index_`` given ``cv_results_``. In that
    case, the ``best_estimator_`` and ``best_params_`` will be set
    according to the returned ``best_index_`` while the ``best_score_``
    attribute will not be available.

    The refitted estimator is made available at the ``best_estimator_``
    attribute and permits using ``predict`` directly on this
    ``GridSearchCV`` instance.

    Also for multiple metric evaluation, the attributes ``best_index_``,
    ``best_score_`` and ``best_params_`` will only be available if
    ``refit`` is set and all of them will be determined w.r.t this specific
    scorer.

    See ``scoring`` parameter to know more about multiple metric
    evaluation.

    See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`
    to see how to design a custom selection strategy using a callable
    via `refit`.

    .. versionchanged:: 0.20
        Support for callable added.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross validation,
    - integer, to specify the number of folds in a `(Stratified)KFold`,
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if the estimator is a classifier and ``y`` is
    either binary or multiclass, :class:`StratifiedKFold` is used. In all
    other cases, :class:`KFold` is used. These splitters are instantiated
    with `shuffle=False` so the splits will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

verbose : int
    Controls the verbosity: the higher, the more messages.

    - >1 : the computation time for each fold and parameter candidate is
      displayed;
    - >2 : the score is also displayed;
    - >3 : the fold and candidate parameter indexes are also displayed
      together with the starting time of the computation.

pre_dispatch : int, or str, default='2*n_jobs'
    Controls the number of jobs that get dispatched during parallel
    execution. Reducing this number can be useful to avoid an
    explosion of memory consumption when more jobs get dispatched
    than CPUs can process. This parameter can be:

        - None, in which case all the jobs are immediately
          created and spawned. Use this for lightweight and
          fast-running jobs, to avoid delays due to on-demand
          spawning of the jobs

        - An int, giving the exact number of total jobs that are
          spawned

        - A str, giving an expression as a function of n_jobs,
          as in '2*n_jobs'

error_score : 'raise' or numeric, default=np.nan
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised. If a numeric value is given,
    FitFailedWarning is raised. This parameter does not affect the refit
    step, which will always raise the error.

return_train_score : bool, default=False
    If ``False``, the ``cv_results_`` attribute will not include training
    scores.
    Computing training scores is used to get insights on how different
    parameter settings impact the overfitting/underfitting trade-off.
    However computing the scores on the training set can be computationally
    expensive and is not strictly required to select the parameters that
    yield the best generalization performance.

    .. versionadded:: 0.19

    .. versionchanged:: 0.21
        Default value was changed from ``True`` to ``False``

Attributes
----------
cv_results_ : dict of numpy (masked) ndarrays
    A dict with keys as column headers and values as columns, that can be
    imported into a pandas ``DataFrame``.

    For instance the below given table

    +------------+-----------+------------+-----------------+---+---------+
    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|
    +============+===========+============+=================+===+=========+
    |  'poly'    |     --    |      2     |       0.80      |...|    2    |
    +------------+-----------+------------+-----------------+---+---------+
    |  'poly'    |     --    |      3     |       0.70      |...|    4    |
    +------------+-----------+------------+-----------------+---+---------+
    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |
    +------------+-----------+------------+-----------------+---+---------+
    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |
    +------------+-----------+------------+-----------------+---+---------+

    will be represented by a ``cv_results_`` dict of::

        {
        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],
                                     mask = [False False False False]...)
        'param_gamma': masked_array(data = [-- -- 0.1 0.2],
                                    mask = [ True  True False False]...),
        'param_degree': masked_array(data = [2.0 3.0 -- --],
                                     mask = [False False  True  True]...),
        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],
        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],
        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],
        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],
        'rank_test_score'    : [2, 4, 3, 1],
        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],
        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],
        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],
        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],
        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],
        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],
        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],
        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],
        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],
        }

    NOTE

    The key ``'params'`` is used to store a list of parameter
    settings dicts for all the parameter candidates.

    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and
    ``std_score_time`` are all in seconds.

    For multi-metric evaluation, the scores for all the scorers are
    available in the ``cv_results_`` dict at the keys ending with that
    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown
    above. ('split0_test_precision', 'mean_train_precision' etc.)

best_estimator_ : estimator
    Estimator that was chosen by the search, i.e. estimator
    which gave highest score (or smallest loss if specified)
    on the left out data. Not available if ``refit=False``.

    See ``refit`` parameter for more information on allowed values.

best_score_ : float
    Mean cross-validated score of the best_estimator

    For multi-metric evaluation, this is present only if ``refit`` is
    specified.

    This attribute is not available if ``refit`` is a function.

best_params_ : dict
    Parameter setting that gave the best results on the hold out data.

    For multi-metric evaluation, this is present only if ``refit`` is
    specified.

best_index_ : int
    The index (of the ``cv_results_`` arrays) which corresponds to the best
    candidate parameter setting.

    The dict at ``search.cv_results_['params'][search.best_index_]`` gives
    the parameter setting for the best model, that gives the highest
    mean score (``search.best_score_``).

    For multi-metric evaluation, this is present only if ``refit`` is
    specified.

scorer_ : function or a dict
    Scorer function used on the held out data to choose the best
    parameters for the model.

    For multi-metric evaluation, this attribute holds the validated
    ``scoring`` dict which maps the scorer key to the scorer callable.

n_splits_ : int
    The number of cross-validation splits (folds/iterations).

refit_time_ : float
    Seconds used for refitting the best model on the whole dataset.

    This is present only if ``refit`` is not False.

    .. versionadded:: 0.20

multimetric_ : bool
    Whether or not the scorers compute several metrics.

classes_ : ndarray of shape (n_classes,)
    The classes labels. This is present only if ``refit`` is specified and
    the underlying estimator is a classifier.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `n_features_in_` when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `feature_names_in_` when fit.

    .. versionadded:: 1.0

See Also
--------
ParameterGrid : Generates all the combinations of a hyperparameter grid.
train_test_split : Utility function to split the data into a development
    set usable for fitting a GridSearchCV instance and an evaluation set
    for its final evaluation.
sklearn.metrics.make_scorer : Make a scorer from a performance metric or
    loss function.

Notes
-----
The parameters selected are those that maximize the score of the left out
data, unless an explicit score is passed in which case it is used instead.

If `n_jobs` was set to a value higher than one, the data is copied for each
point in the grid (and not `n_jobs` times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set `pre_dispatch`. Then, the memory is copied only
`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
n_jobs`.

Examples
--------
>>> from sklearn import svm, datasets
>>> from sklearn.model_selection import GridSearchCV
>>> iris = datasets.load_iris()
>>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
>>> svc = svm.SVC()
>>> clf = GridSearchCV(svc, parameters)
>>> clf.fit(iris.data, iris.target)
GridSearchCV(estimator=SVC(),
             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})
>>> sorted(clf.cv_results_.keys())
['mean_fit_time', 'mean_score_time', 'mean_test_score',...
 'param_C', 'param_kernel', 'params',...
 'rank_test_score', 'split0_test_score',...
 'split2_test_score', ...
 'std_fit_time', 'std_score_time', 'std_test_score']""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                      rdfs:comment """K-fold iterator variant with non-overlapping groups.

Each group will appear exactly once in the test set across all folds (the
number of distinct groups has to be at least equal to the number of folds).

The folds are approximately balanced in the sense that the number of
distinct groups is approximately the same in each fold.

Read more in the :ref:`User Guide <group_k_fold>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=5
    Number of folds. Must be at least 2.

    .. versionchanged:: 0.22
        ``n_splits`` default value changed from 3 to 5.

Notes
-----
Groups appear in an arbitrary order throughout the folds.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import GroupKFold
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> groups = np.array([0, 0, 2, 2, 3, 3])
>>> group_kfold = GroupKFold(n_splits=2)
>>> group_kfold.get_n_splits(X, y, groups)
2
>>> print(group_kfold)
GroupKFold(n_splits=2)
>>> for i, (train_index, test_index) in enumerate(group_kfold.split(X, y, groups)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")
...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")
Fold 0:
  Train: index=[2 3], group=[2 2]
  Test:  index=[0 1 4 5], group=[0 0 3 3]
Fold 1:
  Train: index=[0 1 4 5], group=[0 0 3 3]
  Test:  index=[2 3], group=[2 2]

See Also
--------
LeaveOneGroupOut : For splitting the data according to explicit
    domain-specific stratification of the dataset.

StratifiedKFold : Takes class information into account to avoid building
    folds with imbalanced class proportions (for binary or multiclass
    classification tasks).""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                             rdfs:comment """Shuffle-Group(s)-Out cross-validation iterator.

Provides randomized train/test indices to split data according to a
third-party provided group. This group information can be used to encode
arbitrary domain specific stratifications of the samples as integers.

For instance the groups could be the year of collection of the samples
and thus allow for cross-validation against time-based splits.

The difference between LeavePGroupsOut and GroupShuffleSplit is that
the former generates splits using all subsets of size ``p`` unique groups,
whereas GroupShuffleSplit generates a user-determined number of random
test splits, each with a user-determined fraction of unique groups.

For example, a less computationally intensive alternative to
``LeavePGroupsOut(p=10)`` would be
``GroupShuffleSplit(test_size=10, n_splits=100)``.

Note: The parameters ``test_size`` and ``train_size`` refer to groups, and
not to samples, as in ShuffleSplit.

Read more in the :ref:`User Guide <group_shuffle_split>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=5
    Number of re-shuffling & splitting iterations.

test_size : float, int, default=0.2
    If float, should be between 0.0 and 1.0 and represent the proportion
    of groups to include in the test split (rounded up). If int,
    represents the absolute number of test groups. If None, the value is
    set to the complement of the train size.
    The default will change in version 0.21. It will remain 0.2 only
    if ``train_size`` is unspecified, otherwise it will complement
    the specified ``train_size``.

train_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the
    proportion of the groups to include in the train split. If
    int, represents the absolute number of train groups. If None,
    the value is automatically set to the complement of the test size.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of the training and testing indices produced.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import GroupShuffleSplit
>>> X = np.ones(shape=(8, 2))
>>> y = np.ones(shape=(8, 1))
>>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])
>>> print(groups.shape)
(8,)
>>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)
>>> gss.get_n_splits()
2
>>> print(gss)
GroupShuffleSplit(n_splits=2, random_state=42, test_size=None, train_size=0.7)
>>> for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")
...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")
Fold 0:
  Train: index=[2 3 4 5 6 7], group=[2 2 2 3 3 3]
  Test:  index=[0 1], group=[1 1]
Fold 1:
  Train: index=[0 1 5 6 7], group=[1 1 3 3 3]
  Test:  index=[2 3 4], group=[2 2 2]

See Also
--------
ShuffleSplit : Shuffles samples to create independent test/train sets.

LeavePGroupsOut : Train set leaves out all possible subsets of `p` groups.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HdbscanModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                   rdfs:comment """Cluster data using hierarchical density-based clustering.

HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications
with Noise. Performs :class:`~sklearn.cluster.DBSCAN` over varying epsilon
values and integrates the result to find a clustering that gives the best
stability over epsilon.
This allows HDBSCAN to find clusters of varying densities (unlike
:class:`~sklearn.cluster.DBSCAN`), and be more robust to parameter selection.
Read more in the :ref:`User Guide <hdbscan>`.

For an example of how to use HDBSCAN, as well as a comparison to
:class:`~sklearn.cluster.DBSCAN`, please see the :ref:`plotting demo
<sphx_glr_auto_examples_cluster_plot_hdbscan.py>`.

.. versionadded:: 1.3

Parameters
----------
min_cluster_size : int, default=5
    The minimum number of samples in a group for that group to be
    considered a cluster; groupings smaller than this size will be left
    as noise.

min_samples : int, default=None
    The number of samples in a neighborhood for a point
    to be considered as a core point. This includes the point itself.
    When `None`, defaults to `min_cluster_size`.

cluster_selection_epsilon : float, default=0.0
    A distance threshold. Clusters below this value will be merged.
    See [5]_ for more information.

max_cluster_size : int, default=None
    A limit to the size of clusters returned by the `\"eom\"` cluster
    selection algorithm. There is no limit when `max_cluster_size=None`.
    Has no effect if `cluster_selection_method=\"leaf\"`.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array.

    - If metric is a string or callable, it must be one of
      the options allowed by :func:`~sklearn.metrics.pairwise_distances`
      for its metric parameter.

    - If metric is \"precomputed\", X is assumed to be a distance matrix and
      must be square.

metric_params : dict, default=None
    Arguments passed to the distance metric.

alpha : float, default=1.0
    A distance scaling parameter as used in robust single linkage.
    See [3]_ for more information.

algorithm : {\"auto\", \"brute\", \"kd_tree\", \"ball_tree\"}, default=\"auto\"
    Exactly which algorithm to use for computing core distances; By default
    this is set to `\"auto\"` which attempts to use a
    :class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses
    a :class:`~sklearn.neighbors.BallTree` tree. Both `\"kd_tree\"` and
    `\"ball_tree\"` algorithms use the
    :class:`~sklearn.neighbors.NearestNeighbors` estimator.

    If the `X` passed during `fit` is sparse or `metric` is invalid for
    both :class:`~sklearn.neighbors.KDTree` and
    :class:`~sklearn.neighbors.BallTree`, then it resolves to use the
    `\"brute\"` algorithm.

    .. deprecated:: 1.4
       The `'kdtree'` option was deprecated in version 1.4,
       and will be renamed to `'kd_tree'` in 1.6.

    .. deprecated:: 1.4
       The `'balltree'` option was deprecated in version 1.4,
       and will be renamed to `'ball_tree'` in 1.6.

leaf_size : int, default=40
    Leaf size for trees responsible for fast nearest neighbour queries when
    a KDTree or a BallTree are used as core-distance algorithms. A large
    dataset size and small `leaf_size` may induce excessive memory usage.
    If you are running out of memory consider increasing the `leaf_size`
    parameter. Ignored for `algorithm=\"brute\"`.

n_jobs : int, default=None
    Number of jobs to run in parallel to calculate distances.
    `None` means 1 unless in a :obj:`joblib.parallel_backend` context.
    `-1` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

cluster_selection_method : {\"eom\", \"leaf\"}, default=\"eom\"
    The method used to select clusters from the condensed tree. The
    standard approach for HDBSCAN* is to use an Excess of Mass (`\"eom\"`)
    algorithm to find the most persistent clusters. Alternatively you can
    instead select the clusters at the leaves of the tree -- this provides
    the most fine grained and homogeneous clusters.

allow_single_cluster : bool, default=False
    By default HDBSCAN* will not produce a single cluster, setting this
    to True will override this and allow single cluster results in
    the case that you feel this is a valid result for your dataset.

store_centers : str, default=None
    Which, if any, cluster centers to compute and store. The options are:

    - `None` which does not compute nor store any centers.
    - `\"centroid\"` which calculates the center by taking the weighted
      average of their positions. Note that the algorithm uses the
      euclidean metric and does not guarantee that the output will be
      an observed data point.
    - `\"medoid\"` which calculates the center by taking the point in the
      fitted data which minimizes the distance to all other points in
      the cluster. This is slower than \"centroid\" since it requires
      computing additional pairwise distances between points of the
      same cluster but guarantees the output is an observed data point.
      The medoid is also well-defined for arbitrary metrics, and does not
      depend on a euclidean metric.
    - `\"both\"` which computes and stores both forms of centers.

copy : bool, default=False
    If `copy=True` then any time an in-place modifications would be made
    that would overwrite data passed to :term:`fit`, a copy will first be
    made, guaranteeing that the original data will be unchanged.
    Currently, it only applies when `metric=\"precomputed\"`, when passing
    a dense array or a CSR sparse matrix and when `algorithm=\"brute\"`.

Attributes
----------
labels_ : ndarray of shape (n_samples,)
    Cluster labels for each point in the dataset given to :term:`fit`.
    Outliers are labeled as follows:

    - Noisy samples are given the label -1.
    - Samples with infinite elements (+/- np.inf) are given the label -2.
    - Samples with missing data are given the label -3, even if they
      also have infinite elements.

probabilities_ : ndarray of shape (n_samples,)
    The strength with which each sample is a member of its assigned
    cluster.

    - Clustered samples have probabilities proportional to the degree that
      they persist as part of the cluster.
    - Noisy samples have probability zero.
    - Samples with infinite elements (+/- np.inf) have probability 0.
    - Samples with missing data have probability `np.nan`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

centroids_ : ndarray of shape (n_clusters, n_features)
    A collection containing the centroid of each cluster calculated under
    the standard euclidean metric. The centroids may fall \"outside\" their
    respective clusters if the clusters themselves are non-convex.

    Note that `n_clusters` only counts non-outlier clusters. That is to
    say, the `-1, -2, -3` labels for the outlier clusters are excluded.

medoids_ : ndarray of shape (n_clusters, n_features)
    A collection containing the medoid of each cluster calculated under
    the whichever metric was passed to the `metric` parameter. The
    medoids are points in the original cluster which minimize the average
    distance to all other points in that cluster under the chosen metric.
    These can be thought of as the result of projecting the `metric`-based
    centroid back onto the cluster.

    Note that `n_clusters` only counts non-outlier clusters. That is to
    say, the `-1, -2, -3` labels for the outlier clusters are excluded.

See Also
--------
DBSCAN : Density-Based Spatial Clustering of Applications
    with Noise.
OPTICS : Ordering Points To Identify the Clustering Structure.
Birch : Memory-efficient, online-learning algorithm.

References
----------

.. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering
  based on hierarchical density estimates.
  <10.1007/978-3-642-37456-2_14>`
.. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.
   Hierarchical density estimates for data clustering, visualization,
   and outlier detection.<10.1145/2733381>`

.. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the
   cluster tree.
   <https://papers.nips.cc/paper/2010/hash/
   b534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_

.. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and
   Sander, J. Density-Based Clustering Validation.
   <https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_

.. [5] :arxiv:`Malzer, C., & Baum, M. \"A Hybrid Approach To Hierarchical
   Density-based Cluster Selection.\"<1911.02282>`.

Examples
--------
>>> from sklearn.cluster import HDBSCAN
>>> from sklearn.datasets import load_digits
>>> X, _ = load_digits(return_X_y=True)
>>> hdb = HDBSCAN(min_cluster_size=20)
>>> hdb.fit(X)
HDBSCAN(min_cluster_size=20)
>>> hdb.labels_
array([ 2,  6, -1, ..., -1, -1, -1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                               rdfs:comment """Search over specified parameter values with successive halving.

The search strategy starts evaluating all the candidates with a small
amount of resources and iteratively selects the best candidates, using
more and more resources.

Read more in the :ref:`User guide <successive_halving_user_guide>`.

.. note::

  This estimator is still **experimental** for now: the predictions
  and the API might change without any deprecation cycle. To use it,
  you need to explicitly import ``enable_halving_search_cv``::

    >>> # explicitly require this experimental feature
    >>> from sklearn.experimental import enable_halving_search_cv # noqa
    >>> # now you can import normally from model_selection
    >>> from sklearn.model_selection import HalvingGridSearchCV

Parameters
----------
estimator : estimator object
    This is assumed to implement the scikit-learn estimator interface.
    Either estimator needs to provide a ``score`` function,
    or ``scoring`` must be passed.

param_grid : dict or list of dictionaries
    Dictionary with parameters names (string) as keys and lists of
    parameter settings to try as values, or a list of such
    dictionaries, in which case the grids spanned by each dictionary
    in the list are explored. This enables searching over any sequence
    of parameter settings.

factor : int or float, default=3
    The 'halving' parameter, which determines the proportion of candidates
    that are selected for each subsequent iteration. For example,
    ``factor=3`` means that only one third of the candidates are selected.

resource : ``'n_samples'`` or str, default='n_samples'
    Defines the resource that increases with each iteration. By default,
    the resource is the number of samples. It can also be set to any
    parameter of the base estimator that accepts positive integer
    values, e.g. 'n_iterations' or 'n_estimators' for a gradient
    boosting estimator. In this case ``max_resources`` cannot be 'auto'
    and must be set explicitly.

max_resources : int, default='auto'
    The maximum amount of resource that any candidate is allowed to use
    for a given iteration. By default, this is set to ``n_samples`` when
    ``resource='n_samples'`` (default), else an error is raised.

min_resources : {'exhaust', 'smallest'} or int, default='exhaust'
    The minimum amount of resource that any candidate is allowed to use
    for a given iteration. Equivalently, this defines the amount of
    resources `r0` that are allocated for each candidate at the first
    iteration.

    - 'smallest' is a heuristic that sets `r0` to a small value:

        - ``n_splits * 2`` when ``resource='n_samples'`` for a regression
          problem
        - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a
          classification problem
        - ``1`` when ``resource != 'n_samples'``

    - 'exhaust' will set `r0` such that the **last** iteration uses as
      much resources as possible. Namely, the last iteration will use the
      highest value smaller than ``max_resources`` that is a multiple of
      both ``min_resources`` and ``factor``. In general, using 'exhaust'
      leads to a more accurate estimator, but is slightly more time
      consuming.

    Note that the amount of resources used at each iteration is always a
    multiple of ``min_resources``.

aggressive_elimination : bool, default=False
    This is only relevant in cases where there isn't enough resources to
    reduce the remaining candidates to at most `factor` after the last
    iteration. If ``True``, then the search process will 'replay' the
    first iteration for as long as needed until the number of candidates
    is small enough. This is ``False`` by default, which means that the
    last iteration may evaluate more than ``factor`` candidates. See
    :ref:`aggressive_elimination` for more details.

cv : int, cross-validation generator or iterable, default=5
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - integer, to specify the number of folds in a `(Stratified)KFold`,
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if the estimator is a classifier and ``y`` is
    either binary or multiclass, :class:`StratifiedKFold` is used. In all
    other cases, :class:`KFold` is used. These splitters are instantiated
    with `shuffle=False` so the splits will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. note::
        Due to implementation details, the folds produced by `cv` must be
        the same across multiple calls to `cv.split()`. For
        built-in `scikit-learn` iterators, this can be achieved by
        deactivating shuffling (`shuffle=False`), or by setting the
        `cv`'s `random_state` parameter to an integer.

scoring : str, callable, or None, default=None
    A single string (see :ref:`scoring_parameter`) or a callable
    (see :ref:`scoring`) to evaluate the predictions on the test set.
    If None, the estimator's score method is used.

refit : bool, default=True
    If True, refit an estimator using the best found parameters on the
    whole dataset.

    The refitted estimator is made available at the ``best_estimator_``
    attribute and permits using ``predict`` directly on this
    ``HalvingGridSearchCV`` instance.

error_score : 'raise' or numeric
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised. If a numeric value is given,
    FitFailedWarning is raised. This parameter does not affect the refit
    step, which will always raise the error. Default is ``np.nan``.

return_train_score : bool, default=False
    If ``False``, the ``cv_results_`` attribute will not include training
    scores.
    Computing training scores is used to get insights on how different
    parameter settings impact the overfitting/underfitting trade-off.
    However computing the scores on the training set can be computationally
    expensive and is not strictly required to select the parameters that
    yield the best generalization performance.

random_state : int, RandomState instance or None, default=None
    Pseudo random number generator state used for subsampling the dataset
    when `resources != 'n_samples'`. Ignored otherwise.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

n_jobs : int or None, default=None
    Number of jobs to run in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : int
    Controls the verbosity: the higher, the more messages.

Attributes
----------
n_resources_ : list of int
    The amount of resources used at each iteration.

n_candidates_ : list of int
    The number of candidate parameters that were evaluated at each
    iteration.

n_remaining_candidates_ : int
    The number of candidate parameters that are left after the last
    iteration. It corresponds to `ceil(n_candidates[-1] / factor)`

max_resources_ : int
    The maximum number of resources that any candidate is allowed to use
    for a given iteration. Note that since the number of resources used
    at each iteration must be a multiple of ``min_resources_``, the
    actual number of resources used at the last iteration may be smaller
    than ``max_resources_``.

min_resources_ : int
    The amount of resources that are allocated for each candidate at the
    first iteration.

n_iterations_ : int
    The actual number of iterations that were run. This is equal to
    ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.
    Else, this is equal to ``min(n_possible_iterations_,
    n_required_iterations_)``.

n_possible_iterations_ : int
    The number of iterations that are possible starting with
    ``min_resources_`` resources and without exceeding
    ``max_resources_``.

n_required_iterations_ : int
    The number of iterations that are required to end up with less than
    ``factor`` candidates at the last iteration, starting with
    ``min_resources_`` resources. This will be smaller than
    ``n_possible_iterations_`` when there isn't enough resources.

cv_results_ : dict of numpy (masked) ndarrays
    A dict with keys as column headers and values as columns, that can be
    imported into a pandas ``DataFrame``. It contains lots of information
    for analysing the results of a search.
    Please refer to the :ref:`User guide<successive_halving_cv_results>`
    for details.

best_estimator_ : estimator or dict
    Estimator that was chosen by the search, i.e. estimator
    which gave highest score (or smallest loss if specified)
    on the left out data. Not available if ``refit=False``.

best_score_ : float
    Mean cross-validated score of the best_estimator.

best_params_ : dict
    Parameter setting that gave the best results on the hold out data.

best_index_ : int
    The index (of the ``cv_results_`` arrays) which corresponds to the best
    candidate parameter setting.

    The dict at ``search.cv_results_['params'][search.best_index_]`` gives
    the parameter setting for the best model, that gives the highest
    mean score (``search.best_score_``).

scorer_ : function or a dict
    Scorer function used on the held out data to choose the best
    parameters for the model.

n_splits_ : int
    The number of cross-validation splits (folds/iterations).

refit_time_ : float
    Seconds used for refitting the best model on the whole dataset.

    This is present only if ``refit`` is not False.

multimetric_ : bool
    Whether or not the scorers compute several metrics.

classes_ : ndarray of shape (n_classes,)
    The classes labels. This is present only if ``refit`` is specified and
    the underlying estimator is a classifier.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `n_features_in_` when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `feature_names_in_` when fit.

    .. versionadded:: 1.0

See Also
--------
:class:`HalvingRandomSearchCV`:
    Random search over a set of parameters using successive halving.

Notes
-----
The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.

All parameter combinations scored with a NaN will share the lowest rank.

Examples
--------

>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.experimental import enable_halving_search_cv  # noqa
>>> from sklearn.model_selection import HalvingGridSearchCV
...
>>> X, y = load_iris(return_X_y=True)
>>> clf = RandomForestClassifier(random_state=0)
...
>>> param_grid = {\"max_depth\": [3, None],
...               \"min_samples_split\": [5, 10]}
>>> search = HalvingGridSearchCV(clf, param_grid, resource='n_estimators',
...                              max_resources=10,
...                              random_state=0).fit(X, y)
>>> search.best_params_  # doctest: +SKIP
{'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                 rdfs:comment """Randomized search on hyper parameters.

The search strategy starts evaluating all the candidates with a small
amount of resources and iteratively selects the best candidates, using more
and more resources.

The candidates are sampled at random from the parameter space and the
number of sampled candidates is determined by ``n_candidates``.

Read more in the :ref:`User guide<successive_halving_user_guide>`.

.. note::

  This estimator is still **experimental** for now: the predictions
  and the API might change without any deprecation cycle. To use it,
  you need to explicitly import ``enable_halving_search_cv``::

    >>> # explicitly require this experimental feature
    >>> from sklearn.experimental import enable_halving_search_cv # noqa
    >>> # now you can import normally from model_selection
    >>> from sklearn.model_selection import HalvingRandomSearchCV

Parameters
----------
estimator : estimator object
    This is assumed to implement the scikit-learn estimator interface.
    Either estimator needs to provide a ``score`` function,
    or ``scoring`` must be passed.

param_distributions : dict or list of dicts
    Dictionary with parameters names (`str`) as keys and distributions
    or lists of parameters to try. Distributions must provide a ``rvs``
    method for sampling (such as those from scipy.stats.distributions).
    If a list is given, it is sampled uniformly.
    If a list of dicts is given, first a dict is sampled uniformly, and
    then a parameter is sampled using that dict as above.

n_candidates : \"exhaust\" or int, default=\"exhaust\"
    The number of candidate parameters to sample, at the first
    iteration. Using 'exhaust' will sample enough candidates so that the
    last iteration uses as many resources as possible, based on
    `min_resources`, `max_resources` and `factor`. In this case,
    `min_resources` cannot be 'exhaust'.

factor : int or float, default=3
    The 'halving' parameter, which determines the proportion of candidates
    that are selected for each subsequent iteration. For example,
    ``factor=3`` means that only one third of the candidates are selected.

resource : ``'n_samples'`` or str, default='n_samples'
    Defines the resource that increases with each iteration. By default,
    the resource is the number of samples. It can also be set to any
    parameter of the base estimator that accepts positive integer
    values, e.g. 'n_iterations' or 'n_estimators' for a gradient
    boosting estimator. In this case ``max_resources`` cannot be 'auto'
    and must be set explicitly.

max_resources : int, default='auto'
    The maximum number of resources that any candidate is allowed to use
    for a given iteration. By default, this is set ``n_samples`` when
    ``resource='n_samples'`` (default), else an error is raised.

min_resources : {'exhaust', 'smallest'} or int, default='smallest'
    The minimum amount of resource that any candidate is allowed to use
    for a given iteration. Equivalently, this defines the amount of
    resources `r0` that are allocated for each candidate at the first
    iteration.

    - 'smallest' is a heuristic that sets `r0` to a small value:

        - ``n_splits * 2`` when ``resource='n_samples'`` for a regression
          problem
        - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a
          classification problem
        - ``1`` when ``resource != 'n_samples'``

    - 'exhaust' will set `r0` such that the **last** iteration uses as
      much resources as possible. Namely, the last iteration will use the
      highest value smaller than ``max_resources`` that is a multiple of
      both ``min_resources`` and ``factor``. In general, using 'exhaust'
      leads to a more accurate estimator, but is slightly more time
      consuming. 'exhaust' isn't available when `n_candidates='exhaust'`.

    Note that the amount of resources used at each iteration is always a
    multiple of ``min_resources``.

aggressive_elimination : bool, default=False
    This is only relevant in cases where there isn't enough resources to
    reduce the remaining candidates to at most `factor` after the last
    iteration. If ``True``, then the search process will 'replay' the
    first iteration for as long as needed until the number of candidates
    is small enough. This is ``False`` by default, which means that the
    last iteration may evaluate more than ``factor`` candidates. See
    :ref:`aggressive_elimination` for more details.

cv : int, cross-validation generator or an iterable, default=5
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - integer, to specify the number of folds in a `(Stratified)KFold`,
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if the estimator is a classifier and ``y`` is
    either binary or multiclass, :class:`StratifiedKFold` is used. In all
    other cases, :class:`KFold` is used. These splitters are instantiated
    with `shuffle=False` so the splits will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. note::
        Due to implementation details, the folds produced by `cv` must be
        the same across multiple calls to `cv.split()`. For
        built-in `scikit-learn` iterators, this can be achieved by
        deactivating shuffling (`shuffle=False`), or by setting the
        `cv`'s `random_state` parameter to an integer.

scoring : str, callable, or None, default=None
    A single string (see :ref:`scoring_parameter`) or a callable
    (see :ref:`scoring`) to evaluate the predictions on the test set.
    If None, the estimator's score method is used.

refit : bool, default=True
    If True, refit an estimator using the best found parameters on the
    whole dataset.

    The refitted estimator is made available at the ``best_estimator_``
    attribute and permits using ``predict`` directly on this
    ``HalvingRandomSearchCV`` instance.

error_score : 'raise' or numeric
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised. If a numeric value is given,
    FitFailedWarning is raised. This parameter does not affect the refit
    step, which will always raise the error. Default is ``np.nan``.

return_train_score : bool, default=False
    If ``False``, the ``cv_results_`` attribute will not include training
    scores.
    Computing training scores is used to get insights on how different
    parameter settings impact the overfitting/underfitting trade-off.
    However computing the scores on the training set can be computationally
    expensive and is not strictly required to select the parameters that
    yield the best generalization performance.

random_state : int, RandomState instance or None, default=None
    Pseudo random number generator state used for subsampling the dataset
    when `resources != 'n_samples'`. Also used for random uniform
    sampling from lists of possible values instead of scipy.stats
    distributions.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

n_jobs : int or None, default=None
    Number of jobs to run in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : int
    Controls the verbosity: the higher, the more messages.

Attributes
----------
n_resources_ : list of int
    The amount of resources used at each iteration.

n_candidates_ : list of int
    The number of candidate parameters that were evaluated at each
    iteration.

n_remaining_candidates_ : int
    The number of candidate parameters that are left after the last
    iteration. It corresponds to `ceil(n_candidates[-1] / factor)`

max_resources_ : int
    The maximum number of resources that any candidate is allowed to use
    for a given iteration. Note that since the number of resources used at
    each iteration must be a multiple of ``min_resources_``, the actual
    number of resources used at the last iteration may be smaller than
    ``max_resources_``.

min_resources_ : int
    The amount of resources that are allocated for each candidate at the
    first iteration.

n_iterations_ : int
    The actual number of iterations that were run. This is equal to
    ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.
    Else, this is equal to ``min(n_possible_iterations_,
    n_required_iterations_)``.

n_possible_iterations_ : int
    The number of iterations that are possible starting with
    ``min_resources_`` resources and without exceeding
    ``max_resources_``.

n_required_iterations_ : int
    The number of iterations that are required to end up with less than
    ``factor`` candidates at the last iteration, starting with
    ``min_resources_`` resources. This will be smaller than
    ``n_possible_iterations_`` when there isn't enough resources.

cv_results_ : dict of numpy (masked) ndarrays
    A dict with keys as column headers and values as columns, that can be
    imported into a pandas ``DataFrame``. It contains lots of information
    for analysing the results of a search.
    Please refer to the :ref:`User guide<successive_halving_cv_results>`
    for details.

best_estimator_ : estimator or dict
    Estimator that was chosen by the search, i.e. estimator
    which gave highest score (or smallest loss if specified)
    on the left out data. Not available if ``refit=False``.

best_score_ : float
    Mean cross-validated score of the best_estimator.

best_params_ : dict
    Parameter setting that gave the best results on the hold out data.

best_index_ : int
    The index (of the ``cv_results_`` arrays) which corresponds to the best
    candidate parameter setting.

    The dict at ``search.cv_results_['params'][search.best_index_]`` gives
    the parameter setting for the best model, that gives the highest
    mean score (``search.best_score_``).

scorer_ : function or a dict
    Scorer function used on the held out data to choose the best
    parameters for the model.

n_splits_ : int
    The number of cross-validation splits (folds/iterations).

refit_time_ : float
    Seconds used for refitting the best model on the whole dataset.

    This is present only if ``refit`` is not False.

multimetric_ : bool
    Whether or not the scorers compute several metrics.

classes_ : ndarray of shape (n_classes,)
    The classes labels. This is present only if ``refit`` is specified and
    the underlying estimator is a classifier.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `n_features_in_` when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `feature_names_in_` when fit.

    .. versionadded:: 1.0

See Also
--------
:class:`HalvingGridSearchCV`:
    Search over a grid of parameters using successive halving.

Notes
-----
The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.

All parameter combinations scored with a NaN will share the lowest rank.

Examples
--------

>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.experimental import enable_halving_search_cv  # noqa
>>> from sklearn.model_selection import HalvingRandomSearchCV
>>> from scipy.stats import randint
>>> import numpy as np
...
>>> X, y = load_iris(return_X_y=True)
>>> clf = RandomForestClassifier(random_state=0)
>>> np.random.seed(0)
...
>>> param_distributions = {\"max_depth\": [3, None],
...                        \"min_samples_split\": randint(2, 11)}
>>> search = HalvingRandomSearchCV(clf, param_distributions,
...                                resource='n_estimators',
...                                max_resources=10,
...                                random_state=0).fit(X, y)
>>> search.best_params_  # doctest: +SKIP
{'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HammingLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HammingLossMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                       rdfs:comment """Compute the average Hamming loss.

The Hamming loss is the fraction of labels that are incorrectly predicted.

Read more in the :ref:`User Guide <hamming_loss>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

Returns
-------
loss : float or int
    Return the average Hamming loss between element of ``y_true`` and
    ``y_pred``.

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.
jaccard_score : Compute the Jaccard similarity coefficient score.
zero_one_loss : Compute the Zero-one classification loss. By default, the
    function will return the percentage of imperfectly predicted subsets.

Notes
-----
In multiclass classification, the Hamming loss corresponds to the Hamming
distance between ``y_true`` and ``y_pred`` which is equivalent to the
subset ``zero_one_loss`` function, when `normalize` parameter is set to
True.

In multilabel classification, the Hamming loss is different from the
subset zero-one loss. The zero-one loss considers the entire set of labels
for a given sample incorrect if it does not entirely match the true set of
labels. Hamming loss is more forgiving in that it penalizes only the
individual labels.

The Hamming loss is upperbounded by the subset zero-one loss, when
`normalize` parameter is set to True. It is always between 0 and 1,
lower being better.

References
----------
.. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:
       An Overview. International Journal of Data Warehousing & Mining,
       3(3), 1-13, July-September 2007.

.. [2] `Wikipedia entry on the Hamming distance
       <https://en.wikipedia.org/wiki/Hamming_distance>`_.

Examples
--------
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HdbscanModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HdbscanModule> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                     rdfs:comment """Average hinge loss (non-regularized).

In binary class case, assuming labels in y_true are encoded with +1 and -1,
when a prediction mistake is made, ``margin = y_true * pred_decision`` is
always negative (since the signs disagree), implying ``1 - margin`` is
always greater than 1.  The cumulated hinge loss is therefore an upper
bound of the number of mistakes made by the classifier.

In multiclass case, the function expects that either all the labels are
included in y_true or an optional labels argument is provided which
contains all the labels. The multilabel margin is calculated according
to Crammer-Singer's method. As in the binary case, the cumulated hinge loss
is an upper bound of the number of mistakes made by the classifier.

Read more in the :ref:`User Guide <hinge_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True target, consisting of integers of two values. The positive label
    must be greater than the negative label.

pred_decision : array-like of shape (n_samples,) or (n_samples, n_classes)
    Predicted decisions, as output by decision_function (floats).

labels : array-like, default=None
    Contains all the labels for the problem. Used in multiclass hinge loss.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    Average hinge loss.

References
----------
.. [1] `Wikipedia entry on the Hinge loss
       <https://en.wikipedia.org/wiki/Hinge_loss>`_.

.. [2] Koby Crammer, Yoram Singer. On the Algorithmic
       Implementation of Multiclass Kernel-based Vector
       Machines. Journal of Machine Learning Research 2,
       (2001), 265-292.

.. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models
       by Robert C. Moore, John DeNero
       <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.

Examples
--------
>>> from sklearn import svm
>>> from sklearn.metrics import hinge_loss
>>> X = [[0], [1]]
>>> y = [-1, 1]
>>> est = svm.LinearSVC(dual=\"auto\", random_state=0)
>>> est.fit(X, y)
LinearSVC(dual='auto', random_state=0)
>>> pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision
array([-2.18...,  2.36...,  0.09...])
>>> hinge_loss([-1, 1, 1], pred_decision)
0.30...

In the multiclass case:

>>> import numpy as np
>>> X = np.array([[0], [1], [2], [3]])
>>> Y = np.array([0, 1, 2, 3])
>>> labels = np.array([0, 1, 2, 3])
>>> est = svm.LinearSVC(dual=\"auto\")
>>> est.fit(X, Y)
LinearSVC(dual='auto')
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels=labels)
0.56...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod> rdf:type owl:Class ;
                                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule> ,
                                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                          rdfs:comment """Histogram-based Gradient Boosting Classification Tree.

This estimator is much faster than
:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`
for big datasets (n_samples >= 10 000).

This estimator has native support for missing values (NaNs). During
training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are
assigned to the left or right child consequently. If no missing values
were encountered for a given feature during training, then samples with
missing values are mapped to whichever child has the most samples.

This implementation is inspired by
`LightGBM <https://github.com/Microsoft/LightGBM>`_.

Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.

.. versionadded:: 0.21

Parameters
----------
loss : {'log_loss'}, default='log_loss'
    The loss function to use in the boosting process.

    For binary classification problems, 'log_loss' is also known as logistic loss,
    binomial deviance or binary crossentropy. Internally, the model fits one tree
    per boosting iteration and uses the logistic sigmoid function (expit) as
    inverse link function to compute the predicted positive class probability.

    For multiclass classification problems, 'log_loss' is also known as multinomial
    deviance or categorical crossentropy. Internally, the model fits one tree per
    boosting iteration and per class and uses the softmax function as inverse link
    function to compute the predicted probabilities of the classes.

learning_rate : float, default=0.1
    The learning rate, also known as *shrinkage*. This is used as a
    multiplicative factor for the leaves values. Use ``1`` for no
    shrinkage.
max_iter : int, default=100
    The maximum number of iterations of the boosting process, i.e. the
    maximum number of trees for binary classification. For multiclass
    classification, `n_classes` trees per iteration are built.
max_leaf_nodes : int or None, default=31
    The maximum number of leaves for each tree. Must be strictly greater
    than 1. If None, there is no maximum limit.
max_depth : int or None, default=None
    The maximum depth of each tree. The depth of a tree is the number of
    edges to go from the root to the deepest leaf.
    Depth isn't constrained by default.
min_samples_leaf : int, default=20
    The minimum number of samples per leaf. For small datasets with less
    than a few hundred samples, it is recommended to lower this value
    since only very shallow trees would be built.
l2_regularization : float, default=0
    The L2 regularization parameter. Use ``0`` for no regularization (default).
max_features : float, default=1.0
    Proportion of randomly chosen features in each and every node split.
    This is a form of regularization, smaller values make the trees weaker
    learners and might prevent overfitting.
    If interaction constraints from `interaction_cst` are present, only allowed
    features are taken into account for the subsampling.

    .. versionadded:: 1.4

max_bins : int, default=255
    The maximum number of bins to use for non-missing values. Before
    training, each feature of the input array `X` is binned into
    integer-valued bins, which allows for a much faster training stage.
    Features with a small number of unique values may use less than
    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
    is always reserved for missing values. Must be no larger than 255.
categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None
    Indicates the categorical features.

    - None : no feature will be considered categorical.
    - boolean array-like : boolean mask indicating categorical features.
    - integer array-like : integer indices indicating categorical
      features.
    - str array-like: names of categorical features (assuming the training
      data has feature names).
    - `\"from_dtype\"`: dataframe columns with dtype \"category\" are
      considered to be categorical features. The input must be an object
      exposing a ``__dataframe__`` method such as pandas or polars
      DataFrames to use this feature.

    For each categorical feature, there must be at most `max_bins` unique
    categories. Negative values for categorical features encoded as numeric
    dtypes are treated as missing values. All categorical values are
    converted to floating point numbers. This means that categorical values
    of 1.0 and 1 are treated as the same category.

    Read more in the :ref:`User Guide <categorical_support_gbdt>`.

    .. versionadded:: 0.24

    .. versionchanged:: 1.2
       Added support for feature names.

    .. versionchanged:: 1.4
       Added `\"from_dtype\"` option. The default will change to `\"from_dtype\"` in
       v1.6.

monotonic_cst : array-like of int of shape (n_features) or dict, default=None
    Monotonic constraint to enforce on each feature are specified using the
    following integer values:

    - 1: monotonic increase
    - 0: no constraint
    - -1: monotonic decrease

    If a dict with str keys, map feature to monotonic constraints by name.
    If an array, the features are mapped to constraints by position. See
    :ref:`monotonic_cst_features_names` for a usage example.

    The constraints are only valid for binary classifications and hold
    over the probability of the positive class.
    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 0.23

    .. versionchanged:: 1.2
       Accept dict of constraints with feature names as keys.

interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None
    Specify interaction constraints, the sets of features which can
    interact with each other in child node splits.

    Each item specifies the set of feature indices that are allowed
    to interact with each other. If there are more features than
    specified in these constraints, they are treated as if they were
    specified as an additional set.

    The strings \"pairwise\" and \"no_interactions\" are shorthands for
    allowing only pairwise or no interactions, respectively.

    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`
    is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,
    and specifies that each branch of a tree will either only split
    on features 0 and 1 or only split on features 2, 3 and 4.

    .. versionadded:: 1.2

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble. For results to be valid, the
    estimator should be re-trained on the same data only.
    See :term:`the Glossary <warm_start>`.
early_stopping : 'auto' or bool, default='auto'
    If 'auto', early stopping is enabled if the sample size is larger than
    10000. If True, early stopping is enabled, otherwise early stopping is
    disabled.

    .. versionadded:: 0.23

scoring : str or callable or None, default='loss'
    Scoring parameter to use for early stopping. It can be a single
    string (see :ref:`scoring_parameter`) or a callable (see
    :ref:`scoring`). If None, the estimator's default scorer
    is used. If ``scoring='loss'``, early stopping is checked
    w.r.t the loss value. Only used if early stopping is performed.
validation_fraction : int or float or None, default=0.1
    Proportion (or absolute size) of training data to set aside as
    validation data for early stopping. If None, early stopping is done on
    the training data. Only used if early stopping is performed.
n_iter_no_change : int, default=10
    Used to determine when to \"early stop\". The fitting process is
    stopped when none of the last ``n_iter_no_change`` scores are better
    than the ``n_iter_no_change - 1`` -th-to-last one, up to some
    tolerance. Only used if early stopping is performed.
tol : float, default=1e-7
    The absolute tolerance to use when comparing scores. The higher the
    tolerance, the more likely we are to early stop: higher tolerance
    means that it will be harder for subsequent iterations to be
    considered an improvement upon the reference score.
verbose : int, default=0
    The verbosity level. If not zero, print some information about the
    fitting process.
random_state : int, RandomState instance or None, default=None
    Pseudo-random number generator to control the subsampling in the
    binning process, and the train/validation data split if early stopping
    is enabled.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.
class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form `{class_label: weight}`.
    If not given, all classes are supposed to have weight one.
    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as `n_samples / (n_classes * np.bincount(y))`.
    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if `sample_weight` is specified.

    .. versionadded:: 1.2

Attributes
----------
classes_ : array, shape = (n_classes,)
    Class labels.
do_early_stopping_ : bool
    Indicates whether early stopping is used during training.
n_iter_ : int
    The number of iterations as selected by early stopping, depending on
    the `early_stopping` parameter. Otherwise it corresponds to max_iter.
n_trees_per_iteration_ : int
    The number of tree that are built at each iteration. This is equal to 1
    for binary classification, and to ``n_classes`` for multiclass
    classification.
train_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the training data. The first entry
    is the score of the ensemble before the first iteration. Scores are
    computed according to the ``scoring`` parameter. If ``scoring`` is
    not 'loss', scores are computed on a subset of at most 10 000
    samples. Empty if no early stopping.
validation_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the held-out validation data. The
    first entry is the score of the ensemble before the first iteration.
    Scores are computed according to the ``scoring`` parameter. Empty if
    no early stopping or if ``validation_fraction`` is None.
is_categorical_ : ndarray, shape (n_features, ) or None
    Boolean mask for the categorical features. ``None`` if there are no
    categorical features.
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
GradientBoostingClassifier : Exact gradient boosting method that does not
    scale as good on datasets with a large number of samples.
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
RandomForestClassifier : A meta-estimator that fits a number of decision
    tree classifiers on various sub-samples of the dataset and uses
    averaging to improve the predictive accuracy and control over-fitting.
AdaBoostClassifier : A meta-estimator that begins by fitting a classifier
    on the original dataset and then fits additional copies of the
    classifier on the same dataset where the weights of incorrectly
    classified instances are adjusted such that subsequent classifiers
    focus more on difficult cases.

Examples
--------
>>> from sklearn.ensemble import HistGradientBoostingClassifier
>>> from sklearn.datasets import load_iris
>>> X, y = load_iris(return_X_y=True)
>>> clf = HistGradientBoostingClassifier().fit(X, y)
>>> clf.score(X, y)
1.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod> rdf:type owl:Class ;
                                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule> ,
                                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                         rdfs:comment """Histogram-based Gradient Boosting Regression Tree.

This estimator is much faster than
:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`
for big datasets (n_samples >= 10 000).

This estimator has native support for missing values (NaNs). During
training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are
assigned to the left or right child consequently. If no missing values
were encountered for a given feature during training, then samples with
missing values are mapped to whichever child has the most samples.

This implementation is inspired by
`LightGBM <https://github.com/Microsoft/LightGBM>`_.

Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.

.. versionadded:: 0.21

Parameters
----------
loss : {'squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile'},             default='squared_error'
    The loss function to use in the boosting process. Note that the
    \"squared error\", \"gamma\" and \"poisson\" losses actually implement
    \"half least squares loss\", \"half gamma deviance\" and \"half poisson
    deviance\" to simplify the computation of the gradient. Furthermore,
    \"gamma\" and \"poisson\" losses internally use a log-link, \"gamma\"
    requires ``y > 0`` and \"poisson\" requires ``y >= 0``.
    \"quantile\" uses the pinball loss.

    .. versionchanged:: 0.23
       Added option 'poisson'.

    .. versionchanged:: 1.1
       Added option 'quantile'.

    .. versionchanged:: 1.3
       Added option 'gamma'.

quantile : float, default=None
    If loss is \"quantile\", this parameter specifies which quantile to be estimated
    and must be between 0 and 1.
learning_rate : float, default=0.1
    The learning rate, also known as *shrinkage*. This is used as a
    multiplicative factor for the leaves values. Use ``1`` for no
    shrinkage.
max_iter : int, default=100
    The maximum number of iterations of the boosting process, i.e. the
    maximum number of trees.
max_leaf_nodes : int or None, default=31
    The maximum number of leaves for each tree. Must be strictly greater
    than 1. If None, there is no maximum limit.
max_depth : int or None, default=None
    The maximum depth of each tree. The depth of a tree is the number of
    edges to go from the root to the deepest leaf.
    Depth isn't constrained by default.
min_samples_leaf : int, default=20
    The minimum number of samples per leaf. For small datasets with less
    than a few hundred samples, it is recommended to lower this value
    since only very shallow trees would be built.
l2_regularization : float, default=0
    The L2 regularization parameter. Use ``0`` for no regularization (default).
max_features : float, default=1.0
    Proportion of randomly chosen features in each and every node split.
    This is a form of regularization, smaller values make the trees weaker
    learners and might prevent overfitting.
    If interaction constraints from `interaction_cst` are present, only allowed
    features are taken into account for the subsampling.

    .. versionadded:: 1.4

max_bins : int, default=255
    The maximum number of bins to use for non-missing values. Before
    training, each feature of the input array `X` is binned into
    integer-valued bins, which allows for a much faster training stage.
    Features with a small number of unique values may use less than
    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
    is always reserved for missing values. Must be no larger than 255.
categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None
    Indicates the categorical features.

    - None : no feature will be considered categorical.
    - boolean array-like : boolean mask indicating categorical features.
    - integer array-like : integer indices indicating categorical
      features.
    - str array-like: names of categorical features (assuming the training
      data has feature names).
    - `\"from_dtype\"`: dataframe columns with dtype \"category\" are
      considered to be categorical features. The input must be an object
      exposing a ``__dataframe__`` method such as pandas or polars
      DataFrames to use this feature.

    For each categorical feature, there must be at most `max_bins` unique
    categories. Negative values for categorical features encoded as numeric
    dtypes are treated as missing values. All categorical values are
    converted to floating point numbers. This means that categorical values
    of 1.0 and 1 are treated as the same category.

    Read more in the :ref:`User Guide <categorical_support_gbdt>`.

    .. versionadded:: 0.24

    .. versionchanged:: 1.2
       Added support for feature names.

    .. versionchanged:: 1.4
       Added `\"from_dtype\"` option. The default will change to `\"from_dtype\"` in
       v1.6.

monotonic_cst : array-like of int of shape (n_features) or dict, default=None
    Monotonic constraint to enforce on each feature are specified using the
    following integer values:

    - 1: monotonic increase
    - 0: no constraint
    - -1: monotonic decrease

    If a dict with str keys, map feature to monotonic constraints by name.
    If an array, the features are mapped to constraints by position. See
    :ref:`monotonic_cst_features_names` for a usage example.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 0.23

    .. versionchanged:: 1.2
       Accept dict of constraints with feature names as keys.

interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None
    Specify interaction constraints, the sets of features which can
    interact with each other in child node splits.

    Each item specifies the set of feature indices that are allowed
    to interact with each other. If there are more features than
    specified in these constraints, they are treated as if they were
    specified as an additional set.

    The strings \"pairwise\" and \"no_interactions\" are shorthands for
    allowing only pairwise or no interactions, respectively.

    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`
    is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,
    and specifies that each branch of a tree will either only split
    on features 0 and 1 or only split on features 2, 3 and 4.

    .. versionadded:: 1.2

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble. For results to be valid, the
    estimator should be re-trained on the same data only.
    See :term:`the Glossary <warm_start>`.
early_stopping : 'auto' or bool, default='auto'
    If 'auto', early stopping is enabled if the sample size is larger than
    10000. If True, early stopping is enabled, otherwise early stopping is
    disabled.

    .. versionadded:: 0.23

scoring : str or callable or None, default='loss'
    Scoring parameter to use for early stopping. It can be a single
    string (see :ref:`scoring_parameter`) or a callable (see
    :ref:`scoring`). If None, the estimator's default scorer is used. If
    ``scoring='loss'``, early stopping is checked w.r.t the loss value.
    Only used if early stopping is performed.
validation_fraction : int or float or None, default=0.1
    Proportion (or absolute size) of training data to set aside as
    validation data for early stopping. If None, early stopping is done on
    the training data. Only used if early stopping is performed.
n_iter_no_change : int, default=10
    Used to determine when to \"early stop\". The fitting process is
    stopped when none of the last ``n_iter_no_change`` scores are better
    than the ``n_iter_no_change - 1`` -th-to-last one, up to some
    tolerance. Only used if early stopping is performed.
tol : float, default=1e-7
    The absolute tolerance to use when comparing scores during early
    stopping. The higher the tolerance, the more likely we are to early
    stop: higher tolerance means that it will be harder for subsequent
    iterations to be considered an improvement upon the reference score.
verbose : int, default=0
    The verbosity level. If not zero, print some information about the
    fitting process.
random_state : int, RandomState instance or None, default=None
    Pseudo-random number generator to control the subsampling in the
    binning process, and the train/validation data split if early stopping
    is enabled.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
do_early_stopping_ : bool
    Indicates whether early stopping is used during training.
n_iter_ : int
    The number of iterations as selected by early stopping, depending on
    the `early_stopping` parameter. Otherwise it corresponds to max_iter.
n_trees_per_iteration_ : int
    The number of tree that are built at each iteration. For regressors,
    this is always 1.
train_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the training data. The first entry
    is the score of the ensemble before the first iteration. Scores are
    computed according to the ``scoring`` parameter. If ``scoring`` is
    not 'loss', scores are computed on a subset of at most 10 000
    samples. Empty if no early stopping.
validation_score_ : ndarray, shape (n_iter_+1,)
    The scores at each iteration on the held-out validation data. The
    first entry is the score of the ensemble before the first iteration.
    Scores are computed according to the ``scoring`` parameter. Empty if
    no early stopping or if ``validation_fraction`` is None.
is_categorical_ : ndarray, shape (n_features, ) or None
    Boolean mask for the categorical features. ``None`` if there are no
    categorical features.
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
GradientBoostingRegressor : Exact gradient boosting method that does not
    scale as good on datasets with a large number of samples.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
RandomForestRegressor : A meta-estimator that fits a number of decision
    tree regressors on various sub-samples of the dataset and uses
    averaging to improve the statistical performance and control
    over-fitting.
AdaBoostRegressor : A meta-estimator that begins by fitting a regressor
    on the original dataset and then fits additional copies of the
    regressor on the same dataset but where the weights of instances are
    adjusted according to the error of the current prediction. As such,
    subsequent regressors focus more on difficult cases.

Examples
--------
>>> from sklearn.ensemble import HistGradientBoostingRegressor
>>> from sklearn.datasets import load_diabetes
>>> X, y = load_diabetes(return_X_y=True)
>>> est = HistGradientBoostingRegressor().fit(X, y)
>>> est.score(X, y)
0.92...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod> rdf:type owl:Class ;
                                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                           rdfs:comment """Compute the homogeneity and completeness and V-Measure scores at once.

Those metrics are based on normalized conditional entropy measures of
the clustering labeling to evaluate given the knowledge of a Ground
Truth class labels of the same samples.

A clustering result satisfies homogeneity if all of its clusters
contain only data points which are members of a single class.

A clustering result satisfies completeness if all the data points
that are members of a given class are elements of the same cluster.

Both scores have positive values between 0.0 and 1.0, larger values
being desirable.

Those 3 metrics are independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score values in any way.

V-Measure is furthermore symmetric: swapping ``labels_true`` and
``label_pred`` will give the same score. This does not hold for
homogeneity and completeness. V-Measure is identical to
:func:`normalized_mutual_info_score` with the arithmetic averaging
method.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Gluster labels to evaluate.

beta : float, default=1.0
    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
    If ``beta`` is greater than 1, ``completeness`` is weighted more
    strongly in the calculation. If ``beta`` is less than 1,
    ``homogeneity`` is weighted more strongly.

Returns
-------
homogeneity : float
    Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.

completeness : float
    Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.

v_measure : float
    Harmonic mean of the first two.

See Also
--------
homogeneity_score : Homogeneity metric of cluster labeling.
completeness_score : Completeness metric of cluster labeling.
v_measure_score : V-Measure (NMI with arithmetic mean option).

Examples
--------
>>> from sklearn.metrics import homogeneity_completeness_v_measure
>>> y_true, y_pred = [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 2, 2]
>>> homogeneity_completeness_v_measure(y_true, y_pred)
(0.71..., 0.77..., 0.73...)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityScoreMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                            rdfs:comment """Homogeneity metric of a cluster labeling given a ground truth.

A clustering result satisfies homogeneity if all of its clusters
contain only data points which are members of a single class.

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is not symmetric: switching ``label_true`` with ``label_pred``
will return the :func:`completeness_score` which will be different in
general.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Cluster labels to evaluate.

Returns
-------
homogeneity : float
   Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.

See Also
--------
completeness_score : Completeness metric of cluster labeling.
v_measure_score : V-Measure (NMI with arithmetic mean option).

References
----------

.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
   conditional entropy-based external cluster evaluation measure
   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_

Examples
--------

Perfect labelings are homogeneous::

  >>> from sklearn.metrics.cluster import homogeneity_score
  >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Non-perfect labelings that further split classes into more clusters can be
perfectly homogeneous::

  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))
  1.000000
  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))
  1.000000

Clusters that include samples from different classes do not make for an
homogeneous labeling::

  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))
  0.0...
  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))
  0.0...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                          rdfs:comment """L2-regularized linear regression model that is robust to outliers.

The Huber Regressor optimizes the squared loss for the samples where
``|(y - Xw - c) / sigma| < epsilon`` and the absolute loss for the samples
where ``|(y - Xw - c) / sigma| > epsilon``, where the model coefficients
``w``, the intercept ``c`` and the scale ``sigma`` are parameters
to be optimized. The parameter sigma makes sure that if y is scaled up
or down by a certain factor, one does not need to rescale epsilon to
achieve the same robustness. Note that this does not take into account
the fact that the different features of X may be of different scales.

The Huber loss function has the advantage of not being heavily influenced
by the outliers while not completely ignoring their effect.

Read more in the :ref:`User Guide <huber_regression>`

.. versionadded:: 0.18

Parameters
----------
epsilon : float, default=1.35
    The parameter epsilon controls the number of samples that should be
    classified as outliers. The smaller the epsilon, the more robust it is
    to outliers. Epsilon must be in the range `[1, inf)`.

max_iter : int, default=100
    Maximum number of iterations that
    ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.

alpha : float, default=0.0001
    Strength of the squared L2 regularization. Note that the penalty is
    equal to ``alpha * ||w||^2``.
    Must be in the range `[0, inf)`.

warm_start : bool, default=False
    This is useful if the stored attributes of a previously used model
    has to be reused. If set to False, then the coefficients will
    be rewritten for every call to fit.
    See :term:`the Glossary <warm_start>`.

fit_intercept : bool, default=True
    Whether or not to fit the intercept. This can be set to False
    if the data is already centered around the origin.

tol : float, default=1e-05
    The iteration will stop when
    ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``
    where pg_i is the i-th component of the projected gradient.

Attributes
----------
coef_ : array, shape (n_features,)
    Features got by optimizing the L2-regularized Huber loss.

intercept_ : float
    Bias.

scale_ : float
    The value by which ``|y - Xw - c|`` is scaled down.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations that
    ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.

    .. versionchanged:: 0.20

        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.

outliers_ : array, shape (n_samples,)
    A boolean mask which is set to True where the samples are identified
    as outliers.

See Also
--------
RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.
TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.
SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.

References
----------
.. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics
       Concomitant scale estimates, pg 172
.. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.
       https://statweb.stanford.edu/~owen/reports/hhu.pdf

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import HuberRegressor, LinearRegression
>>> from sklearn.datasets import make_regression
>>> rng = np.random.RandomState(0)
>>> X, y, coef = make_regression(
...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)
>>> X[:4] = rng.uniform(10, 20, (4, 2))
>>> y[:4] = rng.uniform(10, 20, 4)
>>> huber = HuberRegressor().fit(X, y)
>>> huber.score(X, y)
-7.284...
>>> huber.predict(X[:1,])
array([806.7200...])
>>> linear = LinearRegression().fit(X, y)
>>> print(\"True coefficients:\", coef)
True coefficients: [20.4923...  34.1698...]
>>> print(\"Huber coefficients:\", huber.coef_)
Huber coefficients: [17.7906... 31.0106...]
>>> print(\"Linear Regression coefficients:\", linear.coef_)
Linear Regression coefficients: [-1.9221...  7.0226...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule> rdf:type owl:Class ;
                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                          rdfs:comment """Incremental principal components analysis (IPCA).

Linear dimensionality reduction using Singular Value Decomposition of
the data, keeping only the most significant singular vectors to
project the data to a lower dimensional space. The input data is centered
but not scaled for each feature before applying the SVD.

Depending on the size of the input data, this algorithm can be much more
memory efficient than a PCA, and allows sparse input.

This algorithm has constant memory complexity, on the order
of ``batch_size * n_features``, enabling use of np.memmap files without
loading the entire file into memory. For sparse matrices, the input
is converted to dense in batches (in order to be able to subtract the
mean) which avoids storing the entire dense matrix at any one time.

The computational overhead of each SVD is
``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples
remain in memory at a time. There will be ``n_samples / batch_size`` SVD
computations to get the principal components, versus 1 large SVD of
complexity ``O(n_samples * n_features ** 2)`` for PCA.

For a usage example, see
:ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`.

Read more in the :ref:`User Guide <IncrementalPCA>`.

.. versionadded:: 0.16

Parameters
----------
n_components : int, default=None
    Number of components to keep. If ``n_components`` is ``None``,
    then ``n_components`` is set to ``min(n_samples, n_features)``.

whiten : bool, default=False
    When True (False by default) the ``components_`` vectors are divided
    by ``n_samples`` times ``components_`` to ensure uncorrelated outputs
    with unit component-wise variances.

    Whitening will remove some information from the transformed signal
    (the relative variance scales of the components) but can sometimes
    improve the predictive accuracy of the downstream estimators by
    making data respect some hard-wired assumptions.

copy : bool, default=True
    If False, X will be overwritten. ``copy=False`` can be used to
    save memory but is unsafe for general use.

batch_size : int, default=None
    The number of samples to use for each batch. Only used when calling
    ``fit``. If ``batch_size`` is ``None``, then ``batch_size``
    is inferred from the data and set to ``5 * n_features``, to provide a
    balance between approximation accuracy and memory consumption.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Principal axes in feature space, representing the directions of
    maximum variance in the data. Equivalently, the right singular
    vectors of the centered input data, parallel to its eigenvectors.
    The components are sorted by decreasing ``explained_variance_``.

explained_variance_ : ndarray of shape (n_components,)
    Variance explained by each of the selected components.

explained_variance_ratio_ : ndarray of shape (n_components,)
    Percentage of variance explained by each of the selected components.
    If all components are stored, the sum of explained variances is equal
    to 1.0.

singular_values_ : ndarray of shape (n_components,)
    The singular values corresponding to each of the selected components.
    The singular values are equal to the 2-norms of the ``n_components``
    variables in the lower-dimensional space.

mean_ : ndarray of shape (n_features,)
    Per-feature empirical mean, aggregate over calls to ``partial_fit``.

var_ : ndarray of shape (n_features,)
    Per-feature empirical variance, aggregate over calls to
    ``partial_fit``.

noise_variance_ : float
    The estimated noise covariance following the Probabilistic PCA model
    from Tipping and Bishop 1999. See \"Pattern Recognition and
    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or
    http://www.miketipping.com/papers/met-mppca.pdf.

n_components_ : int
    The estimated number of components. Relevant when
    ``n_components=None``.

n_samples_seen_ : int
    The number of samples processed by the estimator. Will be reset on
    new calls to fit, but increments across ``partial_fit`` calls.

batch_size_ : int
    Inferred batch size from ``batch_size``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PCA : Principal component analysis (PCA).
KernelPCA : Kernel Principal component analysis (KPCA).
SparsePCA : Sparse Principal Components Analysis (SparsePCA).
TruncatedSVD : Dimensionality reduction using truncated SVD.

Notes
-----
Implements the incremental PCA model from:
*D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual
Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,
pp. 125-141, May 2008.*
See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf

This model is an extension of the Sequential Karhunen-Loeve Transform from:
:doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and
its Application to Images, IEEE Transactions on Image Processing, Volume 9,
Number 8, pp. 1371-1374, August 2000. <10.1109/83.855432>`

We have specifically abstained from an optimization used by authors of both
papers, a QR decomposition used in specific situations to reduce the
algorithmic complexity of the SVD. The source for this technique is
*Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,
section 5.4.4, pp 252-253.*. This technique has been omitted because it is
advantageous only when decomposing a matrix with ``n_samples`` (rows)
>= 5/3 * ``n_features`` (columns), and hurts the readability of the
implemented algorithm. This would be a good opportunity for future
optimization, if it is deemed necessary.

References
----------
D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual
Tracking, International Journal of Computer Vision, Volume 77,
Issue 1-3, pp. 125-141, May 2008.

G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,
Section 5.4.4, pp. 252-253.

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.decomposition import IncrementalPCA
>>> from scipy import sparse
>>> X, _ = load_digits(return_X_y=True)
>>> transformer = IncrementalPCA(n_components=7, batch_size=200)
>>> # either partially fit on smaller batches of data
>>> transformer.partial_fit(X[:100, :])
IncrementalPCA(batch_size=200, n_components=7)
>>> # or let the fit function itself divide the data into batches
>>> X_sparse = sparse.csr_matrix(X)
>>> X_transformed = transformer.fit_transform(X_sparse)
>>> X_transformed.shape
(1797, 7)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#InstanceBasedRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#InstanceBasedRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """Isolation Forest Algorithm.

Return the anomaly score of each sample using the IsolationForest algorithm

The IsolationForest 'isolates' observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.

Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.

This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.

Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.

Read more in the :ref:`User Guide <isolation_forest>`.

.. versionadded:: 0.18

Parameters
----------
n_estimators : int, default=100
    The number of base estimators in the ensemble.

max_samples : \"auto\", int or float, default=\"auto\"
    The number of samples to draw from X to train each base estimator.
        - If int, then draw `max_samples` samples.
        - If float, then draw `max_samples * X.shape[0]` samples.
        - If \"auto\", then `max_samples=min(256, n_samples)`.

    If max_samples is larger than the number of samples provided,
    all samples will be used for all trees (no sampling).

contamination : 'auto' or float, default='auto'
    The amount of contamination of the data set, i.e. the proportion
    of outliers in the data set. Used when fitting to define the threshold
    on the scores of the samples.

        - If 'auto', the threshold is determined as in the
          original paper.
        - If float, the contamination should be in the range (0, 0.5].

    .. versionchanged:: 0.22
       The default value of ``contamination`` changed from 0.1
       to ``'auto'``.

max_features : int or float, default=1.0
    The number of features to draw from X to train each base estimator.

        - If int, then draw `max_features` features.
        - If float, then draw `max(1, int(max_features * n_features_in_))` features.

    Note: using a float number less than 1.0 or integer less than number of
    features will enable feature subsampling and leads to a longer runtime.

bootstrap : bool, default=False
    If True, individual trees are fit on random subsets of the training
    data sampled with replacement. If False, sampling without replacement
    is performed.

n_jobs : int, default=None
    The number of jobs to run in parallel for both :meth:`fit` and
    :meth:`predict`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo-randomness of the selection of the feature
    and split values for each branching step and each tree in the forest.

    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : int, default=0
    Controls the verbosity of the tree building process.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`the Glossary <warm_start>`.

    .. versionadded:: 0.21

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
    The child estimator template used to create the collection of
    fitted sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of ExtraTreeRegressor instances
    The collection of fitted sub-estimators.

estimators_features_ : list of ndarray
    The subset of drawn features for each base estimator.

estimators_samples_ : list of ndarray
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator.

max_samples_ : int
    The actual number of samples.

offset_ : float
    Offset used to define the decision function from the raw scores. We
    have the relation: ``decision_function = score_samples - offset_``.
    ``offset_`` is defined as follows. When the contamination parameter is
    set to \"auto\", the offset is equal to -0.5 as the scores of inliers are
    close to 0 and the scores of outliers are close to -1. When a
    contamination parameter different than \"auto\" is provided, the offset
    is defined in such a way we obtain the expected number of outliers
    (samples with decision function < 0) in training.

    .. versionadded:: 0.20

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a
    Gaussian distributed dataset.
sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.
    Estimate the support of a high-dimensional distribution.
    The implementation is based on libsvm.
sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection
    using Local Outlier Factor (LOF).

Notes
-----
The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to ``ceil(log_2(n))`` where
:math:`n` is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).

References
----------
.. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"
       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.
.. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based
       anomaly detection.\" ACM Transactions on Knowledge Discovery from
       Data (TKDD) 6.1 (2012): 3.

Examples
--------
>>> from sklearn.ensemble import IsolationForest
>>> X = [[-1.1], [0.3], [0.5], [100]]
>>> clf = IsolationForest(random_state=0).fit(X)
>>> clf.predict([[0.1], [0], [90]])
array([ 1,  1, -1])

For an example of using isolation forest for anomaly detection see
:ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py`.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                        rdfs:comment """Jaccard similarity coefficient score.

The Jaccard index [1], or Jaccard similarity coefficient, defined as
the size of the intersection divided by the size of the union of two label
sets, is used to compare set of predicted labels for a sample to the
corresponding set of labels in ``y_true``.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return the
Jaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`,
`pos_label` is ignored and scores for both classes are computed, then averaged or
both returned (when `average=None`). Similarly, for :term:`multiclass` and
:term:`multilabel` targets, scores for all `labels` are either returned or
averaged depending on the `average` parameter. Use `labels` specify the set of
labels to calculate the score for.

Read more in the :ref:`User Guide <jaccard_similarity_score>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

labels : array-like of shape (n_classes,), default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a \"negative
    class\". Labels not present in the data can be included and will be
    \"assigned\" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"
    Sets the value to return when there is a zero division, i.e. when there
    there are no negative values in predictions and labels. If set to
    \"warn\", this acts like 0, but a warning is also raised.

Returns
-------
score : float or ndarray of shape (n_unique_labels,), dtype=np.float64
    The Jaccard score. When `average` is not `None`, a single scalar is
    returned.

See Also
--------
accuracy_score : Function for calculating the accuracy score.
f1_score : Function for calculating the F1 score.
multilabel_confusion_matrix : Function for computing a confusion matrix                                  for each class or sample.

Notes
-----
:func:`jaccard_score` may be a poor metric if there are no
positives for some samples or classes. Jaccard is undefined if there are
no true or predicted labels, and our implementation will return a score
of 0 with a warning.

References
----------
.. [1] `Wikipedia entry for the Jaccard index
       <https://en.wikipedia.org/wiki/Jaccard_index>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import jaccard_score
>>> y_true = np.array([[0, 1, 1],
...                    [1, 1, 0]])
>>> y_pred = np.array([[1, 1, 1],
...                    [1, 0, 0]])

In the binary case:

>>> jaccard_score(y_true[0], y_pred[0])
0.6666...

In the 2D comparison case (e.g. image similarity):

>>> jaccard_score(y_true, y_pred, average=\"micro\")
0.6

In the multilabel case:

>>> jaccard_score(y_true, y_pred, average='samples')
0.5833...
>>> jaccard_score(y_true, y_pred, average='macro')
0.6666...
>>> jaccard_score(y_true, y_pred, average=None)
array([0.5, 0.5, 1. ])

In the multiclass case:

>>> y_pred = [0, 2, 1, 2]
>>> y_true = [0, 1, 2, 2]
>>> jaccard_score(y_true, y_pred, average=None)
array([1. , 0. , 0.33...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                            rdfs:comment """Bin continuous data into intervals.

Read more in the :ref:`User Guide <preprocessing_discretization>`.

.. versionadded:: 0.20

Parameters
----------
n_bins : int or array-like of shape (n_features,), default=5
    The number of bins to produce. Raises ValueError if ``n_bins < 2``.

encode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'
    Method used to encode the transformed result.

    - 'onehot': Encode the transformed result with one-hot encoding
      and return a sparse matrix. Ignored features are always
      stacked to the right.
    - 'onehot-dense': Encode the transformed result with one-hot encoding
      and return a dense array. Ignored features are always
      stacked to the right.
    - 'ordinal': Return the bin identifier encoded as an integer value.

strategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'
    Strategy used to define the widths of the bins.

    - 'uniform': All bins in each feature have identical widths.
    - 'quantile': All bins in each feature have the same number of points.
    - 'kmeans': Values in each bin have the same nearest center of a 1D
      k-means cluster.

    For an example of the different strategies see:
    :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`.

dtype : {np.float32, np.float64}, default=None
    The desired data-type for the output. If None, output dtype is
    consistent with input dtype. Only np.float32 and np.float64 are
    supported.

    .. versionadded:: 0.24

subsample : int or None, default='warn'
    Maximum number of samples, used to fit the model, for computational
    efficiency. Defaults to 200_000 when `strategy='quantile'` and to `None`
    when `strategy='uniform'` or `strategy='kmeans'`.
    `subsample=None` means that all the training samples are used when
    computing the quantiles that determine the binning thresholds.
    Since quantile computation relies on sorting each column of `X` and
    that sorting has an `n log(n)` time complexity,
    it is recommended to use subsampling on datasets with a
    very large number of samples.

    .. versionchanged:: 1.3
        The default value of `subsample` changed from `None` to `200_000` when
        `strategy=\"quantile\"`.

    .. versionchanged:: 1.5
        The default value of `subsample` changed from `None` to `200_000` when
        `strategy=\"uniform\"` or `strategy=\"kmeans\"`.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for subsampling.
    Pass an int for reproducible results across multiple function calls.
    See the `subsample` parameter for more details.
    See :term:`Glossary <random_state>`.

    .. versionadded:: 1.1

Attributes
----------
bin_edges_ : ndarray of ndarray of shape (n_features,)
    The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
    Ignored features will have empty arrays.

n_bins_ : ndarray of shape (n_features,), dtype=np.int64
    Number of bins per feature. Bins whose width are too small
    (i.e., <= 1e-8) are removed with a warning.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Binarizer : Class used to bin values as ``0`` or
    ``1`` based on a parameter ``threshold``.

Notes
-----

For a visualization of discretization on different datasets refer to
:ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`.
On the effect of discretization on linear models see:
:ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`.

In bin edges for feature ``i``, the first and last values are used only for
``inverse_transform``. During transform, bin edges are extended to::

  np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])

You can combine ``KBinsDiscretizer`` with
:class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess
part of the features.

``KBinsDiscretizer`` might produce constant features (e.g., when
``encode = 'onehot'`` and certain bins do not contain any data).
These features can be removed with feature selection algorithms
(e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).

Examples
--------
>>> from sklearn.preprocessing import KBinsDiscretizer
>>> X = [[-2, 1, -4,   -1],
...      [-1, 2, -3, -0.5],
...      [ 0, 3, -2,  0.5],
...      [ 1, 4, -1,    2]]
>>> est = KBinsDiscretizer(
...     n_bins=3, encode='ordinal', strategy='uniform', subsample=None
... )
>>> est.fit(X)
KBinsDiscretizer(...)
>>> Xt = est.transform(X)
>>> Xt  # doctest: +SKIP
array([[ 0., 0., 0., 0.],
       [ 1., 1., 1., 0.],
       [ 2., 2., 2., 1.],
       [ 2., 2., 2., 2.]])

Sometimes it may be useful to convert the data back into the original
feature space. The ``inverse_transform`` function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.

>>> est.bin_edges_[0]
array([-2., -1.,  0.,  1.])
>>> est.inverse_transform(Xt)
array([[-1.5,  1.5, -3.5, -0.5],
       [-0.5,  2.5, -2.5, -0.5],
       [ 0.5,  3.5, -1.5,  0.5],
       [ 0.5,  3.5, -1.5,  1.5]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KDTreeMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KDTreeMethod> rdf:type owl:Class ;
                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                  rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                 rdfs:comment """K-Fold cross-validator.

Provides train/test indices to split data in train/test sets. Split
dataset into k consecutive folds (without shuffling by default).

Each fold is then used once as a validation while the k - 1 remaining
folds form the training set.

Read more in the :ref:`User Guide <k_fold>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=5
    Number of folds. Must be at least 2.

    .. versionchanged:: 0.22
        ``n_splits`` default value changed from 3 to 5.

shuffle : bool, default=False
    Whether to shuffle the data before splitting into batches.
    Note that the samples within each split will not be shuffled.

random_state : int, RandomState instance or None, default=None
    When `shuffle` is True, `random_state` affects the ordering of the
    indices, which controls the randomness of each fold. Otherwise, this
    parameter has no effect.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import KFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4])
>>> kf = KFold(n_splits=2)
>>> kf.get_n_splits(X)
2
>>> print(kf)
KFold(n_splits=2, random_state=None, shuffle=False)
>>> for i, (train_index, test_index) in enumerate(kf.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[2 3]
  Test:  index=[0 1]
Fold 1:
  Train: index=[0 1]
  Test:  index=[2 3]

Notes
-----
The first ``n_samples % n_splits`` folds have size
``n_samples // n_splits + 1``, other folds have size
``n_samples // n_splits``, where ``n_samples`` is the number of samples.

Randomized CV splitters may return different results for each call of
split. You can make the results identical by setting `random_state`
to an integer.

See Also
--------
StratifiedKFold : Takes class information into account to avoid building
    folds with imbalanced class distributions (for binary or multiclass
    classification tasks).

GroupKFold : K-fold iterator variant with non-overlapping groups.

RepeatedKFold : Repeats K-Fold n times.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod> rdf:type owl:Class ;
                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                  rdfs:comment """K-Means clustering.

Read more in the :ref:`User Guide <k_means>`.

Parameters
----------

n_clusters : int, default=8
    The number of clusters to form as well as the number of
    centroids to generate.

    For an example of how to choose an optimal value for `n_clusters` refer to
    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.

init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'
    Method for initialization:

    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.

    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.

    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.

    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.

    For an example of how to use the different `init` strategy, see the example
    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.

n_init : 'auto' or int, default='auto'
    Number of times the k-means algorithm is run with different centroid
    seeds. The final results is the best output of `n_init` consecutive runs
    in terms of inertia. Several runs are recommended for sparse
    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).

    When `n_init='auto'`, the number of runs depends on the value of init:
    10 if using `init='random'` or `init` is a callable;
    1 if using `init='k-means++'` or `init` is an array-like.

    .. versionadded:: 1.2
       Added 'auto' option for `n_init`.

    .. versionchanged:: 1.4
       Default value for `n_init` changed to `'auto'`.

max_iter : int, default=300
    Maximum number of iterations of the k-means algorithm for a
    single run.

tol : float, default=1e-4
    Relative tolerance with regards to Frobenius norm of the difference
    in the cluster centers of two consecutive iterations to declare
    convergence.

verbose : int, default=0
    Verbosity mode.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization. Use
    an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

copy_x : bool, default=True
    When pre-computing distances it is more numerically accurate to center
    the data first. If copy_x is True (default), then the original data is
    not modified. If False, the original data is modified, and put back
    before the function returns, but small numerical differences may be
    introduced by subtracting and then adding the data mean. Note that if
    the original data is not C-contiguous, a copy will be made even if
    copy_x is False. If the original data is sparse, but not in CSR format,
    a copy will be made even if copy_x is False.

algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"
    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.
    The `\"elkan\"` variation can be more efficient on some datasets with
    well-defined clusters, by using the triangle inequality. However it's
    more memory intensive due to the allocation of an extra array of shape
    `(n_samples, n_clusters)`.

    .. versionchanged:: 0.18
        Added Elkan algorithm

    .. versionchanged:: 1.1
        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".
        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".

Attributes
----------
cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers. If the algorithm stops before fully
    converging (see ``tol`` and ``max_iter``), these will not be
    consistent with ``labels_``.

labels_ : ndarray of shape (n_samples,)
    Labels of each point

inertia_ : float
    Sum of squared distances of samples to their closest cluster center,
    weighted by the sample weights if provided.

n_iter_ : int
    Number of iterations run.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MiniBatchKMeans : Alternative online implementation that does incremental
    updates of the centers positions using mini-batches.
    For large scale learning (say n_samples > 10k) MiniBatchKMeans is
    probably much faster than the default batch implementation.

Notes
-----
The k-means problem is solved using either Lloyd's or Elkan's algorithm.

The average complexity is given by O(k n T), where n is the number of
samples and T is the number of iteration.

The worst case complexity is given by O(n^(k+2/p)) with
n = n_samples, p = n_features.
Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -
SoCG2006.<10.1145/1137856.1137880>` for more details.

In practice, the k-means algorithm is very fast (one of the fastest
clustering algorithms available), but it falls in local minima. That's why
it can be useful to restart it several times.

If the algorithm stops before fully converging (because of ``tol`` or
``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,
i.e. the ``cluster_centers_`` will not be the means of the points in each
cluster. Also, the estimator will reassign ``labels_`` after the last
iteration to make ``labels_`` consistent with ``predict`` on the training
set.

Examples
--------

>>> from sklearn.cluster import KMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [10, 2], [10, 4], [10, 0]])
>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)
>>> kmeans.labels_
array([1, 1, 1, 0, 0, 0], dtype=int32)
>>> kmeans.predict([[0, 0], [12, 3]])
array([1, 0], dtype=int32)
>>> kmeans.cluster_centers_
array([[10.,  2.],
       [ 1.,  2.]])

For a more detailed example of K-Means using the iris dataset see
:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.

For examples of common problems with K-Means and how to address them see
:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.

For an example of how to use K-Means to perform color quantization see
:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.

For a demonstration of how K-Means can be used to cluster text documents see
:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.

For a comparison between K-Means and MiniBatchKMeans refer to example
:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                      rdfs:comment """Imputation for completing missing values using k-Nearest Neighbors.

Each sample's missing values are imputed using the mean value from
`n_neighbors` nearest neighbors found in the training set. Two samples are
close if the features that neither is missing are close.

Read more in the :ref:`User Guide <knnimpute>`.

.. versionadded:: 0.22

Parameters
----------
missing_values : int, float, str, np.nan or None, default=np.nan
    The placeholder for the missing values. All occurrences of
    `missing_values` will be imputed. For pandas' dataframes with
    nullable integer dtypes with missing values, `missing_values`
    should be set to np.nan, since `pd.NA` will be converted to np.nan.

n_neighbors : int, default=5
    Number of neighboring samples to use for imputation.

weights : {'uniform', 'distance'} or callable, default='uniform'
    Weight function used in prediction.  Possible values:

    - 'uniform' : uniform weights. All points in each neighborhood are
      weighted equally.
    - 'distance' : weight points by the inverse of their distance.
      in this case, closer neighbors of a query point will have a
      greater influence than neighbors which are further away.
    - callable : a user-defined function which accepts an
      array of distances, and returns an array of the same shape
      containing the weights.

metric : {'nan_euclidean'} or callable, default='nan_euclidean'
    Distance metric for searching neighbors. Possible values:

    - 'nan_euclidean'
    - callable : a user-defined function which conforms to the definition
      of ``_pairwise_callable(X, Y, metric, **kwds)``. The function
      accepts two arrays, X and Y, and a `missing_values` keyword in
      `kwds` and returns a scalar distance value.

copy : bool, default=True
    If True, a copy of X will be created. If False, imputation will
    be done in-place whenever possible.

add_indicator : bool, default=False
    If True, a :class:`MissingIndicator` transform will stack onto the
    output of the imputer's transform. This allows a predictive estimator
    to account for missingness despite imputation. If a feature has no
    missing values at fit/train time, the feature won't appear on the
    missing indicator even if there are missing values at transform/test
    time.

keep_empty_features : bool, default=False
    If True, features that consist exclusively of missing values when
    `fit` is called are returned in results when `transform` is called.
    The imputed value is always `0`.

    .. versionadded:: 1.2

Attributes
----------
indicator_ : :class:`~sklearn.impute.MissingIndicator`
    Indicator used to add binary indicators for missing values.
    ``None`` if add_indicator is False.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SimpleImputer : Univariate imputer for completing missing values
    with simple strategies.
IterativeImputer : Multivariate imputer that estimates values to impute for
    each feature with missing values from all the others.

References
----------
* `Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor
  Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing
  value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17
  no. 6, 2001 Pages 520-525.
  <https://academic.oup.com/bioinformatics/article/17/6/520/272365>`_

Examples
--------
>>> import numpy as np
>>> from sklearn.impute import KNNImputer
>>> X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]
>>> imputer = KNNImputer(n_neighbors=2)
>>> imputer.fit_transform(X)
array([[1. , 2. , 4. ],
       [3. , 4. , 3. ],
       [5.5, 6. , 5. ],
       [8. , 8. , 7. ]])

For a more detailed example see
:ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                rdfs:comment """Classifier implementing the k-nearest neighbors vote.

Read more in the :ref:`User Guide <classification>`.

Parameters
----------
n_neighbors : int, default=5
    Number of neighbors to use by default for :meth:`kneighbors` queries.

weights : {'uniform', 'distance'}, callable or None, default='uniform'
    Weight function used in prediction.  Possible values:

    - 'uniform' : uniform weights.  All points in each neighborhood
      are weighted equally.
    - 'distance' : weight points by the inverse of their distance.
      in this case, closer neighbors of a query point will have a
      greater influence than neighbors which are further away.
    - [callable] : a user-defined function which accepts an
      array of distances, and returns an array of the same shape
      containing the weights.

    Refer to the example entitled
    :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`
    showing the impact of the `weights` parameter on the decision
    boundary.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

p : float, default=2
    Power parameter for the Minkowski metric. When p = 1, this is equivalent
    to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.
    For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected
    to be positive.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.
    Doesn't affect :meth:`fit` method.

Attributes
----------
classes_ : array of shape (n_classes,)
    Class labels known to the classifier

effective_metric_ : str or callble
    The distance metric used. It will be same as the `metric` parameter
    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to
    'minkowski' and `p` parameter set to 2.

effective_metric_params_ : dict
    Additional keyword arguments for the metric function. For most metrics
    will be same with `metric_params` parameter, but may also contain the
    `p` parameter value if the `effective_metric_` attribute is set to
    'minkowski'.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

outputs_2d_ : bool
    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit
    otherwise True.

See Also
--------
RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.
KNeighborsRegressor: Regression based on k-nearest neighbors.
RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.
NearestNeighbors: Unsupervised learner for implementing neighbor searches.

Notes
-----
See :ref:`Nearest Neighbors <neighbors>` in the online documentation
for a discussion of the choice of ``algorithm`` and ``leaf_size``.

.. warning::

   Regarding the Nearest Neighbors algorithms, if it is found that two
   neighbors, neighbor `k+1` and `k`, have identical distances
   but different labels, the results will depend on the ordering of the
   training data.

https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm

Examples
--------
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y)
KNeighborsClassifier(...)
>>> print(neigh.predict([[1.1]]))
[0]
>>> print(neigh.predict_proba([[0.9]]))
[[0.666... 0.333...]]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                               rdfs:comment """Regression based on k-nearest neighbors.

The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.

Read more in the :ref:`User Guide <regression>`.

.. versionadded:: 0.9

Parameters
----------
n_neighbors : int, default=5
    Number of neighbors to use by default for :meth:`kneighbors` queries.

weights : {'uniform', 'distance'}, callable or None, default='uniform'
    Weight function used in prediction.  Possible values:

    - 'uniform' : uniform weights.  All points in each neighborhood
      are weighted equally.
    - 'distance' : weight points by the inverse of their distance.
      in this case, closer neighbors of a query point will have a
      greater influence than neighbors which are further away.
    - [callable] : a user-defined function which accepts an
      array of distances, and returns an array of the same shape
      containing the weights.

    Uniform weights are used by default.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

p : float, default=2
    Power parameter for the Minkowski metric. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric : str, DistanceMetric object or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    If metric is a DistanceMetric object, it will be passed directly to
    the underlying computation routines.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.
    Doesn't affect :meth:`fit` method.

Attributes
----------
effective_metric_ : str or callable
    The distance metric to use. It will be same as the `metric` parameter
    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to
    'minkowski' and `p` parameter set to 2.

effective_metric_params_ : dict
    Additional keyword arguments for the metric function. For most metrics
    will be same with `metric_params` parameter, but may also contain the
    `p` parameter value if the `effective_metric_` attribute is set to
    'minkowski'.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

See Also
--------
NearestNeighbors : Unsupervised learner for implementing neighbor searches.
RadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.
KNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.
RadiusNeighborsClassifier : Classifier implementing
    a vote among neighbors within a given radius.

Notes
-----
See :ref:`Nearest Neighbors <neighbors>` in the online documentation
for a discussion of the choice of ``algorithm`` and ``leaf_size``.

.. warning::

   Regarding the Nearest Neighbors algorithms, if it is found that two
   neighbors, neighbor `k+1` and `k`, have identical distances but
   different labels, the results will depend on the ordering of the
   training data.

https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm

Examples
--------
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsRegressor
>>> neigh = KNeighborsRegressor(n_neighbors=2)
>>> neigh.fit(X, y)
KNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[0.5]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                 rdfs:comment """Transform X into a (weighted) graph of k nearest neighbors.

The transformed data is a sparse graph as returned by kneighbors_graph.

Read more in the :ref:`User Guide <neighbors_transformer>`.

.. versionadded:: 0.22

Parameters
----------
mode : {'distance', 'connectivity'}, default='distance'
    Type of returned matrix: 'connectivity' will return the connectivity
    matrix with ones and zeros, and 'distance' will return the distances
    between neighbors according to the given metric.

n_neighbors : int, default=5
    Number of neighbors for each sample in the transformed sparse graph.
    For compatibility reasons, as each sample is considered as its own
    neighbor, one extra neighbor will be computed when mode == 'distance'.
    In this case, the sparse graph contains (n_neighbors + 1) neighbors.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    Distance matrices are not supported.

p : float, default=2
    Parameter for the Minkowski metric from
    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
    This parameter is expected to be positive.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    If ``-1``, then the number of jobs is set to the number of CPU cores.

Attributes
----------
effective_metric_ : str or callable
    The distance metric used. It will be same as the `metric` parameter
    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to
    'minkowski' and `p` parameter set to 2.

effective_metric_params_ : dict
    Additional keyword arguments for the metric function. For most metrics
    will be same with `metric_params` parameter, but may also contain the
    `p` parameter value if the `effective_metric_` attribute is set to
    'minkowski'.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

See Also
--------
kneighbors_graph : Compute the weighted graph of k-neighbors for
    points in X.
RadiusNeighborsTransformer : Transform X into a weighted graph of
    neighbors nearer than a radius.

Notes
-----
For an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`
in combination with :class:`~sklearn.manifold.TSNE` see
:ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.

Examples
--------
>>> from sklearn.datasets import load_wine
>>> from sklearn.neighbors import KNeighborsTransformer
>>> X, _ = load_wine(return_X_y=True)
>>> X.shape
(178, 13)
>>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')
>>> X_dist_graph = transformer.fit_transform(X)
>>> X_dist_graph.shape
(178, 178)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelCentererMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelCentererMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                          rdfs:comment """Center an arbitrary kernel matrix :math:`K`.

Let define a kernel :math:`K` such that:

.. math::
    K(X, Y) = \\phi(X) . \\phi(Y)^{T}

:math:`\\phi(X)` is a function mapping of rows of :math:`X` to a
Hilbert space and :math:`K` is of shape `(n_samples, n_samples)`.

This class allows to compute :math:`\\tilde{K}(X, Y)` such that:

.. math::
    \\tilde{K(X, Y)} = \\tilde{\\phi}(X) . \\tilde{\\phi}(Y)^{T}

:math:`\\tilde{\\phi}(X)` is the centered mapped data in the Hilbert
space.

`KernelCenterer` centers the features without explicitly computing the
mapping :math:`\\phi(\\cdot)`. Working with centered kernels is sometime
expected when dealing with algebra computation such as eigendecomposition
for :class:`~sklearn.decomposition.KernelPCA` for instance.

Read more in the :ref:`User Guide <kernel_centering>`.

Attributes
----------
K_fit_rows_ : ndarray of shape (n_samples,)
    Average of each column of kernel matrix.

K_fit_all_ : float
    Average of kernel matrix.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.kernel_approximation.Nystroem : Approximate a kernel map
    using a subset of the training data.

References
----------
.. [1] `Schlkopf, Bernhard, Alexander Smola, and Klaus-Robert Mller.
   \"Nonlinear component analysis as a kernel eigenvalue problem.\"
   Neural computation 10.5 (1998): 1299-1319.
   <https://www.mlpack.org/papers/kpca.pdf>`_

Examples
--------
>>> from sklearn.preprocessing import KernelCenterer
>>> from sklearn.metrics.pairwise import pairwise_kernels
>>> X = [[ 1., -2.,  2.],
...      [ -2.,  1.,  3.],
...      [ 4.,  1., -2.]]
>>> K = pairwise_kernels(X, metric='linear')
>>> K
array([[  9.,   2.,  -2.],
       [  2.,  14., -13.],
       [ -2., -13.,  21.]])
>>> transformer = KernelCenterer().fit(K)
>>> transformer
KernelCenterer()
>>> transformer.transform(K)
array([[  5.,   0.,  -5.],
       [  0.,  14., -14.],
       [ -5., -14.,  19.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Kernel Density Estimation.

Read more in the :ref:`User Guide <kernel_density>`.

Parameters
----------
bandwidth : float or {\"scott\", \"silverman\"}, default=1.0
    The bandwidth of the kernel. If bandwidth is a float, it defines the
    bandwidth of the kernel. If bandwidth is a string, one of the estimation
    methods is implemented.

algorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto'
    The tree algorithm to use.

kernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'
    The kernel to use.

metric : str, default='euclidean'
    Metric to use for distance computation. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    Not all metrics are valid with all algorithms: refer to the
    documentation of :class:`BallTree` and :class:`KDTree`. Note that the
    normalization of the density output is correct only for the Euclidean
    distance metric.

atol : float, default=0
    The desired absolute tolerance of the result.  A larger tolerance will
    generally lead to faster execution.

rtol : float, default=0
    The desired relative tolerance of the result.  A larger tolerance will
    generally lead to faster execution.

breadth_first : bool, default=True
    If true (default), use a breadth-first approach to the problem.
    Otherwise use a depth-first approach.

leaf_size : int, default=40
    Specify the leaf size of the underlying tree.  See :class:`BallTree`
    or :class:`KDTree` for details.

metric_params : dict, default=None
    Additional parameters to be passed to the tree for use with the
    metric.  For more information, see the documentation of
    :class:`BallTree` or :class:`KDTree`.

Attributes
----------
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

tree_ : ``BinaryTree`` instance
    The tree algorithm for fast generalized N-point problems.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

bandwidth_ : float
    Value of the bandwidth, given directly by the bandwidth parameter or
    estimated using the 'scott' or 'silverman' method.

    .. versionadded:: 1.0

See Also
--------
sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point
    problems.
sklearn.neighbors.BallTree : Ball tree for fast generalized N-point
    problems.

Examples
--------
Compute a gaussian kernel density estimate with a fixed bandwidth.

>>> from sklearn.neighbors import KernelDensity
>>> import numpy as np
>>> rng = np.random.RandomState(42)
>>> X = rng.random_sample((100, 3))
>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)
>>> log_density = kde.score_samples(X[:3])
>>> log_density
array([-1.52955942, -1.51462041, -1.60244657])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                     rdfs:comment """Kernel Principal component analysis (KPCA) [1]_.

Non-linear dimensionality reduction through the use of kernels (see
:ref:`metrics`).

It uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD
or the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the
truncated SVD, depending on the shape of the input data and the number of
components to extract. It can also use a randomized truncated SVD by the
method proposed in [3]_, see `eigen_solver`.

For a usage example, see
:ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.

Read more in the :ref:`User Guide <kernel_PCA>`.

Parameters
----------
n_components : int, default=None
    Number of components. If None, all non-zero components are kept.

kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear'
    Kernel used for PCA.

gamma : float, default=None
    Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
    kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.

degree : float, default=3
    Degree for poly kernels. Ignored by other kernels.

coef0 : float, default=1
    Independent term in poly and sigmoid kernels.
    Ignored by other kernels.

kernel_params : dict, default=None
    Parameters (keyword arguments) and
    values for kernel passed as callable object.
    Ignored by other kernels.

alpha : float, default=1.0
    Hyperparameter of the ridge regression that learns the
    inverse transform (when fit_inverse_transform=True).

fit_inverse_transform : bool, default=False
    Learn the inverse transform for non-precomputed kernels
    (i.e. learn to find the pre-image of a point). This method is based
    on [2]_.

eigen_solver : {'auto', 'dense', 'arpack', 'randomized'},             default='auto'
    Select eigensolver to use. If `n_components` is much
    less than the number of training samples, randomized (or arpack to a
    smaller extent) may be more efficient than the dense eigensolver.
    Randomized SVD is performed according to the method of Halko et al
    [3]_.

    auto :
        the solver is selected by a default policy based on n_samples
        (the number of training samples) and `n_components`:
        if the number of components to extract is less than 10 (strict) and
        the number of samples is more than 200 (strict), the 'arpack'
        method is enabled. Otherwise the exact full eigenvalue
        decomposition is computed and optionally truncated afterwards
        ('dense' method).
    dense :
        run exact full eigenvalue decomposition calling the standard
        LAPACK solver via `scipy.linalg.eigh`, and select the components
        by postprocessing
    arpack :
        run SVD truncated to n_components calling ARPACK solver using
        `scipy.sparse.linalg.eigsh`. It requires strictly
        0 < n_components < n_samples
    randomized :
        run randomized SVD by the method of Halko et al. [3]_. The current
        implementation selects eigenvalues based on their module; therefore
        using this method can lead to unexpected results if the kernel is
        not positive semi-definite. See also [4]_.

    .. versionchanged:: 1.0
       `'randomized'` was added.

tol : float, default=0
    Convergence tolerance for arpack.
    If 0, optimal value will be chosen by arpack.

max_iter : int, default=None
    Maximum number of iterations for arpack.
    If None, optimal value will be chosen by arpack.

iterated_power : int >= 0, or 'auto', default='auto'
    Number of iterations for the power method computed by
    svd_solver == 'randomized'. When 'auto', it is set to 7 when
    `n_components < 0.1 * min(X.shape)`, other it is set to 4.

    .. versionadded:: 1.0

remove_zero_eig : bool, default=False
    If True, then all components with zero eigenvalues are removed, so
    that the number of components in the output may be < n_components
    (and sometimes even zero due to numerical instability).
    When n_components is None, this parameter is ignored and components
    with zero eigenvalues are removed regardless.

random_state : int, RandomState instance or None, default=None
    Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int
    for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

    .. versionadded:: 0.18

copy_X : bool, default=True
    If True, input X is copied and stored by the model in the `X_fit_`
    attribute. If no further changes will be done to X, setting
    `copy_X=False` saves memory by storing a reference.

    .. versionadded:: 0.18

n_jobs : int, default=None
    The number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionadded:: 0.18

Attributes
----------
eigenvalues_ : ndarray of shape (n_components,)
    Eigenvalues of the centered kernel matrix in decreasing order.
    If `n_components` and `remove_zero_eig` are not set,
    then all values are stored.

eigenvectors_ : ndarray of shape (n_samples, n_components)
    Eigenvectors of the centered kernel matrix. If `n_components` and
    `remove_zero_eig` are not set, then all components are stored.

dual_coef_ : ndarray of shape (n_samples, n_features)
    Inverse transform matrix. Only available when
    ``fit_inverse_transform`` is True.

X_transformed_fit_ : ndarray of shape (n_samples, n_components)
    Projection of the fitted data on the kernel principal components.
    Only available when ``fit_inverse_transform`` is True.

X_fit_ : ndarray of shape (n_samples, n_features)
    The data used to fit the model. If `copy_X=False`, then `X_fit_` is
    a reference. This attribute is used for the calls to transform.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

gamma_ : float
    Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma`
    is explicitly provided, this is just the same as `gamma`. When `gamma`
    is `None`, this is the actual value of kernel coefficient.

    .. versionadded:: 1.3

See Also
--------
FastICA : A fast algorithm for Independent Component Analysis.
IncrementalPCA : Incremental Principal Component Analysis.
NMF : Non-Negative Matrix Factorization.
PCA : Principal Component Analysis.
SparsePCA : Sparse Principal Component Analysis.
TruncatedSVD : Dimensionality reduction using truncated SVD.

References
----------
.. [1] `Schlkopf, Bernhard, Alexander Smola, and Klaus-Robert Mller.
   \"Kernel principal component analysis.\"
   International conference on artificial neural networks.
   Springer, Berlin, Heidelberg, 1997.
   <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_

.. [2] `Bakr, Gkhan H., Jason Weston, and Bernhard Schlkopf.
   \"Learning to find pre-images.\"
   Advances in neural information processing systems 16 (2004): 449-456.
   <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_

.. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.
   \"Finding structure with randomness: Probabilistic algorithms for
   constructing approximate matrix decompositions.\"
   SIAM review 53.2 (2011): 217-288. <0909.4061>`

.. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.
   \"A randomized algorithm for the decomposition of matrices.\"
   Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.
   <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.decomposition import KernelPCA
>>> X, _ = load_digits(return_X_y=True)
>>> transformer = KernelPCA(n_components=7, kernel='linear')
>>> X_transformed = transformer.fit_transform(X)
>>> X_transformed.shape
(1797, 7)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                          rdfs:comment """Binarize labels in a one-vs-all fashion.

Several regression and binary classification algorithms are
available in scikit-learn. A simple way to extend these algorithms
to the multi-class classification case is to use the so-called
one-vs-all scheme.

At learning time, this simply consists in learning one regressor
or binary classifier per class. In doing so, one needs to convert
multi-class labels to binary labels (belong or does not belong
to the class). `LabelBinarizer` makes this process easy with the
transform method.

At prediction time, one assigns the class for which the corresponding
model gave the greatest confidence. `LabelBinarizer` makes this easy
with the :meth:`inverse_transform` method.

Read more in the :ref:`User Guide <preprocessing_targets>`.

Parameters
----------
neg_label : int, default=0
    Value with which negative labels must be encoded.

pos_label : int, default=1
    Value with which positive labels must be encoded.

sparse_output : bool, default=False
    True if the returned array from transform is desired to be in sparse
    CSR format.

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    Holds the label for each class.

y_type_ : str
    Represents the type of the target data as evaluated by
    :func:`~sklearn.utils.multiclass.type_of_target`. Possible type are
    'continuous', 'continuous-multioutput', 'binary', 'multiclass',
    'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.

sparse_input_ : bool
    `True` if the input data to transform is given as a sparse matrix,
     `False` otherwise.

See Also
--------
label_binarize : Function to perform the transform operation of
    LabelBinarizer with fixed classes.
OneHotEncoder : Encode categorical features using a one-hot aka one-of-K
    scheme.

Examples
--------
>>> from sklearn.preprocessing import LabelBinarizer
>>> lb = LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])
LabelBinarizer()
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6])
array([[1, 0, 0, 0],
       [0, 0, 0, 1]])

Binary targets transform to a column vector

>>> lb = LabelBinarizer()
>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])
array([[1],
       [0],
       [0],
       [1]])

Passing a 2D matrix for multilabel classification

>>> import numpy as np
>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))
LabelBinarizer()
>>> lb.classes_
array([0, 1, 2])
>>> lb.transform([0, 1, 2, 1])
array([[1, 0, 0],
       [0, 1, 0],
       [0, 0, 1],
       [0, 1, 0]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelEncoderMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelEncoderMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                        rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingAveragePrecisionScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingAveragePrecisionScoreMethod> rdf:type owl:Class ;
                                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                             rdfs:comment """Compute ranking-based average precision.

Label ranking average precision (LRAP) is the average over each ground
truth label assigned to each sample, of the ratio of true vs. total
labels with lower score.

This metric is used in multilabel ranking problem, where the goal
is to give better rank to the labels associated to each sample.

The obtained score is always strictly greater than 0 and
the best value is 1.

Read more in the :ref:`User Guide <label_ranking_average_precision>`.

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)
    True binary labels in binary indicator format.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by \"decision_function\" on some classifiers).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.20

Returns
-------
score : float
    Ranking-based average precision score.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_average_precision_score
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_average_precision_score(y_true, y_score)
0.416...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingLossMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                            rdfs:comment """Compute Ranking loss measure.

Compute the average number of label pairs that are incorrectly ordered
given y_score weighted by the size of the label set and the number of
labels not in the label set.

This is similar to the error set size, but weighted by the number of
relevant and irrelevant labels. The best performance is achieved with
a ranking loss of zero.

Read more in the :ref:`User Guide <label_ranking_loss>`.

.. versionadded:: 0.17
   A function *label_ranking_loss*

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)
    True binary labels in binary indicator format.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by \"decision_function\" on some classifiers).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    Average number of label pairs that are incorrectly ordered given
    y_score weighted by the size of the label set and the number of labels not
    in the label set.

References
----------
.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).
       Mining multi-label data. In Data mining and knowledge discovery
       handbook (pp. 667-685). Springer US.

Examples
--------
>>> from sklearn.metrics import label_ranking_loss
>>> y_true = [[1, 0, 0], [0, 0, 1]]
>>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]
>>> label_ranking_loss(y_true, y_score)
0.75...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod> rdf:type owl:Class ;
                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                  rdfs:comment """Cross-validated Least Angle Regression model.

See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

max_iter : int, default=500
    Maximum number of iterations to perform.

precompute : bool, 'auto' or array-like , default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram matrix
    cannot be passed as argument since we will use only subsets of X.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

max_n_alphas : int, default=1000
    The maximum number of points on the path used to compute the
    residuals in the cross-validation.

n_jobs : int or None, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

Attributes
----------
active_ : list of length n_alphas or list of such lists
    Indices of active variables at the end of the path.
    If this is a list of lists, the outer list length is `n_targets`.

coef_ : array-like of shape (n_features,)
    parameter vector (w in the formulation formula)

intercept_ : float
    independent term in decision function

coef_path_ : array-like of shape (n_features, n_alphas)
    the varying values of the coefficients along the path

alpha_ : float
    the estimated regularization parameter alpha

alphas_ : array-like of shape (n_alphas,)
    the different values of alpha along the path

cv_alphas_ : array-like of shape (n_cv_alphas,)
    all the values of alpha along the path for the different folds

mse_path_ : array-like of shape (n_folds, n_cv_alphas)
    the mean square error on left-out for each fold along the path
    (alpha values given by ``cv_alphas``)

n_iter_ : array-like or int
    the number of iterations run by Lars with the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoLarsIC : Lasso model fit with Lars using BIC
    or AIC for model selection.
sklearn.decomposition.sparse_encode : Sparse coding.

Notes
-----
In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

Examples
--------
>>> from sklearn.linear_model import LarsCV
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)
>>> reg = LarsCV(cv=5).fit(X, y)
>>> reg.score(X, y)
0.9996...
>>> reg.alpha_
0.2961...
>>> reg.predict(X[:1,])
array([154.3996...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod> rdf:type owl:Class ;
                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                rdfs:comment """Least Angle Regression model a.k.a. LAR.

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

precompute : bool, 'auto' or array-like , default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument.

n_nonzero_coefs : int, default=500
    Target number of non-zero coefficients. Use ``np.inf`` for no limit.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

fit_path : bool, default=True
    If True the full path is stored in the ``coef_path_`` attribute.
    If you compute the solution for a large problem or many targets,
    setting ``fit_path`` to ``False`` will lead to a speedup, especially
    with a small alpha.

jitter : float, default=None
    Upper bound on a uniform noise parameter to be added to the
    `y` values, to satisfy the model's assumption of
    one-at-a-time computations. Might help with stability.

    .. versionadded:: 0.23

random_state : int, RandomState instance or None, default=None
    Determines random number generation for jittering. Pass an int
    for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.

    .. versionadded:: 0.23

Attributes
----------
alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha >= alpha_min``, whichever
    is smaller. If this is a list of array-like, the length of the outer
    list is `n_targets`.

active_ : list of shape (n_alphas,) or list of such lists
    Indices of active variables at the end of the path.
    If this is a list of list, the length of the outer list is `n_targets`.

coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays
    The varying values of the coefficients along the path. It is not
    present if the ``fit_path`` parameter is ``False``. If this is a list
    of array-like, the length of the outer list is `n_targets`.

coef_ : array-like of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the formulation formula).

intercept_ : float or array-like of shape (n_targets,)
    Independent term in decision function.

n_iter_ : array-like or int
    The number of iterations taken by lars_path to find the
    grid of alphas for each target.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path: Compute Least Angle Regression or Lasso
    path using LARS algorithm.
LarsCV : Cross-validated Least Angle Regression model.
sklearn.decomposition.sparse_encode : Sparse coding.

Examples
--------
>>> from sklearn import linear_model
>>> reg = linear_model.Lars(n_nonzero_coefs=1)
>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
Lars(n_nonzero_coefs=1)
>>> print(reg.coef_)
[ 0. -1.11...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                   rdfs:comment """Lasso linear model with iterative fitting along a regularization path.

See glossary entry for :term:`cross-validation estimator`.

The best model is selected by cross-validation.

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Read more in the :ref:`User Guide <lasso>`.

Parameters
----------
eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If ``None`` alphas are set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

verbose : bool or int, default=False
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

positive : bool, default=False
    If positive, restrict regression coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
alpha_ : float
    The amount of penalization chosen by cross validation.

coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

mse_path_ : ndarray of shape (n_alphas, n_folds)
    Mean square error for the test set on each fold, varying alpha.

alphas_ : ndarray of shape (n_alphas,)
    The grid of alphas used for fitting.

dual_gap_ : float or ndarray of shape (n_targets,)
    The dual gap at the end of the optimization for the optimal alpha
    (``alpha_``).

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso path using LARS
    algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : The Lasso is a linear model that estimates sparse coefficients.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoCV : Lasso linear model with iterative fitting along a regularization
    path.
LassoLarsCV : Cross-validated Lasso using the LARS algorithm.

Notes
-----
In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` argument of the `fit`
method should be directly passed as a Fortran-contiguous numpy array.

 For an example, see
 :ref:`examples/linear_model/plot_lasso_model_selection.py
 <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.

:class:`LassoCV` leads to different results than a hyperparameter
search using :class:`~sklearn.model_selection.GridSearchCV` with a
:class:`Lasso` model. In :class:`LassoCV`, a model for a given
penalty `alpha` is warm started using the coefficients of the
closest model (trained at the previous iteration) on the
regularization path. It tends to speed up the hyperparameter
search.

Examples
--------
>>> from sklearn.linear_model import LassoCV
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(noise=4, random_state=0)
>>> reg = LassoCV(cv=5, random_state=0).fit(X, y)
>>> reg.score(X, y)
0.9993...
>>> reg.predict(X[:1,])
array([-78.4951...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                       rdfs:comment """Cross-validated Lasso, using the LARS algorithm.

See glossary entry for :term:`cross-validation estimator`.

The optimization objective for Lasso is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

max_iter : int, default=500
    Maximum number of iterations to perform.

precompute : bool or 'auto' , default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram matrix
    cannot be passed as argument since we will use only subsets of X.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

max_n_alphas : int, default=1000
    The maximum number of points on the path used to compute the
    residuals in the cross-validation.

n_jobs : int or None, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

positive : bool, default=False
    Restrict coefficients to be >= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    Under the positive restriction the model coefficients do not converge
    to the ordinary-least-squares solution for small values of alpha.
    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >
    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
    algorithm are typically in congruence with the solution of the
    coordinate descent Lasso estimator.
    As a consequence using LassoLarsCV only makes sense for problems where
    a sparse solution is expected and/or reached.

Attributes
----------
coef_ : array-like of shape (n_features,)
    parameter vector (w in the formulation formula)

intercept_ : float
    independent term in decision function.

coef_path_ : array-like of shape (n_features, n_alphas)
    the varying values of the coefficients along the path

alpha_ : float
    the estimated regularization parameter alpha

alphas_ : array-like of shape (n_alphas,)
    the different values of alpha along the path

cv_alphas_ : array-like of shape (n_cv_alphas,)
    all the values of alpha along the path for the different folds

mse_path_ : array-like of shape (n_folds, n_cv_alphas)
    the mean square error on left-out for each fold along the path
    (alpha values given by ``cv_alphas``)

n_iter_ : array-like or int
    the number of iterations run by Lars with the optimal alpha.

active_ : list of int
    Indices of active variables at the end of the path.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoLarsIC : Lasso model fit with Lars using BIC
    or AIC for model selection.
sklearn.decomposition.sparse_encode : Sparse coding.

Notes
-----
The object solves the same problem as the
:class:`~sklearn.linear_model.LassoCV` object. However, unlike the
:class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values
by itself. In general, because of this property, it will be more stable.
However, it is more fragile to heavily multicollinear datasets.

It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if
only a small number of features are selected compared to the total number,
for instance if there are very few samples compared to the number of
features.

In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

Examples
--------
>>> from sklearn.linear_model import LassoLarsCV
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(noise=4.0, random_state=0)
>>> reg = LassoLarsCV(cv=5).fit(X, y)
>>> reg.score(X, y)
0.9993...
>>> reg.alpha_
0.3972...
>>> reg.predict(X[:1,])
array([-78.4831...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                       rdfs:comment """Lasso model fit with Lars using BIC or AIC for model selection.

The optimization objective for Lasso is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

AIC is the Akaike information criterion [2]_ and BIC is the Bayes
Information criterion [3]_. Such criteria are useful to select the value
of the regularization parameter by making a trade-off between the
goodness of fit and the complexity of the model. A good model should
explain well the data while being simple.

Read more in the :ref:`User Guide <lasso_lars_ic>`.

Parameters
----------
criterion : {'aic', 'bic'}, default='aic'
    The type of criterion to use.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

precompute : bool, 'auto' or array-like, default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=500
    Maximum number of iterations to perform. Can be used for
    early stopping.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

positive : bool, default=False
    Restrict coefficients to be >= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    Under the positive restriction the model coefficients do not converge
    to the ordinary-least-squares solution for small values of alpha.
    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >
    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
    algorithm are typically in congruence with the solution of the
    coordinate descent Lasso estimator.
    As a consequence using LassoLarsIC only makes sense for problems where
    a sparse solution is expected and/or reached.

noise_variance : float, default=None
    The estimated noise variance of the data. If `None`, an unbiased
    estimate is computed by an OLS model. However, it is only possible
    in the case where `n_samples > n_features + fit_intercept`.

    .. versionadded:: 1.1

Attributes
----------
coef_ : array-like of shape (n_features,)
    parameter vector (w in the formulation formula)

intercept_ : float
    independent term in decision function.

alpha_ : float
    the alpha parameter chosen by the information criterion

alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha >= alpha_min``, whichever
    is smaller. If a list, it will be of length `n_targets`.

n_iter_ : int
    number of iterations run by lars_path to find the grid of
    alphas.

criterion_ : array-like of shape (n_alphas,)
    The value of the information criteria ('aic', 'bic') across all
    alphas. The alpha which has the smallest information criterion is
    chosen, as specified in [1]_.

noise_variance_ : float
    The estimated noise variance from the data used to compute the
    criterion.

    .. versionadded:: 1.1

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.
sklearn.decomposition.sparse_encode : Sparse coding.

Notes
-----
The number of degrees of freedom is computed as in [1]_.

To have more details regarding the mathematical formulation of the
AIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.

References
----------
.. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.
        \"On the degrees of freedom of the lasso.\"
        The Annals of Statistics 35.5 (2007): 2173-2192.
        <0712.0881>`

.. [2] `Wikipedia entry on the Akaike information criterion
        <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_

.. [3] `Wikipedia entry on the Bayesian information criterion
        <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_

Examples
--------
>>> from sklearn import linear_model
>>> reg = linear_model.LassoLarsIC(criterion='bic')
>>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]
>>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]
>>> reg.fit(X, y)
LassoLarsIC(criterion='bic')
>>> print(reg.coef_)
[ 0.  -1.11...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                     rdfs:comment """Lasso model fit with Least Angle Regression a.k.a. Lars.

It is a Linear Model trained with an L1 prior as regularizer.

The optimization objective for Lasso is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Read more in the :ref:`User Guide <least_angle_regression>`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the penalty term. Defaults to 1.0.
    ``alpha = 0`` is equivalent to an ordinary least square, solved
    by :class:`LinearRegression`. For numerical reasons, using
    ``alpha = 0`` with the LassoLars object is not advised and you
    should prefer the LinearRegression object.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

verbose : bool or int, default=False
    Sets the verbosity amount.

precompute : bool, 'auto' or array-like, default='auto'
    Whether to use a precomputed Gram matrix to speed up
    calculations. If set to ``'auto'`` let us decide. The Gram
    matrix can also be passed as argument.

max_iter : int, default=500
    Maximum number of iterations to perform.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

fit_path : bool, default=True
    If ``True`` the full path is stored in the ``coef_path_`` attribute.
    If you compute the solution for a large problem or many targets,
    setting ``fit_path`` to ``False`` will lead to a speedup, especially
    with a small alpha.

positive : bool, default=False
    Restrict coefficients to be >= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    Under the positive restriction the model coefficients will not converge
    to the ordinary-least-squares solution for small values of alpha.
    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >
    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso
    algorithm are typically in congruence with the solution of the
    coordinate descent Lasso estimator.

jitter : float, default=None
    Upper bound on a uniform noise parameter to be added to the
    `y` values, to satisfy the model's assumption of
    one-at-a-time computations. Might help with stability.

    .. versionadded:: 0.23

random_state : int, RandomState instance or None, default=None
    Determines random number generation for jittering. Pass an int
    for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.

    .. versionadded:: 0.23

Attributes
----------
alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha >= alpha_min``, whichever
    is smaller. If this is a list of array-like, the length of the outer
    list is `n_targets`.

active_ : list of length n_alphas or list of such lists
    Indices of active variables at the end of the path.
    If this is a list of list, the length of the outer list is `n_targets`.

coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays
    If a list is passed it's expected to be one of n_targets such arrays.
    The varying values of the coefficients along the path. It is not
    present if the ``fit_path`` parameter is ``False``. If this is a list
    of array-like, the length of the outer list is `n_targets`.

coef_ : array-like of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the formulation formula).

intercept_ : float or array-like of shape (n_targets,)
    Independent term in decision function.

n_iter_ : array-like or int
    The number of iterations taken by lars_path to find the
    grid of alphas for each target.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Compute Least Angle Regression or Lasso
    path using LARS algorithm.
lasso_path : Compute Lasso path with coordinate descent.
Lasso : Linear Model trained with L1 prior as
    regularizer (aka the Lasso).
LassoCV : Lasso linear model with iterative fitting
    along a regularization path.
LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.
LassoLarsIC : Lasso model fit with Lars using BIC
    or AIC for model selection.
sklearn.decomposition.sparse_encode : Sparse coding.

Examples
--------
>>> from sklearn import linear_model
>>> reg = linear_model.LassoLars(alpha=0.01)
>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])
LassoLars(alpha=0.01)
>>> print(reg.coef_)
[ 0.         -0.955...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment """Linear Model trained with L1 prior as regularizer (aka the Lasso).

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

Technically the Lasso model is optimizing the same objective function as
the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).

Read more in the :ref:`User Guide <lasso>`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the L1 term, controlling regularization
    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.

    When `alpha = 0`, the objective is equivalent to ordinary least
    squares, solved by the :class:`LinearRegression` object. For numerical
    reasons, using `alpha = 0` with the `Lasso` object is not advised.
    Instead, you should use the :class:`LinearRegression` object.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : bool or array-like of shape (n_features, n_features),                 default=False
    Whether to use a precomputed Gram matrix to speed up
    calculations. The Gram matrix can also be passed as argument.
    For sparse input this option is always ``False`` to preserve sparsity.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``, see Notes below.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the cost function formula).

dual_gap_ : float or ndarray of shape (n_targets,)
    Given param alpha, the dual gaps at the end of the optimization,
    same shape as each observation of y.

sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)
    Readonly property derived from ``coef_``.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : int or list of int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
lars_path : Regularization path using LARS.
lasso_path : Regularization path using Lasso.
LassoLars : Lasso Path along the regularization parameter using LARS algorithm.
LassoCV : Lasso alpha parameter by cross-validation.
LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.
sklearn.decomposition.sparse_encode : Sparse coding array estimator.

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.

Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to `1 / (2C)` in other linear
models such as :class:`~sklearn.linear_model.LogisticRegression` or
:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.

The precise stopping criteria based on `tol` are the following: First, check that
that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`
is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.
If so, then additionally check whether the dual gap is smaller than `tol` times
:math:`||y||_2^2 / n_{\\text{samples}}`.

The target can be a 2-dimensional array, resulting in the optimization of the
following objective::

    (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11

where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.
It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which
instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise
sparsity in the coefficients.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.Lasso(alpha=0.1)
>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
Lasso(alpha=0.1)
>>> print(clf.coef_)
[0.85 0.  ]
>>> print(clf.intercept_)
0.15...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                     rdfs:comment """Latent Dirichlet Allocation with online variational Bayes algorithm.

The implementation is based on [1]_ and [2]_.

.. versionadded:: 0.17

Read more in the :ref:`User Guide <LatentDirichletAllocation>`.

Parameters
----------
n_components : int, default=10
    Number of topics.

    .. versionchanged:: 0.19
        ``n_topics`` was renamed to ``n_components``

doc_topic_prior : float, default=None
    Prior of document topic distribution `theta`. If the value is None,
    defaults to `1 / n_components`.
    In [1]_, this is called `alpha`.

topic_word_prior : float, default=None
    Prior of topic word distribution `beta`. If the value is None, defaults
    to `1 / n_components`.
    In [1]_, this is called `eta`.

learning_method : {'batch', 'online'}, default='batch'
    Method used to update `_component`. Only used in :meth:`fit` method.
    In general, if the data size is large, the online update will be much
    faster than the batch update.

    Valid options::

        'batch': Batch variational Bayes method. Use all training data in
            each EM update.
            Old `components_` will be overwritten in each iteration.
        'online': Online variational Bayes method. In each EM update, use
            mini-batch of training data to update the ``components_``
            variable incrementally. The learning rate is controlled by the
            ``learning_decay`` and the ``learning_offset`` parameters.

    .. versionchanged:: 0.20
        The default learning method is now ``\"batch\"``.

learning_decay : float, default=0.7
    It is a parameter that control learning rate in the online learning
    method. The value should be set between (0.5, 1.0] to guarantee
    asymptotic convergence. When the value is 0.0 and batch_size is
    ``n_samples``, the update method is same as batch learning. In the
    literature, this is called kappa.

learning_offset : float, default=10.0
    A (positive) parameter that downweights early iterations in online
    learning.  It should be greater than 1.0. In the literature, this is
    called tau_0.

max_iter : int, default=10
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the :meth:`fit` method, and not the
    :meth:`partial_fit` method.

batch_size : int, default=128
    Number of documents to use in each EM iteration. Only used in online
    learning.

evaluate_every : int, default=-1
    How often to evaluate perplexity. Only used in `fit` method.
    set it to 0 or negative number to not evaluate perplexity in
    training at all. Evaluating perplexity can help you check convergence
    in training process, but it will also increase total training time.
    Evaluating perplexity in every iteration might increase training time
    up to two-fold.

total_samples : int, default=1e6
    Total number of documents. Only used in the :meth:`partial_fit` method.

perp_tol : float, default=1e-1
    Perplexity tolerance in batch learning. Only used when
    ``evaluate_every`` is greater than 0.

mean_change_tol : float, default=1e-3
    Stopping tolerance for updating document topic distribution in E-step.

max_doc_update_iter : int, default=100
    Max number of iterations for updating document topic distribution in
    the E-step.

n_jobs : int, default=None
    The number of jobs to use in the E-step.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : int, default=0
    Verbosity level.

random_state : int, RandomState instance or None, default=None
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Variational parameters for topic word distribution. Since the complete
    conditional for topic word distribution is a Dirichlet,
    ``components_[i, j]`` can be viewed as pseudocount that represents the
    number of times word `j` was assigned to topic `i`.
    It can also be viewed as distribution over the words for each topic
    after normalization:
    ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.

exp_dirichlet_component_ : ndarray of shape (n_components, n_features)
    Exponential value of expectation of log topic word distribution.
    In the literature, this is `exp(E[log(beta)])`.

n_batch_iter_ : int
    Number of iterations of the EM step.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of passes over the dataset.

bound_ : float
    Final perplexity score on training set.

doc_topic_prior_ : float
    Prior of document topic distribution `theta`. If the value is None,
    it is `1 / n_components`.

random_state_ : RandomState instance
    RandomState instance that is generated either from a seed, the random
    number generator or by `np.random`.

topic_word_prior_ : float
    Prior of topic word distribution `beta`. If the value is None, it is
    `1 / n_components`.

See Also
--------
sklearn.discriminant_analysis.LinearDiscriminantAnalysis:
    A classifier with a linear decision boundary, generated by fitting
    class conditional densities to the data and using Bayes' rule.

References
----------
.. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.
       Hoffman, David M. Blei, Francis Bach, 2010
       https://github.com/blei-lab/onlineldavb

.. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,
       David M. Blei, Chong Wang, John Paisley, 2013

Examples
--------
>>> from sklearn.decomposition import LatentDirichletAllocation
>>> from sklearn.datasets import make_multilabel_classification
>>> # This produces a feature matrix of token counts, similar to what
>>> # CountVectorizer would produce on text.
>>> X, _ = make_multilabel_classification(random_state=0)
>>> lda = LatentDirichletAllocation(n_components=5,
...     random_state=0)
>>> lda.fit(X)
LatentDirichletAllocation(...)
>>> # get topics for some given samples:
>>> lda.transform(X[-2:])
array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],
       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                                rdfs:comment """Learning Curve visualization.

It is recommended to use
:meth:`~sklearn.model_selection.LearningCurveDisplay.from_estimator` to
create a :class:`~sklearn.model_selection.LearningCurveDisplay` instance.
All parameters are stored as attributes.

Read more in the :ref:`User Guide <visualizations>` for general information
about the visualization API and
:ref:`detailed documentation <learning_curve>` regarding the learning
curve visualization.

.. versionadded:: 1.2

Parameters
----------
train_sizes : ndarray of shape (n_unique_ticks,)
    Numbers of training examples that has been used to generate the
    learning curve.

train_scores : ndarray of shape (n_ticks, n_cv_folds)
    Scores on training sets.

test_scores : ndarray of shape (n_ticks, n_cv_folds)
    Scores on test set.

score_name : str, default=None
    The name of the score used in `learning_curve`. It will override the name
    inferred from the `scoring` parameter. If `score` is `None`, we use `\"Score\"` if
    `negate_score` is `False` and `\"Negative score\"` otherwise. If `scoring` is a
    string or a callable, we infer the name. We replace `_` by spaces and capitalize
    the first letter. We remove `neg_` and replace it by `\"Negative\"` if
    `negate_score` is `False` or just remove it otherwise.

Attributes
----------
ax_ : matplotlib Axes
    Axes with the learning curve.

figure_ : matplotlib Figure
    Figure containing the learning curve.

errorbar_ : list of matplotlib Artist or None
    When the `std_display_style` is `\"errorbar\"`, this is a list of
    `matplotlib.container.ErrorbarContainer` objects. If another style is
    used, `errorbar_` is `None`.

lines_ : list of matplotlib Artist or None
    When the `std_display_style` is `\"fill_between\"`, this is a list of
    `matplotlib.lines.Line2D` objects corresponding to the mean train and
    test scores. If another style is used, `line_` is `None`.

fill_between_ : list of matplotlib Artist or None
    When the `std_display_style` is `\"fill_between\"`, this is a list of
    `matplotlib.collections.PolyCollection` objects. If another style is
    used, `fill_between_` is `None`.

See Also
--------
sklearn.model_selection.learning_curve : Compute the learning curve.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import LearningCurveDisplay, learning_curve
>>> from sklearn.tree import DecisionTreeClassifier
>>> X, y = load_iris(return_X_y=True)
>>> tree = DecisionTreeClassifier(random_state=0)
>>> train_sizes, train_scores, test_scores = learning_curve(
...     tree, X, y)
>>> display = LearningCurveDisplay(train_sizes=train_sizes,
...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")
>>> display.plot()
<...>
>>> plt.show()""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneGroupOutMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneGroupOutMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                            rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneOutMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneOutMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                       rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                           rdfs:comment """Leave P Group(s) Out cross-validator.

Provides train/test indices to split data according to a third-party
provided group. This group information can be used to encode arbitrary
domain specific stratifications of the samples as integers.

For instance the groups could be the year of collection of the samples
and thus allow for cross-validation against time-based splits.

The difference between LeavePGroupsOut and LeaveOneGroupOut is that
the former builds the test sets with all the samples assigned to
``p`` different values of the groups while the latter uses samples
all assigned the same groups.

Read more in the :ref:`User Guide <leave_p_groups_out>`.

Parameters
----------
n_groups : int
    Number of groups (``p``) to leave out in the test split.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import LeavePGroupsOut
>>> X = np.array([[1, 2], [3, 4], [5, 6]])
>>> y = np.array([1, 2, 1])
>>> groups = np.array([1, 2, 3])
>>> lpgo = LeavePGroupsOut(n_groups=2)
>>> lpgo.get_n_splits(X, y, groups)
3
>>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required
3
>>> print(lpgo)
LeavePGroupsOut(n_groups=2)
>>> for i, (train_index, test_index) in enumerate(lpgo.split(X, y, groups)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")
...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")
Fold 0:
  Train: index=[2], group=[3]
  Test:  index=[0 1], group=[1 2]
Fold 1:
  Train: index=[1], group=[2]
  Test:  index=[0 2], group=[1 3]
Fold 2:
  Train: index=[0], group=[1]
  Test:  index=[1 2], group=[2 3]

See Also
--------
GroupKFold : K-fold iterator variant with non-overlapping groups.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                     rdfs:comment """Leave-P-Out cross-validator.

Provides train/test indices to split data in train/test sets. This results
in testing on all distinct samples of size p, while the remaining n - p
samples form the training set in each iteration.

Note: ``LeavePOut(p)`` is NOT equivalent to
``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.

Due to the high number of iterations which grows combinatorically with the
number of samples this cross-validation method can be very costly. For
large datasets one should favor :class:`KFold`, :class:`StratifiedKFold`
or :class:`ShuffleSplit`.

Read more in the :ref:`User Guide <leave_p_out>`.

Parameters
----------
p : int
    Size of the test sets. Must be strictly less than the number of
    samples.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import LeavePOut
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 3, 4])
>>> lpo = LeavePOut(2)
>>> lpo.get_n_splits(X)
6
>>> print(lpo)
LeavePOut(p=2)
>>> for i, (train_index, test_index) in enumerate(lpo.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[2 3]
  Test:  index=[0 1]
Fold 1:
  Train: index=[1 3]
  Test:  index=[0 2]
Fold 2:
  Train: index=[1 2]
  Test:  index=[0 3]
Fold 3:
  Train: index=[0 3]
  Test:  index=[1 2]
Fold 4:
  Train: index=[0 2]
  Test:  index=[1 3]
Fold 5:
  Train: index=[0 1]
  Test:  index=[2 3]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """Ordinary least squares Linear Regression.

LinearRegression fits a linear model with coefficients w = (w1, ..., wp)
to minimize the residual sum of squares between the observed targets in
the dataset, and the targets predicted by the linear approximation.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

n_jobs : int, default=None
    The number of jobs to use for the computation. This will only provide
    speedup in case of sufficiently large problems, that is if firstly
    `n_targets > 1` and secondly `X` is sparse or if `positive` is set
    to `True`. ``None`` means 1 unless in a
    :obj:`joblib.parallel_backend` context. ``-1`` means using all
    processors. See :term:`Glossary <n_jobs>` for more details.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive. This
    option is only supported for dense arrays.

    .. versionadded:: 0.24

Attributes
----------
coef_ : array of shape (n_features, ) or (n_targets, n_features)
    Estimated coefficients for the linear regression problem.
    If multiple targets are passed during the fit (y 2D), this
    is a 2D array of shape (n_targets, n_features), while if only
    one target is passed, this is a 1D array of length n_features.

rank_ : int
    Rank of matrix `X`. Only available when `X` is dense.

singular_ : array of shape (min(X, y),)
    Singular values of `X`. Only available when `X` is dense.

intercept_ : float or array of shape (n_targets,)
    Independent term in the linear model. Set to 0.0 if
    `fit_intercept = False`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression addresses some of the
    problems of Ordinary Least Squares by imposing a penalty on the
    size of the coefficients with l2 regularization.
Lasso : The Lasso is a linear model that estimates
    sparse coefficients with l1 regularization.
ElasticNet : Elastic-Net is a linear regression
    model trained with both l1 and l2 -norm regularization of the
    coefficients.

Notes
-----
From the implementation point of view, this is just plain Ordinary
Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares
(scipy.optimize.nnls) wrapped as a predictor object.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import LinearRegression
>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
>>> # y = 1 * x_0 + 2 * x_1 + 3
>>> y = np.dot(X, np.array([1, 2])) + 3
>>> reg = LinearRegression().fit(X, y)
>>> reg.score(X, y)
1.0
>>> reg.coef_
array([1., 2.])
>>> reg.intercept_
3.0...
>>> reg.predict(np.array([[3, 5]]))
array([16.])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                     rdfs:comment """Linear Support Vector Classification.

Similar to SVC with parameter kernel='linear', but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.

The main differences between :class:`~sklearn.svm.LinearSVC` and
:class:`~sklearn.svm.SVC` lie in the loss function used by default, and in
the handling of intercept regularization between those two implementations.

This class supports both dense and sparse input and the multiclass support
is handled according to a one-vs-the-rest scheme.

Read more in the :ref:`User Guide <svm_classification>`.

Parameters
----------
penalty : {'l1', 'l2'}, default='l2'
    Specifies the norm used in the penalization. The 'l2'
    penalty is the standard used in SVC. The 'l1' leads to ``coef_``
    vectors that are sparse.

loss : {'hinge', 'squared_hinge'}, default='squared_hinge'
    Specifies the loss function. 'hinge' is the standard SVM loss
    (used e.g. by the SVC class) while 'squared_hinge' is the
    square of the hinge loss. The combination of ``penalty='l1'``
    and ``loss='hinge'`` is not supported.

dual : \"auto\" or bool, default=True
    Select the algorithm to either solve the dual or primal
    optimization problem. Prefer dual=False when n_samples > n_features.
    `dual=\"auto\"` will choose the value of the parameter automatically,
    based on the values of `n_samples`, `n_features`, `loss`, `multi_class`
    and `penalty`. If `n_samples` < `n_features` and optimizer supports
    chosen `loss`, `multi_class` and `penalty`, then dual will be set to True,
    otherwise it will be set to False.

    .. versionchanged:: 1.3
       The `\"auto\"` option is added in version 1.3 and will be the default
       in version 1.5.

tol : float, default=1e-4
    Tolerance for stopping criteria.

C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive.

multi_class : {'ovr', 'crammer_singer'}, default='ovr'
    Determines the multi-class strategy if `y` contains more than
    two classes.
    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while
    ``\"crammer_singer\"`` optimizes a joint objective over all classes.
    While `crammer_singer` is interesting from a theoretical perspective
    as it is consistent, it is seldom used in practice as it rarely leads
    to better accuracy and is more expensive to compute.
    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual
    will be ignored.

fit_intercept : bool, default=True
    Whether or not to fit an intercept. If set to True, the feature vector
    is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where
    1 corresponds to the intercept. If set to False, no intercept will be
    used in calculations (i.e. data is expected to be already centered).

intercept_scaling : float, default=1.0
    When `fit_intercept` is True, the instance vector x becomes ``[x_1,
    ..., x_n, intercept_scaling]``, i.e. a \"synthetic\" feature with a
    constant value equal to `intercept_scaling` is appended to the instance
    vector. The intercept becomes intercept_scaling * synthetic feature
    weight. Note that liblinear internally penalizes the intercept,
    treating it like any other term in the feature vector. To reduce the
    impact of the regularization on the intercept, the `intercept_scaling`
    parameter can be set to a value greater than 1; the higher the value of
    `intercept_scaling`, the lower the impact of regularization on it.
    Then, the weights become `[w_x_1, ..., w_x_n,
    w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent
    the feature weights and the intercept weight is scaled by
    `intercept_scaling`. This scaling allows the intercept term to have a
    different regularization behavior compared to the other features.

class_weight : dict or 'balanced', default=None
    Set the parameter C of class i to ``class_weight[i]*C`` for
    SVC. If not given, all classes are supposed to have
    weight one.
    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

verbose : int, default=0
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in liblinear that, if enabled, may not work
    properly in a multithreaded context.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data for
    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the
    underlying implementation of :class:`LinearSVC` is not random and
    ``random_state`` has no effect on the results.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

max_iter : int, default=1000
    The maximum number of iterations to be run.

Attributes
----------
coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)
    Weights assigned to the features (coefficients in the primal
    problem).

    ``coef_`` is a readonly property derived from ``raw_coef_`` that
    follows the internal memory layout of liblinear.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

classes_ : ndarray of shape (n_classes,)
    The unique classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Maximum number of iterations run across all classes.

See Also
--------
SVC : Implementation of Support Vector Machine classifier using libsvm:
    the kernel can be non-linear but its SMO algorithm does not
    scale to large number of samples as LinearSVC does.

    Furthermore SVC multi-class mode is implemented using one
    vs one scheme while LinearSVC uses one vs the rest. It is
    possible to implement one vs the rest with SVC by using the
    :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.

    Finally SVC can fit dense data without memory copy if the input
    is C-contiguous. Sparse data will still incur memory copy though.

sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same
    cost function as LinearSVC
    by adjusting the penalty and loss parameters. In addition it requires
    less memory, allows incremental (online) learning, and implements
    various loss functions and regularization regimes.

Notes
-----
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon
to have slightly different results for the same input data. If
that happens, try with a smaller ``tol`` parameter.

The underlying implementation, liblinear, uses a sparse internal
representation for the data that will incur a memory copy.

Predict output may not match that of standalone liblinear in certain
cases. See :ref:`differences from liblinear <liblinear_differences>`
in the narrative documentation.

References
----------
`LIBLINEAR: A Library for Large Linear Classification
<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__

Examples
--------
>>> from sklearn.svm import LinearSVC
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_features=4, random_state=0)
>>> clf = make_pipeline(StandardScaler(),
...                     LinearSVC(dual=\"auto\", random_state=0, tol=1e-5))
>>> clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('linearsvc', LinearSVC(dual='auto', random_state=0, tol=1e-05))])

>>> print(clf.named_steps['linearsvc'].coef_)
[[0.141...   0.526... 0.679... 0.493...]]

>>> print(clf.named_steps['linearsvc'].intercept_)
[0.1693...]
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                     rdfs:comment """Linear Support Vector Regression.

Similar to SVR with parameter kernel='linear', but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.

The main differences between :class:`~sklearn.svm.LinearSVR` and
:class:`~sklearn.svm.SVR` lie in the loss function used by default, and in
the handling of intercept regularization between those two implementations.

This class supports both dense and sparse input.

Read more in the :ref:`User Guide <svm_regression>`.

.. versionadded:: 0.16

Parameters
----------
epsilon : float, default=0.0
    Epsilon parameter in the epsilon-insensitive loss function. Note
    that the value of this parameter depends on the scale of the target
    variable y. If unsure, set ``epsilon=0``.

tol : float, default=1e-4
    Tolerance for stopping criteria.

C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive.

loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'
    Specifies the loss function. The epsilon-insensitive loss
    (standard SVR) is the L1 loss, while the squared epsilon-insensitive
    loss ('squared_epsilon_insensitive') is the L2 loss.

fit_intercept : bool, default=True
    Whether or not to fit an intercept. If set to True, the feature vector
    is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where
    1 corresponds to the intercept. If set to False, no intercept will be
    used in calculations (i.e. data is expected to be already centered).

intercept_scaling : float, default=1.0
    When `fit_intercept` is True, the instance vector x becomes `[x_1, ...,
    x_n, intercept_scaling]`, i.e. a \"synthetic\" feature with a constant
    value equal to `intercept_scaling` is appended to the instance vector.
    The intercept becomes intercept_scaling * synthetic feature weight.
    Note that liblinear internally penalizes the intercept, treating it
    like any other term in the feature vector. To reduce the impact of the
    regularization on the intercept, the `intercept_scaling` parameter can
    be set to a value greater than 1; the higher the value of
    `intercept_scaling`, the lower the impact of regularization on it.
    Then, the weights become `[w_x_1, ..., w_x_n,
    w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent
    the feature weights and the intercept weight is scaled by
    `intercept_scaling`. This scaling allows the intercept term to have a
    different regularization behavior compared to the other features.

dual : \"auto\" or bool, default=True
    Select the algorithm to either solve the dual or primal
    optimization problem. Prefer dual=False when n_samples > n_features.
    `dual=\"auto\"` will choose the value of the parameter automatically,
    based on the values of `n_samples`, `n_features` and `loss`. If
    `n_samples` < `n_features` and optimizer supports chosen `loss`,
    then dual will be set to True, otherwise it will be set to False.

    .. versionchanged:: 1.3
       The `\"auto\"` option is added in version 1.3 and will be the default
       in version 1.5.

verbose : int, default=0
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in liblinear that, if enabled, may not work
    properly in a multithreaded context.

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

max_iter : int, default=1000
    The maximum number of iterations to be run.

Attributes
----------
coef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)
    Weights assigned to the features (coefficients in the primal
    problem).

    `coef_` is a readonly property derived from `raw_coef_` that
    follows the internal memory layout of liblinear.

intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Maximum number of iterations run across all classes.

See Also
--------
LinearSVC : Implementation of Support Vector Machine classifier using the
    same library as this class (liblinear).

SVR : Implementation of Support Vector Machine regression using libsvm:
    the kernel can be non-linear but its SMO algorithm does not scale to
    large number of samples as :class:`~sklearn.svm.LinearSVR` does.

sklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost
    function as LinearSVR
    by adjusting the penalty and loss parameters. In addition it requires
    less memory, allows incremental (online) learning, and implements
    various loss functions and regularization regimes.

Examples
--------
>>> from sklearn.svm import LinearSVR
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=4, random_state=0)
>>> regr = make_pipeline(StandardScaler(),
...                      LinearSVR(dual=\"auto\", random_state=0, tol=1e-5))
>>> regr.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('linearsvr', LinearSVR(dual='auto', random_state=0, tol=1e-05))])

>>> print(regr.named_steps['linearsvr'].coef_)
[18.582... 27.023... 44.357... 64.522...]
>>> print(regr.named_steps['linearsvr'].intercept_)
[-4...]
>>> print(regr.predict([[0, 0, 0, 0]]))
[-2.384...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """Unsupervised Outlier Detection using the Local Outlier Factor (LOF).

The anomaly score of each sample is called the Local Outlier Factor.
It measures the local deviation of the density of a given sample with respect
to its neighbors.
It is local in that the anomaly score depends on how isolated the object
is with respect to the surrounding neighborhood.
More precisely, locality is given by k-nearest neighbors, whose distance
is used to estimate the local density.
By comparing the local density of a sample to the local densities of its
neighbors, one can identify samples that have a substantially lower density
than their neighbors. These are considered outliers.

.. versionadded:: 0.19

Parameters
----------
n_neighbors : int, default=20
    Number of neighbors to use by default for :meth:`kneighbors` queries.
    If n_neighbors is larger than the number of samples provided,
    all samples will be used.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can
    affect the speed of the construction and query, as well as the memory
    required to store the tree. The optimal value depends on the
    nature of the problem.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

p : float, default=2
    Parameter for the Minkowski metric from
    :func:`sklearn.metrics.pairwise_distances`. When p = 1, this
    is equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

contamination : 'auto' or float, default='auto'
    The amount of contamination of the data set, i.e. the proportion
    of outliers in the data set. When fitting this is used to define the
    threshold on the scores of the samples.

    - if 'auto', the threshold is determined as in the
      original paper,
    - if a float, the contamination should be in the range (0, 0.5].

    .. versionchanged:: 0.22
       The default value of ``contamination`` changed from 0.1
       to ``'auto'``.

novelty : bool, default=False
    By default, LocalOutlierFactor is only meant to be used for outlier
    detection (novelty=False). Set novelty to True if you want to use
    LocalOutlierFactor for novelty detection. In this case be aware that
    you should only use predict, decision_function and score_samples
    on new unseen data and not on the training set; and note that the
    results obtained this way may differ from the standard LOF results.

    .. versionadded:: 0.20

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
negative_outlier_factor_ : ndarray of shape (n_samples,)
    The opposite LOF of the training samples. The higher, the more normal.
    Inliers tend to have a LOF score close to 1
    (``negative_outlier_factor_`` close to -1), while outliers tend to have
    a larger LOF score.

    The local outlier factor (LOF) of a sample captures its
    supposed 'degree of abnormality'.
    It is the average of the ratio of the local reachability density of
    a sample and those of its k-nearest neighbors.

n_neighbors_ : int
    The actual number of neighbors used for :meth:`kneighbors` queries.

offset_ : float
    Offset used to obtain binary labels from the raw scores.
    Observations having a negative_outlier_factor smaller than `offset_`
    are detected as abnormal.
    The offset is set to -1.5 (inliers score around -1), except when a
    contamination parameter different than \"auto\" is provided. In that
    case, the offset is defined in such a way we obtain the expected
    number of outliers in training.

    .. versionadded:: 0.20

effective_metric_ : str
    The effective metric used for the distance computation.

effective_metric_params_ : dict
    The effective additional keyword arguments for the metric function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    It is the number of samples in the fitted data.

See Also
--------
sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using
    Support Vector Machine.

References
----------
.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).
       LOF: identifying density-based local outliers. In ACM sigmod record.

Examples
--------
>>> import numpy as np
>>> from sklearn.neighbors import LocalOutlierFactor
>>> X = [[-1.1], [0.2], [101.1], [0.3]]
>>> clf = LocalOutlierFactor(n_neighbors=2)
>>> clf.fit_predict(X)
array([ 1,  1, -1,  1])
>>> clf.negative_outlier_factor_
array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                   rdfs:comment """Log loss, aka logistic loss or cross-entropy loss.

This is the loss function used in (multinomial) logistic regression
and extensions of it such as neural networks, defined as the negative
log-likelihood of a logistic model that returns ``y_pred`` probabilities
for its training data ``y_true``.
The log loss is only defined for two or more labels.
For a single sample with true label :math:`y \\in \\{0,1\\}` and
a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log
loss is:

.. math::
    L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))

Read more in the :ref:`User Guide <log_loss>`.

Parameters
----------
y_true : array-like or label indicator matrix
    Ground truth (correct) labels for n_samples samples.

y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)
    Predicted probabilities, as returned by a classifier's
    predict_proba method. If ``y_pred.shape = (n_samples,)``
    the probabilities provided are assumed to be that of the
    positive class. The labels in ``y_pred`` are assumed to be
    ordered alphabetically, as done by
    :class:`~sklearn.preprocessing.LabelBinarizer`.

eps : float or \"auto\", default=\"auto\"
    Log loss is undefined for p=0 or p=1, so probabilities are
    clipped to `max(eps, min(1 - eps, p))`. The default will depend on the
    data type of `y_pred` and is set to `np.finfo(y_pred.dtype).eps`.

    .. versionadded:: 1.2

    .. versionchanged:: 1.2
       The default value changed from `1e-15` to `\"auto\"` that is
       equivalent to `np.finfo(y_pred.dtype).eps`.

    .. deprecated:: 1.3
       `eps` is deprecated in 1.3 and will be removed in 1.5.

normalize : bool, default=True
    If true, return the mean loss per sample.
    Otherwise, return the sum of the per-sample losses.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like, default=None
    If not provided, labels will be inferred from y_true. If ``labels``
    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are
    assumed to be binary and are inferred from ``y_true``.

    .. versionadded:: 0.18

Returns
-------
loss : float
    Log loss, aka logistic loss or cross-entropy loss.

Notes
-----
The logarithm used is the natural logarithm (base-e).

References
----------
C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,
p. 209.

Examples
--------
>>> from sklearn.metrics import log_loss
>>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],
...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])
0.21616...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                               rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                rdfs:comment """Logistic Regression CV (aka logit, MaxEnt) classifier.

See glossary entry for :term:`cross-validation estimator`.

This class implements logistic regression using liblinear, newton-cg, sag
of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
regularization with primal formulation. The liblinear solver supports both
L1 and L2 regularization, with a dual formulation only for the L2 penalty.
Elastic-Net penalty is only supported by the saga solver.

For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter
is selected by the cross-validator
:class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed
using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'
solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).

Read more in the :ref:`User Guide <logistic_regression>`.

Parameters
----------
Cs : int or list of floats, default=10
    Each of the values in Cs describes the inverse of regularization
    strength. If Cs is as an int, then a grid of Cs values are chosen
    in a logarithmic scale between 1e-4 and 1e4.
    Like in support vector machines, smaller values specify stronger
    regularization.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the decision function.

cv : int or cross-validation generator, default=None
    The default cross-validation generator used is Stratified K-Folds.
    If an integer is provided, then it is the number of folds used.
    See the module :mod:`sklearn.model_selection` module for the
    list of possible cross-validation objects.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

dual : bool, default=False
    Dual (constrained) or primal (regularized, see also
    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation
    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
    n_samples > n_features.

penalty : {'l1', 'l2', 'elasticnet'}, default='l2'
    Specify the norm of the penalty:

    - `'l2'`: add a L2 penalty term (used by default);
    - `'l1'`: add a L1 penalty term;
    - `'elasticnet'`: both L1 and L2 penalty terms are added.

    .. warning::
       Some penalties may not work with some solvers. See the parameter
       `solver` below, to know the compatibility between the penalty and
       solver.

scoring : str or callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``. For a list of scoring functions
    that can be used, look at :mod:`sklearn.metrics`. The
    default scoring option used is 'accuracy'.

solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'

    Algorithm to use in the optimization problem. Default is 'lbfgs'.
    To choose a solver, you might want to consider the following aspects:

        - For small datasets, 'liblinear' is a good choice, whereas 'sag'
          and 'saga' are faster for large ones;
        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and
          'lbfgs' handle multinomial loss;
        - 'liblinear' might be slower in :class:`LogisticRegressionCV`
          because it does not handle warm-starting. 'liblinear' is
          limited to one-versus-rest schemes.
        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,
          especially with one-hot encoded categorical features with rare
          categories. Note that it is limited to binary classification and the
          one-versus-rest reduction for multiclass classification. Be aware that
          the memory usage of this solver has a quadratic dependency on
          `n_features` because it explicitly computes the Hessian matrix.

    .. warning::
       The choice of the algorithm depends on the penalty chosen.
       Supported penalties by solver:

       - 'lbfgs'           -   ['l2']
       - 'liblinear'       -   ['l1', 'l2']
       - 'newton-cg'       -   ['l2']
       - 'newton-cholesky' -   ['l2']
       - 'sag'             -   ['l2']
       - 'saga'            -   ['elasticnet', 'l1', 'l2']

    .. note::
       'sag' and 'saga' fast convergence is only guaranteed on features
       with approximately the same scale. You can preprocess the data with
       a scaler from :mod:`sklearn.preprocessing`.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.
    .. versionadded:: 1.2
       newton-cholesky solver.

tol : float, default=1e-4
    Tolerance for stopping criteria.

max_iter : int, default=100
    Maximum number of iterations of the optimization algorithm.

class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

    .. versionadded:: 0.17
       class_weight == 'balanced'

n_jobs : int, default=None
    Number of CPU cores used during the cross-validation loop.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : int, default=0
    For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any
    positive number for verbosity.

refit : bool, default=True
    If set to True, the scores are averaged across all folds, and the
    coefs and the C that corresponds to the best score is taken, and a
    final refit is done using these parameters.
    Otherwise the coefs, intercepts and C that correspond to the
    best scores across folds are averaged.

intercept_scaling : float, default=1
    Useful only when the solver 'liblinear' is used
    and self.fit_intercept is set to True. In this case, x becomes
    [x, self.intercept_scaling],
    i.e. a \"synthetic\" feature with constant value equal to
    intercept_scaling is appended to the instance vector.
    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.

    Note! the synthetic feature weight is subject to l1/l2 regularization
    as all other features.
    To lessen the effect of regularization on synthetic feature weight
    (and therefore on the intercept) intercept_scaling has to be increased.

multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'
    If the option chosen is 'ovr', then a binary problem is fit for each
    label. For 'multinomial' the loss minimised is the multinomial loss fit
    across the entire probability distribution, *even when the data is
    binary*. 'multinomial' is unavailable when solver='liblinear'.
    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
    and otherwise selects 'multinomial'.

    .. versionadded:: 0.18
       Stochastic Average Gradient descent solver for 'multinomial' case.
    .. versionchanged:: 0.22
        Default changed from 'ovr' to 'auto' in 0.22.

random_state : int, RandomState instance, default=None
    Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.
    Note that this only applies to the solver and not the cross-validation
    generator. See :term:`Glossary <random_state>` for details.

l1_ratios : list of float, default=None
    The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.
    Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to
    using ``penalty='l2'``, while 1 is equivalent to using
    ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination
    of L1 and L2.

Attributes
----------
classes_ : ndarray of shape (n_classes, )
    A list of class labels known to the classifier.

coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.

    `coef_` is of shape (1, n_features) when the given problem
    is binary.

intercept_ : ndarray of shape (1,) or (n_classes,)
    Intercept (a.k.a. bias) added to the decision function.

    If `fit_intercept` is set to False, the intercept is set to zero.
    `intercept_` is of shape(1,) when the problem is binary.

Cs_ : ndarray of shape (n_cs)
    Array of C i.e. inverse of regularization parameter values used
    for cross-validation.

l1_ratios_ : ndarray of shape (n_l1_ratios)
    Array of l1_ratios used for cross-validation. If no l1_ratio is used
    (i.e. penalty is not 'elasticnet'), this is set to ``[None]``

coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)
    dict with classes as the keys, and the path of coefficients obtained
    during cross-validating across each fold and then across each Cs
    after doing an OvR for the corresponding class as values.
    If the 'multi_class' option is set to 'multinomial', then
    the coefs_paths are the coefficients corresponding to each class.
    Each dict value has shape ``(n_folds, n_cs, n_features)`` or
    ``(n_folds, n_cs, n_features + 1)`` depending on whether the
    intercept is fit or not. If ``penalty='elasticnet'``, the shape is
    ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or
    ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.

scores_ : dict
    dict with classes as the keys, and the values as the
    grid of scores obtained during cross-validating each fold, after doing
    an OvR for the corresponding class. If the 'multi_class' option
    given is 'multinomial' then the same scores are repeated across
    all classes, since this is the multinomial class. Each dict value
    has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if
    ``penalty='elasticnet'``.

C_ : ndarray of shape (n_classes,) or (n_classes - 1,)
    Array of C that maps to the best scores across every class. If refit is
    set to False, then for each class, the best C is the average of the
    C's that correspond to the best scores for each fold.
    `C_` is of shape(n_classes,) when the problem is binary.

l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)
    Array of l1_ratio that maps to the best scores across every class. If
    refit is set to False, then for each class, the best l1_ratio is the
    average of the l1_ratio's that correspond to the best scores for each
    fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.

n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)
    Actual number of iterations for all classes, folds and Cs.
    In the binary or multinomial cases, the first dimension is equal to 1.
    If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,
    n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
LogisticRegression : Logistic regression without tuning the
    hyperparameter `C`.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegressionCV
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :]).shape
(2, 3)
>>> clf.score(X, y)
0.98...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """Logistic Regression (aka logit, MaxEnt) classifier.

In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the 'multi_class' option is set to 'ovr', and uses the
cross-entropy loss if the 'multi_class' option is set to 'multinomial'.
(Currently the 'multinomial' option is supported only by the 'lbfgs',
'sag', 'saga' and 'newton-cg' solvers.)

This class implements regularized logistic regression using the
'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note
that regularization is applied by default**. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).

The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization
with primal formulation, or no regularization. The 'liblinear' solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
'saga' solver.

Read more in the :ref:`User Guide <logistic_regression>`.

Parameters
----------
penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'
    Specify the norm of the penalty:

    - `None`: no penalty is added;
    - `'l2'`: add a L2 penalty term and it is the default choice;
    - `'l1'`: add a L1 penalty term;
    - `'elasticnet'`: both L1 and L2 penalty terms are added.

    .. warning::
       Some penalties may not work with some solvers. See the parameter
       `solver` below, to know the compatibility between the penalty and
       solver.

    .. versionadded:: 0.19
       l1 penalty with SAGA solver (allowing 'multinomial' + L1)

dual : bool, default=False
    Dual (constrained) or primal (regularized, see also
    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation
    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when
    n_samples > n_features.

tol : float, default=1e-4
    Tolerance for stopping criteria.

C : float, default=1.0
    Inverse of regularization strength; must be a positive float.
    Like in support vector machines, smaller values specify stronger
    regularization.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the decision function.

intercept_scaling : float, default=1
    Useful only when the solver 'liblinear' is used
    and self.fit_intercept is set to True. In this case, x becomes
    [x, self.intercept_scaling],
    i.e. a \"synthetic\" feature with constant value equal to
    intercept_scaling is appended to the instance vector.
    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.

    Note! the synthetic feature weight is subject to l1/l2 regularization
    as all other features.
    To lessen the effect of regularization on synthetic feature weight
    (and therefore on the intercept) intercept_scaling has to be increased.

class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

    .. versionadded:: 0.17
       *class_weight='balanced'*

random_state : int, RandomState instance, default=None
    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the
    data. See :term:`Glossary <random_state>` for details.

solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'

    Algorithm to use in the optimization problem. Default is 'lbfgs'.
    To choose a solver, you might want to consider the following aspects:

        - For small datasets, 'liblinear' is a good choice, whereas 'sag'
          and 'saga' are faster for large ones;
        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and
          'lbfgs' handle multinomial loss;
        - 'liblinear' is limited to one-versus-rest schemes.
        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,
          especially with one-hot encoded categorical features with rare
          categories. Note that it is limited to binary classification and the
          one-versus-rest reduction for multiclass classification. Be aware that
          the memory usage of this solver has a quadratic dependency on
          `n_features` because it explicitly computes the Hessian matrix.

    .. warning::
       The choice of the algorithm depends on the penalty chosen.
       Supported penalties by solver:

       - 'lbfgs'           -   ['l2', None]
       - 'liblinear'       -   ['l1', 'l2']
       - 'newton-cg'       -   ['l2', None]
       - 'newton-cholesky' -   ['l2', None]
       - 'sag'             -   ['l2', None]
       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]

    .. note::
       'sag' and 'saga' fast convergence is only guaranteed on features
       with approximately the same scale. You can preprocess the data with
       a scaler from :mod:`sklearn.preprocessing`.

    .. seealso::
       Refer to the User Guide for more information regarding
       :class:`LogisticRegression` and more specifically the
       :ref:`Table <Logistic_regression>`
       summarizing solver/penalty supports.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.
    .. versionchanged:: 0.22
        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.
    .. versionadded:: 1.2
       newton-cholesky solver.

max_iter : int, default=100
    Maximum number of iterations taken for the solvers to converge.

multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'
    If the option chosen is 'ovr', then a binary problem is fit for each
    label. For 'multinomial' the loss minimised is the multinomial loss fit
    across the entire probability distribution, *even when the data is
    binary*. 'multinomial' is unavailable when solver='liblinear'.
    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
    and otherwise selects 'multinomial'.

    .. versionadded:: 0.18
       Stochastic Average Gradient descent solver for 'multinomial' case.
    .. versionchanged:: 0.22
        Default changed from 'ovr' to 'auto' in 0.22.

verbose : int, default=0
    For the liblinear and lbfgs solvers set verbose to any positive
    number for verbosity.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.

    .. versionadded:: 0.17
       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.

n_jobs : int, default=None
    Number of CPU cores used when parallelizing over classes if
    multi_class='ovr'\". This parameter is ignored when the ``solver`` is
    set to 'liblinear' regardless of whether 'multi_class' is specified or
    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors.
    See :term:`Glossary <n_jobs>` for more details.

l1_ratio : float, default=None
    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only
    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent
    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent
    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a
    combination of L1 and L2.

Attributes
----------

classes_ : ndarray of shape (n_classes, )
    A list of class labels known to the classifier.

coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.

    `coef_` is of shape (1, n_features) when the given problem is binary.
    In particular, when `multi_class='multinomial'`, `coef_` corresponds
    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).

intercept_ : ndarray of shape (1,) or (n_classes,)
    Intercept (a.k.a. bias) added to the decision function.

    If `fit_intercept` is set to False, the intercept is set to zero.
    `intercept_` is of shape (1,) when the given problem is binary.
    In particular, when `multi_class='multinomial'`, `intercept_`
    corresponds to outcome 1 (True) and `-intercept_` corresponds to
    outcome 0 (False).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : ndarray of shape (n_classes,) or (1, )
    Actual number of iterations for all classes. If binary or multinomial,
    it returns only 1 element. For liblinear solver, only the maximum
    number of iteration across all classes is given.

    .. versionchanged:: 0.20

        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.

See Also
--------
SGDClassifier : Incrementally trained logistic regression (when given
    the parameter ``loss=\"log_loss\"``).
LogisticRegressionCV : Logistic regression with built-in cross validation.

Notes
-----
The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.

Predict output may not match that of standalone liblinear in certain
cases. See :ref:`differences from liblinear <liblinear_differences>`
in the narrative documentation.

References
----------

L-BFGS-B -- Software for Large-scale Bound-constrained Optimization
    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html

LIBLINEAR -- A Library for Large Linear Classification
    https://www.csie.ntu.edu.tw/~cjlin/liblinear/

SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach
    Minimizing Finite Sums with the Stochastic Average Gradient
    https://hal.inria.fr/hal-00860051/document

SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).
        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support
        for Non-Strongly Convex Composite Objectives\" <1407.0202>`

Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent
    methods for logistic regression and maximum entropy models.
    Machine Learning 85(1-2):41-75.
    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])
array([0, 0])
>>> clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
>>> clf.score(X, y)
0.97...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Multi-layer Perceptron classifier.

This model optimizes the log-loss function using LBFGS or stochastic
gradient descent.

.. versionadded:: 0.18

Parameters
----------
hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)
    The ith element represents the number of neurons in the ith
    hidden layer.

activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'
    Activation function for the hidden layer.

    - 'identity', no-op activation, useful to implement linear bottleneck,
      returns f(x) = x

    - 'logistic', the logistic sigmoid function,
      returns f(x) = 1 / (1 + exp(-x)).

    - 'tanh', the hyperbolic tan function,
      returns f(x) = tanh(x).

    - 'relu', the rectified linear unit function,
      returns f(x) = max(0, x)

solver : {'lbfgs', 'sgd', 'adam'}, default='adam'
    The solver for weight optimization.

    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.

    - 'sgd' refers to stochastic gradient descent.

    - 'adam' refers to a stochastic gradient-based optimizer proposed
      by Kingma, Diederik, and Jimmy Ba

    Note: The default solver 'adam' works pretty well on relatively
    large datasets (with thousands of training samples or more) in terms of
    both training time and validation score.
    For small datasets, however, 'lbfgs' can converge faster and perform
    better.

alpha : float, default=0.0001
    Strength of the L2 regularization term. The L2 regularization term
    is divided by the sample size when added to the loss.

batch_size : int, default='auto'
    Size of minibatches for stochastic optimizers.
    If the solver is 'lbfgs', the classifier will not use minibatch.
    When set to \"auto\", `batch_size=min(200, n_samples)`.

learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'
    Learning rate schedule for weight updates.

    - 'constant' is a constant learning rate given by
      'learning_rate_init'.

    - 'invscaling' gradually decreases the learning rate at each
      time step 't' using an inverse scaling exponent of 'power_t'.
      effective_learning_rate = learning_rate_init / pow(t, power_t)

    - 'adaptive' keeps the learning rate constant to
      'learning_rate_init' as long as training loss keeps decreasing.
      Each time two consecutive epochs fail to decrease training loss by at
      least tol, or fail to increase validation score by at least tol if
      'early_stopping' is on, the current learning rate is divided by 5.

    Only used when ``solver='sgd'``.

learning_rate_init : float, default=0.001
    The initial learning rate used. It controls the step-size
    in updating the weights. Only used when solver='sgd' or 'adam'.

power_t : float, default=0.5
    The exponent for inverse scaling learning rate.
    It is used in updating effective learning rate when the learning_rate
    is set to 'invscaling'. Only used when solver='sgd'.

max_iter : int, default=200
    Maximum number of iterations. The solver iterates until convergence
    (determined by 'tol') or this number of iterations. For stochastic
    solvers ('sgd', 'adam'), note that this determines the number of epochs
    (how many times each data point will be used), not the number of
    gradient steps.

shuffle : bool, default=True
    Whether to shuffle samples in each iteration. Only used when
    solver='sgd' or 'adam'.

random_state : int, RandomState instance, default=None
    Determines random number generation for weights and bias
    initialization, train-test split if early stopping is used, and batch
    sampling when solver='sgd' or 'adam'.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

tol : float, default=1e-4
    Tolerance for the optimization. When the loss or score is not improving
    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,
    unless ``learning_rate`` is set to 'adaptive', convergence is
    considered to be reached and training stops.

verbose : bool, default=False
    Whether to print progress messages to stdout.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous
    call to fit as initialization, otherwise, just erase the
    previous solution. See :term:`the Glossary <warm_start>`.

momentum : float, default=0.9
    Momentum for gradient descent update. Should be between 0 and 1. Only
    used when solver='sgd'.

nesterovs_momentum : bool, default=True
    Whether to use Nesterov's momentum. Only used when solver='sgd' and
    momentum > 0.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to true, it will automatically set
    aside 10% of training data as validation and terminate training when
    validation score is not improving by at least ``tol`` for
    ``n_iter_no_change`` consecutive epochs. The split is stratified,
    except in a multilabel setting.
    If early stopping is False, then the training stops when the training
    loss does not improve by more than tol for n_iter_no_change consecutive
    passes over the training set.
    Only effective when solver='sgd' or 'adam'.

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

beta_1 : float, default=0.9
    Exponential decay rate for estimates of first moment vector in adam,
    should be in [0, 1). Only used when solver='adam'.

beta_2 : float, default=0.999
    Exponential decay rate for estimates of second moment vector in adam,
    should be in [0, 1). Only used when solver='adam'.

epsilon : float, default=1e-8
    Value for numerical stability in adam. Only used when solver='adam'.

n_iter_no_change : int, default=10
    Maximum number of epochs to not meet ``tol`` improvement.
    Only effective when solver='sgd' or 'adam'.

    .. versionadded:: 0.20

max_fun : int, default=15000
    Only used when solver='lbfgs'. Maximum number of loss function calls.
    The solver iterates until convergence (determined by 'tol'), number
    of iterations reaches max_iter, or this number of loss function calls.
    Note that number of loss function calls will be greater than or equal
    to the number of iterations for the `MLPClassifier`.

    .. versionadded:: 0.22

Attributes
----------
classes_ : ndarray or list of ndarray of shape (n_classes,)
    Class labels for each output.

loss_ : float
    The current loss computed with the loss function.

best_loss_ : float or None
    The minimum loss reached by the solver throughout fitting.
    If `early_stopping=True`, this attribute is set to `None`. Refer to
    the `best_validation_score_` fitted attribute instead.

loss_curve_ : list of shape (`n_iter_`,)
    The ith element in the list represents the loss at the ith iteration.

validation_scores_ : list of shape (`n_iter_`,) or None
    The score at each iteration on a held-out validation set. The score
    reported is the accuracy score. Only available if `early_stopping=True`,
    otherwise the attribute is set to `None`.

best_validation_score_ : float or None
    The best validation score (i.e. accuracy score) that triggered the
    early stopping. Only available if `early_stopping=True`, otherwise the
    attribute is set to `None`.

t_ : int
    The number of training samples seen by the solver during fitting.

coefs_ : list of shape (n_layers - 1,)
    The ith element in the list represents the weight matrix corresponding
    to layer i.

intercepts_ : list of shape (n_layers - 1,)
    The ith element in the list represents the bias vector corresponding to
    layer i + 1.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The number of iterations the solver has run.

n_layers_ : int
    Number of layers.

n_outputs_ : int
    Number of outputs.

out_activation_ : str
    Name of the output activation function.

See Also
--------
MLPRegressor : Multi-layer Perceptron regressor.
BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).

Notes
-----
MLPClassifier trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.

It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.

This implementation works with data represented as dense numpy arrays or
sparse scipy arrays of floating point values.

References
----------
Hinton, Geoffrey E. \"Connectionist learning procedures.\"
Artificial intelligence 40.1 (1989): 185-234.

Glorot, Xavier, and Yoshua Bengio.
\"Understanding the difficulty of training deep feedforward neural networks.\"
International Conference on Artificial Intelligence and Statistics. 2010.

:arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification.\" <1502.01852>`

:arxiv:`Kingma, Diederik, and Jimmy Ba (2014)
\"Adam: A method for stochastic optimization.\" <1412.6980>`

Examples
--------
>>> from sklearn.neural_network import MLPClassifier
>>> from sklearn.datasets import make_classification
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_classification(n_samples=100, random_state=1)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,
...                                                     random_state=1)
>>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)
>>> clf.predict_proba(X_test[:1])
array([[0.038..., 0.961...]])
>>> clf.predict(X_test[:5, :])
array([1, 0, 1, 0, 1])
>>> clf.score(X_test, y_test)
0.8...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                        rdfs:comment """Multi-layer Perceptron regressor.

This model optimizes the squared error using LBFGS or stochastic gradient
descent.

.. versionadded:: 0.18

Parameters
----------
hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)
    The ith element represents the number of neurons in the ith
    hidden layer.

activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'
    Activation function for the hidden layer.

    - 'identity', no-op activation, useful to implement linear bottleneck,
      returns f(x) = x

    - 'logistic', the logistic sigmoid function,
      returns f(x) = 1 / (1 + exp(-x)).

    - 'tanh', the hyperbolic tan function,
      returns f(x) = tanh(x).

    - 'relu', the rectified linear unit function,
      returns f(x) = max(0, x)

solver : {'lbfgs', 'sgd', 'adam'}, default='adam'
    The solver for weight optimization.

    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.

    - 'sgd' refers to stochastic gradient descent.

    - 'adam' refers to a stochastic gradient-based optimizer proposed by
      Kingma, Diederik, and Jimmy Ba

    Note: The default solver 'adam' works pretty well on relatively
    large datasets (with thousands of training samples or more) in terms of
    both training time and validation score.
    For small datasets, however, 'lbfgs' can converge faster and perform
    better.

alpha : float, default=0.0001
    Strength of the L2 regularization term. The L2 regularization term
    is divided by the sample size when added to the loss.

batch_size : int, default='auto'
    Size of minibatches for stochastic optimizers.
    If the solver is 'lbfgs', the regressor will not use minibatch.
    When set to \"auto\", `batch_size=min(200, n_samples)`.

learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'
    Learning rate schedule for weight updates.

    - 'constant' is a constant learning rate given by
      'learning_rate_init'.

    - 'invscaling' gradually decreases the learning rate ``learning_rate_``
      at each time step 't' using an inverse scaling exponent of 'power_t'.
      effective_learning_rate = learning_rate_init / pow(t, power_t)

    - 'adaptive' keeps the learning rate constant to
      'learning_rate_init' as long as training loss keeps decreasing.
      Each time two consecutive epochs fail to decrease training loss by at
      least tol, or fail to increase validation score by at least tol if
      'early_stopping' is on, the current learning rate is divided by 5.

    Only used when solver='sgd'.

learning_rate_init : float, default=0.001
    The initial learning rate used. It controls the step-size
    in updating the weights. Only used when solver='sgd' or 'adam'.

power_t : float, default=0.5
    The exponent for inverse scaling learning rate.
    It is used in updating effective learning rate when the learning_rate
    is set to 'invscaling'. Only used when solver='sgd'.

max_iter : int, default=200
    Maximum number of iterations. The solver iterates until convergence
    (determined by 'tol') or this number of iterations. For stochastic
    solvers ('sgd', 'adam'), note that this determines the number of epochs
    (how many times each data point will be used), not the number of
    gradient steps.

shuffle : bool, default=True
    Whether to shuffle samples in each iteration. Only used when
    solver='sgd' or 'adam'.

random_state : int, RandomState instance, default=None
    Determines random number generation for weights and bias
    initialization, train-test split if early stopping is used, and batch
    sampling when solver='sgd' or 'adam'.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

tol : float, default=1e-4
    Tolerance for the optimization. When the loss or score is not improving
    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,
    unless ``learning_rate`` is set to 'adaptive', convergence is
    considered to be reached and training stops.

verbose : bool, default=False
    Whether to print progress messages to stdout.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous
    call to fit as initialization, otherwise, just erase the
    previous solution. See :term:`the Glossary <warm_start>`.

momentum : float, default=0.9
    Momentum for gradient descent update. Should be between 0 and 1. Only
    used when solver='sgd'.

nesterovs_momentum : bool, default=True
    Whether to use Nesterov's momentum. Only used when solver='sgd' and
    momentum > 0.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set
    aside ``validation_fraction`` of training data as validation and
    terminate training when validation score is not improving by at
    least ``tol`` for ``n_iter_no_change`` consecutive epochs.
    Only effective when solver='sgd' or 'adam'.

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

beta_1 : float, default=0.9
    Exponential decay rate for estimates of first moment vector in adam,
    should be in [0, 1). Only used when solver='adam'.

beta_2 : float, default=0.999
    Exponential decay rate for estimates of second moment vector in adam,
    should be in [0, 1). Only used when solver='adam'.

epsilon : float, default=1e-8
    Value for numerical stability in adam. Only used when solver='adam'.

n_iter_no_change : int, default=10
    Maximum number of epochs to not meet ``tol`` improvement.
    Only effective when solver='sgd' or 'adam'.

    .. versionadded:: 0.20

max_fun : int, default=15000
    Only used when solver='lbfgs'. Maximum number of function calls.
    The solver iterates until convergence (determined by ``tol``), number
    of iterations reaches max_iter, or this number of function calls.
    Note that number of function calls will be greater than or equal to
    the number of iterations for the MLPRegressor.

    .. versionadded:: 0.22

Attributes
----------
loss_ : float
    The current loss computed with the loss function.

best_loss_ : float
    The minimum loss reached by the solver throughout fitting.
    If `early_stopping=True`, this attribute is set to `None`. Refer to
    the `best_validation_score_` fitted attribute instead.
    Only accessible when solver='sgd' or 'adam'.

loss_curve_ : list of shape (`n_iter_`,)
    Loss value evaluated at the end of each training step.
    The ith element in the list represents the loss at the ith iteration.
    Only accessible when solver='sgd' or 'adam'.

validation_scores_ : list of shape (`n_iter_`,) or None
    The score at each iteration on a held-out validation set. The score
    reported is the R2 score. Only available if `early_stopping=True`,
    otherwise the attribute is set to `None`.
    Only accessible when solver='sgd' or 'adam'.

best_validation_score_ : float or None
    The best validation score (i.e. R2 score) that triggered the
    early stopping. Only available if `early_stopping=True`, otherwise the
    attribute is set to `None`.
    Only accessible when solver='sgd' or 'adam'.

t_ : int
    The number of training samples seen by the solver during fitting.
    Mathematically equals `n_iters * X.shape[0]`, it means
    `time_step` and it is used by optimizer's learning rate scheduler.

coefs_ : list of shape (n_layers - 1,)
    The ith element in the list represents the weight matrix corresponding
    to layer i.

intercepts_ : list of shape (n_layers - 1,)
    The ith element in the list represents the bias vector corresponding to
    layer i + 1.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The number of iterations the solver has run.

n_layers_ : int
    Number of layers.

n_outputs_ : int
    Number of outputs.

out_activation_ : str
    Name of the output activation function.

See Also
--------
BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).
MLPClassifier : Multi-layer Perceptron classifier.
sklearn.linear_model.SGDRegressor : Linear model fitted by minimizing
    a regularized empirical loss with SGD.

Notes
-----
MLPRegressor trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.

It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.

This implementation works with data represented as dense and sparse numpy
arrays of floating point values.

References
----------
Hinton, Geoffrey E. \"Connectionist learning procedures.\"
Artificial intelligence 40.1 (1989): 185-234.

Glorot, Xavier, and Yoshua Bengio.
\"Understanding the difficulty of training deep feedforward neural networks.\"
International Conference on Artificial Intelligence and Statistics. 2010.

:arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification.\" <1502.01852>`

:arxiv:`Kingma, Diederik, and Jimmy Ba (2014)
\"Adam: A method for stochastic optimization.\" <1412.6980>`

Examples
--------
>>> from sklearn.neural_network import MLPRegressor
>>> from sklearn.datasets import make_regression
>>> from sklearn.model_selection import train_test_split
>>> X, y = make_regression(n_samples=200, random_state=1)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y,
...                                                     random_state=1)
>>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)
>>> regr.predict(X_test[:2])
array([-0.9..., -7.1...])
>>> regr.score(X_test, y_test)
0.4...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                      rdfs:comment """Make a scorer from a performance metric or loss function.

A scorer is a wrapper around an arbitrary metric or loss function that is called
with the signature `scorer(estimator, X, y_true, **kwargs)`.

It is accepted in all scikit-learn estimators or functions allowing a `scoring`
parameter.

The parameter `response_method` allows to specify which method of the estimator
should be used to feed the scoring/loss function.

Read more in the :ref:`User Guide <scoring>`.

Parameters
----------
score_func : callable
    Score function (or loss function) with signature
    ``score_func(y, y_pred, **kwargs)``.

response_method : {\"predict_proba\", \"decision_function\", \"predict\"} or             list/tuple of such str, default=None

    Specifies the response method to use get prediction from an estimator
    (i.e. :term:`predict_proba`, :term:`decision_function` or
    :term:`predict`). Possible choices are:

    - if `str`, it corresponds to the name to the method to return;
    - if a list or tuple of `str`, it provides the method names in order of
      preference. The method returned corresponds to the first method in
      the list and which is implemented by `estimator`.
    - if `None`, it is equivalent to `\"predict\"`.

    .. versionadded:: 1.4

greater_is_better : bool, default=True
    Whether `score_func` is a score function (default), meaning high is
    good, or a loss function, meaning low is good. In the latter case, the
    scorer object will sign-flip the outcome of the `score_func`.

needs_proba : bool, default=False
    Whether `score_func` requires `predict_proba` to get probability
    estimates out of a classifier.

    If True, for binary `y_true`, the score function is supposed to accept
    a 1D `y_pred` (i.e., probability of the positive class, shape
    `(n_samples,)`).

    .. deprecated:: 1.4
       `needs_proba` is deprecated in version 1.4 and will be removed in
       1.6. Use `response_method=\"predict_proba\"` instead.

needs_threshold : bool, default=False
    Whether `score_func` takes a continuous decision certainty.
    This only works for binary classification using estimators that
    have either a `decision_function` or `predict_proba` method.

    If True, for binary `y_true`, the score function is supposed to accept
    a 1D `y_pred` (i.e., probability of the positive class or the decision
    function, shape `(n_samples,)`).

    For example `average_precision` or the area under the roc curve
    can not be computed using discrete predictions alone.

    .. deprecated:: 1.4
       `needs_threshold` is deprecated in version 1.4 and will be removed
       in 1.6. Use `response_method=(\"decision_function\", \"predict_proba\")`
       instead to preserve the same behaviour.

**kwargs : additional arguments
    Additional parameters to be passed to `score_func`.

Returns
-------
scorer : callable
    Callable object that returns a scalar score; greater is better.

Examples
--------
>>> from sklearn.metrics import fbeta_score, make_scorer
>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
>>> ftwo_scorer
make_scorer(fbeta_score, response_method='predict', beta=2)
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
...                     scoring=ftwo_scorer)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MatthewsCorrcoefMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MatthewsCorrcoefMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                            rdfs:comment """Compute the Matthews correlation coefficient (MCC).

The Matthews correlation coefficient is used in machine learning as a
measure of the quality of binary and multiclass classifications. It takes
into account true and false positives and negatives and is generally
regarded as a balanced measure which can be used even if the classes are of
very different sizes. The MCC is in essence a correlation coefficient value
between -1 and +1. A coefficient of +1 represents a perfect prediction, 0
an average random prediction and -1 an inverse prediction.  The statistic
is also known as the phi coefficient. [source: Wikipedia]

Binary and multiclass labels are supported.  Only in the binary case does
this relate to information about true and false positives and negatives.
See references below.

Read more in the :ref:`User Guide <matthews_corrcoef>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.18

Returns
-------
mcc : float
    The Matthews correlation coefficient (+1 represents a perfect
    prediction, 0 an average random prediction and -1 and inverse
    prediction).

References
----------
.. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the
   accuracy of prediction algorithms for classification: an overview.
   <10.1093/bioinformatics/16.5.412>`

.. [2] `Wikipedia entry for the Matthews Correlation Coefficient
   <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.

.. [3] `Gorodkin, (2004). Comparing two K-category assignments by a
    K-category correlation coefficient
    <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.

.. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN
    Error Measures in MultiClass Prediction
    <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.

Examples
--------
>>> from sklearn.metrics import matthews_corrcoef
>>> y_true = [+1, +1, +1, -1]
>>> y_pred = [+1, -1, +1, +1]
>>> matthews_corrcoef(y_true, y_pred)
-0.33...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                        rdfs:comment """Scale each feature by its maximum absolute value.

This estimator scales and translates each feature individually such
that the maximal absolute value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.

This scaler can also be applied to sparse CSR or CSC matrices.

`MaxAbsScaler` doesn't reduce the effect of outliers; it only linearly
scales them down. For an example visualization, refer to :ref:`Compare
MaxAbsScaler with other scalers <plot_all_scaling_max_abs_scaler_section>`.

.. versionadded:: 0.17

Parameters
----------
copy : bool, default=True
    Set to False to perform inplace scaling and avoid a copy (if the input
    is already a numpy array).

Attributes
----------
scale_ : ndarray of shape (n_features,)
    Per feature relative scaling of the data.

    .. versionadded:: 0.17
       *scale_* attribute.

max_abs_ : ndarray of shape (n_features,)
    Per feature maximum absolute value.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_seen_ : int
    The number of samples processed by the estimator. Will be reset on
    new calls to fit, but increments across ``partial_fit`` calls.

See Also
--------
maxabs_scale : Equivalent function without the estimator API.

Notes
-----
NaNs are treated as missing values: disregarded in fit, and maintained in
transform.

Examples
--------
>>> from sklearn.preprocessing import MaxAbsScaler
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> transformer = MaxAbsScaler().fit(X)
>>> transformer
MaxAbsScaler()
>>> transformer.transform(X)
array([[ 0.5, -1. ,  1. ],
       [ 1. ,  0. ,  0. ],
       [ 0. ,  1. , -0.5]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxErrorMethod> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                    rdfs:comment """The max_error metric calculates the maximum residual error.

Read more in the :ref:`User Guide <max_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

Returns
-------
max_error : float
    A positive floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import max_error
>>> y_true = [3, 2, 7, 1]
>>> y_pred = [4, 2, 7, 1]
>>> max_error(y_true, y_pred)
1""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Mean absolute error regression loss.

Read more in the :ref:`User Guide <mean_absolute_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    MAE output is non-negative floating point. The best value is 0.0.

Examples
--------
>>> from sklearn.metrics import mean_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod> rdf:type owl:Class ;
                                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                       rdfs:comment """Mean absolute percentage error (MAPE) regression loss.

Note here that the output is not a percentage in the range [0, 100]
and a value of 100 does not mean 100% but 1e2. Furthermore, the output
can be arbitrarily high when `y_true` is small (which is specific to the
metric) or when `abs(y_true - y_pred)` is large (which is common for most
regression metrics). Read more in the
:ref:`User Guide <mean_absolute_percentage_error>`.

.. versionadded:: 0.24

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.
    If input is list then the shape must be (n_outputs,).

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute percentage error
    is returned for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    MAPE output is non-negative floating point. The best value is 0.0.
    But note that bad predictions can lead to arbitrarily large
    MAPE values, especially if some `y_true` values are very close to zero.
    Note that we return a large value instead of `inf` when `y_true` is zero.

Examples
--------
>>> from sklearn.metrics import mean_absolute_percentage_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.3273...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.5515...
>>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.6198...
>>> # the value when some element of the y_true is zero is arbitrarily high because
>>> # of the division by epsilon
>>> y_true = [1., 0., 2.4, 7.]
>>> y_pred = [1.2, 0.1, 2.4, 8.]
>>> mean_absolute_percentage_error(y_true, y_pred)
112589990684262.48""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanGammaDevianceMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanGammaDevianceMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Mean Gamma deviance regression loss.

Gamma deviance is equivalent to the Tweedie deviance with
the power parameter `power=2`. It is invariant to scaling of
the target variable, and measures relative errors.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values. Requires y_true > 0.

y_pred : array-like of shape (n_samples,)
    Estimated target values. Requires y_pred > 0.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_gamma_deviance
>>> y_true = [2, 0.5, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_gamma_deviance(y_true, y_pred)
1.0568...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                           rdfs:comment """Pinball loss for quantile regression.

Read more in the :ref:`User Guide <pinball_loss>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

alpha : float, slope of the pinball loss, default=0.5,
    This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,
    `alpha=0.95` is minimized by estimators of the 95th percentile.

multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

    The pinball loss output is a non-negative floating point. The best
    value is 0.0.

Examples
--------
>>> from sklearn.metrics import mean_pinball_loss
>>> y_true = [1, 2, 3]
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)
0.03...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)
0.3...
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)
0.3...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)
0.03...
>>> mean_pinball_loss(y_true, y_true, alpha=0.1)
0.0
>>> mean_pinball_loss(y_true, y_true, alpha=0.9)
0.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPoissonDevianceMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPoissonDevianceMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                               rdfs:comment """Mean Poisson deviance regression loss.

Poisson deviance is equivalent to the Tweedie deviance with
the power parameter `power=1`.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values. Requires y_true >= 0.

y_pred : array-like of shape (n_samples,)
    Estimated target values. Requires y_pred > 0.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_poisson_deviance
>>> y_true = [2, 0, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_poisson_deviance(y_true, y_pred)
1.4260...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                     rdfs:comment """Mean shift clustering using a flat kernel.

Mean shift clustering aims to discover \"blobs\" in a smooth density of
samples. It is a centroid-based algorithm, which works by updating
candidates for centroids to be the mean of the points within a given
region. These candidates are then filtered in a post-processing stage to
eliminate near-duplicates to form the final set of centroids.

Seeding is performed using a binning technique for scalability.

Read more in the :ref:`User Guide <mean_shift>`.

Parameters
----------
bandwidth : float, default=None
    Bandwidth used in the flat kernel.

    If not given, the bandwidth is estimated using
    sklearn.cluster.estimate_bandwidth; see the documentation for that
    function for hints on scalability (see also the Notes, below).

seeds : array-like of shape (n_samples, n_features), default=None
    Seeds used to initialize kernels. If not set,
    the seeds are calculated by clustering.get_bin_seeds
    with bandwidth as the grid size and default values for
    other parameters.

bin_seeding : bool, default=False
    If true, initial kernel locations are not locations of all
    points, but rather the location of the discretized version of
    points, where points are binned onto a grid whose coarseness
    corresponds to the bandwidth. Setting this option to True will speed
    up the algorithm because fewer seeds will be initialized.
    The default value is False.
    Ignored if seeds argument is not None.

min_bin_freq : int, default=1
   To speed up the algorithm, accept only those bins with at least
   min_bin_freq points as seeds.

cluster_all : bool, default=True
    If true, then all points are clustered, even those orphans that are
    not within any kernel. Orphans are assigned to the nearest kernel.
    If false, then orphans are given cluster label -1.

n_jobs : int, default=None
    The number of jobs to use for the computation. The following tasks benefit
    from the parallelization:

    - The search of nearest neighbors for bandwidth estimation and label
      assignments. See the details in the docstring of the
      ``NearestNeighbors`` class.
    - Hill-climbing optimization for all seeds.

    See :term:`Glossary <n_jobs>` for more details.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

max_iter : int, default=300
    Maximum number of iterations, per seed point before the clustering
    operation terminates (for that seed point), if has not converged yet.

    .. versionadded:: 0.22

Attributes
----------
cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers.

labels_ : ndarray of shape (n_samples,)
    Labels of each point.

n_iter_ : int
    Maximum number of iterations performed on each seed.

    .. versionadded:: 0.22

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
KMeans : K-Means clustering.

Notes
-----

Scalability:

Because this implementation uses a flat kernel and
a Ball Tree to look up members of each kernel, the complexity will tend
towards O(T*n*log(n)) in lower dimensions, with n the number of samples
and T the number of points. In higher dimensions the complexity will
tend towards O(T*n^2).

Scalability can be boosted by using fewer seeds, for example by using
a higher value of min_bin_freq in the get_bin_seeds function.

Note that the estimate_bandwidth function is much less scalable than the
mean shift algorithm and will be the bottleneck if it is used.

References
----------

Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward
feature space analysis\". IEEE Transactions on Pattern Analysis and
Machine Intelligence. 2002. pp. 603-619.

Examples
--------
>>> from sklearn.cluster import MeanShift
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = MeanShift(bandwidth=2).fit(X)
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])
>>> clustering.predict([[0, 0], [5, 5]])
array([1, 0])
>>> clustering
MeanShift(bandwidth=2)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                            rdfs:comment """Mean squared error regression loss.

Read more in the :ref:`User Guide <mean_squared_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

squared : bool, default=True
    If True returns MSE value, if False returns RMSE value.

    .. deprecated:: 1.4
       `squared` is deprecated in 1.4 and will be removed in 1.6.
       Use :func:`~sklearn.metrics.root_mean_squared_error`
       instead to calculate the root mean squared error.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> mean_squared_error(y_true, y_pred)
0.708...
>>> mean_squared_error(y_true, y_pred, multioutput='raw_values')
array([0.41666667, 1.        ])
>>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.825...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                               rdfs:comment """Mean squared logarithmic error regression loss.

Read more in the :ref:`User Guide <mean_squared_log_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'

    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors when the input is of multioutput
        format.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

squared : bool, default=True
    If True returns MSLE (mean squared log error) value.
    If False returns RMSLE (root mean squared log error) value.

    .. deprecated:: 1.4
       `squared` is deprecated in 1.4 and will be removed in 1.6.
       Use :func:`~sklearn.metrics.root_mean_squared_log_error`
       instead to calculate the root mean squared logarithmic error.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> mean_squared_log_error(y_true, y_pred)
0.039...
>>> y_true = [[0.5, 1], [1, 2], [7, 6]]
>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]
>>> mean_squared_log_error(y_true, y_pred)
0.044...
>>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')
array([0.00462428, 0.08377444])
>>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.060...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                               rdfs:comment """Mean Tweedie deviance regression loss.

Read more in the :ref:`User Guide <mean_tweedie_deviance>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

power : float, default=0
    Tweedie power parameter. Either power <= 0 or power >= 1.

    The higher `p` the less weight is given to extreme
    deviations between true and predicted targets.

    - power < 0: Extreme stable distribution. Requires: y_pred > 0.
    - power = 0 : Normal distribution, output corresponds to
      mean_squared_error. y_true and y_pred can be any real numbers.
    - power = 1 : Poisson distribution. Requires: y_true >= 0 and
      y_pred > 0.
    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0
      and y_pred > 0.
    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.
    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0
      and y_pred > 0.
    - otherwise : Positive stable distribution. Requires: y_true > 0
      and y_pred > 0.

Returns
-------
loss : float
    A non-negative floating point value (the best value is 0.0).

Examples
--------
>>> from sklearn.metrics import mean_tweedie_deviance
>>> y_true = [2, 0, 1, 4]
>>> y_pred = [0.5, 0.5, 2., 2.]
>>> mean_tweedie_deviance(y_true, y_pred, power=1)
1.4260...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                               rdfs:comment """Median absolute error regression loss.

Median absolute error output is non-negative floating point. The best value
is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values. Array-like value defines
    weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

    .. versionadded:: 0.24

Returns
-------
loss : float or ndarray of floats
    If multioutput is 'raw_values', then mean absolute error is returned
    for each output separately.
    If multioutput is 'uniform_average' or an ndarray of weights, then the
    weighted average of all output errors is returned.

Examples
--------
>>> from sklearn.metrics import median_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> median_absolute_error(y_true, y_pred)
0.75
>>> median_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                        rdfs:comment """Transform features by scaling each feature to a given range.

This estimator scales and translates each feature individually such
that it is in the given range on the training set, e.g. between
zero and one.

The transformation is given by::

    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
    X_scaled = X_std * (max - min) + min

where min, max = feature_range.

This transformation is often used as an alternative to zero mean,
unit variance scaling.

`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly
scales them down into a fixed range, where the largest occurring data point
corresponds to the maximum value and the smallest one corresponds to the
minimum value. For an example visualization, refer to :ref:`Compare
MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.

Read more in the :ref:`User Guide <preprocessing_scaler>`.

Parameters
----------
feature_range : tuple (min, max), default=(0, 1)
    Desired range of transformed data.

copy : bool, default=True
    Set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array).

clip : bool, default=False
    Set to True to clip transformed values of held-out data to
    provided `feature range`.

    .. versionadded:: 0.24

Attributes
----------
min_ : ndarray of shape (n_features,)
    Per feature adjustment for minimum. Equivalent to
    ``min - X.min(axis=0) * self.scale_``

scale_ : ndarray of shape (n_features,)
    Per feature relative scaling of the data. Equivalent to
    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``

    .. versionadded:: 0.17
       *scale_* attribute.

data_min_ : ndarray of shape (n_features,)
    Per feature minimum seen in the data

    .. versionadded:: 0.17
       *data_min_*

data_max_ : ndarray of shape (n_features,)
    Per feature maximum seen in the data

    .. versionadded:: 0.17
       *data_max_*

data_range_ : ndarray of shape (n_features,)
    Per feature range ``(data_max_ - data_min_)`` seen in the data

    .. versionadded:: 0.17
       *data_range_*

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

n_samples_seen_ : int
    The number of samples processed by the estimator.
    It will be reset on new calls to fit, but increments across
    ``partial_fit`` calls.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
minmax_scale : Equivalent function without the estimator API.

Notes
-----
NaNs are treated as missing values: disregarded in fit, and maintained in
transform.

Examples
--------
>>> from sklearn.preprocessing import MinMaxScaler
>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
>>> scaler = MinMaxScaler()
>>> print(scaler.fit(data))
MinMaxScaler()
>>> print(scaler.data_max_)
[ 1. 18.]
>>> print(scaler.transform(data))
[[0.   0.  ]
 [0.25 0.25]
 [0.5  0.5 ]
 [1.   1.  ]]
>>> print(scaler.transform([[2, 2]]))
[[1.5 0. ]]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod> rdf:type owl:Class ;
                                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                       rdfs:comment """Mini-batch dictionary learning.

Finds a dictionary (a set of atoms) that performs well at sparsely
encoding the fitted data.

Solves the optimization problem::

   (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1
                (U,V)
                with || V_k ||_2 <= 1 for all  0 <= k < n_components

||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for
the entry-wise matrix norm which is the sum of the absolute values
of all the entries in the matrix.

Read more in the :ref:`User Guide <DictionaryLearning>`.

Parameters
----------
n_components : int, default=None
    Number of dictionary elements to extract.

alpha : float, default=1
    Sparsity controlling parameter.

max_iter : int, default=1_000
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion heuristics.

    .. versionadded:: 1.1

    .. deprecated:: 1.4
       `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.
       Use the default value (i.e. `1_000`) instead.

fit_algorithm : {'lars', 'cd'}, default='lars'
    The algorithm used:

    - `'lars'`: uses the least angle regression method to solve the lasso
      problem (`linear_model.lars_path`)
    - `'cd'`: uses the coordinate descent method to compute the
      Lasso solution (`linear_model.Lasso`). Lars will be faster if
      the estimated components are sparse.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

batch_size : int, default=256
    Number of samples in each mini-batch.

    .. versionchanged:: 1.3
       The default value of `batch_size` changed from 3 to 256 in version 1.3.

shuffle : bool, default=True
    Whether to shuffle the samples before forming batches.

dict_init : ndarray of shape (n_components, n_features), default=None
    Initial value of the dictionary for warm restart scenarios.

transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'
    Algorithm used to transform the data:

    - `'lars'`: uses the least angle regression method
      (`linear_model.lars_path`);
    - `'lasso_lars'`: uses Lars to compute the Lasso solution.
    - `'lasso_cd'`: uses the coordinate descent method to compute the
      Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster
      if the estimated components are sparse.
    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse
      solution.
    - `'threshold'`: squashes to zero all coefficients less than alpha from
      the projection ``dictionary * X'``.

transform_n_nonzero_coefs : int, default=None
    Number of nonzero coefficients to target in each column of the
    solution. This is only used by `algorithm='lars'` and
    `algorithm='omp'`. If `None`, then
    `transform_n_nonzero_coefs=int(n_features / 10)`.

transform_alpha : float, default=None
    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the
    penalty applied to the L1 norm.
    If `algorithm='threshold'`, `alpha` is the absolute value of the
    threshold below which coefficients will be squashed to zero.
    If `None`, defaults to `alpha`.

    .. versionchanged:: 1.2
        When None, default value changed from 1.0 to `alpha`.

verbose : bool or int, default=False
    To control the verbosity of the procedure.

split_sign : bool, default=False
    Whether to split the sparse feature vector into the concatenation of
    its negative part and its positive part. This can improve the
    performance of downstream classifiers.

random_state : int, RandomState instance or None, default=None
    Used for initializing the dictionary when ``dict_init`` is not
    specified, randomly shuffling the data when ``shuffle`` is set to
    ``True``, and updating the dictionary. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

positive_code : bool, default=False
    Whether to enforce positivity when finding the code.

    .. versionadded:: 0.20

positive_dict : bool, default=False
    Whether to enforce positivity when finding the dictionary.

    .. versionadded:: 0.20

transform_max_iter : int, default=1000
    Maximum number of iterations to perform if `algorithm='lasso_cd'` or
    `'lasso_lars'`.

    .. versionadded:: 0.22

callback : callable, default=None
    A callable that gets invoked at the end of each iteration.

    .. versionadded:: 1.1

tol : float, default=1e-3
    Control early stopping based on the norm of the differences in the
    dictionary between 2 steps.

    To disable early stopping based on changes in the dictionary, set
    `tol` to 0.0.

    .. versionadded:: 1.1

max_no_improvement : int, default=10
    Control early stopping based on the consecutive number of mini batches
    that does not yield an improvement on the smoothed cost function.

    To disable convergence detection based on cost function, set
    `max_no_improvement` to None.

    .. versionadded:: 1.1

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Components extracted from the data.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations over the full dataset.

n_steps_ : int
    Number of mini-batches processed.

    .. versionadded:: 1.1

See Also
--------
DictionaryLearning : Find a dictionary that sparsely encodes data.
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.
SparseCoder : Find a sparse representation of data from a fixed,
    precomputed dictionary.
SparsePCA : Sparse Principal Components Analysis.

References
----------

J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_sparse_coded_signal
>>> from sklearn.decomposition import MiniBatchDictionaryLearning
>>> X, dictionary, code = make_sparse_coded_signal(
...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,
...     random_state=42)
>>> dict_learner = MiniBatchDictionaryLearning(
...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',
...     transform_alpha=0.1, max_iter=20, random_state=42)
>>> X_transformed = dict_learner.fit_transform(X)

We can check the level of sparsity of `X_transformed`:

>>> np.mean(X_transformed == 0) > 0.5
True

We can compare the average squared euclidean norm of the reconstruction
error of the sparse coded signal relative to the squared euclidean norm of
the original signal:

>>> X_hat = X_transformed @ dict_learner.components_
>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))
0.052...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """Mini-Batch K-Means clustering.

Read more in the :ref:`User Guide <mini_batch_kmeans>`.

Parameters
----------

n_clusters : int, default=8
    The number of clusters to form as well as the number of
    centroids to generate.

init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'
    Method for initialization:

    'k-means++' : selects initial cluster centroids using sampling based on
    an empirical probability distribution of the points' contribution to the
    overall inertia. This technique speeds up convergence. The algorithm
    implemented is \"greedy k-means++\". It differs from the vanilla k-means++
    by making several trials at each sampling step and choosing the best centroid
    among them.

    'random': choose `n_clusters` observations (rows) at random from data
    for the initial centroids.

    If an array is passed, it should be of shape (n_clusters, n_features)
    and gives the initial centers.

    If a callable is passed, it should take arguments X, n_clusters and a
    random state and return an initialization.

max_iter : int, default=100
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion heuristics.

batch_size : int, default=1024
    Size of the mini batches.
    For faster computations, you can set the ``batch_size`` greater than
    256 * number of cores to enable parallelism on all cores.

    .. versionchanged:: 1.0
       `batch_size` default changed from 100 to 1024.

verbose : int, default=0
    Verbosity mode.

compute_labels : bool, default=True
    Compute label assignment and inertia for the complete dataset
    once the minibatch optimization has converged in fit.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for centroid initialization and
    random reassignment. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

tol : float, default=0.0
    Control early stopping based on the relative center changes as
    measured by a smoothed, variance-normalized of the mean center
    squared position changes. This early stopping heuristics is
    closer to the one used for the batch variant of the algorithms
    but induces a slight computational and memory overhead over the
    inertia heuristic.

    To disable convergence detection based on normalized center
    change, set tol to 0.0 (default).

max_no_improvement : int, default=10
    Control early stopping based on the consecutive number of mini
    batches that does not yield an improvement on the smoothed inertia.

    To disable convergence detection based on inertia, set
    max_no_improvement to None.

init_size : int, default=None
    Number of samples to randomly sample for speeding up the
    initialization (sometimes at the expense of accuracy): the
    only algorithm is initialized by running a batch KMeans on a
    random subset of the data. This needs to be larger than n_clusters.

    If `None`, the heuristic is `init_size = 3 * batch_size` if
    `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.

n_init : 'auto' or int, default=\"auto\"
    Number of random initializations that are tried.
    In contrast to KMeans, the algorithm is only run once, using the best of
    the `n_init` initializations as measured by inertia. Several runs are
    recommended for sparse high-dimensional problems (see
    :ref:`kmeans_sparse_high_dim`).

    When `n_init='auto'`, the number of runs depends on the value of init:
    3 if using `init='random'` or `init` is a callable;
    1 if using `init='k-means++'` or `init` is an array-like.

    .. versionadded:: 1.2
       Added 'auto' option for `n_init`.

    .. versionchanged:: 1.4
       Default value for `n_init` changed to `'auto'` in version.

reassignment_ratio : float, default=0.01
    Control the fraction of the maximum number of counts for a center to
    be reassigned. A higher value means that low count centers are more
    easily reassigned, which means that the model will take longer to
    converge, but should converge in a better clustering. However, too high
    a value may cause convergence issues, especially with a small batch
    size.

Attributes
----------

cluster_centers_ : ndarray of shape (n_clusters, n_features)
    Coordinates of cluster centers.

labels_ : ndarray of shape (n_samples,)
    Labels of each point (if compute_labels is set to True).

inertia_ : float
    The value of the inertia criterion associated with the chosen
    partition if compute_labels is set to True. If compute_labels is set to
    False, it's an approximation of the inertia based on an exponentially
    weighted average of the batch inertiae.
    The inertia is defined as the sum of square distances of samples to
    their cluster center, weighted by the sample weights if provided.

n_iter_ : int
    Number of iterations over the full dataset.

n_steps_ : int
    Number of minibatches processed.

    .. versionadded:: 1.0

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
KMeans : The classic implementation of the clustering method based on the
    Lloyd's algorithm. It consumes the whole set of input data at each
    iteration.

Notes
-----
See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf

When there are too few points in the dataset, some centers may be
duplicated, which means that a proper clustering in terms of the number
of requesting clusters and the number of returned clusters will not
always match. One solution is to set `reassignment_ratio=0`, which
prevents reassignments of clusters that are too small.

Examples
--------
>>> from sklearn.cluster import MiniBatchKMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 0], [4, 4],
...               [4, 5], [0, 1], [2, 2],
...               [3, 2], [5, 5], [1, -1]])
>>> # manually fit on batches
>>> kmeans = MiniBatchKMeans(n_clusters=2,
...                          random_state=0,
...                          batch_size=6,
...                          n_init=\"auto\")
>>> kmeans = kmeans.partial_fit(X[0:6,:])
>>> kmeans = kmeans.partial_fit(X[6:12,:])
>>> kmeans.cluster_centers_
array([[3.375, 3.  ],
       [0.75 , 0.5 ]])
>>> kmeans.predict([[0, 0], [4, 4]])
array([1, 0], dtype=int32)
>>> # fit on the whole data
>>> kmeans = MiniBatchKMeans(n_clusters=2,
...                          random_state=0,
...                          batch_size=6,
...                          max_iter=10,
...                          n_init=\"auto\").fit(X)
>>> kmeans.cluster_centers_
array([[3.55102041, 2.48979592],
       [1.06896552, 1.        ]])
>>> kmeans.predict([[0, 0], [4, 4]])
array([1, 0], dtype=int32)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                        rdfs:comment """Mini-Batch Non-Negative Matrix Factorization (NMF).

.. versionadded:: 1.1

Find two non-negative matrices, i.e. matrices with all non-negative elements,
(`W`, `H`) whose product approximates the non-negative matrix `X`. This
factorization can be used for example for dimensionality reduction, source
separation or topic extraction.

The objective function is:

    .. math::

        L(W, H) &= 0.5 * ||X - WH||_{loss}^2

        &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1

        &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1

        &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2

        &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2

Where:

:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)

:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)

The generic norm :math:`||X - WH||_{loss}^2` may represent
the Frobenius norm or another supported beta-divergence loss.
The choice between options is controlled by the `beta_loss` parameter.

The objective function is minimized with an alternating minimization of `W`
and `H`.

Note that the transformed data is named `W` and the components matrix is
named `H`. In the NMF literature, the naming convention is usually the opposite
since the data matrix `X` is transposed.

Read more in the :ref:`User Guide <MiniBatchNMF>`.

Parameters
----------
n_components : int or {'auto'} or None, default=None
    Number of components, if `n_components` is not set all features
    are kept.
    If `n_components='auto'`, the number of components is automatically inferred
    from W or H shapes.

    .. versionchanged:: 1.4
        Added `'auto'` value.

init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
    Method used to initialize the procedure.
    Valid options:

    - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,
      otherwise random.

    - `'random'`: non-negative random matrices, scaled with:
      `sqrt(X.mean() / n_components)`

    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)
      initialization (better for sparseness).

    - `'nndsvda'`: NNDSVD with zeros filled with the average of X
      (better when sparsity is not desired).

    - `'nndsvdar'` NNDSVD with zeros filled with small random values
      (generally faster, less accurate alternative to NNDSVDa
      for when sparsity is not desired).

    - `'custom'`: Use custom matrices `W` and `H` which must both be provided.

batch_size : int, default=1024
    Number of samples in each mini-batch. Large batch sizes
    give better long-term convergence at the cost of a slower start.

beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'
    Beta divergence to be minimized, measuring the distance between `X`
    and the dot product `WH`. Note that values different from 'frobenius'
    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
    fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input
    matrix `X` cannot contain zeros.

tol : float, default=1e-4
    Control early stopping based on the norm of the differences in `H`
    between 2 steps. To disable early stopping based on changes in `H`, set
    `tol` to 0.0.

max_no_improvement : int, default=10
    Control early stopping based on the consecutive number of mini batches
    that does not yield an improvement on the smoothed cost function.
    To disable convergence detection based on cost function, set
    `max_no_improvement` to None.

max_iter : int, default=200
    Maximum number of iterations over the complete dataset before
    timing out.

alpha_W : float, default=0.0
    Constant that multiplies the regularization terms of `W`. Set it to zero
    (default) to have no regularization on `W`.

alpha_H : float or \"same\", default=\"same\"
    Constant that multiplies the regularization terms of `H`. Set it to zero to
    have no regularization on `H`. If \"same\" (default), it takes the same value as
    `alpha_W`.

l1_ratio : float, default=0.0
    The regularization mixing parameter, with 0 <= l1_ratio <= 1.
    For l1_ratio = 0 the penalty is an elementwise L2 penalty
    (aka Frobenius Norm).
    For l1_ratio = 1 it is an elementwise L1 penalty.
    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.

forget_factor : float, default=0.7
    Amount of rescaling of past information. Its value could be 1 with
    finite datasets. Choosing values < 1 is recommended with online
    learning as more recent batches will weight more than past batches.

fresh_restarts : bool, default=False
    Whether to completely solve for W at each step. Doing fresh restarts will likely
    lead to a better solution for a same number of iterations but it is much slower.

fresh_restarts_max_iter : int, default=30
    Maximum number of iterations when solving for W at each step. Only used when
    doing fresh restarts. These iterations may be stopped early based on a small
    change of W controlled by `tol`.

transform_max_iter : int, default=None
    Maximum number of iterations when solving for W at transform time.
    If None, it defaults to `max_iter`.

random_state : int, RandomState instance or None, default=None
    Used for initialisation (when ``init`` == 'nndsvdar' or
    'random'), and in Coordinate Descent. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

verbose : bool, default=False
    Whether to be verbose.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Factorization matrix, sometimes called 'dictionary'.

n_components_ : int
    The number of components. It is same as the `n_components` parameter
    if it was given. Otherwise, it will be same as the number of
    features.

reconstruction_err_ : float
    Frobenius norm of the matrix difference, or beta-divergence, between
    the training data `X` and the reconstructed data `WH` from
    the fitted model.

n_iter_ : int
    Actual number of started iterations over the whole dataset.

n_steps_ : int
    Number of mini-batches processed.

n_features_in_ : int
    Number of features seen during :term:`fit`.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

See Also
--------
NMF : Non-negative matrix factorization.
MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent
    data using a sparse code.

References
----------
.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor
   factorizations\" <10.1587/transfun.E92.A.708>`
   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals
   of electronics, communications and computer sciences 92.3: 708-721, 2009.

.. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the
   beta-divergence\" <10.1162/NECO_a_00168>`
   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).

.. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the
   Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`
   Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.

Examples
--------
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
>>> from sklearn.decomposition import MiniBatchNMF
>>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)
>>> W = model.fit_transform(X)
>>> H = model.components_""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                              rdfs:comment """Mini-batch Sparse Principal Components Analysis.

Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.

For an example comparing sparse PCA to PCA, see
:ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`

Read more in the :ref:`User Guide <SparsePCA>`.

Parameters
----------
n_components : int, default=None
    Number of sparse atoms to extract. If None, then ``n_components``
    is set to ``n_features``.

alpha : int, default=1
    Sparsity controlling parameter. Higher values lead to sparser
    components.

ridge_alpha : float, default=0.01
    Amount of ridge shrinkage to apply in order to improve
    conditioning when calling the transform method.

max_iter : int, default=1_000
    Maximum number of iterations over the complete dataset before
    stopping independently of any early stopping criterion heuristics.

    .. versionadded:: 1.2

    .. deprecated:: 1.4
       `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.
       Use the default value (i.e. `100`) instead.

callback : callable, default=None
    Callable that gets invoked every five iterations.

batch_size : int, default=3
    The number of features to take in each mini batch.

verbose : int or bool, default=False
    Controls the verbosity; the higher, the more messages. Defaults to 0.

shuffle : bool, default=True
    Whether to shuffle the data before splitting it in batches.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

method : {'lars', 'cd'}, default='lars'
    Method to be used for optimization.
    lars: uses the least angle regression method to solve the lasso problem
    (linear_model.lars_path)
    cd: uses the coordinate descent method to compute the
    Lasso solution (linear_model.Lasso). Lars will be faster if
    the estimated components are sparse.

random_state : int, RandomState instance or None, default=None
    Used for random shuffling when ``shuffle`` is set to ``True``,
    during online dictionary learning. Pass an int for reproducible results
    across multiple function calls.
    See :term:`Glossary <random_state>`.

tol : float, default=1e-3
    Control early stopping based on the norm of the differences in the
    dictionary between 2 steps.

    To disable early stopping based on changes in the dictionary, set
    `tol` to 0.0.

    .. versionadded:: 1.1

max_no_improvement : int or None, default=10
    Control early stopping based on the consecutive number of mini batches
    that does not yield an improvement on the smoothed cost function.

    To disable convergence detection based on cost function, set
    `max_no_improvement` to `None`.

    .. versionadded:: 1.1

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Sparse components extracted from the data.

n_components_ : int
    Estimated number of components.

    .. versionadded:: 0.23

n_iter_ : int
    Number of iterations run.

mean_ : ndarray of shape (n_features,)
    Per-feature empirical mean, estimated from the training set.
    Equal to ``X.mean(axis=0)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
DictionaryLearning : Find a dictionary that sparsely encodes data.
IncrementalPCA : Incremental principal components analysis.
PCA : Principal component analysis.
SparsePCA : Sparse Principal Components Analysis.
TruncatedSVD : Dimensionality reduction using truncated SVD.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.decomposition import MiniBatchSparsePCA
>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
>>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,
...                                  max_iter=10, random_state=0)
>>> transformer.fit(X)
MiniBatchSparsePCA(...)
>>> X_transformed = transformer.transform(X)
>>> X_transformed.shape
(200, 5)
>>> # most values in the components_ are zero (sparsity)
>>> np.mean(transformer.components_ == 0)
0.9...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                            rdfs:comment """Binary indicators for missing values.

Note that this component typically should not be used in a vanilla
:class:`~sklearn.pipeline.Pipeline` consisting of transformers and a
classifier, but rather could be added using a
:class:`~sklearn.pipeline.FeatureUnion` or
:class:`~sklearn.compose.ColumnTransformer`.

Read more in the :ref:`User Guide <impute>`.

.. versionadded:: 0.20

Parameters
----------
missing_values : int, float, str, np.nan or None, default=np.nan
    The placeholder for the missing values. All occurrences of
    `missing_values` will be imputed. For pandas' dataframes with
    nullable integer dtypes with missing values, `missing_values`
    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.

features : {'missing-only', 'all'}, default='missing-only'
    Whether the imputer mask should represent all or a subset of
    features.

    - If `'missing-only'` (default), the imputer mask will only represent
      features containing missing values during fit time.
    - If `'all'`, the imputer mask will represent all features.

sparse : bool or 'auto', default='auto'
    Whether the imputer mask format should be sparse or dense.

    - If `'auto'` (default), the imputer mask will be of same type as
      input.
    - If `True`, the imputer mask will be a sparse matrix.
    - If `False`, the imputer mask will be a numpy array.

error_on_new : bool, default=True
    If `True`, :meth:`transform` will raise an error when there are
    features with missing values that have no missing values in
    :meth:`fit`. This is applicable only when `features='missing-only'`.

Attributes
----------
features_ : ndarray of shape (n_missing_features,) or (n_features,)
    The features indices which will be returned when calling
    :meth:`transform`. They are computed during :meth:`fit`. If
    `features='all'`, `features_` is equal to `range(n_features)`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SimpleImputer : Univariate imputation of missing values.
IterativeImputer : Multivariate imputation of missing values.

Examples
--------
>>> import numpy as np
>>> from sklearn.impute import MissingIndicator
>>> X1 = np.array([[np.nan, 1, 3],
...                [4, 0, np.nan],
...                [8, 1, 0]])
>>> X2 = np.array([[5, 1, np.nan],
...                [np.nan, 2, 3],
...                [2, 4, 0]])
>>> indicator = MissingIndicator()
>>> indicator.fit(X1)
MissingIndicator()
>>> X2_tr = indicator.transform(X2)
>>> X2_tr
array([[False,  True],
       [ True, False],
       [False, False]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelection
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelection> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                               rdfs:comment """Transform between iterable of iterables and a multilabel format.

Although a list of sets or tuples is a very intuitive format for multilabel
data, it is unwieldy to process. This transformer converts between this
intuitive format and the supported multilabel format: a (samples x classes)
binary matrix indicating the presence of a class label.

Parameters
----------
classes : array-like of shape (n_classes,), default=None
    Indicates an ordering for the class labels.
    All entries should be unique (cannot contain duplicate classes).

sparse_output : bool, default=False
    Set to True if output binary array is desired in CSR sparse format.

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    A copy of the `classes` parameter when provided.
    Otherwise it corresponds to the sorted set of classes found
    when fitting.

See Also
--------
OneHotEncoder : Encode categorical features using a one-hot aka one-of-K
    scheme.

Examples
--------
>>> from sklearn.preprocessing import MultiLabelBinarizer
>>> mlb = MultiLabelBinarizer()
>>> mlb.fit_transform([(1, 2), (3,)])
array([[1, 1, 0],
       [0, 0, 1]])
>>> mlb.classes_
array([1, 2, 3])

>>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])
array([[0, 1, 1],
       [1, 0, 0]])
>>> list(mlb.classes_)
['comedy', 'sci-fi', 'thriller']

A common mistake is to pass in a list, which leads to the following issue:

>>> mlb = MultiLabelBinarizer()
>>> mlb.fit(['sci-fi', 'thriller', 'comedy'])
MultiLabelBinarizer()
>>> mlb.classes_
array(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't',
    'y'], dtype=object)

To correct this, the list of labels should be passed in as:

>>> mlb = MultiLabelBinarizer()
>>> mlb.fit([['sci-fi', 'thriller', 'comedy']])
MultiLabelBinarizer()
>>> mlb.classes_
array(['comedy', 'sci-fi', 'thriller'], dtype=object)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                 rdfs:comment """Multi-task L1/L2 ElasticNet with built-in cross-validation.

See glossary entry for :term:`cross-validation estimator`.

The optimization objective for MultiTaskElasticNet is::

    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2
    + alpha * l1_ratio * ||W||_21
    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2

Where::

    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}

i.e. the sum of norm of each row.

Read more in the :ref:`User Guide <multi_task_elastic_net>`.

.. versionadded:: 0.15

Parameters
----------
l1_ratio : float or list of float, default=0.5
    The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.
    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
    is an L2 penalty.
    For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.
    This parameter can be a list, in which case the different
    values are tested by cross-validation and the one giving the best
    prediction score is used. Note that a good choice of list of
    values for l1_ratio is often to put more values close to 1
    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,
    .9, .95, .99, 1]``.

eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If not provided, set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

verbose : bool or int, default=0
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation. Note that this is
    used only if multiple values for l1_ratio are given.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula).
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

alpha_ : float
    The amount of penalization chosen by cross validation.

mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)
    Mean square error for the test set on each fold, varying alpha.

alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)
    The grid of alphas used for fitting, for each l1_ratio.

l1_ratio_ : float
    Best l1_ratio obtained by cross-validation.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

dual_gap_ : float
    The dual gap at the end of the optimization for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.
ElasticNetCV : Elastic net model with best model selection by
    cross-validation.
MultiTaskLassoCV : Multi-task Lasso model trained with L1 norm
    as regularizer and built-in cross-validation.

Notes
-----
The algorithm used to fit the model is coordinate descent.

In `fit`, once the best parameters `l1_ratio` and `alpha` are found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` and `y` arguments of the
`fit` method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.MultiTaskElasticNetCV(cv=3)
>>> clf.fit([[0,0], [1, 1], [2, 2]],
...         [[0, 0], [1, 1], [2, 2]])
MultiTaskElasticNetCV(cv=3)
>>> print(clf.coef_)
[[0.52875032 0.46958558]
 [0.52875032 0.46958558]]
>>> print(clf.intercept_)
[0.00166409 0.00166409]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                               rdfs:comment """Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.

The optimization objective for MultiTaskElasticNet is::

    (1 / (2 * n_samples)) * ||Y - XW||_Fro^2
    + alpha * l1_ratio * ||W||_21
    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2

Where::

    ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)

i.e. the sum of norms of each row.

Read more in the :ref:`User Guide <multi_task_elastic_net>`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the L1/L2 term. Defaults to 1.0.

l1_ratio : float, default=0.5
    The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.
    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
    is an L2 penalty.
    For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula). If a 1D y is
    passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

dual_gap_ : float
    The dual gaps at the end of the optimization.

eps_ : float
    The tolerance scaled scaled by the variance of the target `y`.

sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)
    Sparse representation of the `coef_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in
    cross-validation.
ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.
MultiTaskLasso : Multi-task Lasso model trained with L1/L2
    mixed-norm as regularizer.

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)
>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
MultiTaskElasticNet(alpha=0.1)
>>> print(clf.coef_)
[[0.45663524 0.45612256]
 [0.45663524 0.45612256]]
>>> print(clf.intercept_)
[0.0872422 0.0872422]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.

See glossary entry for :term:`cross-validation estimator`.

The optimization objective for MultiTaskLasso is::

    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21

Where::

    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}

i.e. the sum of norm of each row.

Read more in the :ref:`User Guide <multi_task_lasso>`.

.. versionadded:: 0.15

Parameters
----------
eps : float, default=1e-3
    Length of the path. ``eps=1e-3`` means that
    ``alpha_min / alpha_max = 1e-3``.

n_alphas : int, default=100
    Number of alphas along the regularization path.

alphas : array-like, default=None
    List of alphas where to compute the models.
    If not provided, set automatically.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - int, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

verbose : bool or int, default=False
    Amount of verbosity.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation. Note that this is
    used only if multiple values for l1_ratio are given.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula).
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

alpha_ : float
    The amount of penalization chosen by cross validation.

mse_path_ : ndarray of shape (n_alphas, n_folds)
    Mean square error for the test set on each fold, varying alpha.

alphas_ : ndarray of shape (n_alphas,)
    The grid of alphas used for fitting.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance for the optimal alpha.

dual_gap_ : float
    The dual gap at the end of the optimization for the optimal alpha.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2
    mixed-norm as regularizer.
ElasticNetCV : Elastic net model with best model selection by
    cross-validation.
MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in
    cross-validation.

Notes
-----
The algorithm used to fit the model is coordinate descent.

In `fit`, once the best parameter `alpha` is found through
cross-validation, the model is fit again using the entire training set.

To avoid unnecessary memory duplication the `X` and `y` arguments of the
`fit` method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
>>> from sklearn.linear_model import MultiTaskLassoCV
>>> from sklearn.datasets import make_regression
>>> from sklearn.metrics import r2_score
>>> X, y = make_regression(n_targets=2, noise=4, random_state=0)
>>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)
>>> r2_score(y, reg.predict(X))
0.9994...
>>> reg.alpha_
0.5713...
>>> reg.predict(X[:1,])
array([[153.7971...,  94.9015...]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                          rdfs:comment """Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.

The optimization objective for Lasso is::

    (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21

Where::

    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}

i.e. the sum of norm of each row.

Read more in the :ref:`User Guide <multi_task_lasso>`.

Parameters
----------
alpha : float, default=1.0
    Constant that multiplies the L1/L2 term. Defaults to 1.0.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

copy_X : bool, default=True
    If ``True``, X will be copied; else, it may be overwritten.

max_iter : int, default=1000
    The maximum number of iterations.

tol : float, default=1e-4
    The tolerance for the optimization: if the updates are
    smaller than ``tol``, the optimization code checks the
    dual gap for optimality and continues until it is smaller
    than ``tol``.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator that selects a random
    feature to update. Used when ``selection`` == 'random'.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

selection : {'cyclic', 'random'}, default='cyclic'
    If set to 'random', a random coefficient is updated every iteration
    rather than looping over features sequentially by default. This
    (setting to 'random') often leads to significantly faster convergence
    especially when tol is higher than 1e-4.

Attributes
----------
coef_ : ndarray of shape (n_targets, n_features)
    Parameter vector (W in the cost function formula).
    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.

intercept_ : ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : int
    Number of iterations run by the coordinate descent solver to reach
    the specified tolerance.

dual_gap_ : ndarray of shape (n_alphas,)
    The dual gaps at the end of the optimization for each alpha.

eps_ : float
    The tolerance scaled scaled by the variance of the target `y`.

sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)
    Sparse representation of the `coef_`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).
MultiTaskLassoCV: Multi-task L1 regularized linear model with built-in
    cross-validation.
MultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.

Notes
-----
The algorithm used to fit the model is coordinate descent.

To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.MultiTaskLasso(alpha=0.1)
>>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])
MultiTaskLasso(alpha=0.1)
>>> print(clf.coef_)
[[0.         0.60809415]
[0.         0.94592424]]
>>> print(clf.intercept_)
[-0.41888636 -0.87382323]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelClassification
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelClassification> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                     rdfs:comment """Compute a confusion matrix for each class or sample.

.. versionadded:: 0.21

Compute class-wise (default) or sample-wise (samplewise=True) multilabel
confusion matrix to evaluate the accuracy of a classification, and output
confusion matrices for each class or sample.

In multilabel confusion matrix :math:`MCM`, the count of true negatives
is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,
true positives is :math:`MCM_{:,1,1}` and false positives is
:math:`MCM_{:,0,1}`.

Multiclass data will be treated as if binarized under a one-vs-rest
transformation. Returned confusion matrices will be in the order of
sorted unique labels in the union of (y_true, y_pred).

Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.

Parameters
----------
y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)
    Ground truth (correct) target values.

y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)
    Estimated targets as returned by a classifier.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

labels : array-like of shape (n_classes,), default=None
    A list of classes or column indices to select some (or to force
    inclusion of classes absent from the data).

samplewise : bool, default=False
    In the multilabel case, this calculates a confusion matrix per sample.

Returns
-------
multi_confusion : ndarray of shape (n_outputs, 2, 2)
    A 2x2 confusion matrix corresponding to each output in the input.
    When calculating class-wise multi_confusion (default), then
    n_outputs = n_labels; when calculating sample-wise multi_confusion
    (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,
    the results will be returned in the order specified in ``labels``,
    otherwise the results will be returned in sorted order by default.

See Also
--------
confusion_matrix : Compute confusion matrix to evaluate the accuracy of a
    classifier.

Notes
-----
The `multilabel_confusion_matrix` calculates class-wise or sample-wise
multilabel confusion matrices, and in multiclass tasks, labels are
binarized under a one-vs-rest way; while
:func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix
for confusion between every two classes.

Examples
--------
Multilabel-indicator case:

>>> import numpy as np
>>> from sklearn.metrics import multilabel_confusion_matrix
>>> y_true = np.array([[1, 0, 1],
...                    [0, 1, 0]])
>>> y_pred = np.array([[1, 0, 0],
...                    [0, 1, 1]])
>>> multilabel_confusion_matrix(y_true, y_pred)
array([[[1, 0],
        [0, 1]],
<BLANKLINE>
       [[1, 0],
        [0, 1]],
<BLANKLINE>
       [[0, 1],
        [1, 0]]])

Multiclass case:

>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]
>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]
>>> multilabel_confusion_matrix(y_true, y_pred,
...                             labels=[\"ant\", \"bird\", \"cat\"])
array([[[3, 1],
        [0, 2]],
<BLANKLINE>
       [[5, 0],
        [1, 0]],
<BLANKLINE>
       [[2, 1],
        [1, 2]]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Naive Bayes classifier for multinomial models.

The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.

Read more in the :ref:`User Guide <multinomial_naive_bayes>`.

Parameters
----------
alpha : float or array-like of shape (n_features,), default=1.0
    Additive (Laplace/Lidstone) smoothing parameter
    (set alpha=0 and force_alpha=True, for no smoothing).

force_alpha : bool, default=True
    If False and alpha is less than 1e-10, it will set alpha to
    1e-10. If True, alpha will remain unchanged. This may cause
    numerical errors if alpha is too close to 0.

    .. versionadded:: 1.2
    .. versionchanged:: 1.4
       The default value of `force_alpha` changed to `True`.

fit_prior : bool, default=True
    Whether to learn class prior probabilities or not.
    If false, a uniform prior will be used.

class_prior : array-like of shape (n_classes,), default=None
    Prior probabilities of the classes. If specified, the priors are not
    adjusted according to the data.

Attributes
----------
class_count_ : ndarray of shape (n_classes,)
    Number of samples encountered for each class during fitting. This
    value is weighted by the sample weight when provided.

class_log_prior_ : ndarray of shape (n_classes,)
    Smoothed empirical log probability for each class.

classes_ : ndarray of shape (n_classes,)
    Class labels known to the classifier

feature_count_ : ndarray of shape (n_classes, n_features)
    Number of samples encountered for each (class, feature)
    during fitting. This value is weighted by the sample weight when
    provided.

feature_log_prob_ : ndarray of shape (n_classes, n_features)
    Empirical log probability of features
    given a class, ``P(x_i|y)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
CategoricalNB : Naive Bayes classifier for categorical features.
ComplementNB : Complement Naive Bayes classifier.
GaussianNB : Gaussian Naive Bayes.

References
----------
C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html

Examples
--------
>>> import numpy as np
>>> rng = np.random.RandomState(1)
>>> X = rng.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB()
>>> print(clf.predict(X[2:3]))
[3]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                             rdfs:comment """Estimate mutual information for a discrete target variable.

Mutual information (MI) [1]_ between two random variables is a non-negative
value, which measures the dependency between the variables. It is equal
to zero if and only if two random variables are independent, and higher
values mean higher dependency.

The function relies on nonparametric methods based on entropy estimation
from k-nearest neighbors distances as described in [2]_ and [3]_. Both
methods are based on the idea originally proposed in [4]_.

It can be used for univariate features selection, read more in the
:ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Feature matrix.

y : array-like of shape (n_samples,)
    Target vector.

discrete_features : 'auto', bool or array-like, default='auto'
    If bool, then determines whether to consider all features discrete
    or continuous. If array, then it should be either a boolean mask
    with shape (n_features,) or array with indices of discrete features.
    If 'auto', it is assigned to False for dense `X` and to True for
    sparse `X`.

n_neighbors : int, default=3
    Number of neighbors to use for MI estimation for continuous variables,
    see [2]_ and [3]_. Higher values reduce variance of the estimation, but
    could introduce a bias.

copy : bool, default=True
    Whether to make a copy of the given data. If set to False, the initial
    data will be overwritten.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for adding small noise to
    continuous variables in order to remove repeated values.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Returns
-------
mi : ndarray, shape (n_features,)
    Estimated mutual information between each feature and the target in
    nat units.

Notes
-----
1. The term \"discrete features\" is used instead of naming them
   \"categorical\", because it describes the essence more accurately.
   For example, pixel intensities of an image are discrete features
   (but hardly categorical) and you will get better results if mark them
   as such. Also note, that treating a continuous variable as discrete and
   vice versa will usually give incorrect results, so be attentive about
   that.
2. True mutual information can't be negative. If its estimate turns out
   to be negative, it is replaced by zero.

References
----------
.. [1] `Mutual Information
       <https://en.wikipedia.org/wiki/Mutual_information>`_
       on Wikipedia.
.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual
       information\". Phys. Rev. E 69, 2004.
.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous
       Data Sets\". PLoS ONE 9(2), 2014.
.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy
       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16

Examples
--------
>>> from sklearn.datasets import make_classification
>>> from sklearn.feature_selection import mutual_info_classif
>>> X, y = make_classification(
...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,
...     shuffle=False, random_state=42
... )
>>> mutual_info_classif(X, y)
array([0.58..., 0.10..., 0.19..., 0.09... , 0.        ,
       0.     , 0.     , 0.     , 0.      , 0.        ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                rdfs:comment """Estimate mutual information for a continuous target variable.

Mutual information (MI) [1]_ between two random variables is a non-negative
value, which measures the dependency between the variables. It is equal
to zero if and only if two random variables are independent, and higher
values mean higher dependency.

The function relies on nonparametric methods based on entropy estimation
from k-nearest neighbors distances as described in [2]_ and [3]_. Both
methods are based on the idea originally proposed in [4]_.

It can be used for univariate features selection, read more in the
:ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
X : array-like or sparse matrix, shape (n_samples, n_features)
    Feature matrix.

y : array-like of shape (n_samples,)
    Target vector.

discrete_features : {'auto', bool, array-like}, default='auto'
    If bool, then determines whether to consider all features discrete
    or continuous. If array, then it should be either a boolean mask
    with shape (n_features,) or array with indices of discrete features.
    If 'auto', it is assigned to False for dense `X` and to True for
    sparse `X`.

n_neighbors : int, default=3
    Number of neighbors to use for MI estimation for continuous variables,
    see [2]_ and [3]_. Higher values reduce variance of the estimation, but
    could introduce a bias.

copy : bool, default=True
    Whether to make a copy of the given data. If set to False, the initial
    data will be overwritten.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for adding small noise to
    continuous variables in order to remove repeated values.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Returns
-------
mi : ndarray, shape (n_features,)
    Estimated mutual information between each feature and the target in
    nat units.

Notes
-----
1. The term \"discrete features\" is used instead of naming them
   \"categorical\", because it describes the essence more accurately.
   For example, pixel intensities of an image are discrete features
   (but hardly categorical) and you will get better results if mark them
   as such. Also note, that treating a continuous variable as discrete and
   vice versa will usually give incorrect results, so be attentive about
   that.
2. True mutual information can't be negative. If its estimate turns out
   to be negative, it is replaced by zero.

References
----------
.. [1] `Mutual Information
       <https://en.wikipedia.org/wiki/Mutual_information>`_
       on Wikipedia.
.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual
       information\". Phys. Rev. E 69, 2004.
.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous
       Data Sets\". PLoS ONE 9(2), 2014.
.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy
       of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16

Examples
--------
>>> from sklearn.datasets import make_regression
>>> from sklearn.feature_selection import mutual_info_regression
>>> X, y = make_regression(
...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
... )
>>> mutual_info_regression(X, y)
array([0.1..., 2.6...  , 0.0...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                           rdfs:comment """Mutual Information between two clusterings.

The Mutual Information is a measure of the similarity between two labels
of the same data. Where :math:`|U_i|` is the number of the samples
in cluster :math:`U_i` and :math:`|V_j|` is the number of the
samples in cluster :math:`V_j`, the Mutual Information
between clusterings :math:`U` and :math:`V` is given as:

.. math::

    MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}
    \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching :math:`U` (i.e
``label_true``) with :math:`V` (i.e. ``label_pred``) will return the
same score value. This can be useful to measure the agreement of two
independent label assignments strategies on the same dataset when the
real ground truth is not known.

Read more in the :ref:`User Guide <mutual_info_score>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=integral
    A clustering of the data into disjoint subsets, called :math:`U` in
    the above formula.

labels_pred : array-like of shape (n_samples,), dtype=integral
    A clustering of the data into disjoint subsets, called :math:`V` in
    the above formula.

contingency : {array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None
    A contingency matrix given by the
    :func:`~sklearn.metrics.cluster.contingency_matrix` function. If value
    is ``None``, it will be computed, otherwise the given value is used,
    with ``labels_true`` and ``labels_pred`` ignored.

Returns
-------
mi : float
   Mutual information, a non-negative value, measured in nats using the
   natural logarithm.

See Also
--------
adjusted_mutual_info_score : Adjusted against chance Mutual Information.
normalized_mutual_info_score : Normalized Mutual Information.

Notes
-----
The logarithm used is the natural logarithm (base-e).

Examples
--------
>>> from sklearn.metrics import mutual_info_score
>>> labels_true = [0, 1, 1, 0, 1, 0]
>>> labels_pred = [0, 1, 0, 0, 1, 1]
>>> mutual_info_score(labels_true, labels_pred)
0.056...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                               rdfs:comment """Non-Negative Matrix Factorization (NMF).

Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)
whose product approximates the non-negative matrix X. This factorization can be used
for example for dimensionality reduction, source separation or topic extraction.

The objective function is:

    .. math::

        L(W, H) &= 0.5 * ||X - WH||_{loss}^2

        &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1

        &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1

        &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2

        &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2

Where:

:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)

:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)

The generic norm :math:`||X - WH||_{loss}` may represent
the Frobenius norm or another supported beta-divergence loss.
The choice between options is controlled by the `beta_loss` parameter.

The regularization terms are scaled by `n_features` for `W` and by `n_samples` for
`H` to keep their impact balanced with respect to one another and to the data fit
term as independent as possible of the size `n_samples` of the training set.

The objective function is minimized with an alternating minimization of W
and H.

Note that the transformed data is named W and the components matrix is named H. In
the NMF literature, the naming convention is usually the opposite since the data
matrix X is transposed.

Read more in the :ref:`User Guide <NMF>`.

Parameters
----------
n_components : int or {'auto'} or None, default=None
    Number of components, if n_components is not set all features
    are kept.
    If `n_components='auto'`, the number of components is automatically inferred
    from W or H shapes.

    .. versionchanged:: 1.4
        Added `'auto'` value.

init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
    Method used to initialize the procedure.
    Valid options:

    - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),
      otherwise random.

    - `'random'`: non-negative random matrices, scaled with:
      `sqrt(X.mean() / n_components)`

    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)
      initialization (better for sparseness)

    - `'nndsvda'`: NNDSVD with zeros filled with the average of X
      (better when sparsity is not desired)

    - `'nndsvdar'` NNDSVD with zeros filled with small random values
      (generally faster, less accurate alternative to NNDSVDa
      for when sparsity is not desired)

    - `'custom'`: Use custom matrices `W` and `H` which must both be provided.

    .. versionchanged:: 1.1
        When `init=None` and n_components is less than n_samples and n_features
        defaults to `nndsvda` instead of `nndsvd`.

solver : {'cd', 'mu'}, default='cd'
    Numerical solver to use:

    - 'cd' is a Coordinate Descent solver.
    - 'mu' is a Multiplicative Update solver.

    .. versionadded:: 0.17
       Coordinate Descent solver.

    .. versionadded:: 0.19
       Multiplicative Update solver.

beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'
    Beta divergence to be minimized, measuring the distance between X
    and the dot product WH. Note that values different from 'frobenius'
    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input
    matrix X cannot contain zeros. Used only in 'mu' solver.

    .. versionadded:: 0.19

tol : float, default=1e-4
    Tolerance of the stopping condition.

max_iter : int, default=200
    Maximum number of iterations before timing out.

random_state : int, RandomState instance or None, default=None
    Used for initialisation (when ``init`` == 'nndsvdar' or
    'random'), and in Coordinate Descent. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary <random_state>`.

alpha_W : float, default=0.0
    Constant that multiplies the regularization terms of `W`. Set it to zero
    (default) to have no regularization on `W`.

    .. versionadded:: 1.0

alpha_H : float or \"same\", default=\"same\"
    Constant that multiplies the regularization terms of `H`. Set it to zero to
    have no regularization on `H`. If \"same\" (default), it takes the same value as
    `alpha_W`.

    .. versionadded:: 1.0

l1_ratio : float, default=0.0
    The regularization mixing parameter, with 0 <= l1_ratio <= 1.
    For l1_ratio = 0 the penalty is an elementwise L2 penalty
    (aka Frobenius Norm).
    For l1_ratio = 1 it is an elementwise L1 penalty.
    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.

    .. versionadded:: 0.17
       Regularization parameter *l1_ratio* used in the Coordinate Descent
       solver.

verbose : int, default=0
    Whether to be verbose.

shuffle : bool, default=False
    If true, randomize the order of coordinates in the CD solver.

    .. versionadded:: 0.17
       *shuffle* parameter used in the Coordinate Descent solver.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Factorization matrix, sometimes called 'dictionary'.

n_components_ : int
    The number of components. It is same as the `n_components` parameter
    if it was given. Otherwise, it will be same as the number of
    features.

reconstruction_err_ : float
    Frobenius norm of the matrix difference, or beta-divergence, between
    the training data ``X`` and the reconstructed data ``WH`` from
    the fitted model.

n_iter_ : int
    Actual number of iterations.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
DictionaryLearning : Find a dictionary that sparsely encodes data.
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.
PCA : Principal component analysis.
SparseCoder : Find a sparse representation of data from a fixed,
    precomputed dictionary.
SparsePCA : Sparse Principal Components Analysis.
TruncatedSVD : Dimensionality reduction using truncated SVD.

References
----------
.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor
   factorizations\" <10.1587/transfun.E92.A.708>`
   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals
   of electronics, communications and computer sciences 92.3: 708-721, 2009.

.. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the
   beta-divergence\" <10.1162/NECO_a_00168>`
   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).

Examples
--------
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
>>> from sklearn.decomposition import NMF
>>> model = NMF(n_components=2, init='random', random_state=0)
>>> W = model.fit_transform(X)
>>> H = model.components_""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                 rdfs:comment """Calculate the euclidean distances in the presence of missing values.

Compute the euclidean distance between each pair of samples in X and Y,
where Y=X is assumed if Y=None. When calculating the distance between a
pair of samples, this formulation ignores feature coordinates with a
missing value in either sample and scales up the weight of the remaining
coordinates:

    dist(x,y) = sqrt(weight * sq. distance from present coordinates)
    where,
    weight = Total # of coordinates / # of present coordinates

For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``
is:

    .. math::
        \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}

If all the coordinates are missing or if there are no common present
coordinates then NaN is returned for that pair.

Read more in the :ref:`User Guide <metrics>`.

.. versionadded:: 0.22

Parameters
----------
X : array-like of shape (n_samples_X, n_features)
    An array where each row is a sample and each column is a feature.

Y : array-like of shape (n_samples_Y, n_features), default=None
    An array where each row is a sample and each column is a feature.
    If `None`, method uses `Y=X`.

squared : bool, default=False
    Return squared Euclidean distances.

missing_values : np.nan, float or int, default=np.nan
    Representation of missing value.

copy : bool, default=True
    Make and use a deep copy of X and Y (if Y exists).

Returns
-------
distances : ndarray of shape (n_samples_X, n_samples_Y)
    Returns the distances between the row vectors of `X`
    and the row vectors of `Y`.

See Also
--------
paired_distances : Distances between pairs of elements of X and Y.

References
----------
* John K. Dixon, \"Pattern Recognition with Partly Missing Data\",
  IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:
  10, pp. 617 - 621, Oct. 1979.
  http://ieeexplore.ieee.org/abstract/document/4310090/

Examples
--------
>>> from sklearn.metrics.pairwise import nan_euclidean_distances
>>> nan = float(\"NaN\")
>>> X = [[0, 1], [1, nan]]
>>> nan_euclidean_distances(X, X) # distance between rows of X
array([[0.        , 1.41421356],
       [1.41421356, 0.        ]])

>>> # get distance to origin
>>> nan_euclidean_distances(X, [[0, 0]])
array([[1.        ],
       [1.41421356]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                     rdfs:comment """Compute Normalized Discounted Cumulative Gain.

Sum the true scores ranked in the order induced by the predicted scores,
after applying a logarithmic discount. Then divide by the best possible
score (Ideal DCG, obtained for a perfect ranking) to obtain a score between
0 and 1.

This ranking metric returns a high value if true labels are ranked high by
``y_score``.

Parameters
----------
y_true : array-like of shape (n_samples, n_labels)
    True targets of multilabel classification, or true scores of entities
    to be ranked. Negative values in `y_true` may result in an output
    that is not between 0 and 1.

y_score : array-like of shape (n_samples, n_labels)
    Target scores, can either be probability estimates, confidence values,
    or non-thresholded measure of decisions (as returned by
    \"decision_function\" on some classifiers).

k : int, default=None
    Only consider the highest k scores in the ranking. If `None`, use all
    outputs.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If `None`, all samples are given the same weight.

ignore_ties : bool, default=False
    Assume that there are no ties in y_score (which is likely to be the
    case if y_score is continuous) for efficiency gains.

Returns
-------
normalized_discounted_cumulative_gain : float in [0., 1.]
    The averaged NDCG scores for all samples.

See Also
--------
dcg_score : Discounted Cumulative Gain (not normalized).

References
----------
`Wikipedia entry for Discounted Cumulative Gain
<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_

Jarvelin, K., & Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.

Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013)

McSherry, F., & Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import ndcg_score
>>> # we have groud-truth relevance of some answers to a query:
>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])
>>> # we predict some scores (relevance) for the answers
>>> scores = np.asarray([[.1, .2, .3, 4, 70]])
>>> ndcg_score(true_relevance, scores)
0.69...
>>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])
>>> ndcg_score(true_relevance, scores)
0.49...
>>> # we can set k to truncate the sum; only top k answers contribute.
>>> ndcg_score(true_relevance, scores, k=4)
0.35...
>>> # the normalization takes k into account so a perfect answer
>>> # would still get 1.0
>>> ndcg_score(true_relevance, true_relevance, k=4)
1.0...
>>> # now we have some ties in our prediction
>>> scores = np.asarray([[1, 0, 0, 0, 1]])
>>> # by default ties are averaged, so here we get the average (normalized)
>>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75
>>> ndcg_score(true_relevance, scores, k=1)
0.75...
>>> # we can choose to ignore ties for faster results, but only
>>> # if we know there aren't ties in our scores, otherwise we get
>>> # wrong results:
>>> ndcg_score(true_relevance,
...           scores, k=1, ignore_ties=True)
0.5...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """Nearest centroid classifier.

Each class is represented by its centroid, with test samples classified to
the class with the nearest centroid.

Read more in the :ref:`User Guide <nearest_centroid_classifier>`.

Parameters
----------
metric : str or callable, default=\"euclidean\"
    Metric to use for distance computation. See the documentation of
    `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values. Note that \"wminkowski\", \"seuclidean\" and \"mahalanobis\" are not
    supported.

    The centroids for the samples corresponding to each class is
    the point from which the sum of the distances (according to the metric)
    of all samples that belong to that particular class are minimized.
    If the `\"manhattan\"` metric is provided, this centroid is the median
    and for all other metrics, the centroid is now set to be the mean.

    .. deprecated:: 1.3
        Support for metrics other than `euclidean` and `manhattan` and for
        callables was deprecated in version 1.3 and will be removed in
        version 1.5.

    .. versionchanged:: 0.19
        `metric='precomputed'` was deprecated and now raises an error

shrink_threshold : float, default=None
    Threshold for shrinking centroids to remove features.

Attributes
----------
centroids_ : array-like of shape (n_classes, n_features)
    Centroid of each class.

classes_ : array of shape (n_classes,)
    The unique classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
KNeighborsClassifier : Nearest neighbors classifier.

Notes
-----
When used for text classification with tf-idf vectors, this classifier is
also known as the Rocchio classifier.

References
----------
Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of
multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.

Examples
--------
>>> from sklearn.neighbors import NearestCentroid
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = NearestCentroid()
>>> clf.fit(X, y)
NearestCentroid()
>>> print(clf.predict([[-0.8, -1]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """Unsupervised learner for implementing neighbor searches.

Read more in the :ref:`User Guide <unsupervised_neighbors>`.

.. versionadded:: 0.9

Parameters
----------
n_neighbors : int, default=5
    Number of neighbors to use by default for :meth:`kneighbors` queries.

radius : float, default=1.0
    Range of parameter space to use by default for :meth:`radius_neighbors`
    queries.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

p : float (positive), default=2
    Parameter for the Minkowski metric from
    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
effective_metric_ : str
    Metric used to compute distances to neighbors.

effective_metric_params_ : dict
    Parameters for the metric used to compute distances to neighbors.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

See Also
--------
KNeighborsClassifier : Classifier implementing the k-nearest neighbors
    vote.
RadiusNeighborsClassifier : Classifier implementing a vote among neighbors
    within a given radius.
KNeighborsRegressor : Regression based on k-nearest neighbors.
RadiusNeighborsRegressor : Regression based on neighbors within a fixed
    radius.
BallTree : Space partitioning data structure for organizing points in a
    multi-dimensional space, used for nearest neighbor search.

Notes
-----
See :ref:`Nearest Neighbors <neighbors>` in the online documentation
for a discussion of the choice of ``algorithm`` and ``leaf_size``.

https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm

Examples
--------
>>> import numpy as np
>>> from sklearn.neighbors import NearestNeighbors
>>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]
>>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)
>>> neigh.fit(samples)
NearestNeighbors(...)
>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)
array([[2, 0]]...)
>>> nbrs = neigh.radius_neighbors(
...    [[0, 0, 1.3]], 0.4, return_distance=False
... )
>>> np.asarray(nbrs[0][0])
array(2)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod> rdf:type owl:Class ;
                                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                          rdfs:comment """Neighborhood Components Analysis.

Neighborhood Component Analysis (NCA) is a machine learning algorithm for
metric learning. It learns a linear transformation in a supervised fashion
to improve the classification accuracy of a stochastic nearest neighbors
rule in the transformed space.

Read more in the :ref:`User Guide <nca>`.

Parameters
----------
n_components : int, default=None
    Preferred dimensionality of the projected space.
    If None it will be set to `n_features`.

init : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'
    Initialization of the linear transformation. Possible options are
    `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy
    array of shape `(n_features_a, n_features_b)`.

    - `'auto'`
        Depending on `n_components`, the most reasonable initialization
        will be chosen. If `n_components <= n_classes` we use `'lda'`, as
        it uses labels information. If not, but
        `n_components < min(n_features, n_samples)`, we use `'pca'`, as
        it projects data in meaningful directions (those of higher
        variance). Otherwise, we just use `'identity'`.

    - `'pca'`
        `n_components` principal components of the inputs passed
        to :meth:`fit` will be used to initialize the transformation.
        (See :class:`~sklearn.decomposition.PCA`)

    - `'lda'`
        `min(n_components, n_classes)` most discriminative
        components of the inputs passed to :meth:`fit` will be used to
        initialize the transformation. (If `n_components > n_classes`,
        the rest of the components will be zero.) (See
        :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)

    - `'identity'`
        If `n_components` is strictly smaller than the
        dimensionality of the inputs passed to :meth:`fit`, the identity
        matrix will be truncated to the first `n_components` rows.

    - `'random'`
        The initial transformation will be a random array of shape
        `(n_components, n_features)`. Each value is sampled from the
        standard normal distribution.

    - numpy array
        `n_features_b` must match the dimensionality of the inputs passed
        to :meth:`fit` and n_features_a must be less than or equal to that.
        If `n_components` is not `None`, `n_features_a` must match it.

warm_start : bool, default=False
    If `True` and :meth:`fit` has been called before, the solution of the
    previous call to :meth:`fit` is used as the initial linear
    transformation (`n_components` and `init` will be ignored).

max_iter : int, default=50
    Maximum number of iterations in the optimization.

tol : float, default=1e-5
    Convergence tolerance for the optimization.

callback : callable, default=None
    If not `None`, this function is called after every iteration of the
    optimizer, taking as arguments the current solution (flattened
    transformation matrix) and the number of iterations. This might be
    useful in case one wants to examine or store the transformation
    found after each iteration.

verbose : int, default=0
    If 0, no progress messages will be printed.
    If 1, progress messages will be printed to stdout.
    If > 1, progress messages will be printed and the `disp`
    parameter of :func:`scipy.optimize.minimize` will be set to
    `verbose - 2`.

random_state : int or numpy.RandomState, default=None
    A pseudo random number generator object or a seed for it if int. If
    `init='random'`, `random_state` is used to initialize the random
    transformation. If `init='pca'`, `random_state` is passed as an
    argument to PCA when initializing the transformation. Pass an int
    for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    The linear transformation learned during fitting.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

n_iter_ : int
    Counts the number of iterations performed by the optimizer.

random_state_ : numpy.RandomState
    Pseudo random number generator object used during initialization.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear
    Discriminant Analysis.
sklearn.decomposition.PCA : Principal component analysis (PCA).

References
----------
.. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.
       \"Neighbourhood Components Analysis\". Advances in Neural Information
       Processing Systems. 17, 513-520, 2005.
       http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf

.. [2] Wikipedia entry on Neighborhood Components Analysis
       https://en.wikipedia.org/wiki/Neighbourhood_components_analysis

Examples
--------
>>> from sklearn.neighbors import NeighborhoodComponentsAnalysis
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y,
... stratify=y, test_size=0.7, random_state=42)
>>> nca = NeighborhoodComponentsAnalysis(random_state=42)
>>> nca.fit(X_train, y_train)
NeighborhoodComponentsAnalysis(...)
>>> knn = KNeighborsClassifier(n_neighbors=3)
>>> knn.fit(X_train, y_train)
KNeighborsClassifier(...)
>>> print(knn.score(X_test, y_test))
0.933333...
>>> knn.fit(nca.transform(X_train), y_train)
KNeighborsClassifier(...)
>>> print(knn.score(nca.transform(X_test), y_test))
0.961904...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                     rdfs:comment """Normalized Mutual Information between two clusterings.

Normalized Mutual Information (NMI) is a normalization of the Mutual
Information (MI) score to scale the results between 0 (no mutual
information) and 1 (perfect correlation). In this function, mutual
information is normalized by some generalized mean of ``H(labels_true)``
and ``H(labels_pred))``, defined by the `average_method`.

This measure is not adjusted for chance. Therefore
:func:`adjusted_mutual_info_score` might be preferred.

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching ``label_true`` with
``label_pred`` will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.

Read more in the :ref:`User Guide <mutual_info_score>`.

Parameters
----------
labels_true : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets.

labels_pred : int array-like of shape (n_samples,)
    A clustering of the data into disjoint subsets.

average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'
    How to compute the normalizer in the denominator.

    .. versionadded:: 0.20

    .. versionchanged:: 0.22
       The default value of ``average_method`` changed from 'geometric' to
       'arithmetic'.

Returns
-------
nmi : float
   Score between 0.0 and 1.0 in normalized nats (based on the natural
   logarithm). 1.0 stands for perfectly complete labeling.

See Also
--------
v_measure_score : V-Measure (NMI with arithmetic mean option).
adjusted_rand_score : Adjusted Rand Index.
adjusted_mutual_info_score : Adjusted Mutual Information (adjusted
    against chance).

Examples
--------

Perfect labelings are both homogeneous and complete, hence have
score 1.0::

  >>> from sklearn.metrics.cluster import normalized_mutual_info_score
  >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
  ... # doctest: +SKIP
  1.0
  >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
  ... # doctest: +SKIP
  1.0

If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the NMI is null::

  >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
  ... # doctest: +SKIP
  0.0""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                      rdfs:comment """Normalize samples individually to unit norm.

Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1, l2 or inf) equals one.

This transformer is able to work both with dense numpy arrays and
scipy.sparse matrix (use CSR format if you want to avoid the burden of
a copy / conversion).

Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.

For an example visualization, refer to :ref:`Compare Normalizer with other
scalers <plot_all_scaling_normalizer_section>`.

Read more in the :ref:`User Guide <preprocessing_normalization>`.

Parameters
----------
norm : {'l1', 'l2', 'max'}, default='l2'
    The norm to use to normalize each non zero sample. If norm='max'
    is used, values will be rescaled by the maximum of the absolute
    values.

copy : bool, default=True
    Set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSR matrix).

Attributes
----------
n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
normalize : Equivalent function without the estimator API.

Notes
-----
This estimator is :term:`stateless` and does not need to be fitted.
However, we recommend to call :meth:`fit_transform` instead of
:meth:`transform`, as parameter validation is only performed in
:meth:`fit`.

Examples
--------
>>> from sklearn.preprocessing import Normalizer
>>> X = [[4, 1, 2, 2],
...      [1, 3, 9, 3],
...      [5, 7, 5, 1]]
>>> transformer = Normalizer().fit(X)  # fit does nothing.
>>> transformer
Normalizer()
>>> transformer.transform(X)
array([[0.8, 0.2, 0.4, 0.4],
       [0.1, 0.3, 0.9, 0.3],
       [0.5, 0.7, 0.5, 0.1]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment """Nu-Support Vector Classification.

Similar to SVC but uses a parameter to control the number of support
vectors.

The implementation is based on libsvm.

Read more in the :ref:`User Guide <svm_classification>`.

Parameters
----------
nu : float, default=0.5
    An upper bound on the fraction of margin errors (see :ref:`User Guide
    <nu_svc>`) and a lower bound of the fraction of support vectors.
    Should be in the interval (0, 1].

kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'
    Specifies the kernel type to be used in the algorithm.
    If none is given, 'rbf' will be used. If a callable is given it is
    used to precompute the kernel matrix. For an intuitive
    visualization of different kernel types see
    :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.

degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Must be non-negative. Ignored by all other kernels.

gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.

    - if ``gamma='scale'`` (default) is passed then it uses
      1 / (n_features * X.var()) as value of gamma,
    - if 'auto', uses 1 / n_features
    - if float, must be non-negative.

    .. versionchanged:: 0.22
       The default value of ``gamma`` changed from 'auto' to 'scale'.

coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.

shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:`User Guide <shrinking_svm>`.

probability : bool, default=False
    Whether to enable probability estimates. This must be enabled prior
    to calling `fit`, will slow down that method as it internally uses
    5-fold cross-validation, and `predict_proba` may be inconsistent with
    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.

tol : float, default=1e-3
    Tolerance for stopping criterion.

cache_size : float, default=200
    Specify the size of the kernel cache (in MB).

class_weight : {dict, 'balanced'}, default=None
    Set the parameter C of class i to class_weight[i]*C for
    SVC. If not given, all classes are supposed to have
    weight one. The \"balanced\" mode uses the values of y to automatically
    adjust weights inversely proportional to class frequencies as
    ``n_samples / (n_classes * np.bincount(y))``.

verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.

max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.

decision_function_shape : {'ovo', 'ovr'}, default='ovr'
    Whether to return a one-vs-rest ('ovr') decision function of shape
    (n_samples, n_classes) as all other classifiers, or the original
    one-vs-one ('ovo') decision function of libsvm which has shape
    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
    ('ovo') is always used as multi-class strategy. The parameter is
    ignored for binary classification.

    .. versionchanged:: 0.19
        decision_function_shape is 'ovr' by default.

    .. versionadded:: 0.17
       *decision_function_shape='ovr'* is recommended.

    .. versionchanged:: 0.17
       Deprecated *decision_function_shape='ovo' and None*.

break_ties : bool, default=False
    If true, ``decision_function_shape='ovr'``, and number of classes > 2,
    :term:`predict` will break ties according to the confidence values of
    :term:`decision_function`; otherwise the first class among the tied
    classes is returned. Please note that breaking ties comes at a
    relatively high computational cost compared to a simple predict.

    .. versionadded:: 0.22

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data for
    probability estimates. Ignored when `probability` is False.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
class_weight_ : ndarray of shape (n_classes,)
    Multipliers of parameter C of each class.
    Computed based on the ``class_weight`` parameter.

classes_ : ndarray of shape (n_classes,)
    The unique classes labels.

coef_ : ndarray of shape (n_classes * (n_classes -1) / 2, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.

    `coef_` is readonly property derived from `dual_coef_` and
    `support_vectors_`.

dual_coef_ : ndarray of shape (n_classes - 1, n_SV)
    Dual coefficients of the support vector in the decision
    function (see :ref:`sgd_mathematical_formulation`), multiplied by
    their targets.
    For multiclass, coefficient for all 1-vs-1 classifiers.
    The layout of the coefficients in the multiclass case is somewhat
    non-trivial. See the :ref:`multi-class section of the User Guide
    <svm_multi_class>` for details.

fit_status_ : int
    0 if correctly fitted, 1 if the algorithm did not converge.

intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)
    Number of iterations run by the optimization routine to fit the model.
    The shape of this attribute depends on the number of models optimized
    which in turn depends on the number of classes.

    .. versionadded:: 1.1

support_ : ndarray of shape (n_SV,)
    Indices of support vectors.

support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.

n_support_ : ndarray of shape (n_classes,), dtype=int32
    Number of support vectors for each class.

fit_status_ : int
    0 if correctly fitted, 1 if the algorithm did not converge.

probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)

probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)
    If `probability=True`, it corresponds to the parameters learned in
    Platt scaling to produce probability estimates from decision values.
    If `probability=False`, it's an empty array. Platt scaling uses the
    logistic function
    ``1 / (1 + exp(decision_value * probA_ + probB_))``
    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For
    more information on the multiclass case and training procedure see
    section 8 of [1]_.

shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector ``X``.

See Also
--------
SVC : Support Vector Machine for classification using libsvm.

LinearSVC : Scalable linear Support Vector Machine for classification using
    liblinear.

References
----------
.. [1] `LIBSVM: A Library for Support Vector Machines
    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_

.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector
    Machines and Comparisons to Regularized Likelihood Methods\"
    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_

Examples
--------
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> y = np.array([1, 1, 2, 2])
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.svm import NuSVC
>>> clf = make_pipeline(StandardScaler(), NuSVC())
>>> clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])
>>> print(clf.predict([[-0.8, -1]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment """Nu Support Vector Regression.

Similar to NuSVC, for regression, uses a parameter nu to control
the number of support vectors. However, unlike NuSVC, where nu
replaces C, here nu replaces the parameter epsilon of epsilon-SVR.

The implementation is based on libsvm.

Read more in the :ref:`User Guide <svm_regression>`.

Parameters
----------
nu : float, default=0.5
    An upper bound on the fraction of training errors and a lower bound of
    the fraction of support vectors. Should be in the interval (0, 1].  By
    default 0.5 will be taken.

C : float, default=1.0
    Penalty parameter C of the error term.

kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'
     Specifies the kernel type to be used in the algorithm.
     If none is given, 'rbf' will be used. If a callable is given it is
     used to precompute the kernel matrix.

degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Must be non-negative. Ignored by all other kernels.

gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.

    - if ``gamma='scale'`` (default) is passed then it uses
      1 / (n_features * X.var()) as value of gamma,
    - if 'auto', uses 1 / n_features
    - if float, must be non-negative.

    .. versionchanged:: 0.22
       The default value of ``gamma`` changed from 'auto' to 'scale'.

coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.

shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:`User Guide <shrinking_svm>`.

tol : float, default=1e-3
    Tolerance for stopping criterion.

cache_size : float, default=200
    Specify the size of the kernel cache (in MB).

verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.

max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.

Attributes
----------
coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.

    `coef_` is readonly property derived from `dual_coef_` and
    `support_vectors_`.

dual_coef_ : ndarray of shape (1, n_SV)
    Coefficients of the support vector in the decision function.

fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)

intercept_ : ndarray of shape (1,)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations run by the optimization routine to fit the model.

    .. versionadded:: 1.1

n_support_ : ndarray of shape (1,), dtype=int32
    Number of support vectors.

shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector ``X``.

support_ : ndarray of shape (n_SV,)
    Indices of support vectors.

support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.

See Also
--------
NuSVC : Support Vector Machine for classification implemented with libsvm
    with a parameter to control the number of support vectors.

SVR : Epsilon Support Vector Machine for regression implemented with
    libsvm.

References
----------
.. [1] `LIBSVM: A Library for Support Vector Machines
    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_

.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector
    Machines and Comparisons to Regularized Likelihood Methods\"
    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_

Examples
--------
>>> from sklearn.svm import NuSVR
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> np.random.seed(0)
>>> y = np.random.randn(n_samples)
>>> X = np.random.randn(n_samples, n_features)
>>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))
>>> regr.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('nusvr', NuSVR(nu=0.1))])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod> rdf:type owl:Class ;
                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                  rdfs:comment """Estimate clustering structure from vector array.

OPTICS (Ordering Points To Identify the Clustering Structure), closely
related to DBSCAN, finds core sample of high density and expands clusters
from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable
neighborhood radius. Better suited for usage on large datasets than the
current sklearn implementation of DBSCAN.

Clusters are then extracted using a DBSCAN-like method
(cluster_method = 'dbscan') or an automatic
technique proposed in [1]_ (cluster_method = 'xi').

This implementation deviates from the original OPTICS by first performing
k-nearest-neighborhood searches on all points to identify core sizes, then
computing only the distances to unprocessed points when constructing the
cluster order. Note that we do not employ a heap to manage the expansion
candidates, so the time complexity will be O(n^2).

Read more in the :ref:`User Guide <optics>`.

Parameters
----------
min_samples : int > 1 or float between 0 and 1, default=5
    The number of samples in a neighborhood for a point to be considered as
    a core point. Also, up and down steep regions can't have more than
    ``min_samples`` consecutive non-steep points. Expressed as an absolute
    number or a fraction of the number of samples (rounded to be at least
    2).

max_eps : float, default=np.inf
    The maximum distance between two samples for one to be considered as
    in the neighborhood of the other. Default value of ``np.inf`` will
    identify clusters across all scales; reducing ``max_eps`` will result
    in shorter run times.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.

    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string. If metric is
    \"precomputed\", `X` is assumed to be a distance matrix and must be
    square.

    Valid values for metric are:

    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
      'manhattan']

    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
      'yule']

    Sparse matrices are only supported by scikit-learn metrics.
    See the documentation for scipy.spatial.distance for details on these
    metrics.

    .. note::
       `'kulsinski'` is deprecated from SciPy 1.9 and will removed in SciPy 1.11.

p : float, default=2
    Parameter for the Minkowski metric from
    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

cluster_method : str, default='xi'
    The extraction method used to extract clusters using the calculated
    reachability and ordering. Possible values are \"xi\" and \"dbscan\".

eps : float, default=None
    The maximum distance between two samples for one to be considered as
    in the neighborhood of the other. By default it assumes the same value
    as ``max_eps``.
    Used only when ``cluster_method='dbscan'``.

xi : float between 0 and 1, default=0.05
    Determines the minimum steepness on the reachability plot that
    constitutes a cluster boundary. For example, an upwards point in the
    reachability plot is defined by the ratio from one point to its
    successor being at most 1-xi.
    Used only when ``cluster_method='xi'``.

predecessor_correction : bool, default=True
    Correct clusters according to the predecessors calculated by OPTICS
    [2]_. This parameter has minimal effect on most datasets.
    Used only when ``cluster_method='xi'``.

min_cluster_size : int > 1 or float between 0 and 1, default=None
    Minimum number of samples in an OPTICS cluster, expressed as an
    absolute number or a fraction of the number of samples (rounded to be
    at least 2). If ``None``, the value of ``min_samples`` is used instead.
    Used only when ``cluster_method='xi'``.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.
    - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.
    - 'brute' will use a brute-force search.
    - 'auto' (default) will attempt to decide the most appropriate
      algorithm based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to :class:`~sklearn.neighbors.BallTree` or
    :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the
    construction and query, as well as the memory required to store the
    tree. The optimal value depends on the nature of the problem.

memory : str or object with the joblib.Memory interface, default=None
    Used to cache the output of the computation of the tree.
    By default, no caching is done. If a string is given, it is the
    path to the caching directory.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
labels_ : ndarray of shape (n_samples,)
    Cluster labels for each point in the dataset given to fit().
    Noisy samples and points which are not included in a leaf cluster
    of ``cluster_hierarchy_`` are labeled as -1.

reachability_ : ndarray of shape (n_samples,)
    Reachability distances per sample, indexed by object order. Use
    ``clust.reachability_[clust.ordering_]`` to access in cluster order.

ordering_ : ndarray of shape (n_samples,)
    The cluster ordered list of sample indices.

core_distances_ : ndarray of shape (n_samples,)
    Distance at which each sample becomes a core point, indexed by object
    order. Points which will never be core have a distance of inf. Use
    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.

predecessor_ : ndarray of shape (n_samples,)
    Point that a sample was reached from, indexed by object order.
    Seed points have a predecessor of -1.

cluster_hierarchy_ : ndarray of shape (n_clusters, 2)
    The list of clusters in the form of ``[start, end]`` in each row, with
    all indices inclusive. The clusters are ordered according to
    ``(end, -start)`` (ascending) so that larger clusters encompassing
    smaller clusters come after those smaller ones. Since ``labels_`` does
    not reflect the hierarchy, usually
    ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also
    note that these indices are of the ``ordering_``, i.e.
    ``X[ordering_][start:end + 1]`` form a cluster.
    Only available when ``cluster_method='xi'``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
DBSCAN : A similar clustering for a specified neighborhood radius (eps).
    Our implementation is optimized for runtime.

References
----------
.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,
   and Jrg Sander. \"OPTICS: ordering points to identify the clustering
   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.

.. [2] Schubert, Erich, Michael Gertz.
   \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of
   the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.

Examples
--------
>>> from sklearn.cluster import OPTICS
>>> import numpy as np
>>> X = np.array([[1, 2], [2, 5], [3, 6],
...               [8, 7], [8, 8], [7, 3]])
>>> clustering = OPTICS(min_samples=2).fit(X)
>>> clustering.labels_
array([0, 0, 0, 1, 1, 1])

For a more detailed example see
:ref:`sphx_glr_auto_examples_cluster_plot_optics.py`.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                       rdfs:comment """Unsupervised Outlier Detection.

Estimate the support of a high-dimensional distribution.

The implementation is based on libsvm.

Read more in the :ref:`User Guide <outlier_detection>`.

Parameters
----------
kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'
     Specifies the kernel type to be used in the algorithm.
     If none is given, 'rbf' will be used. If a callable is given it is
     used to precompute the kernel matrix.

degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Must be non-negative. Ignored by all other kernels.

gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.

    - if ``gamma='scale'`` (default) is passed then it uses
      1 / (n_features * X.var()) as value of gamma,
    - if 'auto', uses 1 / n_features
    - if float, must be non-negative.

    .. versionchanged:: 0.22
       The default value of ``gamma`` changed from 'auto' to 'scale'.

coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.

tol : float, default=1e-3
    Tolerance for stopping criterion.

nu : float, default=0.5
    An upper bound on the fraction of training
    errors and a lower bound of the fraction of support
    vectors. Should be in the interval (0, 1]. By default 0.5
    will be taken.

shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:`User Guide <shrinking_svm>`.

cache_size : float, default=200
    Specify the size of the kernel cache (in MB).

verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.

max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.

Attributes
----------
coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.

    `coef_` is readonly property derived from `dual_coef_` and
    `support_vectors_`.

dual_coef_ : ndarray of shape (1, n_SV)
    Coefficients of the support vectors in the decision function.

fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)

intercept_ : ndarray of shape (1,)
    Constant in the decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations run by the optimization routine to fit the model.

    .. versionadded:: 1.1

n_support_ : ndarray of shape (n_classes,), dtype=int32
    Number of support vectors for each class.

offset_ : float
    Offset used to define the decision function from the raw scores.
    We have the relation: decision_function = score_samples - `offset_`.
    The offset is the opposite of `intercept_` and is provided for
    consistency with other outlier detection algorithms.

    .. versionadded:: 0.20

shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector ``X``.

support_ : ndarray of shape (n_SV,)
    Indices of support vectors.

support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.

See Also
--------
sklearn.linear_model.SGDOneClassSVM : Solves linear One-Class SVM using
    Stochastic Gradient Descent.
sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection using
    Local Outlier Factor (LOF).
sklearn.ensemble.IsolationForest : Isolation Forest Algorithm.

Examples
--------
>>> from sklearn.svm import OneClassSVM
>>> X = [[0], [0.44], [0.45], [0.46], [1]]
>>> clf = OneClassSVM(gamma='auto').fit(X)
>>> clf.predict(X)
array([-1,  1,  1,  1, -1])
>>> clf.score_samples(X)
array([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                         rdfs:comment """Encode categorical features as a one-hot numeric array.

The input to this transformer should be an array-like of integers or
strings, denoting the values taken on by categorical (discrete) features.
The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')
encoding scheme. This creates a binary column for each category and
returns a sparse matrix or dense array (depending on the ``sparse_output``
parameter).

By default, the encoder derives the categories based on the unique values
in each feature. Alternatively, you can also specify the `categories`
manually.

This encoding is needed for feeding categorical data to many scikit-learn
estimators, notably linear models and SVMs with the standard kernels.

Note: a one-hot encoding of y labels should use a LabelBinarizer
instead.

Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
For a comparison of different encoders, refer to:
:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.

Parameters
----------
categories : 'auto' or a list of array-like, default='auto'
    Categories (unique values) per feature:

    - 'auto' : Determine categories automatically from the training data.
    - list : ``categories[i]`` holds the categories expected in the ith
      column. The passed categories should not mix strings and numeric
      values within a single feature, and should be sorted in case of
      numeric values.

    The used categories can be found in the ``categories_`` attribute.

    .. versionadded:: 0.20

drop : {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None
    Specifies a methodology to use to drop one of the categories per
    feature. This is useful in situations where perfectly collinear
    features cause problems, such as when feeding the resulting data
    into an unregularized linear regression model.

    However, dropping one category breaks the symmetry of the original
    representation and can therefore induce a bias in downstream models,
    for instance for penalized linear classification or regression models.

    - None : retain all features (the default).
    - 'first' : drop the first category in each feature. If only one
      category is present, the feature will be dropped entirely.
    - 'if_binary' : drop the first category in each feature with two
      categories. Features with 1 or more than 2 categories are
      left intact.
    - array : ``drop[i]`` is the category in feature ``X[:, i]`` that
      should be dropped.

    When `max_categories` or `min_frequency` is configured to group
    infrequent categories, the dropping behavior is handled after the
    grouping.

    .. versionadded:: 0.21
       The parameter `drop` was added in 0.21.

    .. versionchanged:: 0.23
       The option `drop='if_binary'` was added in 0.23.

    .. versionchanged:: 1.1
        Support for dropping infrequent categories.

sparse_output : bool, default=True
    When ``True``, it returns a :class:`scipy.sparse.csr_matrix`,
    i.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.

    .. versionadded:: 1.2
       `sparse` was renamed to `sparse_output`

dtype : number type, default=np.float64
    Desired dtype of output.

handle_unknown : {'error', 'ignore', 'infrequent_if_exist'},                      default='error'
    Specifies the way unknown categories are handled during :meth:`transform`.

    - 'error' : Raise an error if an unknown category is present during transform.
    - 'ignore' : When an unknown category is encountered during
      transform, the resulting one-hot encoded columns for this feature
      will be all zeros. In the inverse transform, an unknown category
      will be denoted as None.
    - 'infrequent_if_exist' : When an unknown category is encountered
      during transform, the resulting one-hot encoded columns for this
      feature will map to the infrequent category if it exists. The
      infrequent category will be mapped to the last position in the
      encoding. During inverse transform, an unknown category will be
      mapped to the category denoted `'infrequent'` if it exists. If the
      `'infrequent'` category does not exist, then :meth:`transform` and
      :meth:`inverse_transform` will handle an unknown category as with
      `handle_unknown='ignore'`. Infrequent categories exist based on
      `min_frequency` and `max_categories`. Read more in the
      :ref:`User Guide <encoder_infrequent_categories>`.

    .. versionchanged:: 1.1
        `'infrequent_if_exist'` was added to automatically handle unknown
        categories and infrequent categories.

min_frequency : int or float, default=None
    Specifies the minimum frequency below which a category will be
    considered infrequent.

    - If `int`, categories with a smaller cardinality will be considered
      infrequent.

    - If `float`, categories with a smaller cardinality than
      `min_frequency * n_samples`  will be considered infrequent.

    .. versionadded:: 1.1
        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.

max_categories : int, default=None
    Specifies an upper limit to the number of output features for each input
    feature when considering infrequent categories. If there are infrequent
    categories, `max_categories` includes the category representing the
    infrequent categories along with the frequent categories. If `None`,
    there is no limit to the number of output features.

    .. versionadded:: 1.1
        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.

feature_name_combiner : \"concat\" or callable, default=\"concat\"
    Callable with signature `def callable(input_feature, category)` that returns a
    string. This is used to create feature names to be returned by
    :meth:`get_feature_names_out`.

    `\"concat\"` concatenates encoded feature name and category with
    `feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create
    feature names `X_1, X_6, X_7`.

    .. versionadded:: 1.3

Attributes
----------
categories_ : list of arrays
    The categories of each feature determined during fitting
    (in order of the features in X and corresponding with the output
    of ``transform``). This includes the category specified in ``drop``
    (if any).

drop_idx_ : array of shape (n_features,)
    - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category
      to be dropped for each feature.
    - ``drop_idx_[i] = None`` if no category is to be dropped from the
      feature with index ``i``, e.g. when `drop='if_binary'` and the
      feature isn't binary.
    - ``drop_idx_ = None`` if all the transformed features will be
      retained.

    If infrequent categories are enabled by setting `min_frequency` or
    `max_categories` to a non-default value and `drop_idx[i]` corresponds
    to a infrequent category, then the entire infrequent category is
    dropped.

    .. versionchanged:: 0.23
       Added the possibility to contain `None` values.

infrequent_categories_ : list of ndarray
    Defined only if infrequent categories are enabled by setting
    `min_frequency` or `max_categories` to a non-default value.
    `infrequent_categories_[i]` are the infrequent categories for feature
    `i`. If the feature `i` has no infrequent categories
    `infrequent_categories_[i]` is None.

    .. versionadded:: 1.1

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 1.0

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

feature_name_combiner : callable or None
    Callable with signature `def callable(input_feature, category)` that returns a
    string. This is used to create feature names to be returned by
    :meth:`get_feature_names_out`.

    .. versionadded:: 1.3

See Also
--------
OrdinalEncoder : Performs an ordinal (integer)
  encoding of the categorical features.
TargetEncoder : Encodes categorical features using the target.
sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of
  dictionary items (also handles string-valued features).
sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot
  encoding of dictionary items or strings.
LabelBinarizer : Binarizes labels in a one-vs-all
  fashion.
MultiLabelBinarizer : Transforms between iterable of
  iterables and a multilabel format, e.g. a (samples x classes) binary
  matrix indicating the presence of a class label.

Examples
--------
Given a dataset with two features, we let the encoder find the unique
values per feature and transform the data to a binary one-hot encoding.

>>> from sklearn.preprocessing import OneHotEncoder

One can discard categories not seen during `fit`:

>>> enc = OneHotEncoder(handle_unknown='ignore')
>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
>>> enc.fit(X)
OneHotEncoder(handle_unknown='ignore')
>>> enc.categories_
[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
>>> enc.transform([['Female', 1], ['Male', 4]]).toarray()
array([[1., 0., 1., 0., 0.],
       [0., 1., 0., 0., 0.]])
>>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])
array([['Male', 1],
       [None, 2]], dtype=object)
>>> enc.get_feature_names_out(['gender', 'group'])
array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)

One can always drop the first column for each feature:

>>> drop_enc = OneHotEncoder(drop='first').fit(X)
>>> drop_enc.categories_
[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
>>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()
array([[0., 0., 0.],
       [1., 1., 0.]])

Or drop a column for feature only having 2 categories:

>>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)
>>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()
array([[0., 1., 0., 0.],
       [1., 0., 1., 0.]])

One can change the way feature names are created.

>>> def custom_combiner(feature, category):
...     return str(feature) + \"_\" + type(category).__name__ + \"_\" + str(category)
>>> custom_fnames_enc = OneHotEncoder(feature_name_combiner=custom_combiner).fit(X)
>>> custom_fnames_enc.get_feature_names_out()
array(['x0_str_Female', 'x0_str_Male', 'x1_int_1', 'x1_int_2', 'x1_int_3'],
      dtype=object)

Infrequent categories are enabled by setting `max_categories` or `min_frequency`.

>>> import numpy as np
>>> X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T
>>> ohe = OneHotEncoder(max_categories=3, sparse_output=False).fit(X)
>>> ohe.infrequent_categories_
[array(['a', 'd'], dtype=object)]
>>> ohe.transform([[\"a\"], [\"b\"]])
array([[0., 0., 1.],
       [1., 0., 0.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """One-vs-one multiclass strategy.

This strategy consists in fitting one classifier per class pair.
At prediction time, the class which received the most votes is selected.
Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers,
this method is usually slower than one-vs-the-rest, due to its
O(n_classes^2) complexity. However, this method may be advantageous for
algorithms such as kernel algorithms which don't scale well with
`n_samples`. This is because each individual learning problem only involves
a small subset of the data whereas, with one-vs-the-rest, the complete
dataset is used `n_classes` times.

Read more in the :ref:`User Guide <ovo_classification>`.

Parameters
----------
estimator : estimator object
    A regressor or a classifier that implements :term:`fit`.
    When a classifier is passed, :term:`decision_function` will be used
    in priority and it will fallback to :term:`predict_proba` if it is not
    available.
    When a regressor is passed, :term:`predict` is used.

n_jobs : int, default=None
    The number of jobs to use for the computation: the `n_classes * (
    n_classes - 1) / 2` OVO problems are computed in parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
estimators_ : list of ``n_classes * (n_classes - 1) / 2`` estimators
    Estimators used for predictions.

classes_ : numpy array of shape [n_classes]
    Array containing labels.

n_classes_ : int
    Number of classes.

pairwise_indices_ : list, length = ``len(estimators_)``, or ``None``
    Indices of samples used when training the estimators.
    ``None`` when ``estimator``'s `pairwise` tag is False.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
OneVsRestClassifier : One-vs-all multiclass strategy.
OutputCodeClassifier : (Error-Correcting) Output-Code multiclass strategy.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.multiclass import OneVsOneClassifier
>>> from sklearn.svm import LinearSVC
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, shuffle=True, random_state=0)
>>> clf = OneVsOneClassifier(
...     LinearSVC(dual=\"auto\", random_state=0)).fit(X_train, y_train)
>>> clf.predict(X_test[:10])
array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                               rdfs:comment """One-vs-the-rest (OvR) multiclass strategy.

Also known as one-vs-all, this strategy consists in fitting one classifier
per class. For each classifier, the class is fitted against all the other
classes. In addition to its computational efficiency (only `n_classes`
classifiers are needed), one advantage of this approach is its
interpretability. Since each class is represented by one and one classifier
only, it is possible to gain knowledge about the class by inspecting its
corresponding classifier. This is the most commonly used strategy for
multiclass classification and is a fair default choice.

OneVsRestClassifier can also be used for multilabel classification. To use
this feature, provide an indicator matrix for the target `y` when calling
`.fit`. In other words, the target labels should be formatted as a 2D
binary (0/1) matrix, where [i, j] == 1 indicates the presence of label j
in sample i. This estimator uses the binary relevance method to perform
multilabel classification, which involves training one binary classifier
independently for each label.

Read more in the :ref:`User Guide <ovr_classification>`.

Parameters
----------
estimator : estimator object
    A regressor or a classifier that implements :term:`fit`.
    When a classifier is passed, :term:`decision_function` will be used
    in priority and it will fallback to :term:`predict_proba` if it is not
    available.
    When a regressor is passed, :term:`predict` is used.

n_jobs : int, default=None
    The number of jobs to use for the computation: the `n_classes`
    one-vs-rest problems are computed in parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionchanged:: 0.20
       `n_jobs` default changed from 1 to None

verbose : int, default=0
    The verbosity level, if non zero, progress messages are printed.
    Below 50, the output is sent to stderr. Otherwise, the output is sent
    to stdout. The frequency of the messages increases with the verbosity
    level, reporting all iterations at 10. See :class:`joblib.Parallel` for
    more details.

    .. versionadded:: 1.1

Attributes
----------
estimators_ : list of `n_classes` estimators
    Estimators used for predictions.

classes_ : array, shape = [`n_classes`]
    Class labels.

n_classes_ : int
    Number of classes.

label_binarizer_ : LabelBinarizer object
    Object used to transform multiclass labels to binary labels and
    vice-versa.

multilabel_ : boolean
    Whether a OneVsRestClassifier is a multilabel classifier.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 1.0

See Also
--------
OneVsOneClassifier : One-vs-one multiclass strategy.
OutputCodeClassifier : (Error-Correcting) Output-Code multiclass strategy.
sklearn.multioutput.MultiOutputClassifier : Alternate way of extending an
    estimator for multilabel classification.
sklearn.preprocessing.MultiLabelBinarizer : Transform iterable of iterables
    to binary indicator matrix.

Examples
--------
>>> import numpy as np
>>> from sklearn.multiclass import OneVsRestClassifier
>>> from sklearn.svm import SVC
>>> X = np.array([
...     [10, 10],
...     [8, 10],
...     [-5, 5.5],
...     [-5.4, 5.5],
...     [-20, -20],
...     [-15, -20]
... ])
>>> y = np.array([0, 0, 1, 1, 2, 2])
>>> clf = OneVsRestClassifier(SVC()).fit(X, y)
>>> clf.predict([[-19, -20], [9, 9], [-5, 5]])
array([2, 0, 1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                          rdfs:comment """Encode categorical features as an integer array.

The input to this transformer should be an array-like of integers or
strings, denoting the values taken on by categorical (discrete) features.
The features are converted to ordinal integers. This results in
a single column of integers (0 to n_categories - 1) per feature.

Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
For a comparison of different encoders, refer to:
:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.

.. versionadded:: 0.20

Parameters
----------
categories : 'auto' or a list of array-like, default='auto'
    Categories (unique values) per feature:

    - 'auto' : Determine categories automatically from the training data.
    - list : ``categories[i]`` holds the categories expected in the ith
      column. The passed categories should not mix strings and numeric
      values, and should be sorted in case of numeric values.

    The used categories can be found in the ``categories_`` attribute.

dtype : number type, default=np.float64
    Desired dtype of output.

handle_unknown : {'error', 'use_encoded_value'}, default='error'
    When set to 'error' an error will be raised in case an unknown
    categorical feature is present during transform. When set to
    'use_encoded_value', the encoded value of unknown categories will be
    set to the value given for the parameter `unknown_value`. In
    :meth:`inverse_transform`, an unknown category will be denoted as None.

    .. versionadded:: 0.24

unknown_value : int or np.nan, default=None
    When the parameter handle_unknown is set to 'use_encoded_value', this
    parameter is required and will set the encoded value of unknown
    categories. It has to be distinct from the values used to encode any of
    the categories in `fit`. If set to np.nan, the `dtype` parameter must
    be a float dtype.

    .. versionadded:: 0.24

encoded_missing_value : int or np.nan, default=np.nan
    Encoded value of missing categories. If set to `np.nan`, then the `dtype`
    parameter must be a float dtype.

    .. versionadded:: 1.1

min_frequency : int or float, default=None
    Specifies the minimum frequency below which a category will be
    considered infrequent.

    - If `int`, categories with a smaller cardinality will be considered
      infrequent.

    - If `float`, categories with a smaller cardinality than
      `min_frequency * n_samples`  will be considered infrequent.

    .. versionadded:: 1.3
        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.

max_categories : int, default=None
    Specifies an upper limit to the number of output categories for each input
    feature when considering infrequent categories. If there are infrequent
    categories, `max_categories` includes the category representing the
    infrequent categories along with the frequent categories. If `None`,
    there is no limit to the number of output features.

    `max_categories` do **not** take into account missing or unknown
    categories. Setting `unknown_value` or `encoded_missing_value` to an
    integer will increase the number of unique integer codes by one each.
    This can result in up to `max_categories + 2` integer codes.

    .. versionadded:: 1.3
        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.

Attributes
----------
categories_ : list of arrays
    The categories of each feature determined during ``fit`` (in order of
    the features in X and corresponding with the output of ``transform``).
    This does not include categories that weren't seen during ``fit``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 1.0

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

infrequent_categories_ : list of ndarray
    Defined only if infrequent categories are enabled by setting
    `min_frequency` or `max_categories` to a non-default value.
    `infrequent_categories_[i]` are the infrequent categories for feature
    `i`. If the feature `i` has no infrequent categories
    `infrequent_categories_[i]` is None.

    .. versionadded:: 1.3

See Also
--------
OneHotEncoder : Performs a one-hot encoding of categorical features. This encoding
    is suitable for low to medium cardinality categorical variables, both in
    supervised and unsupervised settings.
TargetEncoder : Encodes categorical features using supervised signal
    in a classification or regression pipeline. This encoding is typically
    suitable for high cardinality categorical variables.
LabelEncoder : Encodes target labels with values between 0 and
    ``n_classes-1``.

Notes
-----
With a high proportion of `nan` values, inferring categories becomes slow with
Python versions before 3.10. The handling of `nan` values was improved
from Python 3.10 onwards, (c.f.
`bpo-43475 <https://github.com/python/cpython/issues/87641>`_).

Examples
--------
Given a dataset with two features, we let the encoder find the unique
values per feature and transform the data to an ordinal encoding.

>>> from sklearn.preprocessing import OrdinalEncoder
>>> enc = OrdinalEncoder()
>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
>>> enc.fit(X)
OrdinalEncoder()
>>> enc.categories_
[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
>>> enc.transform([['Female', 3], ['Male', 1]])
array([[0., 2.],
       [1., 0.]])

>>> enc.inverse_transform([[1, 0], [0, 1]])
array([['Male', 1],
       ['Female', 2]], dtype=object)

By default, :class:`OrdinalEncoder` is lenient towards missing values by
propagating them.

>>> import numpy as np
>>> X = [['Male', 1], ['Female', 3], ['Female', np.nan]]
>>> enc.fit_transform(X)
array([[ 1.,  0.],
       [ 0.,  1.],
       [ 0., nan]])

You can use the parameter `encoded_missing_value` to encode missing values.

>>> enc.set_params(encoded_missing_value=-1).fit_transform(X)
array([[ 1.,  0.],
       [ 0.,  1.],
       [ 0., -1.]])

Infrequent categories are enabled by setting `max_categories` or `min_frequency`.
In the following example, \"a\" and \"d\" are considered infrequent and grouped
together into a single category, \"b\" and \"c\" are their own categories, unknown
values are encoded as 3 and missing values are encoded as 4.

>>> X_train = np.array(
...     [[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3 + [np.nan]],
...     dtype=object).T
>>> enc = OrdinalEncoder(
...     handle_unknown=\"use_encoded_value\", unknown_value=3,
...     max_categories=3, encoded_missing_value=4)
>>> _ = enc.fit(X_train)
>>> X_test = np.array([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [np.nan]], dtype=object)
>>> enc.transform(X_test)
array([[2.],
       [0.],
       [1.],
       [2.],
       [3.],
       [4.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod> rdf:type owl:Class ;
                                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                       rdfs:comment """Cross-validated Orthogonal Matching Pursuit model (OMP).

See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide <omp>`.

Parameters
----------
copy : bool, default=True
    Whether the design matrix X must be copied by the algorithm. A false
    value is only helpful if X is already Fortran-ordered, otherwise a
    copy is made anyway.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

max_iter : int, default=None
    Maximum numbers of iterations to perform, therefore maximum features
    to include. 10% of ``n_features`` but at least 5 if available.

cv : int, cross-validation generator or iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : bool or int, default=False
    Sets the verbosity amount.

Attributes
----------
intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the problem formulation).

n_nonzero_coefs_ : int
    Estimated number of non-zero coefficients giving the best mean squared
    error over the cross-validation folds.

n_iter_ : int or array-like
    Number of active features across every target for the model refit with
    the best hyperparameters got by cross-validating across all folds.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.
orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit
    problems using only the Gram matrix X.T * X and the product X.T * y.
lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).
LarsCV : Cross-validated Least Angle Regression model.
LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.
sklearn.decomposition.sparse_encode : Generic sparse coding.
    Each column of the result is the solution to a Lasso problem.

Notes
-----
In `fit`, once the optimal number of non-zero coefficients is found through
cross-validation, the model is fit again using the entire training set.

Examples
--------
>>> from sklearn.linear_model import OrthogonalMatchingPursuitCV
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=100, n_informative=10,
...                        noise=4, random_state=0)
>>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)
>>> reg.score(X, y)
0.9991...
>>> reg.n_nonzero_coefs_
10
>>> reg.predict(X[:1,])
array([-78.3854...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                     rdfs:comment """Orthogonal Matching Pursuit model (OMP).

Read more in the :ref:`User Guide <omp>`.

Parameters
----------
n_nonzero_coefs : int, default=None
    Desired number of non-zero entries in the solution. If None (by
    default) this value is set to 10% of n_features.

tol : float, default=None
    Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

precompute : 'auto' or bool, default='auto'
    Whether to use a precomputed Gram and Xy matrix to speed up
    calculations. Improves performance when :term:`n_targets` or
    :term:`n_samples` is very large. Note that if you already have such
    matrices, you can pass them directly to the fit method.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Parameter vector (w in the formula).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function.

n_iter_ : int or array-like
    Number of active features across every target.

n_nonzero_coefs_ : int
    The number of non-zero coefficients in the solution. If
    `n_nonzero_coefs` is None and `tol` is None this value is either set
    to 10% of `n_features` or 1, whichever is greater.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.
orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit
    problems using only the Gram matrix X.T * X and the product X.T * y.
lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
sklearn.decomposition.sparse_encode : Generic sparse coding.
    Each column of the result is the solution to a Lasso problem.
OrthogonalMatchingPursuitCV : Cross-validated
    Orthogonal Matching Pursuit model (OMP).

Notes
-----
Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)

This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf

Examples
--------
>>> from sklearn.linear_model import OrthogonalMatchingPursuit
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(noise=4, random_state=0)
>>> reg = OrthogonalMatchingPursuit().fit(X, y)
>>> reg.score(X, y)
0.9991...
>>> reg.predict(X[:1,])
array([-78.3854...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                rdfs:comment """(Error-Correcting) Output-Code multiclass strategy.

Output-code based strategies consist in representing each class with a
binary code (an array of 0s and 1s). At fitting time, one binary
classifier per bit in the code book is fitted.  At prediction time, the
classifiers are used to project new points in the class space and the class
closest to the points is chosen. The main advantage of these strategies is
that the number of classifiers used can be controlled by the user, either
for compressing the model (0 < `code_size` < 1) or for making the model more
robust to errors (`code_size` > 1). See the documentation for more details.

Read more in the :ref:`User Guide <ecoc>`.

Parameters
----------
estimator : estimator object
    An estimator object implementing :term:`fit` and one of
    :term:`decision_function` or :term:`predict_proba`.

code_size : float, default=1.5
    Percentage of the number of classes to be used to create the code book.
    A number between 0 and 1 will require fewer classifiers than
    one-vs-the-rest. A number greater than 1 will require more classifiers
    than one-vs-the-rest.

random_state : int, RandomState instance, default=None
    The generator used to initialize the codebook.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

n_jobs : int, default=None
    The number of jobs to use for the computation: the multiclass problems
    are computed in parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
estimators_ : list of `int(n_classes * code_size)` estimators
    Estimators used for predictions.

classes_ : ndarray of shape (n_classes,)
    Array containing labels.

code_book_ : ndarray of shape (n_classes, `len(estimators_)`)
    Binary array containing the code of each class.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 1.0

See Also
--------
OneVsRestClassifier : One-vs-all multiclass strategy.
OneVsOneClassifier : One-vs-one multiclass strategy.

References
----------

.. [1] \"Solving multiclass learning problems via error-correcting output
   codes\",
   Dietterich T., Bakiri G.,
   Journal of Artificial Intelligence Research 2,
   1995.

.. [2] \"The error coding method and PICTs\",
   James G., Hastie T.,
   Journal of Computational and Graphical statistics 7,
   1998.

.. [3] \"The Elements of Statistical Learning\",
   Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)
   2008.

Examples
--------
>>> from sklearn.multiclass import OutputCodeClassifier
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=100, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = OutputCodeClassifier(
...     estimator=RandomForestClassifier(random_state=0),
...     random_state=0).fit(X, y)
>>> clf.predict([[0, 0, 0, 0]])
array([1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                               rdfs:comment """Principal component analysis (PCA).

Linear dimensionality reduction using Singular Value Decomposition of the
data to project it to a lower dimensional space. The input data is centered
but not scaled for each feature before applying the SVD.

It uses the LAPACK implementation of the full SVD or a randomized truncated
SVD by the method of Halko et al. 2009, depending on the shape of the input
data and the number of components to extract.

It can also use the scipy.sparse.linalg ARPACK implementation of the
truncated SVD.

Notice that this class does not support sparse input. See
:class:`TruncatedSVD` for an alternative with sparse data.

For a usage example, see
:ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`

Read more in the :ref:`User Guide <PCA>`.

Parameters
----------
n_components : int, float or 'mle', default=None
    Number of components to keep.
    if n_components is not set all components are kept::

        n_components == min(n_samples, n_features)

    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's
    MLE is used to guess the dimension. Use of ``n_components == 'mle'``
    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.

    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the
    number of components such that the amount of variance that needs to be
    explained is greater than the percentage specified by n_components.

    If ``svd_solver == 'arpack'``, the number of components must be
    strictly less than the minimum of n_features and n_samples.

    Hence, the None case results in::

        n_components == min(n_samples, n_features) - 1

copy : bool, default=True
    If False, data passed to fit are overwritten and running
    fit(X).transform(X) will not yield the expected results,
    use fit_transform(X) instead.

whiten : bool, default=False
    When True (False by default) the `components_` vectors are multiplied
    by the square root of n_samples and then divided by the singular values
    to ensure uncorrelated outputs with unit component-wise variances.

    Whitening will remove some information from the transformed signal
    (the relative variance scales of the components) but can sometime
    improve the predictive accuracy of the downstream estimators by
    making their data respect some hard-wired assumptions.

svd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'
    If auto :
        The solver is selected by a default policy based on `X.shape` and
        `n_components`: if the input data is larger than 500x500 and the
        number of components to extract is lower than 80% of the smallest
        dimension of the data, then the more efficient 'randomized'
        method is enabled. Otherwise the exact full SVD is computed and
        optionally truncated afterwards.
    If full :
        run exact full SVD calling the standard LAPACK solver via
        `scipy.linalg.svd` and select the components by postprocessing
    If arpack :
        run SVD truncated to n_components calling ARPACK solver via
        `scipy.sparse.linalg.svds`. It requires strictly
        0 < n_components < min(X.shape)
    If randomized :
        run randomized SVD by the method of Halko et al.

    .. versionadded:: 0.18.0

tol : float, default=0.0
    Tolerance for singular values computed by svd_solver == 'arpack'.
    Must be of range [0.0, infinity).

    .. versionadded:: 0.18.0

iterated_power : int or 'auto', default='auto'
    Number of iterations for the power method computed by
    svd_solver == 'randomized'.
    Must be of range [0, infinity).

    .. versionadded:: 0.18.0

n_oversamples : int, default=10
    This parameter is only relevant when `svd_solver=\"randomized\"`.
    It corresponds to the additional number of random vectors to sample the
    range of `X` so as to ensure proper conditioning. See
    :func:`~sklearn.utils.extmath.randomized_svd` for more details.

    .. versionadded:: 1.1

power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
    Power iteration normalizer for randomized SVD solver.
    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`
    for more details.

    .. versionadded:: 1.1

random_state : int, RandomState instance or None, default=None
    Used when the 'arpack' or 'randomized' solvers are used. Pass an int
    for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

    .. versionadded:: 0.18.0

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Principal axes in feature space, representing the directions of
    maximum variance in the data. Equivalently, the right singular
    vectors of the centered input data, parallel to its eigenvectors.
    The components are sorted by decreasing ``explained_variance_``.

explained_variance_ : ndarray of shape (n_components,)
    The amount of variance explained by each of the selected components.
    The variance estimation uses `n_samples - 1` degrees of freedom.

    Equal to n_components largest eigenvalues
    of the covariance matrix of X.

    .. versionadded:: 0.18

explained_variance_ratio_ : ndarray of shape (n_components,)
    Percentage of variance explained by each of the selected components.

    If ``n_components`` is not set then all components are stored and the
    sum of the ratios is equal to 1.0.

singular_values_ : ndarray of shape (n_components,)
    The singular values corresponding to each of the selected components.
    The singular values are equal to the 2-norms of the ``n_components``
    variables in the lower-dimensional space.

    .. versionadded:: 0.19

mean_ : ndarray of shape (n_features,)
    Per-feature empirical mean, estimated from the training set.

    Equal to `X.mean(axis=0)`.

n_components_ : int
    The estimated number of components. When n_components is set
    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this
    number is estimated from input data. Otherwise it equals the parameter
    n_components, or the lesser value of n_features and n_samples
    if n_components is None.

n_samples_ : int
    Number of samples in the training data.

noise_variance_ : float
    The estimated noise covariance following the Probabilistic PCA model
    from Tipping and Bishop 1999. See \"Pattern Recognition and
    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or
    http://www.miketipping.com/papers/met-mppca.pdf. It is required to
    compute the estimated data covariance and score samples.

    Equal to the average of (min(n_features, n_samples) - n_components)
    smallest eigenvalues of the covariance matrix of X.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
KernelPCA : Kernel Principal Component Analysis.
SparsePCA : Sparse Principal Component Analysis.
TruncatedSVD : Dimensionality reduction using truncated SVD.
IncrementalPCA : Incremental Principal Component Analysis.

References
----------
For n_components == 'mle', this class uses the method from:
`Minka, T. P.. \"Automatic choice of dimensionality for PCA\".
In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_

Implements the probabilistic PCA model from:
`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal
component analysis\". Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 61(3), 611-622.
<http://www.miketipping.com/papers/met-mppca.pdf>`_
via the score and score_samples methods.

For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.

For svd_solver == 'randomized', see:
:doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).
\"Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions\".
SIAM review, 53(2), 217-288.
<10.1137/090771806>`
and also
:doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).
\"A randomized algorithm for the decomposition of matrices\".
Applied and Computational Harmonic Analysis, 30(1), 47-68.
<10.1016/j.acha.2010.02.003>`

Examples
--------
>>> import numpy as np
>>> from sklearn.decomposition import PCA
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> pca = PCA(n_components=2)
>>> pca.fit(X)
PCA(n_components=2)
>>> print(pca.explained_variance_ratio_)
[0.9924... 0.0075...]
>>> print(pca.singular_values_)
[6.30061... 0.54980...]

>>> pca = PCA(n_components=2, svd_solver='full')
>>> pca.fit(X)
PCA(n_components=2, svd_solver='full')
>>> print(pca.explained_variance_ratio_)
[0.9924... 0.00755...]
>>> print(pca.singular_values_)
[6.30061... 0.54980...]

>>> pca = PCA(n_components=1, svd_solver='arpack')
>>> pca.fit(X)
PCA(n_components=1, svd_solver='arpack')
>>> print(pca.explained_variance_ratio_)
[0.99244...]
>>> print(pca.singular_values_)
[6.30061...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairConfusionMatrixMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairConfusionMatrixMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                               rdfs:comment """Pair confusion matrix arising from two clusterings [1]_.

The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix
between two clusterings by considering all pairs of samples and counting
pairs that are assigned into the same or into different clusters under
the true and predicted clusterings.

Considering a pair of samples that is clustered together a positive pair,
then as in binary classification the count of true negatives is
:math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is
:math:`C_{11}` and false positives is :math:`C_{01}`.

Read more in the :ref:`User Guide <pair_confusion_matrix>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=integral
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,), dtype=integral
    Cluster labels to evaluate.

Returns
-------
C : ndarray of shape (2, 2), dtype=np.int64
    The contingency matrix.

See Also
--------
sklearn.metrics.rand_score : Rand Score.
sklearn.metrics.adjusted_rand_score : Adjusted Rand Score.
sklearn.metrics.adjusted_mutual_info_score : Adjusted Mutual Information.

References
----------
.. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"
       Journal of Classification 2, 193218 (1985).
       <10.1007/BF01908075>`

Examples
--------
Perfectly matching labelings have all non-zero entries on the
diagonal regardless of actual label values:

  >>> from sklearn.metrics.cluster import pair_confusion_matrix
  >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
  array([[8, 0],
         [0, 4]]...

Labelings that assign all classes members to the same clusters
are complete but may be not always pure, hence penalized, and
have some off-diagonal non-zero entries:

  >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
  array([[8, 2],
         [0, 2]]...

Note that the matrix is not symmetric.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod> rdf:type owl:Class ;
                                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                   rdfs:comment """Compute minimum distances between one point and a set of points.

This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance).

This is mostly equivalent to calling:

    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)

but uses much less memory, and is faster for large arrays.

This function works with dense 2D arrays only.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
    Array containing points.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
    Arrays containing points.

axis : int, default=1
    Axis along which the argmin and distances are to be computed.

metric : str or callable, default=\"euclidean\"
    Metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.

    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    Distance matrices are not supported.

    Valid values for metric are:

    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
      'manhattan']

    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
      'yule']

    See the documentation for scipy.spatial.distance for details on these
    metrics.

    .. note::
       `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.

    .. note::
       `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).

metric_kwargs : dict, default=None
    Keyword arguments to pass to specified metric function.

Returns
-------
argmin : numpy.ndarray
    Y[argmin[i], :] is the row in Y that is closest to X[i, :].

See Also
--------
pairwise_distances : Distances between every pair of samples of X and Y.
pairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also
    returns the distances.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_distances_argmin
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> pairwise_distances_argmin(X, Y)
array([0, 1])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod> rdf:type owl:Class ;
                                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                      rdfs:comment """Compute minimum distances between one point and a set of points.

This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance). The minimal distances are
also returned.

This is mostly equivalent to calling:

    (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),
     pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))

but uses much less memory, and is faster for large arrays.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
    Array containing points.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
    Array containing points.

axis : int, default=1
    Axis along which the argmin and distances are to be computed.

metric : str or callable, default='euclidean'
    Metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.

    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    Distance matrices are not supported.

    Valid values for metric are:

    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
      'manhattan']

    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',
      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
      'yule']

    See the documentation for scipy.spatial.distance for details on these
    metrics.

    .. note::
       `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.

    .. note::
       `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).

metric_kwargs : dict, default=None
    Keyword arguments to pass to specified metric function.

Returns
-------
argmin : ndarray
    Y[argmin[i], :] is the row in Y that is closest to X[i, :].

distances : ndarray
    The array of minimum distances. `distances[i]` is the distance between
    the i-th row in X and the argmin[i]-th row in Y.

See Also
--------
pairwise_distances : Distances between every pair of samples of X and Y.
pairwise_distances_argmin : Same as `pairwise_distances_argmin_min` but only
    returns the argmins.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_distances_argmin_min
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> argmin, distances = pairwise_distances_argmin_min(X, Y)
>>> argmin
array([0, 1])
>>> distances
array([1., 1.])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod> rdf:type owl:Class ;
                                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                    rdfs:comment """Generate a distance matrix chunk by chunk with optional reduction.

In cases where not all of a pairwise distance matrix needs to be
stored at once, this is used to calculate pairwise distances in
``working_memory``-sized chunks.  If ``reduce_func`` is given, it is
run on each chunk and its return values are concatenated into lists,
arrays or sparse matrices.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)
    Array of pairwise distances between samples, or a feature array.
    The shape the array should be (n_samples_X, n_samples_X) if
    metric='precomputed' and (n_samples_X, n_features) otherwise.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None
    An optional second feature array. Only allowed if
    metric != \"precomputed\".

reduce_func : callable, default=None
    The function which is applied on each chunk of the distance matrix,
    reducing it to needed values.  ``reduce_func(D_chunk, start)``
    is called repeatedly, where ``D_chunk`` is a contiguous vertical
    slice of the pairwise distance matrix, starting at row ``start``.
    It should return one of: None; an array, a list, or a sparse matrix
    of length ``D_chunk.shape[0]``; or a tuple of such objects.
    Returning None is useful for in-place operations, rather than
    reductions.

    If None, pairwise_distances_chunked returns a generator of vertical
    chunks of the distance matrix.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by scipy.spatial.distance.pdist for its metric parameter,
    or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.
    If metric is \"precomputed\", X is assumed to be a distance matrix.
    Alternatively, if metric is a callable function, it is called on
    each pair of instances (rows) and the resulting value recorded.
    The callable should take two arrays from X as input and return a
    value indicating the distance between them.

n_jobs : int, default=None
    The number of jobs to use for the computation. This works by
    breaking down the pairwise matrix into n_jobs even slices and
    computing them in parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

working_memory : float, default=None
    The sought maximum memory for temporary distance matrix chunks.
    When None (default), the value of
    ``sklearn.get_config()['working_memory']`` is used.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a scipy.spatial.distance metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Yields
------
D_chunk : {ndarray, sparse matrix}
    A contiguous slice of distance matrix, optionally processed by
    ``reduce_func``.

Examples
--------
Without reduce_func:

>>> import numpy as np
>>> from sklearn.metrics import pairwise_distances_chunked
>>> X = np.random.RandomState(0).rand(5, 3)
>>> D_chunk = next(pairwise_distances_chunked(X))
>>> D_chunk
array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],
       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],
       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],
       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],
       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])

Retrieve all neighbors and average distance within radius r:

>>> r = .2
>>> def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r) for d in D_chunk]
...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)
...     return neigh, avg_dist
>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)
>>> neigh, avg_dist = next(gen)
>>> neigh
[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]
>>> avg_dist
array([0.039..., 0.        , 0.        , 0.039..., 0.        ])

Where r is defined per sample, we need to make use of ``start``:

>>> r = [.2, .4, .4, .3, .1]
>>> def reduce_func(D_chunk, start):
...     neigh = [np.flatnonzero(d < r[i])
...              for i, d in enumerate(D_chunk, start)]
...     return neigh
>>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))
>>> neigh
[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]

Force row-by-row generation by reducing ``working_memory``:

>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,
...                                  working_memory=0)
>>> next(gen)
[array([0, 3])]
>>> next(gen)
[array([0, 1])]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Compute the distance matrix from a vector array X and optional Y.

This method takes either a vector array or a distance matrix, and returns
a distance matrix. If the input is a vector array, the distances are
computed. If the input is a distances matrix, it is returned instead.

This method provides a safe way to take a distance matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.

If Y is given (default is None), then the returned matrix is the pairwise
distance between the arrays from both X and Y.

Valid values for metric are:

- From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
  'manhattan']. These metrics support sparse matrix
  inputs.
  ['nan_euclidean'] but it does not yet support sparse matrices.

- From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',
  'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',
  'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']
  See the documentation for scipy.spatial.distance for details on these
  metrics. These metrics do not support sparse matrix inputs.

.. note::
    `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.

.. note::
    `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).

Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are
valid scipy.spatial.distance metrics), the scikit-learn implementation
will be used, which is faster and has support for sparse matrices (except
for 'cityblock'). For a verbose description of the metrics from
scikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`
function.

Read more in the :ref:`User Guide <metrics>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)
    Array of pairwise distances between samples, or a feature array.
    The shape of the array should be (n_samples_X, n_samples_X) if
    metric == \"precomputed\" and (n_samples_X, n_features) otherwise.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None
    An optional second feature array. Only allowed if
    metric != \"precomputed\".

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by scipy.spatial.distance.pdist for its metric parameter, or
    a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.
    If metric is \"precomputed\", X is assumed to be a distance matrix.
    Alternatively, if metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays from X as input and return a value indicating
    the distance between them.

n_jobs : int, default=None
    The number of jobs to use for the computation. This works by breaking
    down the pairwise matrix into n_jobs even slices and computing them in
    parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

force_all_finite : bool or 'allow-nan', default=True
    Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored
    for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The
    possibilities are:

    - True: Force all values of array to be finite.
    - False: accepts np.inf, np.nan, pd.NA in array.
    - 'allow-nan': accepts only np.nan and pd.NA values in array. Values
      cannot be infinite.

    .. versionadded:: 0.22
       ``force_all_finite`` accepts the string ``'allow-nan'``.

    .. versionchanged:: 0.23
       Accepts `pd.NA` and converts it into `np.nan`.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a scipy.spatial.distance metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Returns
-------
D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)
    A distance matrix D such that D_{i, j} is the distance between the
    ith and jth vectors of the given matrix X, if Y is None.
    If Y is not None, then D_{i, j} is the distance between the ith array
    from X and the jth array from Y.

See Also
--------
pairwise_distances_chunked : Performs the same calculation as this
    function, but returns a generator of chunks of the distance matrix, in
    order to limit memory usage.
sklearn.metrics.pairwise.paired_distances : Computes the distances between
    corresponding elements of two arrays.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_distances
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> pairwise_distances(X, Y, metric='sqeuclidean')
array([[1., 2.],
       [2., 1.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                           rdfs:comment """Compute the kernel between arrays X and optional array Y.

This method takes either a vector array or a kernel matrix, and returns
a kernel matrix. If the input is a vector array, the kernels are
computed. If the input is a kernel matrix, it is returned instead.

This method provides a safe way to take a kernel matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.

If Y is given (default is None), then the returned matrix is the pairwise
kernel between the arrays from both X and Y.

Valid values for metric are:
    ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',
    'laplacian', 'sigmoid', 'cosine']

Read more in the :ref:`User Guide <metrics>`.

Parameters
----------
X : {array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)
    Array of pairwise kernels between samples, or a feature array.
    The shape of the array should be (n_samples_X, n_samples_X) if
    metric == \"precomputed\" and (n_samples_X, n_features) otherwise.

Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None
    A second feature array only if X has shape (n_samples_X, n_features).

metric : str or callable, default=\"linear\"
    The metric to use when calculating kernel between instances in a
    feature array. If metric is a string, it must be one of the metrics
    in ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.
    If metric is \"precomputed\", X is assumed to be a kernel matrix.
    Alternatively, if metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two rows from X as input and return the corresponding
    kernel value as a single number. This means that callables from
    :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on
    matrices, not single samples. Use the string identifying the kernel
    instead.

filter_params : bool, default=False
    Whether to filter invalid parameters or not.

n_jobs : int, default=None
    The number of jobs to use for the computation. This works by breaking
    down the pairwise matrix into n_jobs even slices and computing them in
    parallel.

    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the kernel function.

Returns
-------
K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)
    A kernel matrix K such that K_{i, j} is the kernel between the
    ith and jth vectors of the given matrix X, if Y is None.
    If Y is not None, then K_{i, j} is the kernel between the ith array
    from X and the jth array from Y.

Notes
-----
If metric is 'precomputed', Y is ignored and X is returned.

Examples
--------
>>> from sklearn.metrics.pairwise import pairwise_kernels
>>> X = [[0, 0, 0], [1, 1, 1]]
>>> Y = [[1, 0, 0], [1, 1, 0]]
>>> pairwise_kernels(X, Y, metric='linear')
array([[0., 0.],
       [1., 2.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod> rdf:type owl:Class ;
                                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                       rdfs:comment """Passive Aggressive Classifier.

Read more in the :ref:`User Guide <passive_aggressive>`.

Parameters
----------
C : float, default=1.0
    Maximum step size (regularization). Defaults to 1.0.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss > previous_loss - tol).

    .. versionadded:: 0.19

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set aside
    a stratified fraction of training data as validation and terminate
    training when validation score is not improving by at least `tol` for
    `n_iter_no_change` consecutive epochs.

    .. versionadded:: 0.20

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

    .. versionadded:: 0.20

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before early stopping.

    .. versionadded:: 0.20

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.

loss : str, default=\"hinge\"
    The loss function to be used:
    hinge: equivalent to PA-I in the reference paper.
    squared_hinge: equivalent to PA-II in the reference paper.

n_jobs : int or None, default=None
    The number of CPUs to use to do the OVA (One Versus All, for
    multi-class problems) computation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

random_state : int, RandomState instance, default=None
    Used to shuffle the training data, when ``shuffle`` is set to
    ``True``. Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary <random_state>`.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.

class_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None
    Preset for the class_weight fit parameter.

    Weights associated with classes. If not given, all classes
    are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

    .. versionadded:: 0.17
       parameter *class_weight* to automatically weight samples.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights and stores the
    result in the ``coef_`` attribute. If set to an int greater than 1,
    averaging will begin once the total number of samples seen reaches
    average. So average=10 will begin averaging after seeing 10 samples.

    .. versionadded:: 0.19
       parameter *average* to use weights averaging in SGD.

Attributes
----------
coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.
    For multiclass fits, it is the maximum over every binary fit.

classes_ : ndarray of shape (n_classes,)
    The unique classes labels.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

loss_function_ : callable
    Loss function used by the algorithm.

See Also
--------
SGDClassifier : Incrementally trained logistic regression.
Perceptron : Linear perceptron classifier.

References
----------
Online Passive-Aggressive Algorithms
<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)

Examples
--------
>>> from sklearn.linear_model import PassiveAggressiveClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_features=4, random_state=0)
>>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,
... tol=1e-3)
>>> clf.fit(X, y)
PassiveAggressiveClassifier(random_state=0)
>>> print(clf.coef_)
[[0.26642044 0.45070924 0.67251877 0.64185414]]
>>> print(clf.intercept_)
[1.84127814]
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod> rdf:type owl:Class ;
                                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                      rdfs:comment """Passive Aggressive Regressor.

Read more in the :ref:`User Guide <passive_aggressive>`.

Parameters
----------

C : float, default=1.0
    Maximum step size (regularization). Defaults to 1.0.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered. Defaults to True.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss > previous_loss - tol).

    .. versionadded:: 0.19

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation.
    score is not improving. If set to True, it will automatically set aside
    a fraction of training data as validation and terminate
    training when validation score is not improving by at least tol for
    n_iter_no_change consecutive epochs.

    .. versionadded:: 0.20

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

    .. versionadded:: 0.20

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before early stopping.

    .. versionadded:: 0.20

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.

loss : str, default=\"epsilon_insensitive\"
    The loss function to be used:
    epsilon_insensitive: equivalent to PA-I in the reference paper.
    squared_epsilon_insensitive: equivalent to PA-II in the reference
    paper.

epsilon : float, default=0.1
    If the difference between the current prediction and the correct label
    is below this threshold, the model is not updated.

random_state : int, RandomState instance, default=None
    Used to shuffle the training data, when ``shuffle`` is set to
    ``True``. Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary <random_state>`.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights and stores the
    result in the ``coef_`` attribute. If set to an int greater than 1,
    averaging will begin once the total number of samples seen reaches
    average. So average=10 will begin averaging after seeing 10 samples.

    .. versionadded:: 0.19
       parameter *average* to use weights averaging in SGD.

Attributes
----------
coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]
    Weights assigned to the features.

intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

See Also
--------
SGDRegressor : Linear model fitted by minimizing a regularized
    empirical loss with SGD.

References
----------
Online Passive-Aggressive Algorithms
<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).

Examples
--------
>>> from sklearn.linear_model import PassiveAggressiveRegressor
>>> from sklearn.datasets import make_regression

>>> X, y = make_regression(n_features=4, random_state=0)
>>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,
... tol=1e-3)
>>> regr.fit(X, y)
PassiveAggressiveRegressor(max_iter=100, random_state=0)
>>> print(regr.coef_)
[20.48736655 34.18818427 67.59122734 87.94731329]
>>> print(regr.intercept_)
[-0.02306214]
>>> print(regr.predict([[0, 0, 0, 0]]))
[-0.02306214]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod> rdf:type owl:Class ;
                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                      rdfs:comment """Linear perceptron classifier.

The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier`
by fixing the `loss` and `learning_rate` parameters as::

    SGDClassifier(loss=\"perceptron\", learning_rate=\"constant\")

Other available parameters are described below and are forwarded to
:class:`~sklearn.linear_model.SGDClassifier`.

Read more in the :ref:`User Guide <perceptron>`.

Parameters
----------

penalty : {'l2','l1','elasticnet'}, default=None
    The penalty (aka regularization term) to be used.

alpha : float, default=0.0001
    Constant that multiplies the regularization term if regularization is
    used.

l1_ratio : float, default=0.15
    The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.
    `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.
    Only used if `penalty='elasticnet'`.

    .. versionadded:: 0.24

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`partial_fit` method.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss > previous_loss - tol).

    .. versionadded:: 0.19

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.

eta0 : float, default=1
    Constant by which the updates are multiplied.

n_jobs : int, default=None
    The number of CPUs to use to do the OVA (One Versus All, for
    multi-class problems) computation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

random_state : int, RandomState instance or None, default=0
    Used to shuffle the training data, when ``shuffle`` is set to
    ``True``. Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary <random_state>`.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set aside
    a stratified fraction of training data as validation and terminate
    training when validation score is not improving by at least `tol` for
    `n_iter_no_change` consecutive epochs.

    .. versionadded:: 0.20

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if early_stopping is True.

    .. versionadded:: 0.20

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before early stopping.

    .. versionadded:: 0.20

class_weight : dict, {class_label: weight} or \"balanced\", default=None
    Preset for the class_weight fit parameter.

    Weights associated with classes. If not given, all classes
    are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution. See
    :term:`the Glossary <warm_start>`.

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    The unique classes labels.

coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

loss_function_ : concreteLossFunction
    The function that determines the loss, or difference between the
    output of the algorithm and the target values.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.
    For multiclass fits, it is the maximum over every binary fit.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

See Also
--------
sklearn.linear_model.SGDClassifier : Linear classifiers
    (SVM, logistic regression, etc.) with SGD training.

Notes
-----
``Perceptron`` is a classification algorithm which shares the same
underlying implementation with ``SGDClassifier``. In fact,
``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",
eta0=1, learning_rate=\"constant\", penalty=None)`.

References
----------
https://en.wikipedia.org/wiki/Perceptron and references therein.

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.linear_model import Perceptron
>>> X, y = load_digits(return_X_y=True)
>>> clf = Perceptron(tol=1e-3, random_state=0)
>>> clf.fit(X, y)
Perceptron()
>>> clf.score(X, y)
0.939...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculation
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculation> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """Generalized Linear Model with a Poisson distribution.

This regressor uses the 'log' link function.

Read more in the :ref:`User Guide <Generalized_linear_models>`.

.. versionadded:: 0.23

Parameters
----------
alpha : float, default=1
    Constant that multiplies the L2 penalty term and determines the
    regularization strength. ``alpha = 0`` is equivalent to unpenalized
    GLMs. In this case, the design matrix `X` must have full column rank
    (no collinearities).
    Values of `alpha` must be in the range `[0.0, inf)`.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the linear predictor (`X @ coef + intercept`).

solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'
    Algorithm to use in the optimization problem:

    'lbfgs'
        Calls scipy's L-BFGS-B optimizer.

    'newton-cholesky'
        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to
        iterated reweighted least squares) with an inner Cholesky based solver.
        This solver is a good choice for `n_samples` >> `n_features`, especially
        with one-hot encoded categorical features with rare categories. Be aware
        that the memory usage of this solver has a quadratic dependency on
        `n_features` because it explicitly computes the Hessian matrix.

        .. versionadded:: 1.2

max_iter : int, default=100
    The maximal number of iterations for the solver.
    Values must be in the range `[1, inf)`.

tol : float, default=1e-4
    Stopping criterion. For the lbfgs solver,
    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``
    where ``g_j`` is the j-th component of the gradient (derivative) of
    the objective function.
    Values must be in the range `(0.0, inf)`.

warm_start : bool, default=False
    If set to ``True``, reuse the solution of the previous call to ``fit``
    as initialization for ``coef_`` and ``intercept_`` .

verbose : int, default=0
    For the lbfgs solver set verbose to any positive number for verbosity.
    Values must be in the range `[0, inf)`.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the linear predictor (`X @ coef_ +
    intercept_`) in the GLM.

intercept_ : float
    Intercept (a.k.a. bias) added to linear predictor.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Actual number of iterations used in the solver.

See Also
--------
TweedieRegressor : Generalized Linear Model with a Tweedie distribution.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.PoissonRegressor()
>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]
>>> y = [12, 17, 22, 21]
>>> clf.fit(X, y)
PoissonRegressor()
>>> clf.score(X, y)
0.990...
>>> clf.coef_
array([0.121..., 0.158...])
>>> clf.intercept_
2.088...
>>> clf.predict([[1, 1], [3, 4]])
array([10.676..., 21.875...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                              rdfs:comment """Generate polynomial and interaction features.

Generate a new feature matrix consisting of all polynomial combinations
of the features with degree less than or equal to the specified degree.
For example, if an input sample is two dimensional and of the form
[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].

Read more in the :ref:`User Guide <polynomial_features>`.

Parameters
----------
degree : int or tuple (min_degree, max_degree), default=2
    If a single int is given, it specifies the maximal degree of the
    polynomial features. If a tuple `(min_degree, max_degree)` is passed,
    then `min_degree` is the minimum and `max_degree` is the maximum
    polynomial degree of the generated features. Note that `min_degree=0`
    and `min_degree=1` are equivalent as outputting the degree zero term is
    determined by `include_bias`.

interaction_only : bool, default=False
    If `True`, only interaction features are produced: features that are
    products of at most `degree` *distinct* input features, i.e. terms with
    power of 2 or higher of the same input feature are excluded:

        - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc.
        - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.

include_bias : bool, default=True
    If `True` (default), then include a bias column, the feature in which
    all polynomial powers are zero (i.e. a column of ones - acts as an
    intercept term in a linear model).

order : {'C', 'F'}, default='C'
    Order of output array in the dense case. `'F'` order is faster to
    compute, but may slow down subsequent estimators.

    .. versionadded:: 0.21

Attributes
----------
powers_ : ndarray of shape (`n_output_features_`, `n_features_in_`)
    `powers_[i, j]` is the exponent of the jth input in the ith output.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_output_features_ : int
    The total number of polynomial output features. The number of output
    features is computed by iterating over all suitably sized combinations
    of input features.

See Also
--------
SplineTransformer : Transformer that generates univariate B-spline bases
    for features.

Notes
-----
Be aware that the number of features in the output array scales
polynomially in the number of features of the input array, and
exponentially in the degree. High degrees can cause overfitting.

See :ref:`examples/linear_model/plot_polynomial_interpolation.py
<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`

Examples
--------
>>> import numpy as np
>>> from sklearn.preprocessing import PolynomialFeatures
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)
array([[ 1.,  0.,  1.,  0.,  0.,  1.],
       [ 1.,  2.,  3.,  4.,  6.,  9.],
       [ 1.,  4.,  5., 16., 20., 25.]])
>>> poly = PolynomialFeatures(interaction_only=True)
>>> poly.fit_transform(X)
array([[ 1.,  0.,  1.,  0.],
       [ 1.,  2.,  3.,  6.],
       [ 1.,  4.,  5., 20.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                            rdfs:comment """Apply a power transform featurewise to make data more Gaussian-like.

Power transforms are a family of parametric, monotonic transformations
that are applied to make data more Gaussian-like. This is useful for
modeling issues related to heteroscedasticity (non-constant variance),
or other situations where normality is desired.

Currently, PowerTransformer supports the Box-Cox transform and the
Yeo-Johnson transform. The optimal parameter for stabilizing variance and
minimizing skewness is estimated through maximum likelihood.

Box-Cox requires input data to be strictly positive, while Yeo-Johnson
supports both positive or negative data.

By default, zero-mean, unit-variance normalization is applied to the
transformed data.

For an example visualization, refer to :ref:`Compare PowerTransformer with
other scalers <plot_all_scaling_power_transformer_section>`. To see the
effect of Box-Cox and Yeo-Johnson transformations on different
distributions, see:
:ref:`sphx_glr_auto_examples_preprocessing_plot_map_data_to_normal.py`.

Read more in the :ref:`User Guide <preprocessing_transformer>`.

.. versionadded:: 0.20

Parameters
----------
method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'
    The power transform method. Available methods are:

    - 'yeo-johnson' [1]_, works with positive and negative values
    - 'box-cox' [2]_, only works with strictly positive values

standardize : bool, default=True
    Set to True to apply zero-mean, unit-variance normalization to the
    transformed output.

copy : bool, default=True
    Set to False to perform inplace computation during transformation.

Attributes
----------
lambdas_ : ndarray of float of shape (n_features,)
    The parameters of the power transformation for the selected features.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
power_transform : Equivalent function without the estimator API.

QuantileTransformer : Maps data to a standard normal distribution with
    the parameter `output_distribution='normal'`.

Notes
-----
NaNs are treated as missing values: disregarded in ``fit``, and maintained
in ``transform``.

References
----------

.. [1] :doi:`I.K. Yeo and R.A. Johnson, \"A new family of power
       transformations to improve normality or symmetry.\" Biometrika,
       87(4), pp.954-959, (2000). <10.1093/biomet/87.4.954>`

.. [2] :doi:`G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\",
       Journal of the Royal Statistical Society B, 26, 211-252 (1964).
       <10.1111/j.2517-6161.1964.tb00553.x>`

Examples
--------
>>> import numpy as np
>>> from sklearn.preprocessing import PowerTransformer
>>> pt = PowerTransformer()
>>> data = [[1, 2], [3, 2], [4, 5]]
>>> print(pt.fit(data))
PowerTransformer()
>>> print(pt.lambdas_)
[ 1.386... -3.100...]
>>> print(pt.transform(data))
[[-1.316... -0.707...]
 [ 0.209... -0.707...]
 [ 1.106...  1.414...]]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                rdfs:comment """Compute precision-recall pairs for different probability thresholds.

Note: this implementation is restricted to the binary classification task.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The last precision and recall values are 1. and 0. respectively and do not
have a corresponding threshold. This ensures that the graph starts on the
y axis.

The first precision and recall values are precision=class balance and recall=1.0
which corresponds to a classifier that always predicts the positive class.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True binary labels. If labels are not either {-1, 1} or {0, 1}, then
    pos_label should be explicitly given.

probas_pred : array-like of shape (n_samples,)
    Target scores, can either be probability estimates of the positive
    class, or non-thresholded measure of decisions (as returned by
    `decision_function` on some classifiers).

pos_label : int, float, bool or str, default=None
    The label of the positive class.
    When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},
    ``pos_label`` is set to 1, otherwise an error will be raised.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

drop_intermediate : bool, default=False
    Whether to drop some suboptimal thresholds which would not appear
    on a plotted precision-recall curve. This is useful in order to create
    lighter precision-recall curves.

    .. versionadded:: 1.3

Returns
-------
precision : ndarray of shape (n_thresholds + 1,)
    Precision values such that element i is the precision of
    predictions with score >= thresholds[i] and the last element is 1.

recall : ndarray of shape (n_thresholds + 1,)
    Decreasing recall values such that element i is the recall of
    predictions with score >= thresholds[i] and the last element is 0.

thresholds : ndarray of shape (n_thresholds,)
    Increasing thresholds on the decision function used to compute
    precision and recall where `n_thresholds = len(np.unique(probas_pred))`.

See Also
--------
PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given
    a binary classifier.
PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve
    using predictions from a binary classifier.
average_precision_score : Compute average precision from prediction scores.
det_curve: Compute error rates for different probability thresholds.
roc_curve : Compute Receiver operating characteristic (ROC) curve.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_curve
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> precision, recall, thresholds = precision_recall_curve(
...     y_true, y_scores)
>>> precision
array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])
>>> recall
array([1. , 1. , 0.5, 0.5, 0. ])
>>> thresholds
array([0.1 , 0.35, 0.4 , 0.8 ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod> rdf:type owl:Class ;
                                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                        rdfs:comment """Compute precision, recall, F-measure and support for each class.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label a negative sample as
positive.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The F-beta score can be interpreted as a weighted harmonic mean of
the precision and recall, where an F-beta score reaches its best
value at 1 and worst score at 0.

The F-beta score weights recall more than precision by a factor of
``beta``. ``beta == 1.0`` means recall and precision are equally important.

The support is the number of occurrences of each class in ``y_true``.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and metrics for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
metrics for all `labels` are either returned or averaged depending on the `average`
parameter. Use `labels` specify the set of labels to calculate metrics for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

beta : float, default=1.0
    The strength of recall versus precision in the F-score.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a \"negative
    class\". Labels not present in the data can be included and will be
    \"assigned\" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'binary', 'micro', 'macro', 'samples', 'weighted'},             default=None
    If ``None``, the metrics for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

warn_for : list, tuple or set, for internal use
    This determines which warnings will be made in the case that this
    function is being used to return only one of its metrics.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"
    Sets the value to return when there is a zero division:
       - recall: when there are no positive labels
       - precision: when there are no positive predictions
       - f-score: both

    Notes:
    - If set to \"warn\", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
precision : float (if average is not None) or array of float, shape =        [n_unique_labels]
    Precision score.

recall : float (if average is not None) or array of float, shape =        [n_unique_labels]
    Recall score.

fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]
    F-beta score.

support : None (if average is not None) or array of int, shape =        [n_unique_labels]
    The number of occurrences of each label in ``y_true``.

Notes
-----
When ``true positive + false positive == 0``, precision is undefined.
When ``true positive + false negative == 0``, recall is undefined. When
``true positive + false negative + false positive == 0``, f-score is
undefined. In such cases, by default the metric will be set to 0, and
``UndefinedMetricWarning`` will be raised. This behavior can be modified
with ``zero_division``.

References
----------
.. [1] `Wikipedia entry for the Precision and recall
       <https://en.wikipedia.org/wiki/Precision_and_recall>`_.

.. [2] `Wikipedia entry for the F1-score
       <https://en.wikipedia.org/wiki/F1_score>`_.

.. [3] `Discriminative Methods for Multi-labeled Classification Advances
       in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu
       Godbole, Sunita Sarawagi
       <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_fscore_support
>>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
>>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
>>> precision_recall_fscore_support(y_true, y_pred, average='macro')
(0.22..., 0.33..., 0.26..., None)
>>> precision_recall_fscore_support(y_true, y_pred, average='micro')
(0.33..., 0.33..., 0.33..., None)
>>> precision_recall_fscore_support(y_true, y_pred, average='weighted')
(0.22..., 0.33..., 0.26..., None)

It is possible to compute per-label precisions, recalls, F1-scores and
supports instead of averaging:

>>> precision_recall_fscore_support(y_true, y_pred, average=None,
... labels=['pig', 'dog', 'cat'])
(array([0.        , 0.        , 0.66...]),
 array([0., 0., 1.]), array([0. , 0. , 0.8]),
 array([2, 2, 2]))""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                          rdfs:comment """Compute the precision.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.

The best value is 1 and the worst value is 0.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and precision for both classes are computed, then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
precision for all `labels` are either returned or averaged depending on the
`average` parameter. Use `labels` specify the set of labels to calculate precision
for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a \"negative
    class\". Labels not present in the data can be included and will be
    \"assigned\" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"
    Sets the value to return when there is a zero division.

    Notes:
    - If set to \"warn\", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)
    Precision of the positive class in binary classification or weighted
    average of the precision of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute precision, recall, F-measure and
    support for each class.
recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the
    number of true positives and ``fn`` the number of false negatives.
PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given
    an estimator and some data.
PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given
    binary class predictions.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.

Notes
-----
When ``true positive + false positive == 0``, precision returns 0 and
raises ``UndefinedMetricWarning``. This behavior can be
modified with ``zero_division``.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import precision_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> precision_score(y_true, y_pred, average='macro')
0.22...
>>> precision_score(y_true, y_pred, average='micro')
0.33...
>>> precision_score(y_true, y_pred, average='weighted')
0.22...
>>> precision_score(y_true, y_pred, average=None)
array([0.66..., 0.        , 0.        ])
>>> y_pred = [0, 0, 0, 0, 0, 0]
>>> precision_score(y_true, y_pred, average=None)
array([0.33..., 0.        , 0.        ])
>>> precision_score(y_true, y_pred, average=None, zero_division=1)
array([0.33..., 1.        , 1.        ])
>>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)
array([0.33...,        nan,        nan])

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> precision_score(y_true, y_pred, average=None)
array([0.5, 1. , 1. ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PredefinedSplitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PredefinedSplitMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                           rdfs:comment """Predefined split cross-validator.

Provides train/test indices to split data into train/test sets using a
predefined scheme specified by the user with the ``test_fold`` parameter.

Read more in the :ref:`User Guide <predefined_split>`.

.. versionadded:: 0.16

Parameters
----------
test_fold : array-like of shape (n_samples,)
    The entry ``test_fold[i]`` represents the index of the test set that
    sample ``i`` belongs to. It is possible to exclude sample ``i`` from
    any test set (i.e. include sample ``i`` in every training set) by
    setting ``test_fold[i]`` equal to -1.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import PredefinedSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> test_fold = [0, 1, -1, 1]
>>> ps = PredefinedSplit(test_fold)
>>> ps.get_n_splits()
2
>>> print(ps)
PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))
>>> for i, (train_index, test_index) in enumerate(ps.split()):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[1 2 3]
  Test:  index=[0]
Fold 1:
  Train: index=[0 2]
  Test:  index=[1 3]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                             rdfs:comment """Linear regression model that predicts conditional quantiles.

The linear :class:`QuantileRegressor` optimizes the pinball loss for a
desired `quantile` and is robust to outliers.

This model uses an L1 regularization like
:class:`~sklearn.linear_model.Lasso`.

Read more in the :ref:`User Guide <quantile_regression>`.

.. versionadded:: 1.0

Parameters
----------
quantile : float, default=0.5
    The quantile that the model tries to predict. It must be strictly
    between 0 and 1. If 0.5 (default), the model predicts the 50%
    quantile, i.e. the median.

alpha : float, default=1.0
    Regularization constant that multiplies the L1 penalty term.

fit_intercept : bool, default=True
    Whether or not to fit the intercept.

solver : {'highs-ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='highs'
    Method used by :func:`scipy.optimize.linprog` to solve the linear
    programming formulation.

    From `scipy>=1.6.0`, it is recommended to use the highs methods because
    they are the fastest ones. Solvers \"highs-ds\", \"highs-ipm\" and \"highs\"
    support sparse input data and, in fact, always convert to sparse csc.

    From `scipy>=1.11.0`, \"interior-point\" is not available anymore.

    .. versionchanged:: 1.4
       The default of `solver` changed to `\"highs\"` in version 1.4.

solver_options : dict, default=None
    Additional parameters passed to :func:`scipy.optimize.linprog` as
    options. If `None` and if `solver='interior-point'`, then
    `{\"lstsq\": True}` is passed to :func:`scipy.optimize.linprog` for the
    sake of stability.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the features.

intercept_ : float
    The intercept of the model, aka bias term.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    The actual number of iterations performed by the solver.

See Also
--------
Lasso : The Lasso is a linear model that estimates sparse coefficients
    with l1 regularization.
HuberRegressor : Linear regression model that is robust to outliers.

Examples
--------
>>> from sklearn.linear_model import QuantileRegressor
>>> import numpy as np
>>> n_samples, n_features = 10, 2
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> # the two following lines are optional in practice
>>> from sklearn.utils.fixes import sp_version, parse_version
>>> solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"
>>> reg = QuantileRegressor(quantile=0.8, solver=solver).fit(X, y)
>>> np.mean(y <= reg.predict(X))
0.8""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod> rdf:type owl:Class ;
                                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                               rdfs:comment """Transform features using quantiles information.

This method transforms the features to follow a uniform or a normal
distribution. Therefore, for a given feature, this transformation tends
to spread out the most frequent values. It also reduces the impact of
(marginal) outliers: this is therefore a robust preprocessing scheme.

The transformation is applied on each feature independently. First an
estimate of the cumulative distribution function of a feature is
used to map the original values to a uniform distribution. The obtained
values are then mapped to the desired output distribution using the
associated quantile function. Features values of new/unseen data that fall
below or above the fitted range will be mapped to the bounds of the output
distribution. Note that this transform is non-linear. It may distort linear
correlations between variables measured at the same scale but renders
variables measured at different scales more directly comparable.

For example visualizations, refer to :ref:`Compare QuantileTransformer with
other scalers <plot_all_scaling_quantile_transformer_section>`.

Read more in the :ref:`User Guide <preprocessing_transformer>`.

.. versionadded:: 0.19

Parameters
----------
n_quantiles : int, default=1000 or n_samples
    Number of quantiles to be computed. It corresponds to the number
    of landmarks used to discretize the cumulative distribution function.
    If n_quantiles is larger than the number of samples, n_quantiles is set
    to the number of samples as a larger number of quantiles does not give
    a better approximation of the cumulative distribution function
    estimator.

output_distribution : {'uniform', 'normal'}, default='uniform'
    Marginal distribution for the transformed data. The choices are
    'uniform' (default) or 'normal'.

ignore_implicit_zeros : bool, default=False
    Only applies to sparse matrices. If True, the sparse entries of the
    matrix are discarded to compute the quantile statistics. If False,
    these entries are treated as zeros.

subsample : int, default=10_000
    Maximum number of samples used to estimate the quantiles for
    computational efficiency. Note that the subsampling procedure may
    differ for value-identical sparse and dense matrices.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for subsampling and smoothing
    noise.
    Please see ``subsample`` for more details.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

copy : bool, default=True
    Set to False to perform inplace transformation and avoid a copy (if the
    input is already a numpy array).

Attributes
----------
n_quantiles_ : int
    The actual number of quantiles used to discretize the cumulative
    distribution function.

quantiles_ : ndarray of shape (n_quantiles, n_features)
    The values corresponding the quantiles of reference.

references_ : ndarray of shape (n_quantiles, )
    Quantiles of references.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
quantile_transform : Equivalent function without the estimator API.
PowerTransformer : Perform mapping to a normal distribution using a power
    transform.
StandardScaler : Perform standardization that is faster, but less robust
    to outliers.
RobustScaler : Perform robust standardization that removes the influence
    of outliers but does not put outliers and inliers on the same scale.

Notes
-----
NaNs are treated as missing values: disregarded in fit, and maintained in
transform.

Examples
--------
>>> import numpy as np
>>> from sklearn.preprocessing import QuantileTransformer
>>> rng = np.random.RandomState(0)
>>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)
>>> qt = QuantileTransformer(n_quantiles=10, random_state=0)
>>> qt.fit_transform(X)
array([...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                   rdfs:comment """:math:`R^2` (coefficient of determination) regression score function.

Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). In the general case when the true y is
non-constant, a constant model that always predicts the average y
disregarding the input features would get a :math:`R^2` score of 0.0.

In the particular case when ``y_true`` is constant, the :math:`R^2` score
is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``
(imperfect predictions). To prevent such non-finite numbers to pollute
higher-level experiments such as a grid search cross-validation, by default
these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect
predictions) respectively. You can set ``force_finite`` to ``False`` to
prevent this fix from happening.

Note: when the prediction residuals have zero mean, the :math:`R^2` score
is identical to the
:func:`Explained Variance score <explained_variance_score>`.

Read more in the :ref:`User Guide <r2_score>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'

    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.
    Default is \"uniform_average\".

    'raw_values' :
        Returns a full set of scores in case of multioutput input.

    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.

    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.

    .. versionchanged:: 0.19
        Default value of multioutput is 'uniform_average'.

force_finite : bool, default=True
    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant
    data should be replaced with real numbers (``1.0`` if prediction is
    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting
    for hyperparameters' search procedures (e.g. grid search
    cross-validation).

    .. versionadded:: 1.1

Returns
-------
z : float or ndarray of floats
    The :math:`R^2` score or ndarray of scores if 'multioutput' is
    'raw_values'.

Notes
-----
This is not a symmetric function.

Unlike most other scores, :math:`R^2` score may be negative (it need not
actually be the square of a quantity R).

This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.

References
----------
.. [1] `Wikipedia entry on the Coefficient of determination
        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_

Examples
--------
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred,
...          multioutput='variance_weighted')
0.938...
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> r2_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> r2_score(y_true, y_pred)
0.0
>>> y_true = [1, 2, 3]
>>> y_pred = [3, 2, 1]
>>> r2_score(y_true, y_pred)
-3.0
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> r2_score(y_true, y_pred)
1.0
>>> r2_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> r2_score(y_true, y_pred)
0.0
>>> r2_score(y_true, y_pred, force_finite=False)
-inf""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """RANSAC (RANdom SAmple Consensus) algorithm.

RANSAC is an iterative algorithm for the robust estimation of parameters
from a subset of inliers from the complete data set.

Read more in the :ref:`User Guide <ransac_regression>`.

Parameters
----------
estimator : object, default=None
    Base estimator object which implements the following methods:

     * `fit(X, y)`: Fit model to given training data and target values.
     * `score(X, y)`: Returns the mean accuracy on the given test data,
       which is used for the stop criterion defined by `stop_score`.
       Additionally, the score is used to decide which of two equally
       large consensus sets is chosen as the better one.
     * `predict(X)`: Returns predicted values using the linear model,
       which is used to compute residual error using loss function.

    If `estimator` is None, then
    :class:`~sklearn.linear_model.LinearRegression` is used for
    target values of dtype float.

    Note that the current implementation only supports regression
    estimators.

min_samples : int (>= 1) or float ([0, 1]), default=None
    Minimum number of samples chosen randomly from original data. Treated
    as an absolute number of samples for `min_samples >= 1`, treated as a
    relative number `ceil(min_samples * X.shape[0])` for
    `min_samples < 1`. This is typically chosen as the minimal number of
    samples necessary to estimate the given `estimator`. By default a
    :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and
    `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly
    dependent upon the model, so if a `estimator` other than
    :class:`~sklearn.linear_model.LinearRegression` is used, the user must
    provide a value.

residual_threshold : float, default=None
    Maximum residual for a data sample to be classified as an inlier.
    By default the threshold is chosen as the MAD (median absolute
    deviation) of the target values `y`. Points whose residuals are
    strictly equal to the threshold are considered as inliers.

is_data_valid : callable, default=None
    This function is called with the randomly selected data before the
    model is fitted to it: `is_data_valid(X, y)`. If its return value is
    False the current randomly chosen sub-sample is skipped.

is_model_valid : callable, default=None
    This function is called with the estimated model and the randomly
    selected data: `is_model_valid(model, X, y)`. If its return value is
    False the current randomly chosen sub-sample is skipped.
    Rejecting samples with this function is computationally costlier than
    with `is_data_valid`. `is_model_valid` should therefore only be used if
    the estimated model is needed for making the rejection decision.

max_trials : int, default=100
    Maximum number of iterations for random sample selection.

max_skips : int, default=np.inf
    Maximum number of iterations that can be skipped due to finding zero
    inliers or invalid data defined by ``is_data_valid`` or invalid models
    defined by ``is_model_valid``.

    .. versionadded:: 0.19

stop_n_inliers : int, default=np.inf
    Stop iteration if at least this number of inliers are found.

stop_score : float, default=np.inf
    Stop iteration if score is greater equal than this threshold.

stop_probability : float in range [0, 1], default=0.99
    RANSAC iteration stops if at least one outlier-free set of the training
    data is sampled in RANSAC. This requires to generate at least N
    samples (iterations)::

        N >= log(1 - probability) / log(1 - e**m)

    where the probability (confidence) is typically set to high value such
    as 0.99 (the default) and e is the current fraction of inliers w.r.t.
    the total number of samples.

loss : str, callable, default='absolute_error'
    String inputs, 'absolute_error' and 'squared_error' are supported which
    find the absolute error and squared error per sample respectively.

    If ``loss`` is a callable, then it should be a function that takes
    two arrays as inputs, the true and predicted value and returns a 1-D
    array with the i-th value of the array corresponding to the loss
    on ``X[i]``.

    If the loss on a sample is greater than the ``residual_threshold``,
    then this sample is classified as an outlier.

    .. versionadded:: 0.18

random_state : int, RandomState instance, default=None
    The generator used to initialize the centers.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
estimator_ : object
    Best fitted model (copy of the `estimator` object).

n_trials_ : int
    Number of random selection trials until one of the stop criteria is
    met. It is always ``<= max_trials``.

inlier_mask_ : bool array of shape [n_samples]
    Boolean mask of inliers classified as ``True``.

n_skips_no_inliers_ : int
    Number of iterations skipped due to finding zero inliers.

    .. versionadded:: 0.19

n_skips_invalid_data_ : int
    Number of iterations skipped due to invalid data defined by
    ``is_data_valid``.

    .. versionadded:: 0.19

n_skips_invalid_model_ : int
    Number of iterations skipped due to an invalid model defined by
    ``is_model_valid``.

    .. versionadded:: 0.19

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
HuberRegressor : Linear regression model that is robust to outliers.
TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.
SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.

References
----------
.. [1] https://en.wikipedia.org/wiki/RANSAC
.. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf
.. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf

Examples
--------
>>> from sklearn.linear_model import RANSACRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(
...     n_samples=200, n_features=2, noise=4.0, random_state=0)
>>> reg = RANSACRegressor(random_state=0).fit(X, y)
>>> reg.score(X, y)
0.9885...
>>> reg.predict(X[:1,])
array([-31.9417...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                 rdfs:comment """Recursive feature elimination with cross-validation to select features.

The number of features selected is tuned automatically by fitting an :class:`RFE`
selector on the different cross-validation splits (provided by the `cv` parameter).
The performance of the :class:`RFE` selector are evaluated using `scorer` for
different number of selected features and aggregated together. Finally, the scores
are averaged across folds and the number of features selected is set to the number
of features that maximize the cross-validation score.
See glossary entry for :term:`cross-validation estimator`.

Read more in the :ref:`User Guide <rfe>`.

Parameters
----------
estimator : ``Estimator`` instance
    A supervised learning estimator with a ``fit`` method that provides
    information about feature importance either through a ``coef_``
    attribute or through a ``feature_importances_`` attribute.

step : int or float, default=1
    If greater than or equal to 1, then ``step`` corresponds to the
    (integer) number of features to remove at each iteration.
    If within (0.0, 1.0), then ``step`` corresponds to the percentage
    (rounded down) of features to remove at each iteration.
    Note that the last iteration may remove fewer than ``step`` features in
    order to reach ``min_features_to_select``.

min_features_to_select : int, default=1
    The minimum number of features to be selected. This number of features
    will always be scored, even if the difference between the original
    feature count and ``min_features_to_select`` isn't divisible by
    ``step``.

    .. versionadded:: 0.20

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross-validation,
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if ``y`` is binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used. If the
    estimator is a classifier or if ``y`` is neither binary nor multiclass,
    :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value of None changed from 3-fold to 5-fold.

scoring : str, callable or None, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.

verbose : int, default=0
    Controls verbosity of output.

n_jobs : int or None, default=None
    Number of cores to run in parallel while fitting across folds.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionadded:: 0.18

importance_getter : str or callable, default='auto'
    If 'auto', uses the feature importance either through a `coef_`
    or `feature_importances_` attributes of estimator.

    Also accepts a string that specifies an attribute name/path
    for extracting feature importance.
    For example, give `regressor_.coef_` in case of
    :class:`~sklearn.compose.TransformedTargetRegressor`  or
    `named_steps.clf.feature_importances_` in case of
    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.

    If `callable`, overrides the default feature importance getter.
    The callable is passed with the fitted estimator and it should
    return importance for each feature.

    .. versionadded:: 0.24

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    The classes labels. Only available when `estimator` is a classifier.

estimator_ : ``Estimator`` instance
    The fitted estimator used to select features.

cv_results_ : dict of ndarrays
    A dict with keys:

    split(k)_test_score : ndarray of shape (n_subsets_of_features,)
        The cross-validation scores across (k)th fold.

    mean_test_score : ndarray of shape (n_subsets_of_features,)
        Mean of scores over the folds.

    std_test_score : ndarray of shape (n_subsets_of_features,)
        Standard deviation of scores over the folds.

    .. versionadded:: 1.0

n_features_ : int
    The number of selected features with cross-validation.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

ranking_ : narray of shape (n_features,)
    The feature ranking, such that `ranking_[i]`
    corresponds to the ranking
    position of the i-th feature.
    Selected (i.e., estimated best)
    features are assigned rank 1.

support_ : ndarray of shape (n_features,)
    The mask of selected features.

See Also
--------
RFE : Recursive feature elimination.

Notes
-----
The size of all values in ``cv_results_`` is equal to
``ceil((n_features - min_features_to_select) / step) + 1``,
where step is the number of features removed at each iteration.

Allows NaN/Inf in the input if the underlying estimator does as well.

References
----------

.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection
       for cancer classification using support vector machines\",
       Mach. Learn., 46(1-3), 389--422, 2002.

Examples
--------
The following example shows how to retrieve the a-priori not known 5
informative features in the Friedman #1 dataset.

>>> from sklearn.datasets import make_friedman1
>>> from sklearn.feature_selection import RFECV
>>> from sklearn.svm import SVR
>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
>>> estimator = SVR(kernel=\"linear\")
>>> selector = RFECV(estimator, step=1, cv=5)
>>> selector = selector.fit(X, y)
>>> selector.support_
array([ True,  True,  True,  True,  True, False, False, False, False,
       False])
>>> selector.ranking_
array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                               rdfs:comment """Feature ranking with recursive feature elimination.

Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination
(RFE) is to select features by recursively considering smaller and smaller
sets of features. First, the estimator is trained on the initial set of
features and the importance of each feature is obtained either through
any specific attribute or callable.
Then, the least important features are pruned from current set of features.
That procedure is recursively repeated on the pruned set until the desired
number of features to select is eventually reached.

Read more in the :ref:`User Guide <rfe>`.

Parameters
----------
estimator : ``Estimator`` instance
    A supervised learning estimator with a ``fit`` method that provides
    information about feature importance
    (e.g. `coef_`, `feature_importances_`).

n_features_to_select : int or float, default=None
    The number of features to select. If `None`, half of the features are
    selected. If integer, the parameter is the absolute number of features
    to select. If float between 0 and 1, it is the fraction of features to
    select.

    .. versionchanged:: 0.24
       Added float values for fractions.

step : int or float, default=1
    If greater than or equal to 1, then ``step`` corresponds to the
    (integer) number of features to remove at each iteration.
    If within (0.0, 1.0), then ``step`` corresponds to the percentage
    (rounded down) of features to remove at each iteration.

verbose : int, default=0
    Controls verbosity of output.

importance_getter : str or callable, default='auto'
    If 'auto', uses the feature importance either through a `coef_`
    or `feature_importances_` attributes of estimator.

    Also accepts a string that specifies an attribute name/path
    for extracting feature importance (implemented with `attrgetter`).
    For example, give `regressor_.coef_` in case of
    :class:`~sklearn.compose.TransformedTargetRegressor`  or
    `named_steps.clf.feature_importances_` in case of
    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.

    If `callable`, overrides the default feature importance getter.
    The callable is passed with the fitted estimator and it should
    return importance for each feature.

    .. versionadded:: 0.24

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    The classes labels. Only available when `estimator` is a classifier.

estimator_ : ``Estimator`` instance
    The fitted estimator used to select features.

n_features_ : int
    The number of selected features.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

ranking_ : ndarray of shape (n_features,)
    The feature ranking, such that ``ranking_[i]`` corresponds to the
    ranking position of the i-th feature. Selected (i.e., estimated
    best) features are assigned rank 1.

support_ : ndarray of shape (n_features,)
    The mask of selected features.

See Also
--------
RFECV : Recursive feature elimination with built-in cross-validated
    selection of the best number of features.
SelectFromModel : Feature selection based on thresholds of importance
    weights.
SequentialFeatureSelector : Sequential cross-validation based feature
    selection. Does not rely on importance weights.

Notes
-----
Allows NaN/Inf in the input if the underlying estimator does as well.

References
----------

.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection
       for cancer classification using support vector machines\",
       Mach. Learn., 46(1-3), 389--422, 2002.

Examples
--------
The following example shows how to retrieve the 5 most informative
features in the Friedman #1 dataset.

>>> from sklearn.datasets import make_friedman1
>>> from sklearn.feature_selection import RFE
>>> from sklearn.svm import SVR
>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
>>> estimator = SVR(kernel=\"linear\")
>>> selector = RFE(estimator, n_features_to_select=5, step=1)
>>> selector = selector.fit(X, y)
>>> selector.support_
array([ True,  True,  True,  True,  True, False, False, False, False,
       False])
>>> selector.ranking_
array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                     rdfs:comment """Classifier implementing a vote among neighbors within a given radius.

Read more in the :ref:`User Guide <classification>`.

Parameters
----------
radius : float, default=1.0
    Range of parameter space to use by default for :meth:`radius_neighbors`
    queries.

weights : {'uniform', 'distance'}, callable or None, default='uniform'
    Weight function used in prediction.  Possible values:

    - 'uniform' : uniform weights.  All points in each neighborhood
      are weighted equally.
    - 'distance' : weight points by the inverse of their distance.
      in this case, closer neighbors of a query point will have a
      greater influence than neighbors which are further away.
    - [callable] : a user-defined function which accepts an
      array of distances, and returns an array of the same shape
      containing the weights.

    Uniform weights are used by default.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

p : float, default=2
    Power parameter for the Minkowski metric. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
    This parameter is expected to be positive.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

outlier_label : {manual label, 'most_frequent'}, default=None
    Label for outlier samples (samples with no neighbors in given radius).

    - manual label: str or int label (should be the same type as y)
      or list of manual labels if multi-output is used.
    - 'most_frequent' : assign the most frequent label of y to outliers.
    - None : when any outlier is detected, ValueError will be raised.

    The outlier label should be selected from among the unique 'Y' labels.
    If it is specified with a different value a warning will be raised and
    all class probabilities of outliers will be assigned to be 0.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
classes_ : ndarray of shape (n_classes,)
    Class labels known to the classifier.

effective_metric_ : str or callable
    The distance metric used. It will be same as the `metric` parameter
    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to
    'minkowski' and `p` parameter set to 2.

effective_metric_params_ : dict
    Additional keyword arguments for the metric function. For most metrics
    will be same with `metric_params` parameter, but may also contain the
    `p` parameter value if the `effective_metric_` attribute is set to
    'minkowski'.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

outlier_label_ : int or array-like of shape (n_class,)
    Label which is given for outlier samples (samples with no neighbors
    on given radius).

outputs_2d_ : bool
    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit
    otherwise True.

See Also
--------
KNeighborsClassifier : Classifier implementing the k-nearest neighbors
    vote.
RadiusNeighborsRegressor : Regression based on neighbors within a
    fixed radius.
KNeighborsRegressor : Regression based on k-nearest neighbors.
NearestNeighbors : Unsupervised learner for implementing neighbor
    searches.

Notes
-----
See :ref:`Nearest Neighbors <neighbors>` in the online documentation
for a discussion of the choice of ``algorithm`` and ``leaf_size``.

https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm

Examples
--------
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import RadiusNeighborsClassifier
>>> neigh = RadiusNeighborsClassifier(radius=1.0)
>>> neigh.fit(X, y)
RadiusNeighborsClassifier(...)
>>> print(neigh.predict([[1.5]]))
[0]
>>> print(neigh.predict_proba([[1.0]]))
[[0.66666667 0.33333333]]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod> rdf:type owl:Class ;
                                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                    rdfs:comment """Regression based on neighbors within a fixed radius.

The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.

Read more in the :ref:`User Guide <regression>`.

.. versionadded:: 0.9

Parameters
----------
radius : float, default=1.0
    Range of parameter space to use by default for :meth:`radius_neighbors`
    queries.

weights : {'uniform', 'distance'}, callable or None, default='uniform'
    Weight function used in prediction.  Possible values:

    - 'uniform' : uniform weights.  All points in each neighborhood
      are weighted equally.
    - 'distance' : weight points by the inverse of their distance.
      in this case, closer neighbors of a query point will have a
      greater influence than neighbors which are further away.
    - [callable] : a user-defined function which accepts an
      array of distances, and returns an array of the same shape
      containing the weights.

    Uniform weights are used by default.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

p : float, default=2
    Power parameter for the Minkowski metric. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is \"precomputed\", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only \"nonzero\" elements may be considered neighbors.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
effective_metric_ : str or callable
    The distance metric to use. It will be same as the `metric` parameter
    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to
    'minkowski' and `p` parameter set to 2.

effective_metric_params_ : dict
    Additional keyword arguments for the metric function. For most metrics
    will be same with `metric_params` parameter, but may also contain the
    `p` parameter value if the `effective_metric_` attribute is set to
    'minkowski'.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

See Also
--------
NearestNeighbors : Unsupervised learner for implementing neighbor searches.
KNeighborsRegressor : Regression based on k-nearest neighbors.
KNeighborsClassifier : Classifier based on the k-nearest neighbors.
RadiusNeighborsClassifier : Classifier based on neighbors within a given radius.

Notes
-----
See :ref:`Nearest Neighbors <neighbors>` in the online documentation
for a discussion of the choice of ``algorithm`` and ``leaf_size``.

https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm

Examples
--------
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import RadiusNeighborsRegressor
>>> neigh = RadiusNeighborsRegressor(radius=1.0)
>>> neigh.fit(X, y)
RadiusNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[0.5]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod> rdf:type owl:Class ;
                                                                                                                      rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule> ,
                                                                                                                                      <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                      rdfs:comment """Transform X into a (weighted) graph of neighbors nearer than a radius.

The transformed data is a sparse graph as returned by
`radius_neighbors_graph`.

Read more in the :ref:`User Guide <neighbors_transformer>`.

.. versionadded:: 0.22

Parameters
----------
mode : {'distance', 'connectivity'}, default='distance'
    Type of returned matrix: 'connectivity' will return the connectivity
    matrix with ones and zeros, and 'distance' will return the distances
    between neighbors according to the given metric.

radius : float, default=1.0
    Radius of neighborhood in the transformed sparse graph.

algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:

    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.

    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.

leaf_size : int, default=30
    Leaf size passed to BallTree or KDTree.  This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.

metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is \"minkowski\", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.

    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.

    Distance matrices are not supported.

p : float, default=2
    Parameter for the Minkowski metric from
    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
    This parameter is expected to be positive.

metric_params : dict, default=None
    Additional keyword arguments for the metric function.

n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    If ``-1``, then the number of jobs is set to the number of CPU cores.

Attributes
----------
effective_metric_ : str or callable
    The distance metric used. It will be same as the `metric` parameter
    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to
    'minkowski' and `p` parameter set to 2.

effective_metric_params_ : dict
    Additional keyword arguments for the metric function. For most metrics
    will be same with `metric_params` parameter, but may also contain the
    `p` parameter value if the `effective_metric_` attribute is set to
    'minkowski'.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_fit_ : int
    Number of samples in the fitted data.

See Also
--------
kneighbors_graph : Compute the weighted graph of k-neighbors for
    points in X.
KNeighborsTransformer : Transform X into a weighted graph of k
    nearest neighbors.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import load_wine
>>> from sklearn.cluster import DBSCAN
>>> from sklearn.neighbors import RadiusNeighborsTransformer
>>> from sklearn.pipeline import make_pipeline
>>> X, _ = load_wine(return_X_y=True)
>>> estimator = make_pipeline(
...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),
...     DBSCAN(eps=25.0, metric='precomputed'))
>>> X_clustered = estimator.fit_predict(X)
>>> clusters, counts = np.unique(X_clustered, return_counts=True)
>>> print(counts)
[ 29  15 111  11  12]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandScoreMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                     rdfs:comment """Rand index.

The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings [1]_ [2]_.

The raw RI score [3]_ is:

    RI = (number of agreeing pairs) / (number of pairs)

Read more in the :ref:`User Guide <rand_score>`.

Parameters
----------
labels_true : array-like of shape (n_samples,), dtype=integral
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,), dtype=integral
    Cluster labels to evaluate.

Returns
-------
RI : float
   Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for
   perfect match.

See Also
--------
adjusted_rand_score: Adjusted Rand Score.
adjusted_mutual_info_score: Adjusted Mutual Information.

References
----------
.. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"
   Journal of Classification 2, 193218 (1985).
   <10.1007/BF01908075>`.

.. [2] `Wikipedia: Simple Matching Coefficient
    <https://en.wikipedia.org/wiki/Simple_matching_coefficient>`_

.. [3] `Wikipedia: Rand Index <https://en.wikipedia.org/wiki/Rand_index>`_

Examples
--------
Perfectly matching labelings have a score of 1 even

  >>> from sklearn.metrics.cluster import rand_score
  >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized:

  >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])
  0.83...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod> rdf:type owl:Class ;
                                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                  rdfs:comment """A random forest classifier.

A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
Trees in the forest use the best split strategy, i.e. equivalent to passing
`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.
The sub-sample size is controlled with the `max_samples` parameter if
`bootstrap=True` (default), otherwise the whole dataset is used to build
each tree.

For a comparison between tree-based ensemble models see the example
:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"
    The function to measure the quality of a split. Supported criteria are
    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the
    Shannon information gain, see :ref:`tree_mathematical_formulation`.
    Note: This parameter is tree-specific.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=True
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.accuracy_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls both the randomness of the bootstrapping of the samples used
    when building trees (if ``bootstrap=True``) and the sampling of the
    features to consider when looking for the best split at each node
    (if ``max_features < n_features``).
    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    The \"balanced_subsample\" mode is the same as \"balanced\" except that
    weights are computed based on the bootstrap sample for every tree
    grown.

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonic increase
      - 0: no constraint
      - -1: monotonic decrease

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multiclass classifications (i.e. when `n_classes > 2`),
      - multioutput classifications (i.e. when `n_outputs_ > 1`),
      - classifications trained on data with missing values.

    The constraints hold over the probability of the positive class.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeClassifier
    The collection of fitted sub-estimators.

classes_ : ndarray of shape (n_classes,) or a list of such arrays
    The classes labels (single output problem), or a list of arrays of
    class labels (multi-output problem).

n_classes_ : int or list
    The number of classes (single output problem), or a list containing the
    number of classes for each output (multi-output problem).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    `oob_decision_function_` might contain NaN. This attribute exists
    only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized
    tree classifiers.
sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient
    Boosting Classification Tree, very fast for big datasets (n_samples >=
    10_000).

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
``max_features=n_features`` and ``bootstrap=False``, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, ``random_state`` has to be fixed.

References
----------
.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.

Examples
--------
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = RandomForestClassifier(max_depth=2, random_state=0)
>>> clf.fit(X, y)
RandomForestClassifier(...)
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod> rdf:type owl:Class ;
                                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                 rdfs:comment """A random forest regressor.

A random forest is a meta estimator that fits a number of decision tree
regressors on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
Trees in the forest use the best split strategy, i.e. equivalent to passing
`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.
The sub-sample size is controlled with the `max_samples` parameter if
`bootstrap=True` (default), otherwise the whole dataset is used to build
each tree.

For a comparison between tree-based ensemble models see the example
:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.

Read more in the :ref:`User Guide <forest>`.

Parameters
----------
n_estimators : int, default=100
    The number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"
    The function to measure the quality of a split. Supported criteria
    are \"squared_error\" for the mean squared error, which is equal to
    variance reduction as feature selection criterion and minimizes the L2
    loss using the mean of each terminal node, \"friedman_mse\", which uses
    mean squared error with Friedman's improvement score for potential
    splits, \"absolute_error\" for the mean absolute error, which minimizes
    the L1 loss using the median of each terminal node, and \"poisson\" which
    uses reduction in Poisson deviance to find splits.
    Training using \"absolute_error\" is significantly slower
    than when using \"squared_error\".

    .. versionadded:: 0.18
       Mean Absolute Error (MAE) criterion.

    .. versionadded:: 1.0
       Poisson criterion.

max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0
    The number of features to consider when looking for the best split:

    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `max(1, int(max_features * n_features_in_))` features are considered at each
      split.
    - If \"sqrt\", then `max_features=sqrt(n_features)`.
    - If \"log2\", then `max_features=log2(n_features)`.
    - If None or 1.0, then `max_features=n_features`.

    .. note::
        The default of 1.0 is equivalent to bagged trees and more
        randomness can be achieved by setting smaller values, e.g. 0.3.

    .. versionchanged:: 1.1
        The default of `max_features` changed from `\"auto\"` to 1.0.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

bootstrap : bool, default=True
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.

oob_score : bool or callable, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    By default, :func:`~sklearn.metrics.r2_score` is used.
    Provide a callable with signature `metric(y_true, y_pred)` to use a
    custom metric. Only available if `bootstrap=True`.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls both the randomness of the bootstrapping of the samples used
    when building trees (if ``bootstrap=True``) and the sampling of the
    features to consider when looking for the best split at each node
    (if ``max_features < n_features``).
    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
    :ref:`minimal_cost_complexity_pruning` for details.

    .. versionadded:: 0.22

max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.

    - If None (default), then draw `X.shape[0]` samples.
    - If int, then draw `max_samples` samples.
    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,
      `max_samples` should be in the interval `(0.0, 1.0]`.

    .. versionadded:: 0.22

monotonic_cst : array-like of int of shape (n_features), default=None
    Indicates the monotonicity constraint to enforce on each feature.
      - 1: monotonically increasing
      - 0: no constraint
      - -1: monotonically decreasing

    If monotonic_cst is None, no constraints are applied.

    Monotonicity constraints are not supported for:
      - multioutput regressions (i.e. when `n_outputs_ > 1`),
      - regressions trained on data with missing values.

    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.

    .. versionadded:: 1.4

Attributes
----------
estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.

feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.

    Warning: impurity-based feature importances can be misleading for
    high cardinality features (many unique values). See
    :func:`sklearn.inspection.permutation_importance` as an alternative.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when ``oob_score`` is True.

oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
    Prediction computed with out-of-bag estimate on the training set.
    This attribute exists only when ``oob_score`` is True.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized
    tree regressors.
sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient
    Boosting Regression Tree, very fast for big datasets (n_samples >=
    10_000).

Notes
-----
The default values for the parameters controlling the size of the trees
(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.

The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
``max_features=n_features`` and ``bootstrap=False``, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, ``random_state`` has to be fixed.

The default value ``max_features=1.0`` uses ``n_features``
rather than ``n_features / 3``. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].

References
----------
.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.

.. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized
       trees\", Machine Learning, 63(1), 3-42, 2006.

Examples
--------
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = RandomForestRegressor(max_depth=2, random_state=0)
>>> regr.fit(X, y)
RandomForestRegressor(...)
>>> print(regr.predict([[0, 0, 0, 0]]))
[-8.32987858]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                rdfs:comment """An ensemble of totally random trees.

An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as there are trees in
the forest.

The dimensionality of the resulting representation is
``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.

Read more in the :ref:`User Guide <random_trees_embedding>`.

Parameters
----------
n_estimators : int, default=100
    Number of trees in the forest.

    .. versionchanged:: 0.22
       The default value of ``n_estimators`` changed from 10 to 100
       in 0.22.

max_depth : int, default=5
    The maximum depth of each tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.

min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` is the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` is the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_leaf_nodes : int, default=None
    Grow trees with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

sparse_output : bool, default=True
    Whether or not to return a sparse CSR matrix, as default behavior,
    or to return a dense array compatible with dense pipeline operators.

n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,
    :meth:`decision_path` and :meth:`apply` are all parallelized over the
    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
    context. ``-1`` means using all processors. See :term:`Glossary
    <n_jobs>` for more details.

random_state : int, RandomState instance or None, default=None
    Controls the generation of the random `y` used to fit the trees
    and the draw of the splits for each feature at the trees' nodes.
    See :term:`Glossary <random_state>` for details.

verbose : int, default=0
    Controls the verbosity when fitting and predicting.

warm_start : bool, default=False
    When set to ``True``, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:`Glossary <warm_start>` and
    :ref:`gradient_boosting_warm_start` for details.

Attributes
----------
estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
    The child estimator template used to create the collection of fitted
    sub-estimators.

    .. versionadded:: 1.2
       `base_estimator_` was renamed to `estimator_`.

estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances
    The collection of fitted sub-estimators.

feature_importances_ : ndarray of shape (n_features,)
    The feature importances (the higher, the more important the feature).

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

one_hot_encoder_ : OneHotEncoder instance
    One-hot encoder used to create the sparse embedding.

estimators_samples_ : list of arrays
    The subset of drawn samples (i.e., the in-bag samples) for each base
    estimator. Each subset is defined by an array of the indices selected.

    .. versionadded:: 1.4

See Also
--------
ExtraTreesClassifier : An extra-trees classifier.
ExtraTreesRegressor : An extra-trees regressor.
RandomForestClassifier : A random forest classifier.
RandomForestRegressor : A random forest regressor.
sklearn.tree.ExtraTreeClassifier: An extremely randomized
    tree classifier.
sklearn.tree.ExtraTreeRegressor : An extremely randomized
    tree regressor.

References
----------
.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",
       Machine Learning, 63(1), 3-42, 2006.
.. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative
       visual codebooks using randomized clustering forests\"
       NIPS 2007

Examples
--------
>>> from sklearn.ensemble import RandomTreesEmbedding
>>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]
>>> random_trees = RandomTreesEmbedding(
...    n_estimators=5, random_state=0, max_depth=1).fit(X)
>>> X_sparse_embedding = random_trees.transform(X)
>>> X_sparse_embedding.toarray()
array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],
       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """Randomized search on hyper parameters.

RandomizedSearchCV implements a \"fit\" and a \"score\" method.
It also implements \"score_samples\", \"predict\", \"predict_proba\",
\"decision_function\", \"transform\" and \"inverse_transform\" if they are
implemented in the estimator used.

The parameters of the estimator used to apply these methods are optimized
by cross-validated search over parameter settings.

In contrast to GridSearchCV, not all parameter values are tried out, but
rather a fixed number of parameter settings is sampled from the specified
distributions. The number of parameter settings that are tried is
given by n_iter.

If all parameters are presented as a list,
sampling without replacement is performed. If at least one parameter
is given as a distribution, sampling with replacement is used.
It is highly recommended to use continuous distributions for continuous
parameters.

Read more in the :ref:`User Guide <randomized_parameter_search>`.

.. versionadded:: 0.14

Parameters
----------
estimator : estimator object
    An object of that type is instantiated for each grid point.
    This is assumed to implement the scikit-learn estimator interface.
    Either estimator needs to provide a ``score`` function,
    or ``scoring`` must be passed.

param_distributions : dict or list of dicts
    Dictionary with parameters names (`str`) as keys and distributions
    or lists of parameters to try. Distributions must provide a ``rvs``
    method for sampling (such as those from scipy.stats.distributions).
    If a list is given, it is sampled uniformly.
    If a list of dicts is given, first a dict is sampled uniformly, and
    then a parameter is sampled using that dict as above.

n_iter : int, default=10
    Number of parameter settings that are sampled. n_iter trades
    off runtime vs quality of the solution.

scoring : str, callable, list, tuple or dict, default=None
    Strategy to evaluate the performance of the cross-validated model on
    the test set.

    If `scoring` represents a single score, one can use:

    - a single string (see :ref:`scoring_parameter`);
    - a callable (see :ref:`scoring`) that returns a single value.

    If `scoring` represents multiple scores, one can use:

    - a list or tuple of unique strings;
    - a callable returning a dictionary where the keys are the metric
      names and the values are the metric scores;
    - a dictionary with metric names as keys and callables a values.

    See :ref:`multimetric_grid_search` for an example.

    If None, the estimator's score method is used.

n_jobs : int, default=None
    Number of jobs to run in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionchanged:: v0.20
       `n_jobs` default changed from 1 to None

refit : bool, str, or callable, default=True
    Refit an estimator using the best found parameters on the whole
    dataset.

    For multiple metric evaluation, this needs to be a `str` denoting the
    scorer that would be used to find the best parameters for refitting
    the estimator at the end.

    Where there are considerations other than maximum score in
    choosing a best estimator, ``refit`` can be set to a function which
    returns the selected ``best_index_`` given the ``cv_results``. In that
    case, the ``best_estimator_`` and ``best_params_`` will be set
    according to the returned ``best_index_`` while the ``best_score_``
    attribute will not be available.

    The refitted estimator is made available at the ``best_estimator_``
    attribute and permits using ``predict`` directly on this
    ``RandomizedSearchCV`` instance.

    Also for multiple metric evaluation, the attributes ``best_index_``,
    ``best_score_`` and ``best_params_`` will only be available if
    ``refit`` is set and all of them will be determined w.r.t this specific
    scorer.

    See ``scoring`` parameter to know more about multiple metric
    evaluation.

    .. versionchanged:: 0.20
        Support for callable added.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross validation,
    - integer, to specify the number of folds in a `(Stratified)KFold`,
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if the estimator is a classifier and ``y`` is
    either binary or multiclass, :class:`StratifiedKFold` is used. In all
    other cases, :class:`KFold` is used. These splitters are instantiated
    with `shuffle=False` so the splits will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

verbose : int
    Controls the verbosity: the higher, the more messages.

    - >1 : the computation time for each fold and parameter candidate is
      displayed;
    - >2 : the score is also displayed;
    - >3 : the fold and candidate parameter indexes are also displayed
      together with the starting time of the computation.

pre_dispatch : int, or str, default='2*n_jobs'
    Controls the number of jobs that get dispatched during parallel
    execution. Reducing this number can be useful to avoid an
    explosion of memory consumption when more jobs get dispatched
    than CPUs can process. This parameter can be:

        - None, in which case all the jobs are immediately
          created and spawned. Use this for lightweight and
          fast-running jobs, to avoid delays due to on-demand
          spawning of the jobs

        - An int, giving the exact number of total jobs that are
          spawned

        - A str, giving an expression as a function of n_jobs,
          as in '2*n_jobs'

random_state : int, RandomState instance or None, default=None
    Pseudo random number generator state used for random uniform sampling
    from lists of possible values instead of scipy.stats distributions.
    Pass an int for reproducible output across multiple
    function calls.
    See :term:`Glossary <random_state>`.

error_score : 'raise' or numeric, default=np.nan
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised. If a numeric value is given,
    FitFailedWarning is raised. This parameter does not affect the refit
    step, which will always raise the error.

return_train_score : bool, default=False
    If ``False``, the ``cv_results_`` attribute will not include training
    scores.
    Computing training scores is used to get insights on how different
    parameter settings impact the overfitting/underfitting trade-off.
    However computing the scores on the training set can be computationally
    expensive and is not strictly required to select the parameters that
    yield the best generalization performance.

    .. versionadded:: 0.19

    .. versionchanged:: 0.21
        Default value was changed from ``True`` to ``False``

Attributes
----------
cv_results_ : dict of numpy (masked) ndarrays
    A dict with keys as column headers and values as columns, that can be
    imported into a pandas ``DataFrame``.

    For instance the below given table

    +--------------+-------------+-------------------+---+---------------+
    | param_kernel | param_gamma | split0_test_score |...|rank_test_score|
    +==============+=============+===================+===+===============+
    |    'rbf'     |     0.1     |       0.80        |...|       1       |
    +--------------+-------------+-------------------+---+---------------+
    |    'rbf'     |     0.2     |       0.84        |...|       3       |
    +--------------+-------------+-------------------+---+---------------+
    |    'rbf'     |     0.3     |       0.70        |...|       2       |
    +--------------+-------------+-------------------+---+---------------+

    will be represented by a ``cv_results_`` dict of::

        {
        'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],
                                      mask = False),
        'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),
        'split0_test_score'  : [0.80, 0.84, 0.70],
        'split1_test_score'  : [0.82, 0.50, 0.70],
        'mean_test_score'    : [0.81, 0.67, 0.70],
        'std_test_score'     : [0.01, 0.24, 0.00],
        'rank_test_score'    : [1, 3, 2],
        'split0_train_score' : [0.80, 0.92, 0.70],
        'split1_train_score' : [0.82, 0.55, 0.70],
        'mean_train_score'   : [0.81, 0.74, 0.70],
        'std_train_score'    : [0.01, 0.19, 0.00],
        'mean_fit_time'      : [0.73, 0.63, 0.43],
        'std_fit_time'       : [0.01, 0.02, 0.01],
        'mean_score_time'    : [0.01, 0.06, 0.04],
        'std_score_time'     : [0.00, 0.00, 0.00],
        'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],
        }

    NOTE

    The key ``'params'`` is used to store a list of parameter
    settings dicts for all the parameter candidates.

    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and
    ``std_score_time`` are all in seconds.

    For multi-metric evaluation, the scores for all the scorers are
    available in the ``cv_results_`` dict at the keys ending with that
    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown
    above. ('split0_test_precision', 'mean_train_precision' etc.)

best_estimator_ : estimator
    Estimator that was chosen by the search, i.e. estimator
    which gave highest score (or smallest loss if specified)
    on the left out data. Not available if ``refit=False``.

    For multi-metric evaluation, this attribute is present only if
    ``refit`` is specified.

    See ``refit`` parameter for more information on allowed values.

best_score_ : float
    Mean cross-validated score of the best_estimator.

    For multi-metric evaluation, this is not available if ``refit`` is
    ``False``. See ``refit`` parameter for more information.

    This attribute is not available if ``refit`` is a function.

best_params_ : dict
    Parameter setting that gave the best results on the hold out data.

    For multi-metric evaluation, this is not available if ``refit`` is
    ``False``. See ``refit`` parameter for more information.

best_index_ : int
    The index (of the ``cv_results_`` arrays) which corresponds to the best
    candidate parameter setting.

    The dict at ``search.cv_results_['params'][search.best_index_]`` gives
    the parameter setting for the best model, that gives the highest
    mean score (``search.best_score_``).

    For multi-metric evaluation, this is not available if ``refit`` is
    ``False``. See ``refit`` parameter for more information.

scorer_ : function or a dict
    Scorer function used on the held out data to choose the best
    parameters for the model.

    For multi-metric evaluation, this attribute holds the validated
    ``scoring`` dict which maps the scorer key to the scorer callable.

n_splits_ : int
    The number of cross-validation splits (folds/iterations).

refit_time_ : float
    Seconds used for refitting the best model on the whole dataset.

    This is present only if ``refit`` is not False.

    .. versionadded:: 0.20

multimetric_ : bool
    Whether or not the scorers compute several metrics.

classes_ : ndarray of shape (n_classes,)
    The classes labels. This is present only if ``refit`` is specified and
    the underlying estimator is a classifier.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `n_features_in_` when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if
    `best_estimator_` is defined (see the documentation for the `refit`
    parameter for more details) and that `best_estimator_` exposes
    `feature_names_in_` when fit.

    .. versionadded:: 1.0

See Also
--------
GridSearchCV : Does exhaustive search over a grid of parameters.
ParameterSampler : A generator over parameter settings, constructed from
    param_distributions.

Notes
-----
The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.

If `n_jobs` was set to a value higher than one, the data is copied for each
parameter setting(and not `n_jobs` times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set `pre_dispatch`. Then, the memory is copied only
`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
n_jobs`.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.model_selection import RandomizedSearchCV
>>> from scipy.stats import uniform
>>> iris = load_iris()
>>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,
...                               random_state=0)
>>> distributions = dict(C=uniform(loc=0, scale=4),
...                      penalty=['l2', 'l1'])
>>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)
>>> search = clf.fit(iris.data, iris.target)
>>> search.best_params_
{'C': 2..., 'penalty': 'l1'}""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                       rdfs:comment """Compute the recall.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The best value is 1 and the worst value is 0.

Support beyond term:`binary` targets is achieved by treating :term:`multiclass`
and :term:`multilabel` data as a collection of binary problems, one for each
label. For the :term:`binary` case, setting `average='binary'` will return
recall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored
and recall for both classes are computed then averaged or both returned (when
`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,
recall for all `labels` are either returned or averaged depending on the `average`
parameter. Use `labels` specify the set of labels to calculate recall for.

Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) target values.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Estimated targets as returned by a classifier.

labels : array-like, default=None
    The set of labels to include when `average != 'binary'`, and their
    order if `average is None`. Labels present in the data can be
    excluded, for example in multiclass classification to exclude a \"negative
    class\". Labels not present in the data can be included and will be
    \"assigned\" 0 samples. For multilabel targets, labels are column indices.
    By default, all labels in `y_true` and `y_pred` are used in sorted order.

    .. versionchanged:: 0.17
       Parameter `labels` improved for multiclass problem.

pos_label : int, float, bool or str, default=1
    The class to report if `average='binary'` and the data is binary,
    otherwise this parameter is ignored.
    For multiclass or multilabel targets, set `labels=[pos_label]` and
    `average != 'binary'` to report metrics for one label only.

average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'
    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:

    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall. Weighted recall
        is equal to accuracy.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"
    Sets the value to return when there is a zero division.

    Notes:
    - If set to \"warn\", this acts like 0, but a warning is also raised.
    - If set to `np.nan`, such values will be excluded from the average.

    .. versionadded:: 1.3
       `np.nan` option was added.

Returns
-------
recall : float (if average is not None) or array of float of shape              (n_unique_labels,)
    Recall of the positive class in binary classification or weighted
    average of the recall of each class for the multiclass task.

See Also
--------
precision_recall_fscore_support : Compute precision, recall, F-measure and
    support for each class.
precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the
    number of true positives and ``fp`` the number of false positives.
balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced
    datasets.
multilabel_confusion_matrix : Compute a confusion matrix for each class or
    sample.
PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given
    an estimator and some data.
PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given
    binary class predictions.

Notes
-----
When ``true positive + false negative == 0``, recall returns 0 and raises
``UndefinedMetricWarning``. This behavior can be modified with
``zero_division``.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import recall_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> recall_score(y_true, y_pred, average='macro')
0.33...
>>> recall_score(y_true, y_pred, average='micro')
0.33...
>>> recall_score(y_true, y_pred, average='weighted')
0.33...
>>> recall_score(y_true, y_pred, average=None)
array([1., 0., 0.])
>>> y_true = [0, 0, 0, 0, 0, 0]
>>> recall_score(y_true, y_pred, average=None)
array([0.5, 0. , 0. ])
>>> recall_score(y_true, y_pred, average=None, zero_division=1)
array([0.5, 1. , 1. ])
>>> recall_score(y_true, y_pred, average=None, zero_division=np.nan)
array([0.5, nan, nan])

>>> # multilabel classification
>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]
>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]
>>> recall_score(y_true, y_pred, average=None)
array([1. , 1. , 0.5])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                         rdfs:comment """Repeated K-Fold cross validator.

Repeats K-Fold n times with different randomization in each repetition.

Read more in the :ref:`User Guide <repeated_k_fold>`.

Parameters
----------
n_splits : int, default=5
    Number of folds. Must be at least 2.

n_repeats : int, default=10
    Number of times cross-validator needs to be repeated.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of each repeated cross-validation instance.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import RepeatedKFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)
>>> rkf.get_n_splits(X, y)
4
>>> print(rkf)
RepeatedKFold(n_repeats=2, n_splits=2, random_state=2652124)
>>> for i, (train_index, test_index) in enumerate(rkf.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
...
Fold 0:
  Train: index=[0 1]
  Test:  index=[2 3]
Fold 1:
  Train: index=[2 3]
  Test:  index=[0 1]
Fold 2:
  Train: index=[1 2]
  Test:  index=[0 3]
Fold 3:
  Train: index=[0 3]
  Test:  index=[1 2]

Notes
-----
Randomized CV splitters may return different results for each call of
split. You can make the results identical by setting `random_state`
to an integer.

See Also
--------
RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod> rdf:type owl:Class ;
                                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                                   rdfs:comment """Repeated Stratified K-Fold cross validator.

Repeats Stratified K-Fold n times with different randomization in each
repetition.

Read more in the :ref:`User Guide <repeated_k_fold>`.

Parameters
----------
n_splits : int, default=5
    Number of folds. Must be at least 2.

n_repeats : int, default=10
    Number of times cross-validator needs to be repeated.

random_state : int, RandomState instance or None, default=None
    Controls the generation of the random states for each repetition.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import RepeatedStratifiedKFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,
...     random_state=36851234)
>>> rskf.get_n_splits(X, y)
4
>>> print(rskf)
RepeatedStratifiedKFold(n_repeats=2, n_splits=2, random_state=36851234)
>>> for i, (train_index, test_index) in enumerate(rskf.split(X, y)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
...
Fold 0:
  Train: index=[1 2]
  Test:  index=[0 3]
Fold 1:
  Train: index=[0 3]
  Test:  index=[1 2]
Fold 2:
  Train: index=[1 3]
  Test:  index=[0 2]
Fold 3:
  Train: index=[0 2]
  Test:  index=[1 3]

Notes
-----
Randomized CV splitters may return different results for each call of
split. You can make the results identical by setting `random_state`
to an integer.

See Also
--------
RepeatedKFold : Repeats K-Fold n times.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                   rdfs:comment """Ridge regression with built-in cross-validation.

See glossary entry for :term:`cross-validation estimator`.

By default, it performs efficient Leave-One-Out Cross-Validation.

Read more in the :ref:`User Guide <ridge_regression>`.

Parameters
----------
alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)
    Array of alpha values to try.
    Regularization strength; must be a positive float. Regularization
    improves the conditioning of the problem and reduces the variance of
    the estimates. Larger values specify stronger regularization.
    Alpha corresponds to ``1 / (2C)`` in other linear models such as
    :class:`~sklearn.linear_model.LogisticRegression` or
    :class:`~sklearn.svm.LinearSVC`.
    If using Leave-One-Out cross-validation, alphas must be positive.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

scoring : str, callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.
    If None, the negative mean squared error if cv is 'auto' or None
    (i.e. when using leave-one-out cross-validation), and r2 score
    otherwise.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the efficient Leave-One-Out cross-validation
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if ``y`` is binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used, else,
    :class:`~sklearn.model_selection.KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

gcv_mode : {'auto', 'svd', 'eigen'}, default='auto'
    Flag indicating which strategy to use when performing
    Leave-One-Out Cross-Validation. Options are::

        'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'
        'svd' : force use of singular value decomposition of X when X is
            dense, eigenvalue decomposition of X^T.X when X is sparse.
        'eigen' : force computation via eigendecomposition of X.X^T

    The 'auto' mode is the default and is intended to pick the cheaper
    option of the two depending on the shape of the training data.

store_cv_values : bool, default=False
    Flag indicating if the cross-validation values corresponding to
    each alpha should be stored in the ``cv_values_`` attribute (see
    below). This flag is only compatible with ``cv=None`` (i.e. using
    Leave-One-Out Cross-Validation).

alpha_per_target : bool, default=False
    Flag indicating whether to optimize the alpha value (picked from the
    `alphas` parameter list) for each target separately (for multi-output
    settings: multiple prediction targets). When set to `True`, after
    fitting, the `alpha_` attribute will contain a value for each target.
    When set to `False`, a single alpha is used for all targets.

    .. versionadded:: 0.24

Attributes
----------
cv_values_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional
    Cross-validation values for each alpha (only available if
    ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been
    called, this attribute will contain the mean squared errors if
    `scoring is None` otherwise it will contain standardized per point
    prediction values.

coef_ : ndarray of shape (n_features) or (n_targets, n_features)
    Weight vector(s).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

alpha_ : float or ndarray of shape (n_targets,)
    Estimated regularization parameter, or, if ``alpha_per_target=True``,
    the estimated regularization parameter for each target.

best_score_ : float or ndarray of shape (n_targets,)
    Score of base estimator with best alpha, or, if
    ``alpha_per_target=True``, a score for each target.

    .. versionadded:: 0.23

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression.
RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.
RidgeClassifierCV : Ridge classifier with built-in cross validation.

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.linear_model import RidgeCV
>>> X, y = load_diabetes(return_X_y=True)
>>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
>>> clf.score(X, y)
0.5166...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                             rdfs:comment """Ridge classifier with built-in cross-validation.

See glossary entry for :term:`cross-validation estimator`.

By default, it performs Leave-One-Out Cross-Validation. Currently,
only the n_features > n_samples case is handled efficiently.

Read more in the :ref:`User Guide <ridge_regression>`.

Parameters
----------
alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)
    Array of alpha values to try.
    Regularization strength; must be a positive float. Regularization
    improves the conditioning of the problem and reduces the variance of
    the estimates. Larger values specify stronger regularization.
    Alpha corresponds to ``1 / (2C)`` in other linear models such as
    :class:`~sklearn.linear_model.LogisticRegression` or
    :class:`~sklearn.svm.LinearSVC`.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

scoring : str, callable, default=None
    A string (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the efficient Leave-One-Out cross-validation
    - integer, to specify the number of folds.
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

store_cv_values : bool, default=False
    Flag indicating if the cross-validation values corresponding to
    each alpha should be stored in the ``cv_values_`` attribute (see
    below). This flag is only compatible with ``cv=None`` (i.e. using
    Leave-One-Out Cross-Validation).

Attributes
----------
cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional
    Cross-validation values for each alpha (only if ``store_cv_values=True`` and
    ``cv=None``). After ``fit()`` has been called, this attribute will
    contain the mean squared errors if `scoring is None` otherwise it
    will contain standardized per point prediction values.

coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)
    Coefficient of the features in the decision function.

    ``coef_`` is of shape (1, n_features) when the given problem is binary.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

alpha_ : float
    Estimated regularization parameter.

best_score_ : float
    Score of base estimator with best alpha.

    .. versionadded:: 0.23

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression.
RidgeClassifier : Ridge classifier.
RidgeCV : Ridge regression with built-in cross validation.

Notes
-----
For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.

Examples
--------
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.linear_model import RidgeClassifierCV
>>> X, y = load_breast_cancer(return_X_y=True)
>>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
>>> clf.score(X, y)
0.9630...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """Classifier using Ridge regression.

This classifier first converts the target values into ``{-1, 1}`` and
then treats the problem as a regression task (multi-output regression in
the multiclass case).

Read more in the :ref:`User Guide <ridge_regression>`.

Parameters
----------
alpha : float, default=1.0
    Regularization strength; must be a positive float. Regularization
    improves the conditioning of the problem and reduces the variance of
    the estimates. Larger values specify stronger regularization.
    Alpha corresponds to ``1 / (2C)`` in other linear models such as
    :class:`~sklearn.linear_model.LogisticRegression` or
    :class:`~sklearn.svm.LinearSVC`.

fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set to false, no
    intercept will be used in calculations (e.g. data is expected to be
    already centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

max_iter : int, default=None
    Maximum number of iterations for conjugate gradient solver.
    The default value is determined by scipy.sparse.linalg.

tol : float, default=1e-4
    The precision of the solution (`coef_`) is determined by `tol` which
    specifies a different convergence criterion for each solver:

    - 'svd': `tol` has no impact.

    - 'cholesky': `tol` has no impact.

    - 'sparse_cg': norm of residuals smaller than `tol`.

    - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,
      which control the norm of the residual vector in terms of the norms of
      matrix and coefficients.

    - 'sag' and 'saga': relative change of coef smaller than `tol`.

    - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|
      smaller than `tol`.

    .. versionchanged:: 1.2
       Default value changed from 1e-3 to 1e-4 for consistency with other linear
       models.

class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'
    Solver to use in the computational routines:

    - 'auto' chooses the solver automatically based on the type of data.

    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge
      coefficients. It is the most stable solver, in particular more stable
      for singular matrices than 'cholesky' at the cost of being slower.

    - 'cholesky' uses the standard scipy.linalg.solve function to
      obtain a closed-form solution.

    - 'sparse_cg' uses the conjugate gradient solver as found in
      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
      more appropriate than 'cholesky' for large-scale data
      (possibility to set `tol` and `max_iter`).

    - 'lsqr' uses the dedicated regularized least-squares routine
      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
      procedure.

    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
      its unbiased and more flexible version named SAGA. Both methods
      use an iterative procedure, and are often faster than other solvers
      when both n_samples and n_features are large. Note that 'sag' and
      'saga' fast convergence is only guaranteed on features with
      approximately the same scale. You can preprocess the data with a
      scaler from sklearn.preprocessing.

      .. versionadded:: 0.17
         Stochastic Average Gradient descent solver.
      .. versionadded:: 0.19
         SAGA solver.

    - 'lbfgs' uses L-BFGS-B algorithm implemented in
      `scipy.optimize.minimize`. It can be used only when `positive`
      is True.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.
    Only 'lbfgs' solver is supported in this case.

random_state : int, RandomState instance, default=None
    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.
    See :term:`Glossary <random_state>` for details.

Attributes
----------
coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.

    ``coef_`` is of shape (1, n_features) when the given problem is binary.

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

n_iter_ : None or ndarray of shape (n_targets,)
    Actual number of iterations for each target. Available only for
    sag and lsqr solvers. Other solvers will return None.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
Ridge : Ridge regression.
RidgeClassifierCV :  Ridge classifier with built-in cross validation.

Notes
-----
For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.

Examples
--------
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.linear_model import RidgeClassifier
>>> X, y = load_breast_cancer(return_X_y=True)
>>> clf = RidgeClassifier().fit(X, y)
>>> clf.score(X, y)
0.9595...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod> rdf:type owl:Class ;
                                                                                                 rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                 <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                 rdfs:comment """Linear least squares with l2 regularization.

Minimizes the objective function::

||y - Xw||^2_2 + alpha * ||w||^2_2

This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape (n_samples, n_targets)).

Read more in the :ref:`User Guide <ridge_regression>`.

Parameters
----------
alpha : {float, ndarray of shape (n_targets,)}, default=1.0
    Constant that multiplies the L2 term, controlling regularization
    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.

    When `alpha = 0`, the objective is equivalent to ordinary least
    squares, solved by the :class:`LinearRegression` object. For numerical
    reasons, using `alpha = 0` with the `Ridge` object is not advised.
    Instead, you should use the :class:`LinearRegression` object.

    If an array is passed, penalties are assumed to be specific to the
    targets. Hence they must correspond in number.

fit_intercept : bool, default=True
    Whether to fit the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. ``X`` and ``y`` are expected to be centered).

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

max_iter : int, default=None
    Maximum number of iterations for conjugate gradient solver.
    For 'sparse_cg' and 'lsqr' solvers, the default value is determined
    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.
    For 'lbfgs' solver, the default value is 15000.

tol : float, default=1e-4
    The precision of the solution (`coef_`) is determined by `tol` which
    specifies a different convergence criterion for each solver:

    - 'svd': `tol` has no impact.

    - 'cholesky': `tol` has no impact.

    - 'sparse_cg': norm of residuals smaller than `tol`.

    - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,
      which control the norm of the residual vector in terms of the norms of
      matrix and coefficients.

    - 'sag' and 'saga': relative change of coef smaller than `tol`.

    - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|
      smaller than `tol`.

    .. versionchanged:: 1.2
       Default value changed from 1e-3 to 1e-4 for consistency with other linear
       models.

solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'
    Solver to use in the computational routines:

    - 'auto' chooses the solver automatically based on the type of data.

    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge
      coefficients. It is the most stable solver, in particular more stable
      for singular matrices than 'cholesky' at the cost of being slower.

    - 'cholesky' uses the standard scipy.linalg.solve function to
      obtain a closed-form solution.

    - 'sparse_cg' uses the conjugate gradient solver as found in
      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
      more appropriate than 'cholesky' for large-scale data
      (possibility to set `tol` and `max_iter`).

    - 'lsqr' uses the dedicated regularized least-squares routine
      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
      procedure.

    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses
      its improved, unbiased version named SAGA. Both methods also use an
      iterative procedure, and are often faster than other solvers when
      both n_samples and n_features are large. Note that 'sag' and
      'saga' fast convergence is only guaranteed on features with
      approximately the same scale. You can preprocess the data with a
      scaler from sklearn.preprocessing.

    - 'lbfgs' uses L-BFGS-B algorithm implemented in
      `scipy.optimize.minimize`. It can be used only when `positive`
      is True.

    All solvers except 'svd' support both dense and sparse data. However, only
    'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when
    `fit_intercept` is True.

    .. versionadded:: 0.17
       Stochastic Average Gradient descent solver.
    .. versionadded:: 0.19
       SAGA solver.

positive : bool, default=False
    When set to ``True``, forces the coefficients to be positive.
    Only 'lbfgs' solver is supported in this case.

random_state : int, RandomState instance, default=None
    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.
    See :term:`Glossary <random_state>` for details.

    .. versionadded:: 0.17
       `random_state` to support Stochastic Average Gradient.

Attributes
----------
coef_ : ndarray of shape (n_features,) or (n_targets, n_features)
    Weight vector(s).

intercept_ : float or ndarray of shape (n_targets,)
    Independent term in decision function. Set to 0.0 if
    ``fit_intercept = False``.

n_iter_ : None or ndarray of shape (n_targets,)
    Actual number of iterations for each target. Available only for
    sag and lsqr solvers. Other solvers will return None.

    .. versionadded:: 0.17

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
RidgeClassifier : Ridge classifier.
RidgeCV : Ridge regression with built-in cross validation.
:class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression
    combines ridge regression with the kernel trick.

Notes
-----
Regularization improves the conditioning of the problem and
reduces the variance of the estimates. Larger values specify stronger
regularization. Alpha corresponds to ``1 / (2C)`` in other linear
models such as :class:`~sklearn.linear_model.LogisticRegression` or
:class:`~sklearn.svm.LinearSVC`.

Examples
--------
>>> from sklearn.linear_model import Ridge
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> clf = Ridge(alpha=1.0)
>>> clf.fit(X, y)
Ridge()""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                        rdfs:comment """Scale features using statistics that are robust to outliers.

This Scaler removes the median and scales the data according to
the quantile range (defaults to IQR: Interquartile Range).
The IQR is the range between the 1st quartile (25th quantile)
and the 3rd quartile (75th quantile).

Centering and scaling happen independently on each feature by
computing the relevant statistics on the samples in the training
set. Median and interquartile range are then stored to be used on
later data using the :meth:`transform` method.

Standardization of a dataset is a common preprocessing for many machine
learning estimators. Typically this is done by removing the mean and
scaling to unit variance. However, outliers can often influence the sample
mean / variance in a negative way. In such cases, using the median and the
interquartile range often give better results. For an example visualization
and comparison to other scalers, refer to :ref:`Compare RobustScaler with
other scalers <plot_all_scaling_robust_scaler_section>`.

.. versionadded:: 0.17

Read more in the :ref:`User Guide <preprocessing_scaler>`.

Parameters
----------
with_centering : bool, default=True
    If `True`, center the data before scaling.
    This will cause :meth:`transform` to raise an exception when attempted
    on sparse matrices, because centering them entails building a dense
    matrix which in common use cases is likely to be too large to fit in
    memory.

with_scaling : bool, default=True
    If `True`, scale the data to interquartile range.

quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0)
    Quantile range used to calculate `scale_`. By default this is equal to
    the IQR, i.e., `q_min` is the first quantile and `q_max` is the third
    quantile.

    .. versionadded:: 0.18

copy : bool, default=True
    If `False`, try to avoid a copy and do inplace scaling instead.
    This is not guaranteed to always work inplace; e.g. if the data is
    not a NumPy array or scipy.sparse CSR matrix, a copy may still be
    returned.

unit_variance : bool, default=False
    If `True`, scale data so that normally distributed features have a
    variance of 1. In general, if the difference between the x-values of
    `q_max` and `q_min` for a standard normal distribution is greater
    than 1, the dataset will be scaled down. If less than 1, the dataset
    will be scaled up.

    .. versionadded:: 0.24

Attributes
----------
center_ : array of floats
    The median value for each feature in the training set.

scale_ : array of floats
    The (scaled) interquartile range for each feature in the training set.

    .. versionadded:: 0.17
       *scale_* attribute.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
robust_scale : Equivalent function without the estimator API.
sklearn.decomposition.PCA : Further removes the linear correlation across
    features with 'whiten=True'.

Notes
-----

https://en.wikipedia.org/wiki/Median
https://en.wikipedia.org/wiki/Interquartile_range

Examples
--------
>>> from sklearn.preprocessing import RobustScaler
>>> X = [[ 1., -2.,  2.],
...      [ -2.,  1.,  3.],
...      [ 4.,  1., -2.]]
>>> transformer = RobustScaler().fit(X)
>>> transformer
RobustScaler()
>>> transformer.transform(X)
array([[ 0. , -2. ,  0. ],
       [-1. ,  0. ,  0.4],
       [ 1. ,  0. , -1.6]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                       rdfs:comment """Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.

Note: this implementation can be used with binary, multiclass and
multilabel classification, but some restrictions apply (see Parameters).

Read more in the :ref:`User Guide <roc_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_classes)
    True labels or binary label indicators. The binary and multiclass cases
    expect labels with shape (n_samples,) while the multilabel case expects
    binary label indicators with shape (n_samples, n_classes).

y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
    Target scores.

    * In the binary case, it corresponds to an array of shape
      `(n_samples,)`. Both probability estimates and non-thresholded
      decision values can be provided. The probability estimates correspond
      to the **probability of the class with the greater label**,
      i.e. `estimator.classes_[1]` and thus
      `estimator.predict_proba(X, y)[:, 1]`. The decision values
      corresponds to the output of `estimator.decision_function(X, y)`.
      See more information in the :ref:`User guide <roc_auc_binary>`;
    * In the multiclass case, it corresponds to an array of shape
      `(n_samples, n_classes)` of probability estimates provided by the
      `predict_proba` method. The probability estimates **must**
      sum to 1 across the possible classes. In addition, the order of the
      class scores must correspond to the order of ``labels``,
      if provided, or else to the numerical or lexicographical order of
      the labels in ``y_true``. See more information in the
      :ref:`User guide <roc_auc_multiclass>`;
    * In the multilabel case, it corresponds to an array of shape
      `(n_samples, n_classes)`. Probability estimates are provided by the
      `predict_proba` method and the non-thresholded decision values by
      the `decision_function` method. The probability estimates correspond
      to the **probability of the class with the greater label for each
      output** of the classifier. See more information in the
      :ref:`User guide <roc_auc_multilabel>`.

average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'
    If ``None``, the scores for each class are returned.
    Otherwise, this determines the type of averaging performed on the data.
    Note: multiclass ROC AUC currently only handles the 'macro' and
    'weighted' averages. For multiclass targets, `average=None` is only
    implemented for `multi_class='ovr'` and `average='micro'` is only
    implemented for `multi_class='ovr'`.

    ``'micro'``:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    ``'samples'``:
        Calculate metrics for each instance, and find their average.

    Will be ignored when ``y_true`` is binary.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

max_fpr : float > 0 and <= 1, default=None
    If not ``None``, the standardized partial AUC [2]_ over the range
    [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,
    should be either equal to ``None`` or ``1.0`` as AUC ROC partial
    computation currently is not supported for multiclass.

multi_class : {'raise', 'ovr', 'ovo'}, default='raise'
    Only used for multiclass targets. Determines the type of configuration
    to use. The default value raises an error, so either
    ``'ovr'`` or ``'ovo'`` must be passed explicitly.

    ``'ovr'``:
        Stands for One-vs-rest. Computes the AUC of each class
        against the rest [3]_ [4]_. This
        treats the multiclass case in the same way as the multilabel case.
        Sensitive to class imbalance even when ``average == 'macro'``,
        because class imbalance affects the composition of each of the
        'rest' groupings.
    ``'ovo'``:
        Stands for One-vs-one. Computes the average AUC of all
        possible pairwise combinations of classes [5]_.
        Insensitive to class imbalance when
        ``average == 'macro'``.

labels : array-like of shape (n_classes,), default=None
    Only used for multiclass targets. List of labels that index the
    classes in ``y_score``. If ``None``, the numerical or lexicographical
    order of the labels in ``y_true`` is used.

Returns
-------
auc : float
    Area Under the Curve score.

See Also
--------
average_precision_score : Area under the precision-recall curve.
roc_curve : Compute Receiver operating characteristic (ROC) curve.
RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
    (ROC) curve given an estimator and some data.
RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
    (ROC) curve given the true and predicted values.

Notes
-----
The Gini Coefficient is a summary measure of the ranking ability of binary
classifiers. It is expressed using the area under of the ROC as follows:

G = 2 * AUC - 1

Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation
will ensure that random guessing will yield a score of 0 in expectation, and it is
upper bounded by 1.

References
----------
.. [1] `Wikipedia entry for the Receiver operating characteristic
        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_

.. [2] `Analyzing a portion of the ROC curve. McClish, 1989
        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_

.. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
       probability estimation trees (Section 6.2), CeDER Working Paper
       #IS-00-04, Stern School of Business, New York University.

.. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern
        Recognition Letters, 27(8), 861-874.
        <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_

.. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
        Under the ROC Curve for Multiple Class Classification Problems.
        Machine Learning, 45(2), 171-186.
        <http://link.springer.com/article/10.1023/A:1010920819831>`_
.. [6] `Wikipedia entry for the Gini coefficient
        <https://en.wikipedia.org/wiki/Gini_coefficient>`_

Examples
--------
Binary case:

>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.metrics import roc_auc_score
>>> X, y = load_breast_cancer(return_X_y=True)
>>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)
>>> roc_auc_score(y, clf.predict_proba(X)[:, 1])
0.99...
>>> roc_auc_score(y, clf.decision_function(X))
0.99...

Multiclass case:

>>> from sklearn.datasets import load_iris
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)
>>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
0.99...

Multilabel case:

>>> import numpy as np
>>> from sklearn.datasets import make_multilabel_classification
>>> from sklearn.multioutput import MultiOutputClassifier
>>> X, y = make_multilabel_classification(random_state=0)
>>> clf = MultiOutputClassifier(clf).fit(X, y)
>>> # get a list of n_output containing probability arrays of shape
>>> # (n_samples, n_classes)
>>> y_pred = clf.predict_proba(X)
>>> # extract the positive columns for each output
>>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])
>>> roc_auc_score(y, y_pred, average=None)
array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
>>> from sklearn.linear_model import RidgeClassifierCV
>>> clf = RidgeClassifierCV().fit(X, y)
>>> roc_auc_score(y, clf.decision_function(X), average=None)
array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod> rdf:type owl:Class ;
                                                                                                    rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                    <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                    rdfs:comment """Compute Receiver operating characteristic (ROC).

Note: this implementation is restricted to the binary classification task.

Read more in the :ref:`User Guide <roc_metrics>`.

Parameters
----------
y_true : array-like of shape (n_samples,)
    True binary labels. If labels are not either {-1, 1} or {0, 1}, then
    pos_label should be explicitly given.

y_score : array-like of shape (n_samples,)
    Target scores, can either be probability estimates of the positive
    class, confidence values, or non-thresholded measure of decisions
    (as returned by \"decision_function\" on some classifiers).

pos_label : int, float, bool or str, default=None
    The label of the positive class.
    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},
    ``pos_label`` is set to 1, otherwise an error will be raised.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

drop_intermediate : bool, default=True
    Whether to drop some suboptimal thresholds which would not appear
    on a plotted ROC curve. This is useful in order to create lighter
    ROC curves.

    .. versionadded:: 0.17
       parameter *drop_intermediate*.

Returns
-------
fpr : ndarray of shape (>2,)
    Increasing false positive rates such that element i is the false
    positive rate of predictions with score >= `thresholds[i]`.

tpr : ndarray of shape (>2,)
    Increasing true positive rates such that element `i` is the true
    positive rate of predictions with score >= `thresholds[i]`.

thresholds : ndarray of shape (n_thresholds,)
    Decreasing thresholds on the decision function used to compute
    fpr and tpr. `thresholds[0]` represents no instances being predicted
    and is arbitrarily set to `np.inf`.

See Also
--------
RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic
    (ROC) curve given an estimator and some data.
RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic
    (ROC) curve given the true and predicted values.
det_curve: Compute error rates for different probability thresholds.
roc_auc_score : Compute the area under the ROC curve.

Notes
-----
Since the thresholds are sorted from low to high values, they
are reversed upon returning them to ensure they correspond to both ``fpr``
and ``tpr``, which are sorted in reversed order during their calculation.

An arbitrary threshold is added for the case `tpr=0` and `fpr=0` to
ensure that the curve starts at `(0, 0)`. This threshold corresponds to the
`np.inf`.

References
----------
.. [1] `Wikipedia entry for the Receiver operating characteristic
        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_

.. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition
       Letters, 2006, 27(8):861-874.

Examples
--------
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
>>> fpr
array([0. , 0. , 0.5, 0.5, 1. ])
>>> tpr
array([0. , 0.5, 0.5, 1. , 1. ])
>>> thresholds
array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                rdfs:comment """Root mean squared error regression loss.

Read more in the :ref:`User Guide <mean_squared_error>`.

.. versionadded:: 1.4

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors in case of multioutput input.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import root_mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> root_mean_squared_error(y_true, y_pred)
0.612...
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> root_mean_squared_error(y_true, y_pred)
0.822...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod> rdf:type owl:Class ;
                                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                                   <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                                   rdfs:comment """Root mean squared logarithmic error regression loss.

Read more in the :ref:`User Guide <mean_squared_log_error>`.

.. versionadded:: 1.4

Parameters
----------
y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.

y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'

    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.

    'raw_values' :
        Returns a full set of errors when the input is of multioutput
        format.

    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.

Returns
-------
loss : float or ndarray of floats
    A non-negative floating point value (the best value is 0.0), or an
    array of floating point values, one for each individual target.

Examples
--------
>>> from sklearn.metrics import root_mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> root_mean_squared_log_error(y_true, y_pred)
0.199...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                         rdfs:comment """Linear classifiers (SVM, logistic regression, etc.) with SGD training.

This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning via the `partial_fit` method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.

This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).

The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.

Read more in the :ref:`User Guide <sgd>`.

Parameters
----------
loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'
    The loss function to be used.

    - 'hinge' gives a linear SVM.
    - 'log_loss' gives logistic regression, a probabilistic classifier.
    - 'modified_huber' is another smooth loss that brings tolerance to
      outliers as well as probability estimates.
    - 'squared_hinge' is like hinge but is quadratically penalized.
    - 'perceptron' is the linear loss used by the perceptron algorithm.
    - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and
      'squared_epsilon_insensitive' are designed for regression but can be useful
      in classification as well; see
      :class:`~sklearn.linear_model.SGDRegressor` for a description.

    More details about the losses formulas can be found in the
    :ref:`User Guide <sgd_mathematical_formulation>`.

penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'
    The penalty (aka regularization term) to be used. Defaults to 'l2'
    which is the standard regularizer for linear SVM models. 'l1' and
    'elasticnet' might bring sparsity to the model (feature selection)
    not achievable with 'l2'. No penalty is added when set to `None`.

alpha : float, default=0.0001
    Constant that multiplies the regularization term. The higher the
    value, the stronger the regularization. Also used to compute the
    learning rate when `learning_rate` is set to 'optimal'.
    Values must be in the range `[0.0, inf)`.

l1_ratio : float, default=0.15
    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.
    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
    Only used if `penalty` is 'elasticnet'.
    Values must be in the range `[0.0, 1.0]`.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`partial_fit` method.
    Values must be in the range `[1, inf)`.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, training will stop
    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive
    epochs.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.19

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.
    Values must be in the range `[0, inf)`.

epsilon : float, default=0.1
    Epsilon in the epsilon-insensitive loss functions; only if `loss` is
    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.
    For 'huber', determines the threshold at which it becomes less
    important to get the prediction exactly right.
    For epsilon-insensitive, any differences between the current prediction
    and the correct label are ignored if they are less than this threshold.
    Values must be in the range `[0.0, inf)`.

n_jobs : int, default=None
    The number of CPUs to use to do the OVA (One Versus All, for
    multi-class problems) computation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

random_state : int, RandomState instance, default=None
    Used for shuffling the data, when ``shuffle`` is set to ``True``.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.
    Integer values must be in the range `[0, 2**32 - 1]`.

learning_rate : str, default='optimal'
    The learning rate schedule:

    - 'constant': `eta = eta0`
    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`
      where `t0` is chosen by a heuristic proposed by Leon Bottou.
    - 'invscaling': `eta = eta0 / pow(t, power_t)`
    - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.
      Each time n_iter_no_change consecutive epochs fail to decrease the
      training loss by tol or fail to increase validation score by tol if
      `early_stopping` is `True`, the current learning rate is divided by 5.

        .. versionadded:: 0.20
            Added 'adaptive' option

eta0 : float, default=0.0
    The initial learning rate for the 'constant', 'invscaling' or
    'adaptive' schedules. The default value is 0.0 as eta0 is not used by
    the default schedule 'optimal'.
    Values must be in the range `[0.0, inf)`.

power_t : float, default=0.5
    The exponent for inverse scaling learning rate.
    Values must be in the range `(-inf, inf)`.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to `True`, it will automatically set aside
    a stratified fraction of training data as validation and terminate
    training when validation score returned by the `score` method is not
    improving by at least tol for n_iter_no_change consecutive epochs.

    .. versionadded:: 0.20
        Added 'early_stopping' option

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if `early_stopping` is True.
    Values must be in the range `(0.0, 1.0)`.

    .. versionadded:: 0.20
        Added 'validation_fraction' option

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before stopping
    fitting.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Integer values must be in the range `[1, max_iter)`.

    .. versionadded:: 0.20
        Added 'n_iter_no_change' option

class_weight : dict, {class_label: weight} or \"balanced\", default=None
    Preset for the class_weight fit parameter.

    Weights associated with classes. If not given, all classes
    are supposed to have weight one.

    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.
    If a dynamic learning rate is used, the learning rate is adapted
    depending on the number of samples already seen. Calling ``fit`` resets
    this counter, while ``partial_fit`` will result in increasing the
    existing counter.

average : bool or int, default=False
    When set to `True`, computes the averaged SGD weights across all
    updates and stores the result in the ``coef_`` attribute. If set to
    an int greater than 1, averaging will begin once the total number of
    samples seen reaches `average`. So ``average=10`` will begin
    averaging after seeing 10 samples.
    Integer values must be in the range `[1, n_samples]`.

Attributes
----------
coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.

n_iter_ : int
    The actual number of iterations before reaching the stopping criterion.
    For multiclass fits, it is the maximum over every binary fit.

loss_function_ : concrete ``LossFunction``

    .. deprecated:: 1.4
        Attribute `loss_function_` was deprecated in version 1.4 and will be
        removed in 1.6.

classes_ : array of shape (n_classes,)

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.svm.LinearSVC : Linear support vector classification.
LogisticRegression : Logistic regression.
Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to
    ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",
    penalty=None)``.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import SGDClassifier
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> Y = np.array([1, 1, 2, 2])
>>> # Always scale the input. The most convenient way is to use a pipeline.
>>> clf = make_pipeline(StandardScaler(),
...                     SGDClassifier(max_iter=1000, tol=1e-3))
>>> clf.fit(X, Y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdclassifier', SGDClassifier())])
>>> print(clf.predict([[-0.8, -1]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                          rdfs:comment """Solves linear One-Class SVM using Stochastic Gradient Descent.

This implementation is meant to be used with a kernel approximation
technique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results
similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by
default.

Read more in the :ref:`User Guide <sgd_online_one_class_svm>`.

.. versionadded:: 1.0

Parameters
----------
nu : float, default=0.5
    The nu parameter of the One Class SVM: an upper bound on the
    fraction of training errors and a lower bound of the fraction of
    support vectors. Should be in the interval (0, 1]. By default 0.5
    will be taken.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. Defaults to True.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    `partial_fit`. Defaults to 1000.
    Values must be in the range `[1, inf)`.

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, the iterations will stop
    when (loss > previous_loss - tol). Defaults to 1e-3.
    Values must be in the range `[0.0, inf)`.

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.
    Defaults to True.

verbose : int, default=0
    The verbosity level.

random_state : int, RandomState instance or None, default=None
    The seed of the pseudo random number generator to use when shuffling
    the data.  If int, random_state is the seed used by the random number
    generator; If RandomState instance, random_state is the random number
    generator; If None, the random number generator is the RandomState
    instance used by `np.random`.

learning_rate : {'constant', 'optimal', 'invscaling', 'adaptive'}, default='optimal'
    The learning rate schedule to use with `fit`. (If using `partial_fit`,
    learning rate must be controlled directly).

    - 'constant': `eta = eta0`
    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`
      where t0 is chosen by a heuristic proposed by Leon Bottou.
    - 'invscaling': `eta = eta0 / pow(t, power_t)`
    - 'adaptive': eta = eta0, as long as the training keeps decreasing.
      Each time n_iter_no_change consecutive epochs fail to decrease the
      training loss by tol or fail to increase validation score by tol if
      early_stopping is True, the current learning rate is divided by 5.

eta0 : float, default=0.0
    The initial learning rate for the 'constant', 'invscaling' or
    'adaptive' schedules. The default value is 0.0 as eta0 is not used by
    the default schedule 'optimal'.
    Values must be in the range `[0.0, inf)`.

power_t : float, default=0.5
    The exponent for inverse scaling learning rate.
    Values must be in the range `(-inf, inf)`.

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.
    If a dynamic learning rate is used, the learning rate is adapted
    depending on the number of samples already seen. Calling ``fit`` resets
    this counter, while ``partial_fit``  will result in increasing the
    existing counter.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights and stores the
    result in the ``coef_`` attribute. If set to an int greater than 1,
    averaging will begin once the total number of samples seen reaches
    average. So ``average=10`` will begin averaging after seeing 10
    samples.

Attributes
----------
coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features.

offset_ : ndarray of shape (1,)
    Offset used to define the decision function from the raw scores.
    We have the relation: decision_function = score_samples - offset.

n_iter_ : int
    The actual number of iterations to reach the stopping criterion.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

loss_function_ : concrete ``LossFunction``

    .. deprecated:: 1.4
        ``loss_function_`` was deprecated in version 1.4 and will be removed in
        1.6.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.

Notes
-----
This estimator has a linear complexity in the number of training samples
and is thus better suited than the `sklearn.svm.OneClassSVM`
implementation for datasets with a large number of training samples (say
> 10,000).

Examples
--------
>>> import numpy as np
>>> from sklearn import linear_model
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> clf = linear_model.SGDOneClassSVM(random_state=42)
>>> clf.fit(X)
SGDOneClassSVM(random_state=42)

>>> print(clf.predict([[4, 4]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                        rdfs:comment """Linear model fitted by minimizing a regularized empirical loss with SGD.

SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).

The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.

This implementation works with data represented as dense numpy arrays of
floating point values for the features.

Read more in the :ref:`User Guide <sgd>`.

Parameters
----------
loss : str, default='squared_error'
    The loss function to be used. The possible values are 'squared_error',
    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'

    The 'squared_error' refers to the ordinary least squares fit.
    'huber' modifies 'squared_error' to focus less on getting outliers
    correct by switching from squared to linear loss past a distance of
    epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is
    linear past that; this is the loss function used in SVR.
    'squared_epsilon_insensitive' is the same but becomes squared loss past
    a tolerance of epsilon.

    More details about the losses formulas can be found in the
    :ref:`User Guide <sgd_mathematical_formulation>`.

penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'
    The penalty (aka regularization term) to be used. Defaults to 'l2'
    which is the standard regularizer for linear SVM models. 'l1' and
    'elasticnet' might bring sparsity to the model (feature selection)
    not achievable with 'l2'. No penalty is added when set to `None`.

alpha : float, default=0.0001
    Constant that multiplies the regularization term. The higher the
    value, the stronger the regularization. Also used to compute the
    learning rate when `learning_rate` is set to 'optimal'.
    Values must be in the range `[0.0, inf)`.

l1_ratio : float, default=0.15
    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.
    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
    Only used if `penalty` is 'elasticnet'.
    Values must be in the range `[0.0, 1.0]`.

fit_intercept : bool, default=True
    Whether the intercept should be estimated or not. If False, the
    data is assumed to be already centered.

max_iter : int, default=1000
    The maximum number of passes over the training data (aka epochs).
    It only impacts the behavior in the ``fit`` method, and not the
    :meth:`partial_fit` method.
    Values must be in the range `[1, inf)`.

    .. versionadded:: 0.19

tol : float or None, default=1e-3
    The stopping criterion. If it is not None, training will stop
    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive
    epochs.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Values must be in the range `[0.0, inf)`.

    .. versionadded:: 0.19

shuffle : bool, default=True
    Whether or not the training data should be shuffled after each epoch.

verbose : int, default=0
    The verbosity level.
    Values must be in the range `[0, inf)`.

epsilon : float, default=0.1
    Epsilon in the epsilon-insensitive loss functions; only if `loss` is
    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.
    For 'huber', determines the threshold at which it becomes less
    important to get the prediction exactly right.
    For epsilon-insensitive, any differences between the current prediction
    and the correct label are ignored if they are less than this threshold.
    Values must be in the range `[0.0, inf)`.

random_state : int, RandomState instance, default=None
    Used for shuffling the data, when ``shuffle`` is set to ``True``.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

learning_rate : str, default='invscaling'
    The learning rate schedule:

    - 'constant': `eta = eta0`
    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`
      where t0 is chosen by a heuristic proposed by Leon Bottou.
    - 'invscaling': `eta = eta0 / pow(t, power_t)`
    - 'adaptive': eta = eta0, as long as the training keeps decreasing.
      Each time n_iter_no_change consecutive epochs fail to decrease the
      training loss by tol or fail to increase validation score by tol if
      early_stopping is True, the current learning rate is divided by 5.

        .. versionadded:: 0.20
            Added 'adaptive' option

eta0 : float, default=0.01
    The initial learning rate for the 'constant', 'invscaling' or
    'adaptive' schedules. The default value is 0.01.
    Values must be in the range `[0.0, inf)`.

power_t : float, default=0.25
    The exponent for inverse scaling learning rate.
    Values must be in the range `(-inf, inf)`.

early_stopping : bool, default=False
    Whether to use early stopping to terminate training when validation
    score is not improving. If set to True, it will automatically set aside
    a fraction of training data as validation and terminate
    training when validation score returned by the `score` method is not
    improving by at least `tol` for `n_iter_no_change` consecutive
    epochs.

    .. versionadded:: 0.20
        Added 'early_stopping' option

validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if `early_stopping` is True.
    Values must be in the range `(0.0, 1.0)`.

    .. versionadded:: 0.20
        Added 'validation_fraction' option

n_iter_no_change : int, default=5
    Number of iterations with no improvement to wait before stopping
    fitting.
    Convergence is checked against the training loss or the
    validation loss depending on the `early_stopping` parameter.
    Integer values must be in the range `[1, max_iter)`.

    .. versionadded:: 0.20
        Added 'n_iter_no_change' option

warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    See :term:`the Glossary <warm_start>`.

    Repeatedly calling fit or partial_fit when warm_start is True can
    result in a different solution than when calling fit a single time
    because of the way the data is shuffled.
    If a dynamic learning rate is used, the learning rate is adapted
    depending on the number of samples already seen. Calling ``fit`` resets
    this counter, while ``partial_fit``  will result in increasing the
    existing counter.

average : bool or int, default=False
    When set to True, computes the averaged SGD weights across all
    updates and stores the result in the ``coef_`` attribute. If set to
    an int greater than 1, averaging will begin once the total number of
    samples seen reaches `average`. So ``average=10`` will begin
    averaging after seeing 10 samples.

Attributes
----------
coef_ : ndarray of shape (n_features,)
    Weights assigned to the features.

intercept_ : ndarray of shape (1,)
    The intercept term.

n_iter_ : int
    The actual number of iterations before reaching the stopping criterion.

t_ : int
    Number of weight updates performed during training.
    Same as ``(n_iter_ * n_samples + 1)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
HuberRegressor : Linear regression model that is robust to outliers.
Lars : Least Angle Regression model.
Lasso : Linear Model trained with L1 prior as regularizer.
RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.
Ridge : Linear least squares with l2 regularization.
sklearn.svm.SVR : Epsilon-Support Vector Regression.
TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import SGDRegressor
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> # Always scale the input. The most convenient way is to use a pipeline.
>>> reg = make_pipeline(StandardScaler(),
...                     SGDRegressor(max_iter=1000, tol=1e-3))
>>> reg.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdregressor', SGDRegressor())])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                               rdfs:comment """C-Support Vector Classification.

The implementation is based on libsvm. The fit time scales at least
quadratically with the number of samples and may be impractical
beyond tens of thousands of samples. For large datasets
consider using :class:`~sklearn.svm.LinearSVC` or
:class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a
:class:`~sklearn.kernel_approximation.Nystroem` transformer or
other :ref:`kernel_approximation`.

The multiclass support is handled according to a one-vs-one scheme.

For details on the precise mathematical formulation of the provided
kernel functions and how `gamma`, `coef0` and `degree` affect each
other, see the corresponding section in the narrative documentation:
:ref:`svm_kernels`.

To learn how to tune SVC's hyperparameters, see the following example:
:ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`

Read more in the :ref:`User Guide <svm_classification>`.

Parameters
----------
C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive. The penalty
    is a squared l2 penalty.

kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'
    Specifies the kernel type to be used in the algorithm. If
    none is given, 'rbf' will be used. If a callable is given it is used to
    pre-compute the kernel matrix from data matrices; that matrix should be
    an array of shape ``(n_samples, n_samples)``. For an intuitive
    visualization of different kernel types see
    :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.

degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Must be non-negative. Ignored by all other kernels.

gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.

    - if ``gamma='scale'`` (default) is passed then it uses
      1 / (n_features * X.var()) as value of gamma,
    - if 'auto', uses 1 / n_features
    - if float, must be non-negative.

    .. versionchanged:: 0.22
       The default value of ``gamma`` changed from 'auto' to 'scale'.

coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.

shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:`User Guide <shrinking_svm>`.

probability : bool, default=False
    Whether to enable probability estimates. This must be enabled prior
    to calling `fit`, will slow down that method as it internally uses
    5-fold cross-validation, and `predict_proba` may be inconsistent with
    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.

tol : float, default=1e-3
    Tolerance for stopping criterion.

cache_size : float, default=200
    Specify the size of the kernel cache (in MB).

class_weight : dict or 'balanced', default=None
    Set the parameter C of class i to class_weight[i]*C for
    SVC. If not given, all classes are supposed to have
    weight one.
    The \"balanced\" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``.

verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.

max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.

decision_function_shape : {'ovo', 'ovr'}, default='ovr'
    Whether to return a one-vs-rest ('ovr') decision function of shape
    (n_samples, n_classes) as all other classifiers, or the original
    one-vs-one ('ovo') decision function of libsvm which has shape
    (n_samples, n_classes * (n_classes - 1) / 2). However, note that
    internally, one-vs-one ('ovo') is always used as a multi-class strategy
    to train models; an ovr matrix is only constructed from the ovo matrix.
    The parameter is ignored for binary classification.

    .. versionchanged:: 0.19
        decision_function_shape is 'ovr' by default.

    .. versionadded:: 0.17
       *decision_function_shape='ovr'* is recommended.

    .. versionchanged:: 0.17
       Deprecated *decision_function_shape='ovo' and None*.

break_ties : bool, default=False
    If true, ``decision_function_shape='ovr'``, and number of classes > 2,
    :term:`predict` will break ties according to the confidence values of
    :term:`decision_function`; otherwise the first class among the tied
    classes is returned. Please note that breaking ties comes at a
    relatively high computational cost compared to a simple predict.

    .. versionadded:: 0.22

random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data for
    probability estimates. Ignored when `probability` is False.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
class_weight_ : ndarray of shape (n_classes,)
    Multipliers of parameter C for each class.
    Computed based on the ``class_weight`` parameter.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.

    `coef_` is a readonly property derived from `dual_coef_` and
    `support_vectors_`.

dual_coef_ : ndarray of shape (n_classes -1, n_SV)
    Dual coefficients of the support vector in the decision
    function (see :ref:`sgd_mathematical_formulation`), multiplied by
    their targets.
    For multiclass, coefficient for all 1-vs-1 classifiers.
    The layout of the coefficients in the multiclass case is somewhat
    non-trivial. See the :ref:`multi-class section of the User Guide
    <svm_multi_class>` for details.

fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)

intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)
    Number of iterations run by the optimization routine to fit the model.
    The shape of this attribute depends on the number of models optimized
    which in turn depends on the number of classes.

    .. versionadded:: 1.1

support_ : ndarray of shape (n_SV)
    Indices of support vectors.

support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors. An empty array if kernel is precomputed.

n_support_ : ndarray of shape (n_classes,), dtype=int32
    Number of support vectors for each class.

probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)
probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)
    If `probability=True`, it corresponds to the parameters learned in
    Platt scaling to produce probability estimates from decision values.
    If `probability=False`, it's an empty array. Platt scaling uses the
    logistic function
    ``1 / (1 + exp(decision_value * probA_ + probB_))``
    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For
    more information on the multiclass case and training procedure see
    section 8 of [1]_.

shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector ``X``.

See Also
--------
SVR : Support Vector Machine for Regression implemented using libsvm.

LinearSVC : Scalable Linear Support Vector Machine for classification
    implemented using liblinear. Check the See Also section of
    LinearSVC for more comparison element.

References
----------
.. [1] `LIBSVM: A Library for Support Vector Machines
    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_

.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector
    Machines and Comparisons to Regularized Likelihood Methods\"
    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_

Examples
--------
>>> import numpy as np
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> y = np.array([1, 1, 2, 2])
>>> from sklearn.svm import SVC
>>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
>>> clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc', SVC(gamma='auto'))])

>>> print(clf.predict([[-0.8, -1]]))
[1]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> ,
                                                                                                               <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                               rdfs:comment """Epsilon-Support Vector Regression.

The free parameters in the model are C and epsilon.

The implementation is based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to datasets with more than a couple of 10000 samples. For large
datasets consider using :class:`~sklearn.svm.LinearSVR` or
:class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a
:class:`~sklearn.kernel_approximation.Nystroem` transformer or
other :ref:`kernel_approximation`.

Read more in the :ref:`User Guide <svm_regression>`.

Parameters
----------
kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'
     Specifies the kernel type to be used in the algorithm.
     If none is given, 'rbf' will be used. If a callable is given it is
     used to precompute the kernel matrix.

degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Must be non-negative. Ignored by all other kernels.

gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.

    - if ``gamma='scale'`` (default) is passed then it uses
      1 / (n_features * X.var()) as value of gamma,
    - if 'auto', uses 1 / n_features
    - if float, must be non-negative.

    .. versionchanged:: 0.22
       The default value of ``gamma`` changed from 'auto' to 'scale'.

coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.

tol : float, default=1e-3
    Tolerance for stopping criterion.

C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive.
    The penalty is a squared l2 penalty.

epsilon : float, default=0.1
     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
     within which no penalty is associated in the training loss function
     with points predicted within a distance epsilon from the actual
     value. Must be non-negative.

shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:`User Guide <shrinking_svm>`.

cache_size : float, default=200
    Specify the size of the kernel cache (in MB).

verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.

max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.

Attributes
----------
coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.

    `coef_` is readonly property derived from `dual_coef_` and
    `support_vectors_`.

dual_coef_ : ndarray of shape (1, n_SV)
    Coefficients of the support vector in the decision function.

fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)

intercept_ : ndarray of shape (1,)
    Constants in decision function.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_iter_ : int
    Number of iterations run by the optimization routine to fit the model.

    .. versionadded:: 1.1

n_support_ : ndarray of shape (1,), dtype=int32
    Number of support vectors.

shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector ``X``.

support_ : ndarray of shape (n_SV,)
    Indices of support vectors.

support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.

See Also
--------
NuSVR : Support Vector Machine for regression implemented using libsvm
    using a parameter to control the number of support vectors.

LinearSVR : Scalable Linear Support Vector Machine for regression
    implemented using liblinear.

References
----------
.. [1] `LIBSVM: A Library for Support Vector Machines
    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_

.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector
    Machines and Comparisons to Regularized Likelihood Methods\"
    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_

Examples
--------
>>> from sklearn.svm import SVR
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))
>>> regr.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svr', SVR(epsilon=0.2))])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                     rdfs:comment """Filter: Select the p-values for an estimated false discovery rate.

This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound
on the expected false discovery rate.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
score_func : callable, default=f_classif
    Function taking two arrays X and y, and returning a pair of arrays
    (scores, pvalues).
    Default is f_classif (see below \"See Also\"). The default function only
    works with classification tasks.

alpha : float, default=5e-2
    The highest uncorrected p-value for features to keep.

Attributes
----------
scores_ : array-like of shape (n_features,)
    Scores of features.

pvalues_ : array-like of shape (n_features,)
    p-values of feature scores.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
mutual_info_classif : Mutual information for a discrete target.
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.
mutual_info_regression : Mutual information for a continuous target.
SelectPercentile : Select features based on percentile of the highest
    scores.
SelectKBest : Select features based on the k highest scores.
SelectFpr : Select features based on a false positive rate test.
SelectFwe : Select features based on family-wise error rate.
GenericUnivariateSelect : Univariate feature selector with configurable
    mode.

References
----------
https://en.wikipedia.org/wiki/False_discovery_rate

Examples
--------
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.feature_selection import SelectFdr, chi2
>>> X, y = load_breast_cancer(return_X_y=True)
>>> X.shape
(569, 30)
>>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)
>>> X_new.shape
(569, 16)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                     rdfs:comment """Filter: Select the pvalues below alpha based on a FPR test.

FPR test stands for False Positive Rate test. It controls the total
amount of false detections.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
score_func : callable, default=f_classif
    Function taking two arrays X and y, and returning a pair of arrays
    (scores, pvalues).
    Default is f_classif (see below \"See Also\"). The default function only
    works with classification tasks.

alpha : float, default=5e-2
    Features with p-values less than `alpha` are selected.

Attributes
----------
scores_ : array-like of shape (n_features,)
    Scores of features.

pvalues_ : array-like of shape (n_features,)
    p-values of feature scores.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
chi2 : Chi-squared stats of non-negative features for classification tasks.
mutual_info_classif: Mutual information for a discrete target.
f_regression : F-value between label/feature for regression tasks.
mutual_info_regression : Mutual information for a continuous target.
SelectPercentile : Select features based on percentile of the highest
    scores.
SelectKBest : Select features based on the k highest scores.
SelectFdr : Select features based on an estimated false discovery rate.
SelectFwe : Select features based on family-wise error rate.
GenericUnivariateSelect : Univariate feature selector with configurable
    mode.

Examples
--------
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.feature_selection import SelectFpr, chi2
>>> X, y = load_breast_cancer(return_X_y=True)
>>> X.shape
(569, 30)
>>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)
>>> X_new.shape
(569, 16)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                           rdfs:comment """Meta-transformer for selecting features based on importance weights.

.. versionadded:: 0.17

Read more in the :ref:`User Guide <select_from_model>`.

Parameters
----------
estimator : object
    The base estimator from which the transformer is built.
    This can be both a fitted (if ``prefit`` is set to True)
    or a non-fitted estimator. The estimator should have a
    ``feature_importances_`` or ``coef_`` attribute after fitting.
    Otherwise, the ``importance_getter`` parameter should be used.

threshold : str or float, default=None
    The threshold value to use for feature selection. Features whose
    absolute importance value is greater or equal are kept while the others
    are discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value
    is the median (resp. the mean) of the feature importances. A scaling
    factor (e.g., \"1.25*mean\") may also be used. If None and if the
    estimator has a parameter penalty set to l1, either explicitly
    or implicitly (e.g, Lasso), the threshold used is 1e-5.
    Otherwise, \"mean\" is used by default.

prefit : bool, default=False
    Whether a prefit model is expected to be passed into the constructor
    directly or not.
    If `True`, `estimator` must be a fitted estimator.
    If `False`, `estimator` is fitted and updated by calling
    `fit` and `partial_fit`, respectively.

norm_order : non-zero int, inf, -inf, default=1
    Order of the norm used to filter the vectors of coefficients below
    ``threshold`` in the case where the ``coef_`` attribute of the
    estimator is of dimension 2.

max_features : int, callable, default=None
    The maximum number of features to select.

    - If an integer, then it specifies the maximum number of features to
      allow.
    - If a callable, then it specifies how to calculate the maximum number of
      features allowed by using the output of `max_features(X)`.
    - If `None`, then all features are kept.

    To only select based on ``max_features``, set ``threshold=-np.inf``.

    .. versionadded:: 0.20
    .. versionchanged:: 1.1
       `max_features` accepts a callable.

importance_getter : str or callable, default='auto'
    If 'auto', uses the feature importance either through a ``coef_``
    attribute or ``feature_importances_`` attribute of estimator.

    Also accepts a string that specifies an attribute name/path
    for extracting feature importance (implemented with `attrgetter`).
    For example, give `regressor_.coef_` in case of
    :class:`~sklearn.compose.TransformedTargetRegressor`  or
    `named_steps.clf.feature_importances_` in case of
    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.

    If `callable`, overrides the default feature importance getter.
    The callable is passed with the fitted estimator and it should
    return importance for each feature.

    .. versionadded:: 0.24

Attributes
----------
estimator_ : estimator
    The base estimator from which the transformer is built. This attribute
    exist only when `fit` has been called.

    - If `prefit=True`, it is a deep copy of `estimator`.
    - If `prefit=False`, it is a clone of `estimator` and fit on the data
      passed to `fit` or `partial_fit`.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 0.24

max_features_ : int
    Maximum number of features calculated during :term:`fit`. Only defined
    if the ``max_features`` is not `None`.

    - If `max_features` is an `int`, then `max_features_ = max_features`.
    - If `max_features` is a callable, then `max_features_ = max_features(X)`.

    .. versionadded:: 1.1

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

threshold_ : float
    The threshold value used for feature selection.

See Also
--------
RFE : Recursive feature elimination based on importance weights.
RFECV : Recursive feature elimination with built-in cross-validated
    selection of the best number of features.
SequentialFeatureSelector : Sequential cross-validation based feature
    selection. Does not rely on importance weights.

Notes
-----
Allows NaN/Inf in the input if the underlying estimator does as well.

Examples
--------
>>> from sklearn.feature_selection import SelectFromModel
>>> from sklearn.linear_model import LogisticRegression
>>> X = [[ 0.87, -1.34,  0.31 ],
...      [-2.79, -0.02, -0.85 ],
...      [-1.34, -0.48, -2.55 ],
...      [ 1.92,  1.48,  0.65 ]]
>>> y = [0, 1, 0, 1]
>>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)
>>> selector.estimator_.coef_
array([[-0.3252...,  0.8345...,  0.4976...]])
>>> selector.threshold_
0.55249...
>>> selector.get_support()
array([False,  True, False])
>>> selector.transform(X)
array([[-1.34],
       [-0.02],
       [-0.48],
       [ 1.48]])

Using a callable to create a selector that can use no more than half
of the input features.

>>> def half_callable(X):
...     return round(len(X[0]) / 2)
>>> half_selector = SelectFromModel(estimator=LogisticRegression(),
...                                 max_features=half_callable)
>>> _ = half_selector.fit(X, y)
>>> half_selector.max_features_
2""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                     rdfs:comment """Filter: Select the p-values corresponding to Family-wise error rate.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
score_func : callable, default=f_classif
    Function taking two arrays X and y, and returning a pair of arrays
    (scores, pvalues).
    Default is f_classif (see below \"See Also\"). The default function only
    works with classification tasks.

alpha : float, default=5e-2
    The highest uncorrected p-value for features to keep.

Attributes
----------
scores_ : array-like of shape (n_features,)
    Scores of features.

pvalues_ : array-like of shape (n_features,)
    p-values of feature scores.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.
SelectPercentile : Select features based on percentile of the highest
    scores.
SelectKBest : Select features based on the k highest scores.
SelectFpr : Select features based on a false positive rate test.
SelectFdr : Select features based on an estimated false discovery rate.
GenericUnivariateSelect : Univariate feature selector with configurable
    mode.

Examples
--------
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.feature_selection import SelectFwe, chi2
>>> X, y = load_breast_cancer(return_X_y=True)
>>> X.shape
(569, 30)
>>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)
>>> X_new.shape
(569, 15)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                       rdfs:comment """Select features according to the k highest scores.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
score_func : callable, default=f_classif
    Function taking two arrays X and y, and returning a pair of arrays
    (scores, pvalues) or a single array with scores.
    Default is f_classif (see below \"See Also\"). The default function only
    works with classification tasks.

    .. versionadded:: 0.18

k : int or \"all\", default=10
    Number of top features to select.
    The \"all\" option bypasses selection, for use in a parameter search.

Attributes
----------
scores_ : array-like of shape (n_features,)
    Scores of features.

pvalues_ : array-like of shape (n_features,)
    p-values of feature scores, None if `score_func` returned only scores.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
f_classif: ANOVA F-value between label/feature for classification tasks.
mutual_info_classif: Mutual information for a discrete target.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
mutual_info_regression: Mutual information for a continuous target.
SelectPercentile: Select features based on percentile of the highest
    scores.
SelectFpr : Select features based on a false positive rate test.
SelectFdr : Select features based on an estimated false discovery rate.
SelectFwe : Select features based on family-wise error rate.
GenericUnivariateSelect : Univariate feature selector with configurable
    mode.

Notes
-----
Ties between features with equal scores will be broken in an unspecified
way.

This filter supports unsupervised feature selection that only requests `X` for
computing the scores.

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> X, y = load_digits(return_X_y=True)
>>> X.shape
(1797, 64)
>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)
>>> X_new.shape
(1797, 20)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                            rdfs:comment """Select features according to a percentile of the highest scores.

Read more in the :ref:`User Guide <univariate_feature_selection>`.

Parameters
----------
score_func : callable, default=f_classif
    Function taking two arrays X and y, and returning a pair of arrays
    (scores, pvalues) or a single array with scores.
    Default is f_classif (see below \"See Also\"). The default function only
    works with classification tasks.

    .. versionadded:: 0.18

percentile : int, default=10
    Percent of features to keep.

Attributes
----------
scores_ : array-like of shape (n_features,)
    Scores of features.

pvalues_ : array-like of shape (n_features,)
    p-values of feature scores, None if `score_func` returned only scores.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
mutual_info_classif : Mutual information for a discrete target.
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.
mutual_info_regression : Mutual information for a continuous target.
SelectKBest : Select features based on the k highest scores.
SelectFpr : Select features based on a false positive rate test.
SelectFdr : Select features based on an estimated false discovery rate.
SelectFwe : Select features based on family-wise error rate.
GenericUnivariateSelect : Univariate feature selector with configurable
    mode.

Notes
-----
Ties between features with equal scores will be broken in an unspecified
way.

This filter supports unsupervised feature selection that only requests `X` for
computing the scores.

Examples
--------
>>> from sklearn.datasets import load_digits
>>> from sklearn.feature_selection import SelectPercentile, chi2
>>> X, y = load_digits(return_X_y=True)
>>> X.shape
(1797, 64)
>>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)
>>> X_new.shape
(1797, 7)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod> rdf:type owl:Class ;
                                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                                     rdfs:comment """Transformer that performs Sequential Feature Selection.

This Sequential Feature Selector adds (forward selection) or
removes (backward selection) features to form a feature subset in a
greedy fashion. At each stage, this estimator chooses the best feature to
add or remove based on the cross-validation score of an estimator. In
the case of unsupervised learning, this Sequential Feature Selector
looks only at the features (X), not the desired outputs (y).

Read more in the :ref:`User Guide <sequential_feature_selection>`.

.. versionadded:: 0.24

Parameters
----------
estimator : estimator instance
    An unfitted estimator.

n_features_to_select : \"auto\", int or float, default=\"auto\"
    If `\"auto\"`, the behaviour depends on the `tol` parameter:

    - if `tol` is not `None`, then features are selected while the score
      change does not exceed `tol`.
    - otherwise, half of the features are selected.

    If integer, the parameter is the absolute number of features to select.
    If float between 0 and 1, it is the fraction of features to select.

    .. versionadded:: 1.1
       The option `\"auto\"` was added in version 1.1.

    .. versionchanged:: 1.3
       The default changed from `\"warn\"` to `\"auto\"` in 1.3.

tol : float, default=None
    If the score is not incremented by at least `tol` between two
    consecutive feature additions or removals, stop adding or removing.

    `tol` can be negative when removing features using `direction=\"backward\"`.
    It can be useful to reduce the number of features at the cost of a small
    decrease in the score.

    `tol` is enabled only when `n_features_to_select` is `\"auto\"`.

    .. versionadded:: 1.1

direction : {'forward', 'backward'}, default='forward'
    Whether to perform forward selection or backward selection.

scoring : str or callable, default=None
    A single str (see :ref:`scoring_parameter`) or a callable
    (see :ref:`scoring`) to evaluate the predictions on the test set.

    NOTE that when using a custom scorer, it should return a single
    value.

    If None, the estimator's score method is used.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross validation,
    - integer, to specify the number of folds in a `(Stratified)KFold`,
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For integer/None inputs, if the estimator is a classifier and ``y`` is
    either binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other
    cases, :class:`~sklearn.model_selection.KFold` is used. These splitters
    are instantiated with `shuffle=False` so the splits will be the same
    across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

n_jobs : int, default=None
    Number of jobs to run in parallel. When evaluating a new feature to
    add or remove, the cross-validation procedure is parallel over the
    folds.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

Attributes
----------
n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying estimator exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_features_to_select_ : int
    The number of features that were selected.

support_ : ndarray of shape (n_features,), dtype=bool
    The mask of selected features.

See Also
--------
GenericUnivariateSelect : Univariate feature selector with configurable
    strategy.
RFE : Recursive feature elimination based on importance weights.
RFECV : Recursive feature elimination based on importance weights, with
    automatic selection of the number of features.
SelectFromModel : Feature selection based on thresholds of importance
    weights.

Examples
--------
>>> from sklearn.feature_selection import SequentialFeatureSelector
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.datasets import load_iris
>>> X, y = load_iris(return_X_y=True)
>>> knn = KNeighborsClassifier(n_neighbors=3)
>>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)
>>> sfs.fit(X, y)
SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),
                          n_features_to_select=3)
>>> sfs.get_support()
array([ True, False,  True,  True])
>>> sfs.transform(X).shape
(150, 3)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                        rdfs:comment """Random permutation cross-validator.

Yields indices to split data into training and test sets.

Note: contrary to other cross-validation strategies, random splits
do not guarantee that all folds will be different, although this is
still very likely for sizeable datasets.

Read more in the :ref:`User Guide <ShuffleSplit>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=10
    Number of re-shuffling & splitting iterations.

test_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the proportion
    of the dataset to include in the test split. If int, represents the
    absolute number of test samples. If None, the value is set to the
    complement of the train size. If ``train_size`` is also None, it will
    be set to 0.1.

train_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the
    proportion of the dataset to include in the train split. If
    int, represents the absolute number of train samples. If None,
    the value is automatically set to the complement of the test size.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of the training and testing indices produced.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import ShuffleSplit
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])
>>> y = np.array([1, 2, 1, 2, 1, 2])
>>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)
>>> rs.get_n_splits(X)
5
>>> print(rs)
ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)
>>> for i, (train_index, test_index) in enumerate(rs.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[1 3 0 4]
  Test:  index=[5 2]
Fold 1:
  Train: index=[4 0 2 5]
  Test:  index=[1 3]
Fold 2:
  Train: index=[1 2 4 0]
  Test:  index=[3 5]
Fold 3:
  Train: index=[3 4 1 0]
  Test:  index=[5 2]
Fold 4:
  Train: index=[3 5 1 0]
  Test:  index=[2 4]
>>> # Specify train and test size
>>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,
...                   random_state=0)
>>> for i, (train_index, test_index) in enumerate(rs.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[1 3 0]
  Test:  index=[5 2]
Fold 1:
  Train: index=[4 0 2]
  Test:  index=[1 3]
Fold 2:
  Train: index=[1 2 4]
  Test:  index=[3 5]
Fold 3:
  Train: index=[3 4 1]
  Test:  index=[5 2]
Fold 4:
  Train: index=[3 5 1]
  Test:  index=[2 4]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Compute the Silhouette Coefficient for each sample.

The Silhouette Coefficient is a measure of how well samples are clustered
with samples that are similar to themselves. Clustering models with a high
Silhouette Coefficient are said to be dense, where samples in the same
cluster are similar to each other, and well separated, where samples in
different clusters are not very similar to each other.

The Silhouette Coefficient is calculated using the mean intra-cluster
distance (``a``) and the mean nearest-cluster distance (``b``) for each
sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
b)``.
Note that Silhouette Coefficient is only defined if number of labels
is 2 ``<= n_labels <= n_samples - 1``.

This function returns the Silhouette Coefficient for each sample.

The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters.

Read more in the :ref:`User Guide <silhouette_coefficient>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise
    An array of pairwise distances between samples, or a feature array. If
    a sparse matrix is provided, CSR format should be favoured avoiding
    an additional copy.

labels : array-like of shape (n_samples,)
    Label values for each sample.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by :func:`~sklearn.metrics.pairwise_distances`.
    If ``X`` is the distance array itself, use \"precomputed\" as the metric.
    Precomputed distance matrices must have 0 along the diagonal.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a ``scipy.spatial.distance`` metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Returns
-------
silhouette : array-like of shape (n_samples,)
    Silhouette Coefficients for each sample.

References
----------

.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the
   Interpretation and Validation of Cluster Analysis\". Computational
   and Applied Mathematics 20: 53-65.
   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_

.. [2] `Wikipedia entry on the Silhouette Coefficient
   <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_

Examples
--------
>>> from sklearn.metrics import silhouette_samples
>>> from sklearn.datasets import make_blobs
>>> from sklearn.cluster import KMeans
>>> X, y = make_blobs(n_samples=50, random_state=42)
>>> kmeans = KMeans(n_clusters=3, random_state=42)
>>> labels = kmeans.fit_predict(X)
>>> silhouette_samples(X, labels)
array([...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                           rdfs:comment """Compute the mean Silhouette Coefficient of all samples.

The Silhouette Coefficient is calculated using the mean intra-cluster
distance (``a``) and the mean nearest-cluster distance (``b``) for each
sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
b)``.  To clarify, ``b`` is the distance between a sample and the nearest
cluster that the sample is not a part of.
Note that Silhouette Coefficient is only defined if number of labels
is ``2 <= n_labels <= n_samples - 1``.

This function returns the mean Silhouette Coefficient over all samples.
To obtain the values for each sample, use :func:`silhouette_samples`.

The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters. Negative values generally indicate that a sample has
been assigned to the wrong cluster, as a different cluster is more similar.

Read more in the :ref:`User Guide <silhouette_coefficient>`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise
    An array of pairwise distances between samples, or a feature array.

labels : array-like of shape (n_samples,)
    Predicted labels for each sample.

metric : str or callable, default='euclidean'
    The metric to use when calculating distance between instances in a
    feature array. If metric is a string, it must be one of the options
    allowed by :func:`~sklearn.metrics.pairwise_distances`. If ``X`` is
    the distance array itself, use ``metric=\"precomputed\"``.

sample_size : int, default=None
    The size of the sample to use when computing the Silhouette Coefficient
    on a random subset of the data.
    If ``sample_size is None``, no sampling is used.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for selecting a subset of samples.
    Used when ``sample_size is not None``.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary <random_state>`.

**kwds : optional keyword parameters
    Any further parameters are passed directly to the distance function.
    If using a scipy.spatial.distance metric, the parameters are still
    metric dependent. See the scipy docs for usage examples.

Returns
-------
silhouette : float
    Mean Silhouette Coefficient for all samples.

References
----------

.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the
   Interpretation and Validation of Cluster Analysis\". Computational
   and Applied Mathematics 20: 53-65.
   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_

.. [2] `Wikipedia entry on the Silhouette Coefficient
       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_

Examples
--------
>>> from sklearn.datasets import make_blobs
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import silhouette_score
>>> X, y = make_blobs(random_state=42)
>>> kmeans = KMeans(n_clusters=2, random_state=42)
>>> silhouette_score(X, kmeans.fit_predict(X))
0.49...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                         rdfs:comment """Univariate imputer for completing missing values with simple strategies.

Replace missing values using a descriptive statistic (e.g. mean, median, or
most frequent) along each column, or using a constant value.

Read more in the :ref:`User Guide <impute>`.

.. versionadded:: 0.20
   `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`
   estimator which is now removed.

Parameters
----------
missing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan
    The placeholder for the missing values. All occurrences of
    `missing_values` will be imputed. For pandas' dataframes with
    nullable integer dtypes with missing values, `missing_values`
    can be set to either `np.nan` or `pd.NA`.

strategy : str, default='mean'
    The imputation strategy.

    - If \"mean\", then replace missing values using the mean along
      each column. Can only be used with numeric data.
    - If \"median\", then replace missing values using the median along
      each column. Can only be used with numeric data.
    - If \"most_frequent\", then replace missing using the most frequent
      value along each column. Can be used with strings or numeric data.
      If there is more than one such value, only the smallest is returned.
    - If \"constant\", then replace missing values with fill_value. Can be
      used with strings or numeric data.

    .. versionadded:: 0.20
       strategy=\"constant\" for fixed value imputation.

fill_value : str or numerical value, default=None
    When strategy == \"constant\", `fill_value` is used to replace all
    occurrences of missing_values. For string or object data types,
    `fill_value` must be a string.
    If `None`, `fill_value` will be 0 when imputing numerical
    data and \"missing_value\" for strings or object data types.

copy : bool, default=True
    If True, a copy of X will be created. If False, imputation will
    be done in-place whenever possible. Note that, in the following cases,
    a new copy will always be made, even if `copy=False`:

    - If `X` is not an array of floating values;
    - If `X` is encoded as a CSR matrix;
    - If `add_indicator=True`.

add_indicator : bool, default=False
    If True, a :class:`MissingIndicator` transform will stack onto output
    of the imputer's transform. This allows a predictive estimator
    to account for missingness despite imputation. If a feature has no
    missing values at fit/train time, the feature won't appear on
    the missing indicator even if there are missing values at
    transform/test time.

keep_empty_features : bool, default=False
    If True, features that consist exclusively of missing values when
    `fit` is called are returned in results when `transform` is called.
    The imputed value is always `0` except when `strategy=\"constant\"`
    in which case `fill_value` will be used instead.

    .. versionadded:: 1.2

Attributes
----------
statistics_ : array of shape (n_features,)
    The imputation fill value for each feature.
    Computing statistics can result in `np.nan` values.
    During :meth:`transform`, features corresponding to `np.nan`
    statistics will be discarded.

indicator_ : :class:`~sklearn.impute.MissingIndicator`
    Indicator used to add binary indicators for missing values.
    `None` if `add_indicator=False`.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
IterativeImputer : Multivariate imputer that estimates values to impute for
    each feature with missing values from all the others.
KNNImputer : Multivariate imputer that estimates missing features using
    nearest samples.

Notes
-----
Columns which only contained missing values at :meth:`fit` are discarded
upon :meth:`transform` if strategy is not `\"constant\"`.

In a prediction context, simple imputation usually performs poorly when
associated with a weak learner. However, with a powerful learner, it can
lead to as good or better performance than complex imputation such as
:class:`~sklearn.impute.IterativeImputer` or :class:`~sklearn.impute.KNNImputer`.

Examples
--------
>>> import numpy as np
>>> from sklearn.impute import SimpleImputer
>>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
>>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
SimpleImputer()
>>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
>>> print(imp_mean.transform(X))
[[ 7.   2.   3. ]
 [ 4.   3.5  6. ]
 [10.   3.5  9. ]]

For a more detailed example see
:ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> rdf:type owl:Class ;
                                                                                                   rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                       rdfs:comment """Sparse coding.

Finds a sparse representation of data against a fixed, precomputed
dictionary.

Each row of the result is the solution to a sparse coding problem.
The goal is to find a sparse array `code` such that::

    X ~= code * dictionary

Read more in the :ref:`User Guide <SparseCoder>`.

Parameters
----------
dictionary : ndarray of shape (n_components, n_features)
    The dictionary atoms used for sparse coding. Lines are assumed to be
    normalized to unit norm.

transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'
    Algorithm used to transform the data:

    - `'lars'`: uses the least angle regression method
      (`linear_model.lars_path`);
    - `'lasso_lars'`: uses Lars to compute the Lasso solution;
    - `'lasso_cd'`: uses the coordinate descent method to compute the
      Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if
      the estimated components are sparse;
    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse
      solution;
    - `'threshold'`: squashes to zero all coefficients less than alpha from
      the projection ``dictionary * X'``.

transform_n_nonzero_coefs : int, default=None
    Number of nonzero coefficients to target in each column of the
    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`
    and is overridden by `alpha` in the `omp` case. If `None`, then
    `transform_n_nonzero_coefs=int(n_features / 10)`.

transform_alpha : float, default=None
    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the
    penalty applied to the L1 norm.
    If `algorithm='threshold'`, `alpha` is the absolute value of the
    threshold below which coefficients will be squashed to zero.
    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of
    the reconstruction error targeted. In this case, it overrides
    `n_nonzero_coefs`.
    If `None`, default to 1.

split_sign : bool, default=False
    Whether to split the sparse feature vector into the concatenation of
    its negative part and its positive part. This can improve the
    performance of downstream classifiers.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

positive_code : bool, default=False
    Whether to enforce positivity when finding the code.

    .. versionadded:: 0.20

transform_max_iter : int, default=1000
    Maximum number of iterations to perform if `algorithm='lasso_cd'` or
    `lasso_lars`.

    .. versionadded:: 0.22

Attributes
----------
n_components_ : int
    Number of atoms.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
DictionaryLearning : Find a dictionary that sparsely encodes data.
MiniBatchDictionaryLearning : A faster, less accurate, version of the
    dictionary learning algorithm.
MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.
SparsePCA : Sparse Principal Components Analysis.
sparse_encode : Sparse coding where each row of the result is the solution
    to a sparse coding problem.

Examples
--------
>>> import numpy as np
>>> from sklearn.decomposition import SparseCoder
>>> X = np.array([[-1, -1, -1], [0, 0, 3]])
>>> dictionary = np.array(
...     [[0, 1, 0],
...      [-1, -1, 2],
...      [1, 1, 1],
...      [0, 1, 1],
...      [0, 2, 1]],
...    dtype=np.float64
... )
>>> coder = SparseCoder(
...     dictionary=dictionary, transform_algorithm='lasso_lars',
...     transform_alpha=1e-10,
... )
>>> coder.transform(X)
array([[ 0.,  0., -1.,  0.,  0.],
       [ 0.,  1.,  1.,  0.,  0.]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod> rdf:type owl:Class ;
                                                                                                     rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                     <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                     rdfs:comment """Sparse Principal Components Analysis (SparsePCA).

Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.

Read more in the :ref:`User Guide <SparsePCA>`.

Parameters
----------
n_components : int, default=None
    Number of sparse atoms to extract. If None, then ``n_components``
    is set to ``n_features``.

alpha : float, default=1
    Sparsity controlling parameter. Higher values lead to sparser
    components.

ridge_alpha : float, default=0.01
    Amount of ridge shrinkage to apply in order to improve
    conditioning when calling the transform method.

max_iter : int, default=1000
    Maximum number of iterations to perform.

tol : float, default=1e-8
    Tolerance for the stopping condition.

method : {'lars', 'cd'}, default='lars'
    Method to be used for optimization.
    lars: uses the least angle regression method to solve the lasso problem
    (linear_model.lars_path)
    cd: uses the coordinate descent method to compute the
    Lasso solution (linear_model.Lasso). Lars will be faster if
    the estimated components are sparse.

n_jobs : int, default=None
    Number of parallel jobs to run.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

U_init : ndarray of shape (n_samples, n_components), default=None
    Initial values for the loadings for warm restart scenarios. Only used
    if `U_init` and `V_init` are not None.

V_init : ndarray of shape (n_components, n_features), default=None
    Initial values for the components for warm restart scenarios. Only used
    if `U_init` and `V_init` are not None.

verbose : int or bool, default=False
    Controls the verbosity; the higher, the more messages. Defaults to 0.

random_state : int, RandomState instance or None, default=None
    Used during dictionary learning. Pass an int for reproducible results
    across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    Sparse components extracted from the data.

error_ : ndarray
    Vector of errors at each iteration.

n_components_ : int
    Estimated number of components.

    .. versionadded:: 0.23

n_iter_ : int
    Number of iterations run.

mean_ : ndarray of shape (n_features,)
    Per-feature empirical mean, estimated from the training set.
    Equal to ``X.mean(axis=0)``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PCA : Principal Component Analysis implementation.
MiniBatchSparsePCA : Mini batch variant of `SparsePCA` that is faster but less
    accurate.
DictionaryLearning : Generic dictionary learning problem using a sparse code.

Examples
--------
>>> import numpy as np
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.decomposition import SparsePCA
>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)
>>> transformer = SparsePCA(n_components=5, random_state=0)
>>> transformer.fit(X)
SparsePCA(...)
>>> X_transformed = transformer.transform(X)
>>> X_transformed.shape
(200, 5)
>>> # most values in the components_ are zero (sparsity)
>>> np.mean(transformer.components_ == 0)
0.9666...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                rdfs:comment """Spectral biclustering (Kluger, 2003).

Partitions rows and columns under the assumption that the data has
an underlying checkerboard structure. For instance, if there are
two row partitions and three column partitions, each row will
belong to three biclusters, and each column will belong to two
biclusters. The outer product of the corresponding row and column
label vectors gives this checkerboard structure.

Read more in the :ref:`User Guide <spectral_biclustering>`.

Parameters
----------
n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3
    The number of row and column clusters in the checkerboard
    structure.

method : {'bistochastic', 'scale', 'log'}, default='bistochastic'
    Method of normalizing and converting singular vectors into
    biclusters. May be one of 'scale', 'bistochastic', or 'log'.
    The authors recommend using 'log'. If the data is sparse,
    however, log normalization will not work, which is why the
    default is 'bistochastic'.

    .. warning::
       if `method='log'`, the data must not be sparse.

n_components : int, default=6
    Number of singular vectors to check.

n_best : int, default=3
    Number of best singular vectors to which to project the data
    for clustering.

svd_method : {'randomized', 'arpack'}, default='randomized'
    Selects the algorithm for finding singular vectors. May be
    'randomized' or 'arpack'. If 'randomized', uses
    :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster
    for large matrices. If 'arpack', uses
    `scipy.sparse.linalg.svds`, which is more accurate, but
    possibly slower in some cases.

n_svd_vecs : int, default=None
    Number of vectors to use in calculating the SVD. Corresponds
    to `ncv` when `svd_method=arpack` and `n_oversamples` when
    `svd_method` is 'randomized`.

mini_batch : bool, default=False
    Whether to use mini-batch k-means, which is faster but may get
    different results.

init : {'k-means++', 'random'} or ndarray of shape (n_clusters, n_features),             default='k-means++'
    Method for initialization of k-means algorithm; defaults to
    'k-means++'.

n_init : int, default=10
    Number of random initializations that are tried with the
    k-means algorithm.

    If mini-batch k-means is used, the best initialization is
    chosen and the algorithm runs once. Otherwise, the algorithm
    is run for each initialization and the best solution chosen.

random_state : int, RandomState instance, default=None
    Used for randomizing the singular value decomposition and the k-means
    initialization. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

Attributes
----------
rows_ : array-like of shape (n_row_clusters, n_rows)
    Results of the clustering. `rows[i, r]` is True if
    cluster `i` contains row `r`. Available only after calling ``fit``.

columns_ : array-like of shape (n_column_clusters, n_columns)
    Results of the clustering, like `rows`.

row_labels_ : array-like of shape (n_rows,)
    Row partition labels.

column_labels_ : array-like of shape (n_cols,)
    Column partition labels.

biclusters_ : tuple of two ndarrays
    The tuple contains the `rows_` and `columns_` arrays.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SpectralCoclustering : Spectral Co-Clustering algorithm (Dhillon, 2001).

References
----------

* :doi:`Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray
  data: coclustering genes and conditions.
  <10.1101/gr.648603>`

Examples
--------
>>> from sklearn.cluster import SpectralBiclustering
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)
>>> clustering.row_labels_
array([1, 1, 1, 0, 0, 0], dtype=int32)
>>> clustering.column_labels_
array([1, 0], dtype=int32)
>>> clustering
SpectralBiclustering(n_clusters=2, random_state=0)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """Apply clustering to a projection of the normalized Laplacian.

In practice Spectral Clustering is very useful when the structure of
the individual clusters is highly non-convex, or more generally when
a measure of the center and spread of the cluster is not a suitable
description of the complete cluster, such as when clusters are
nested circles on the 2D plane.

If the affinity matrix is the adjacency matrix of a graph, this method
can be used to find normalized graph cuts [1]_, [2]_.

When calling ``fit``, an affinity matrix is constructed using either
a kernel function such the Gaussian (aka RBF) kernel with Euclidean
distance ``d(X, X)``::

        np.exp(-gamma * d(X,X) ** 2)

or a k-nearest neighbors connectivity matrix.

Alternatively, a user-provided affinity matrix can be specified by
setting ``affinity='precomputed'``.

Read more in the :ref:`User Guide <spectral_clustering>`.

Parameters
----------
n_clusters : int, default=8
    The dimension of the projection subspace.

eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None
    The eigenvalue decomposition strategy to use. AMG requires pyamg
    to be installed. It can be faster on very large, sparse problems,
    but may also lead to instabilities. If None, then ``'arpack'`` is
    used. See [4]_ for more details regarding `'lobpcg'`.

n_components : int, default=None
    Number of eigenvectors to use for the spectral embedding. If None,
    defaults to `n_clusters`.

random_state : int, RandomState instance, default=None
    A pseudo random number generator used for the initialization
    of the lobpcg eigenvectors decomposition when `eigen_solver ==
    'amg'`, and for the K-Means initialization. Use an int to make
    the results deterministic across calls (See
    :term:`Glossary <random_state>`).

    .. note::
        When using `eigen_solver == 'amg'`,
        it is necessary to also fix the global numpy seed with
        `np.random.seed(int)` to get deterministic results. See
        https://github.com/pyamg/pyamg/issues/139 for further
        information.

n_init : int, default=10
    Number of time the k-means algorithm will be run with different
    centroid seeds. The final results will be the best output of n_init
    consecutive runs in terms of inertia. Only used if
    ``assign_labels='kmeans'``.

gamma : float, default=1.0
    Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.
    Ignored for ``affinity='nearest_neighbors'``.

affinity : str or callable, default='rbf'
    How to construct the affinity matrix.
     - 'nearest_neighbors': construct the affinity matrix by computing a
       graph of nearest neighbors.
     - 'rbf': construct the affinity matrix using a radial basis function
       (RBF) kernel.
     - 'precomputed': interpret ``X`` as a precomputed affinity matrix,
       where larger values indicate greater similarity between instances.
     - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph
       of precomputed distances, and construct a binary affinity matrix
       from the ``n_neighbors`` nearest neighbors of each instance.
     - one of the kernels supported by
       :func:`~sklearn.metrics.pairwise.pairwise_kernels`.

    Only kernels that produce similarity scores (non-negative values that
    increase with similarity) should be used. This property is not checked
    by the clustering algorithm.

n_neighbors : int, default=10
    Number of neighbors to use when constructing the affinity matrix using
    the nearest neighbors method. Ignored for ``affinity='rbf'``.

eigen_tol : float, default=\"auto\"
    Stopping criterion for eigen decomposition of the Laplacian matrix.
    If `eigen_tol=\"auto\"` then the passed tolerance will depend on the
    `eigen_solver`:

    - If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;
    - If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then
      `eigen_tol=None` which configures the underlying `lobpcg` solver to
      automatically resolve the value according to their heuristics. See,
      :func:`scipy.sparse.linalg.lobpcg` for details.

    Note that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`
    values of `tol<1e-5` may lead to convergence issues and should be
    avoided.

    .. versionadded:: 1.2
       Added 'auto' option.

assign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'
    The strategy for assigning labels in the embedding space. There are two
    ways to assign labels after the Laplacian embedding. k-means is a
    popular choice, but it can be sensitive to initialization.
    Discretization is another approach which is less sensitive to random
    initialization [3]_.
    The cluster_qr method [5]_ directly extract clusters from eigenvectors
    in spectral clustering. In contrast to k-means and discretization, cluster_qr
    has no tuning parameters and runs no iterations, yet may outperform
    k-means and discretization in terms of both quality and speed.

    .. versionchanged:: 1.1
       Added new labeling method 'cluster_qr'.

degree : float, default=3
    Degree of the polynomial kernel. Ignored by other kernels.

coef0 : float, default=1
    Zero coefficient for polynomial and sigmoid kernels.
    Ignored by other kernels.

kernel_params : dict of str to any, default=None
    Parameters (keyword arguments) and values for kernel passed as
    callable object. Ignored by other kernels.

n_jobs : int, default=None
    The number of parallel jobs to run when `affinity='nearest_neighbors'`
    or `affinity='precomputed_nearest_neighbors'`. The neighbors search
    will be done in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : bool, default=False
    Verbosity mode.

    .. versionadded:: 0.24

Attributes
----------
affinity_matrix_ : array-like of shape (n_samples, n_samples)
    Affinity matrix used for clustering. Available only after calling
    ``fit``.

labels_ : ndarray of shape (n_samples,)
    Labels of each point

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
sklearn.cluster.KMeans : K-Means clustering.
sklearn.cluster.DBSCAN : Density-Based Spatial Clustering of
    Applications with Noise.

Notes
-----
A distance matrix for which 0 indicates identical elements and high values
indicate very dissimilar elements can be transformed into an affinity /
similarity matrix that is well-suited for the algorithm by
applying the Gaussian (aka RBF, heat) kernel::

    np.exp(- dist_matrix ** 2 / (2. * delta ** 2))

where ``delta`` is a free parameter representing the width of the Gaussian
kernel.

An alternative is to take a symmetric version of the k-nearest neighbors
connectivity matrix of the points.

If the pyamg package is installed, it is used: this greatly
speeds up computation.

References
----------
.. [1] :doi:`Normalized cuts and image segmentation, 2000
       Jianbo Shi, Jitendra Malik
       <10.1109/34.868688>`

.. [2] :doi:`A Tutorial on Spectral Clustering, 2007
       Ulrike von Luxburg
       <10.1007/s11222-007-9033-z>`

.. [3] `Multiclass spectral clustering, 2003
       Stella X. Yu, Jianbo Shi
       <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_

.. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:
       Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001
       A. V. Knyazev
       SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.
       <10.1137/S1064827500366124>`

.. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019
       Anil Damle, Victor Minden, Lexing Ying
       <10.1093/imaiai/iay008>`

Examples
--------
>>> from sklearn.cluster import SpectralClustering
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = SpectralClustering(n_clusters=2,
...         assign_labels='discretize',
...         random_state=0).fit(X)
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])
>>> clustering
SpectralClustering(assign_labels='discretize', n_clusters=2,
    random_state=0)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                                rdfs:comment """Spectral Co-Clustering algorithm (Dhillon, 2001).

Clusters rows and columns of an array `X` to solve the relaxed
normalized cut of the bipartite graph created from `X` as follows:
the edge between row vertex `i` and column vertex `j` has weight
`X[i, j]`.

The resulting bicluster structure is block-diagonal, since each
row and each column belongs to exactly one bicluster.

Supports sparse matrices, as long as they are nonnegative.

Read more in the :ref:`User Guide <spectral_coclustering>`.

Parameters
----------
n_clusters : int, default=3
    The number of biclusters to find.

svd_method : {'randomized', 'arpack'}, default='randomized'
    Selects the algorithm for finding singular vectors. May be
    'randomized' or 'arpack'. If 'randomized', use
    :func:`sklearn.utils.extmath.randomized_svd`, which may be faster
    for large matrices. If 'arpack', use
    :func:`scipy.sparse.linalg.svds`, which is more accurate, but
    possibly slower in some cases.

n_svd_vecs : int, default=None
    Number of vectors to use in calculating the SVD. Corresponds
    to `ncv` when `svd_method=arpack` and `n_oversamples` when
    `svd_method` is 'randomized`.

mini_batch : bool, default=False
    Whether to use mini-batch k-means, which is faster but may get
    different results.

init : {'k-means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++'
    Method for initialization of k-means algorithm; defaults to
    'k-means++'.

n_init : int, default=10
    Number of random initializations that are tried with the
    k-means algorithm.

    If mini-batch k-means is used, the best initialization is
    chosen and the algorithm runs once. Otherwise, the algorithm
    is run for each initialization and the best solution chosen.

random_state : int, RandomState instance, default=None
    Used for randomizing the singular value decomposition and the k-means
    initialization. Use an int to make the randomness deterministic.
    See :term:`Glossary <random_state>`.

Attributes
----------
rows_ : array-like of shape (n_row_clusters, n_rows)
    Results of the clustering. `rows[i, r]` is True if
    cluster `i` contains row `r`. Available only after calling ``fit``.

columns_ : array-like of shape (n_column_clusters, n_columns)
    Results of the clustering, like `rows`.

row_labels_ : array-like of shape (n_rows,)
    The bicluster label of each row.

column_labels_ : array-like of shape (n_cols,)
    The bicluster label of each column.

biclusters_ : tuple of two ndarrays
    The tuple contains the `rows_` and `columns_` arrays.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SpectralBiclustering : Partitions rows and columns under the assumption
    that the data has an underlying checkerboard structure.

References
----------
* :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using
  bipartite spectral graph partitioning.
  <10.1145/502512.502550>`

Examples
--------
>>> from sklearn.cluster import SpectralCoclustering
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [1, 0],
...               [4, 7], [3, 5], [3, 6]])
>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)
>>> clustering.row_labels_ #doctest: +SKIP
array([0, 1, 1, 0, 0, 0], dtype=int32)
>>> clustering.column_labels_ #doctest: +SKIP
array([0, 0], dtype=int32)
>>> clustering
SpectralCoclustering(n_clusters=2, random_state=0)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                             rdfs:comment """Generate univariate B-spline bases for features.

Generate a new feature matrix consisting of
`n_splines=n_knots + degree - 1` (`n_knots - 1` for
`extrapolation=\"periodic\"`) spline basis functions
(B-splines) of polynomial order=`degree` for each feature.

In order to learn more about the SplineTransformer class go to:
:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`

Read more in the :ref:`User Guide <spline_transformer>`.

.. versionadded:: 1.0

Parameters
----------
n_knots : int, default=5
    Number of knots of the splines if `knots` equals one of
    {'uniform', 'quantile'}. Must be larger or equal 2. Ignored if `knots`
    is array-like.

degree : int, default=3
    The polynomial degree of the spline basis. Must be a non-negative
    integer.

knots : {'uniform', 'quantile'} or array-like of shape         (n_knots, n_features), default='uniform'
    Set knot positions such that first knot <= features <= last knot.

    - If 'uniform', `n_knots` number of knots are distributed uniformly
      from min to max values of the features.
    - If 'quantile', they are distributed uniformly along the quantiles of
      the features.
    - If an array-like is given, it directly specifies the sorted knot
      positions including the boundary knots. Note that, internally,
      `degree` number of knots are added before the first knot, the same
      after the last knot.

extrapolation : {'error', 'constant', 'linear', 'continue', 'periodic'},         default='constant'
    If 'error', values outside the min and max values of the training
    features raises a `ValueError`. If 'constant', the value of the
    splines at minimum and maximum value of the features is used as
    constant extrapolation. If 'linear', a linear extrapolation is used.
    If 'continue', the splines are extrapolated as is, i.e. option
    `extrapolate=True` in :class:`scipy.interpolate.BSpline`. If
    'periodic', periodic splines with a periodicity equal to the distance
    between the first and last knot are used. Periodic splines enforce
    equal function values and derivatives at the first and last knot.
    For example, this makes it possible to avoid introducing an arbitrary
    jump between Dec 31st and Jan 1st in spline features derived from a
    naturally periodic \"day-of-year\" input feature. In this case it is
    recommended to manually set the knot values to control the period.

include_bias : bool, default=True
    If False, then the last spline element inside the data range
    of a feature is dropped. As B-splines sum to one over the spline basis
    functions for each data point, they implicitly include a bias term,
    i.e. a column of ones. It acts as an intercept term in a linear models.

order : {'C', 'F'}, default='C'
    Order of output array in the dense case. `'F'` order is faster to compute, but
    may slow down subsequent estimators.

sparse_output : bool, default=False
    Will return sparse CSR matrix if set True else will return an array. This
    option is only available with `scipy>=1.8`.

    .. versionadded:: 1.2

Attributes
----------
bsplines_ : list of shape (n_features,)
    List of BSplines objects, one for each feature.

n_features_in_ : int
    The total number of input features.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_features_out_ : int
    The total number of output features, which is computed as
    `n_features * n_splines`, where `n_splines` is
    the number of bases elements of the B-splines,
    `n_knots + degree - 1` for non-periodic splines and
    `n_knots - 1` for periodic ones.
    If `include_bias=False`, then it is only
    `n_features * (n_splines - 1)`.

See Also
--------
KBinsDiscretizer : Transformer that bins continuous data into intervals.

PolynomialFeatures : Transformer that generates polynomial and interaction
    features.

Notes
-----
High degrees and a high number of knots can cause overfitting.

See :ref:`examples/linear_model/plot_polynomial_interpolation.py
<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.preprocessing import SplineTransformer
>>> X = np.arange(6).reshape(6, 1)
>>> spline = SplineTransformer(degree=2, n_knots=3)
>>> spline.fit_transform(X)
array([[0.5 , 0.5 , 0.  , 0.  ],
       [0.18, 0.74, 0.08, 0.  ],
       [0.02, 0.66, 0.32, 0.  ],
       [0.  , 0.32, 0.66, 0.02],
       [0.  , 0.08, 0.74, 0.18],
       [0.  , 0.  , 0.5 , 0.5 ]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                       rdfs:comment "" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod> rdf:type owl:Class ;
                                                                                                              rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                              <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                              rdfs:comment """Stack of estimators with a final classifier.

Stacked generalization consists in stacking the output of individual
estimator and use a classifier to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.

Note that `estimators_` are fitted on the full `X` while `final_estimator_`
is trained using cross-validated predictions of the base estimators using
`cross_val_predict`.

Read more in the :ref:`User Guide <stacking>`.

.. versionadded:: 0.22

Parameters
----------
estimators : list of (str, estimator)
    Base estimators which will be stacked together. Each element of the
    list is defined as a tuple of string (i.e. name) and an estimator
    instance. An estimator can be set to 'drop' using `set_params`.

    The type of estimator is generally expected to be a classifier.
    However, one can pass a regressor for some use case (e.g. ordinal
    regression).

final_estimator : estimator, default=None
    A classifier which will be used to combine the base estimators.
    The default classifier is a
    :class:`~sklearn.linear_model.LogisticRegression`.

cv : int, cross-validation generator, iterable, or \"prefit\", default=None
    Determines the cross-validation splitting strategy used in
    `cross_val_predict` to train `final_estimator`. Possible inputs for
    cv are:

    * None, to use the default 5-fold cross validation,
    * integer, to specify the number of folds in a (Stratified) KFold,
    * An object to be used as a cross-validation generator,
    * An iterable yielding train, test splits,
    * `\"prefit\"` to assume the `estimators` are prefit. In this case, the
      estimators will not be refitted.

    For integer/None inputs, if the estimator is a classifier and y is
    either binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used.
    In all other cases, :class:`~sklearn.model_selection.KFold` is used.
    These splitters are instantiated with `shuffle=False` so the splits
    will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    If \"prefit\" is passed, it is assumed that all `estimators` have
    been fitted already. The `final_estimator_` is trained on the `estimators`
    predictions on the full training set and are **not** cross validated
    predictions. Please note that if the models have been trained on the same
    data to train the stacking model, there is a very high risk of overfitting.

    .. versionadded:: 1.1
        The 'prefit' option was added in 1.1

    .. note::
       A larger number of split will provide no benefits if the number
       of training samples is large enough. Indeed, the training time
       will increase. ``cv`` is not used for model evaluation but for
       prediction.

stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'
    Methods called for each base estimator. It can be:

    * if 'auto', it will try to invoke, for each estimator,
      `'predict_proba'`, `'decision_function'` or `'predict'` in that
      order.
    * otherwise, one of `'predict_proba'`, `'decision_function'` or
      `'predict'`. If the method is not implemented by the estimator, it
      will raise an error.

n_jobs : int, default=None
    The number of jobs to run in parallel all `estimators` `fit`.
    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means
    using all processors. See Glossary for more details.

passthrough : bool, default=False
    When False, only the predictions of estimators will be used as
    training data for `final_estimator`. When True, the
    `final_estimator` is trained on the predictions as well as the
    original training data.

verbose : int, default=0
    Verbosity level.

Attributes
----------
classes_ : ndarray of shape (n_classes,) or list of ndarray if `y`         is of type `\"multilabel-indicator\"`.
    Class labels.

estimators_ : list of estimators
    The elements of the `estimators` parameter, having been fitted on the
    training data. If an estimator has been set to `'drop'`, it
    will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`
    is set to `estimators` and is not fitted again.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying classifier exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

final_estimator_ : estimator
    The classifier which predicts given the output of `estimators_`.

stack_method_ : list of str
    The method used by each base estimator.

See Also
--------
StackingRegressor : Stack of estimators with a final regressor.

Notes
-----
When `predict_proba` is used by each estimator (i.e. most of the time for
`stack_method='auto'` or specifically for `stack_method='predict_proba'`),
The first column predicted by each estimator will be dropped in the case
of a binary classification problem. Indeed, both feature will be perfectly
collinear.

In some cases (e.g. ordinal regression), one can pass regressors as the
first layer of the :class:`StackingClassifier`. However, note that `y` will
be internally encoded in a numerically increasing order or lexicographic
order. If this ordering is not adequate, one should manually numerically
encode the classes in the desired order.

References
----------
.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2
   (1992): 241-259.

Examples
--------
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.svm import LinearSVC
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.ensemble import StackingClassifier
>>> X, y = load_iris(return_X_y=True)
>>> estimators = [
...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
...     ('svr', make_pipeline(StandardScaler(),
...                           LinearSVC(dual=\"auto\", random_state=42)))
... ]
>>> clf = StackingClassifier(
...     estimators=estimators, final_estimator=LogisticRegression()
... )
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, stratify=y, random_state=42
... )
>>> clf.fit(X_train, y_train).score(X_test, y_test)
0.9...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                             rdfs:comment """Stack of estimators with a final regressor.

Stacked generalization consists in stacking the output of individual
estimator and use a regressor to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.

Note that `estimators_` are fitted on the full `X` while `final_estimator_`
is trained using cross-validated predictions of the base estimators using
`cross_val_predict`.

Read more in the :ref:`User Guide <stacking>`.

.. versionadded:: 0.22

Parameters
----------
estimators : list of (str, estimator)
    Base estimators which will be stacked together. Each element of the
    list is defined as a tuple of string (i.e. name) and an estimator
    instance. An estimator can be set to 'drop' using `set_params`.

final_estimator : estimator, default=None
    A regressor which will be used to combine the base estimators.
    The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.

cv : int, cross-validation generator, iterable, or \"prefit\", default=None
    Determines the cross-validation splitting strategy used in
    `cross_val_predict` to train `final_estimator`. Possible inputs for
    cv are:

    * None, to use the default 5-fold cross validation,
    * integer, to specify the number of folds in a (Stratified) KFold,
    * An object to be used as a cross-validation generator,
    * An iterable yielding train, test splits.
    * \"prefit\" to assume the `estimators` are prefit, and skip cross validation

    For integer/None inputs, if the estimator is a classifier and y is
    either binary or multiclass,
    :class:`~sklearn.model_selection.StratifiedKFold` is used.
    In all other cases, :class:`~sklearn.model_selection.KFold` is used.
    These splitters are instantiated with `shuffle=False` so the splits
    will be the same across calls.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    If \"prefit\" is passed, it is assumed that all `estimators` have
    been fitted already. The `final_estimator_` is trained on the `estimators`
    predictions on the full training set and are **not** cross validated
    predictions. Please note that if the models have been trained on the same
    data to train the stacking model, there is a very high risk of overfitting.

    .. versionadded:: 1.1
        The 'prefit' option was added in 1.1

    .. note::
       A larger number of split will provide no benefits if the number
       of training samples is large enough. Indeed, the training time
       will increase. ``cv`` is not used for model evaluation but for
       prediction.

n_jobs : int, default=None
    The number of jobs to run in parallel for `fit` of all `estimators`.
    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means
    using all processors. See Glossary for more details.

passthrough : bool, default=False
    When False, only the predictions of estimators will be used as
    training data for `final_estimator`. When True, the
    `final_estimator` is trained on the predictions as well as the
    original training data.

verbose : int, default=0
    Verbosity level.

Attributes
----------
estimators_ : list of estimator
    The elements of the `estimators` parameter, having been fitted on the
    training data. If an estimator has been set to `'drop'`, it
    will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`
    is set to `estimators` and is not fitted again.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying regressor exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

final_estimator_ : estimator
    The regressor to stacked the base estimators fitted.

stack_method_ : list of str
    The method used by each base estimator.

See Also
--------
StackingClassifier : Stack of estimators with a final classifier.

References
----------
.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2
   (1992): 241-259.

Examples
--------
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.linear_model import RidgeCV
>>> from sklearn.svm import LinearSVR
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.ensemble import StackingRegressor
>>> X, y = load_diabetes(return_X_y=True)
>>> estimators = [
...     ('lr', RidgeCV()),
...     ('svr', LinearSVR(dual=\"auto\", random_state=42))
... ]
>>> reg = StackingRegressor(
...     estimators=estimators,
...     final_estimator=RandomForestRegressor(n_estimators=10,
...                                           random_state=42)
... )
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=42
... )
>>> reg.fit(X_train, y_train).score(X_test, y_test)
0.3...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                          rdfs:comment """Standardize features by removing the mean and scaling to unit variance.

The standard score of a sample `x` is calculated as:

    z = (x - u) / s

where `u` is the mean of the training samples or zero if `with_mean=False`,
and `s` is the standard deviation of the training samples or one if
`with_std=False`.

Centering and scaling happen independently on each feature by computing
the relevant statistics on the samples in the training set. Mean and
standard deviation are then stored to be used on later data using
:meth:`transform`.

Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the
individual features do not more or less look like standard normally
distributed data (e.g. Gaussian with 0 mean and unit variance).

For instance many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the L1 and L2 regularizers of linear models) assume that
all features are centered around 0 and have variance in the same
order. If a feature has a variance that is orders of magnitude larger
than others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.

`StandardScaler` is sensitive to outliers, and the features may scale
differently from each other in the presence of outliers. For an example
visualization, refer to :ref:`Compare StandardScaler with other scalers
<plot_all_scaling_standard_scaler_section>`.

This scaler can also be applied to sparse CSR or CSC matrices by passing
`with_mean=False` to avoid breaking the sparsity structure of the data.

Read more in the :ref:`User Guide <preprocessing_scaler>`.

Parameters
----------
copy : bool, default=True
    If False, try to avoid a copy and do inplace scaling instead.
    This is not guaranteed to always work inplace; e.g. if the data is
    not a NumPy array or scipy.sparse CSR matrix, a copy may still be
    returned.

with_mean : bool, default=True
    If True, center the data before scaling.
    This does not work (and will raise an exception) when attempted on
    sparse matrices, because centering them entails building a dense
    matrix which in common use cases is likely to be too large to fit in
    memory.

with_std : bool, default=True
    If True, scale the data to unit variance (or equivalently,
    unit standard deviation).

Attributes
----------
scale_ : ndarray of shape (n_features,) or None
    Per feature relative scaling of the data to achieve zero mean and unit
    variance. Generally this is calculated using `np.sqrt(var_)`. If a
    variance is zero, we can't achieve unit variance, and the data is left
    as-is, giving a scaling factor of 1. `scale_` is equal to `None`
    when `with_std=False`.

    .. versionadded:: 0.17
       *scale_*

mean_ : ndarray of shape (n_features,) or None
    The mean value for each feature in the training set.
    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.

var_ : ndarray of shape (n_features,) or None
    The variance for each feature in the training set. Used to compute
    `scale_`. Equal to ``None`` when ``with_mean=False`` and
    ``with_std=False``.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

n_samples_seen_ : int or ndarray of shape (n_features,)
    The number of samples processed by the estimator for each feature.
    If there are no missing samples, the ``n_samples_seen`` will be an
    integer, otherwise it will be an array of dtype int. If
    `sample_weights` are used it will be a float (if no missing data)
    or an array of dtype float that sums the weights seen so far.
    Will be reset on new calls to fit, but increments across
    ``partial_fit`` calls.

See Also
--------
scale : Equivalent function without the estimator API.

:class:`~sklearn.decomposition.PCA` : Further removes the linear
    correlation across features with 'whiten=True'.

Notes
-----
NaNs are treated as missing values: disregarded in fit, and maintained in
transform.

We use a biased estimator for the standard deviation, equivalent to
`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to
affect model performance.

Examples
--------
>>> from sklearn.preprocessing import StandardScaler
>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]
>>> scaler = StandardScaler()
>>> print(scaler.fit(data))
StandardScaler()
>>> print(scaler.mean_)
[0.5 0.5]
>>> print(scaler.transform(data))
[[-1. -1.]
 [-1. -1.]
 [ 1.  1.]
 [ 1.  1.]]
>>> print(scaler.transform([[2, 2]]))
[[3. 3.]]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod> rdf:type owl:Class ;
                                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                                <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                                rdfs:comment """Stratified K-Fold iterator variant with non-overlapping groups.

This cross-validation object is a variation of StratifiedKFold attempts to
return stratified folds with non-overlapping groups. The folds are made by
preserving the percentage of samples for each class.

Each group will appear exactly once in the test set across all folds (the
number of distinct groups has to be at least equal to the number of folds).

The difference between :class:`~sklearn.model_selection.GroupKFold`
and :class:`~sklearn.model_selection.StratifiedGroupKFold` is that
the former attempts to create balanced folds such that the number of
distinct groups is approximately the same in each fold, whereas
StratifiedGroupKFold attempts to create folds which preserve the
percentage of samples for each class as much as possible given the
constraint of non-overlapping groups between splits.

Read more in the :ref:`User Guide <cross_validation>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=5
    Number of folds. Must be at least 2.

shuffle : bool, default=False
    Whether to shuffle each class's samples before splitting into batches.
    Note that the samples within each split will not be shuffled.
    This implementation can only shuffle groups that have approximately the
    same y distribution, no global shuffle will be performed.

random_state : int or RandomState instance, default=None
    When `shuffle` is True, `random_state` affects the ordering of the
    indices, which controls the randomness of each fold for each class.
    Otherwise, leave `random_state` as `None`.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import StratifiedGroupKFold
>>> X = np.ones((17, 2))
>>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])
>>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])
>>> sgkf = StratifiedGroupKFold(n_splits=3)
>>> sgkf.get_n_splits(X, y)
3
>>> print(sgkf)
StratifiedGroupKFold(n_splits=3, random_state=None, shuffle=False)
>>> for i, (train_index, test_index) in enumerate(sgkf.split(X, y, groups)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"         group={groups[train_index]}\")
...     print(f\"  Test:  index={test_index}\")
...     print(f\"         group={groups[test_index]}\")
Fold 0:
  Train: index=[ 0  1  2  3  7  8  9 10 11 15 16]
         group=[1 1 2 2 4 5 5 5 5 8 8]
  Test:  index=[ 4  5  6 12 13 14]
         group=[3 3 3 6 6 7]
Fold 1:
  Train: index=[ 4  5  6  7  8  9 10 11 12 13 14]
         group=[3 3 3 4 5 5 5 5 6 6 7]
  Test:  index=[ 0  1  2  3 15 16]
         group=[1 1 2 2 8 8]
Fold 2:
  Train: index=[ 0  1  2  3  4  5  6 12 13 14 15 16]
         group=[1 1 2 2 3 3 3 6 6 7 8 8]
  Test:  index=[ 7  8  9 10 11]
         group=[4 5 5 5 5]

Notes
-----
The implementation is designed to:

* Mimic the behavior of StratifiedKFold as much as possible for trivial
  groups (e.g. when each group contains only one sample).
* Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to
  ``y = [1, 0]`` should not change the indices generated.
* Stratify based on samples as much as possible while keeping
  non-overlapping groups constraint. That means that in some cases when
  there is a small number of groups containing a large number of samples
  the stratification will not be possible and the behavior will be close
  to GroupKFold.

See also
--------
StratifiedKFold: Takes class information into account to build folds which
    retain class distributions (for binary or multiclass classification
    tasks).

GroupKFold: K-fold iterator variant with non-overlapping groups.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                           rdfs:comment """Stratified K-Fold cross-validator.

Provides train/test indices to split data in train/test sets.

This cross-validation object is a variation of KFold that returns
stratified folds. The folds are made by preserving the percentage of
samples for each class.

Read more in the :ref:`User Guide <stratified_k_fold>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=5
    Number of folds. Must be at least 2.

    .. versionchanged:: 0.22
        ``n_splits`` default value changed from 3 to 5.

shuffle : bool, default=False
    Whether to shuffle each class's samples before splitting into batches.
    Note that the samples within each split will not be shuffled.

random_state : int, RandomState instance or None, default=None
    When `shuffle` is True, `random_state` affects the ordering of the
    indices, which controls the randomness of each fold for each class.
    Otherwise, leave `random_state` as `None`.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import StratifiedKFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> skf = StratifiedKFold(n_splits=2)
>>> skf.get_n_splits(X, y)
2
>>> print(skf)
StratifiedKFold(n_splits=2, random_state=None, shuffle=False)
>>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[1 3]
  Test:  index=[0 2]
Fold 1:
  Train: index=[0 2]
  Test:  index=[1 3]

Notes
-----
The implementation is designed to:

* Generate test sets such that all contain the same distribution of
  classes, or as close as possible.
* Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to
  ``y = [1, 0]`` should not change the indices generated.
* Preserve order dependencies in the dataset ordering, when
  ``shuffle=False``: all samples from class k in some test set were
  contiguous in y, or separated in y by samples from classes other than k.
* Generate test sets where the smallest and largest differ by at most one
  sample.

.. versionchanged:: 0.22
    The previous implementation did not follow the last constraint.

See Also
--------
RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod> rdf:type owl:Class ;
                                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                                  rdfs:comment """Stratified ShuffleSplit cross-validator.

Provides train/test indices to split data in train/test sets.

This cross-validation object is a merge of StratifiedKFold and
ShuffleSplit, which returns stratified randomized folds. The folds
are made by preserving the percentage of samples for each class.

Note: like the ShuffleSplit strategy, stratified random splits
do not guarantee that all folds will be different, although this is
still very likely for sizeable datasets.

Read more in the :ref:`User Guide <stratified_shuffle_split>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

Parameters
----------
n_splits : int, default=10
    Number of re-shuffling & splitting iterations.

test_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the proportion
    of the dataset to include in the test split. If int, represents the
    absolute number of test samples. If None, the value is set to the
    complement of the train size. If ``train_size`` is also None, it will
    be set to 0.1.

train_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the
    proportion of the dataset to include in the train split. If
    int, represents the absolute number of train samples. If None,
    the value is automatically set to the complement of the test size.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of the training and testing indices produced.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import StratifiedShuffleSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 0, 1, 1, 1])
>>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)
>>> sss.get_n_splits(X, y)
5
>>> print(sss)
StratifiedShuffleSplit(n_splits=5, random_state=0, ...)
>>> for i, (train_index, test_index) in enumerate(sss.split(X, y)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[5 2 3]
  Test:  index=[4 1 0]
Fold 1:
  Train: index=[5 1 4]
  Test:  index=[0 2 3]
Fold 2:
  Train: index=[5 0 2]
  Test:  index=[4 3 1]
Fold 3:
  Train: index=[4 1 0]
  Test:  index=[2 3 5]
Fold 4:
  Train: index=[0 5 1]
  Test:  index=[3 4 2]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SupportVectorRegression
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SupportVectorRegression> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule> rdf:type owl:Class ;
                                                                                               rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule> ;
                                                                                                         rdfs:comment """Target Encoder for regression and classification targets.

Each category is encoded based on a shrunk estimate of the average target
values for observations belonging to the category. The encoding scheme mixes
the global target mean with the target mean conditioned on the value of the
category (see [MIC]_).

When the target type is \"multiclass\", encodings are based
on the conditional probability estimate for each class. The target is first
binarized using the \"one-vs-all\" scheme via
:class:`~sklearn.preprocessing.LabelBinarizer`, then the average target
value for each class and each category is used for encoding, resulting in
`n_features` * `n_classes` encoded output features.

:class:`TargetEncoder` considers missing values, such as `np.nan` or `None`,
as another category and encodes them like any other category. Categories
that are not seen during :meth:`fit` are encoded with the target mean, i.e.
`target_mean_`.

For a demo on the importance of the `TargetEncoder` internal cross-fitting,
see
:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder_cross_val.py`.
For a comparison of different encoders, refer to
:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`. Read
more in the :ref:`User Guide <target_encoder>`.

.. note::
    `fit(X, y).transform(X)` does not equal `fit_transform(X, y)` because a
    :term:`cross fitting` scheme is used in `fit_transform` for encoding.
    See the :ref:`User Guide <target_encoder>` for details.

.. versionadded:: 1.3

Parameters
----------
categories : \"auto\" or list of shape (n_features,) of array-like, default=\"auto\"
    Categories (unique values) per feature:

    - `\"auto\"` : Determine categories automatically from the training data.
    - list : `categories[i]` holds the categories expected in the i-th column. The
      passed categories should not mix strings and numeric values within a single
      feature, and should be sorted in case of numeric values.

    The used categories are stored in the `categories_` fitted attribute.

target_type : {\"auto\", \"continuous\", \"binary\", \"multiclass\"}, default=\"auto\"
    Type of target.

    - `\"auto\"` : Type of target is inferred with
      :func:`~sklearn.utils.multiclass.type_of_target`.
    - `\"continuous\"` : Continuous target
    - `\"binary\"` : Binary target
    - `\"multiclass\"` : Multiclass target

    .. note::
        The type of target inferred with `\"auto\"` may not be the desired target
        type used for modeling. For example, if the target consisted of integers
        between 0 and 100, then :func:`~sklearn.utils.multiclass.type_of_target`
        will infer the target as `\"multiclass\"`. In this case, setting
        `target_type=\"continuous\"` will specify the target as a regression
        problem. The `target_type_` attribute gives the target type used by the
        encoder.

    .. versionchanged:: 1.4
       Added the option 'multiclass'.

smooth : \"auto\" or float, default=\"auto\"
    The amount of mixing of the target mean conditioned on the value of the
    category with the global target mean. A larger `smooth` value will put
    more weight on the global target mean.
    If `\"auto\"`, then `smooth` is set to an empirical Bayes estimate.

cv : int, default=5
    Determines the number of folds in the :term:`cross fitting` strategy used in
    :meth:`fit_transform`. For classification targets, `StratifiedKFold` is used
    and for continuous targets, `KFold` is used.

shuffle : bool, default=True
    Whether to shuffle the data in :meth:`fit_transform` before splitting into
    folds. Note that the samples within each split will not be shuffled.

random_state : int, RandomState instance or None, default=None
    When `shuffle` is True, `random_state` affects the ordering of the
    indices, which controls the randomness of each fold. Otherwise, this
    parameter has no effect.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

Attributes
----------
encodings_ : list of shape (n_features,) or (n_features * n_classes) of                     ndarray
    Encodings learnt on all of `X`.
    For feature `i`, `encodings_[i]` are the encodings matching the
    categories listed in `categories_[i]`. When `target_type_` is
    \"multiclass\", the encoding for feature `i` and class `j` is stored in
    `encodings_[j + (i * len(classes_))]`. E.g., for 2 features (f) and
    3 classes (c), encodings are ordered:
    f0_c0, f0_c1, f0_c2, f1_c0, f1_c1, f1_c2,

categories_ : list of shape (n_features,) of ndarray
    The categories of each input feature determined during fitting or
    specified in `categories`
    (in order of the features in `X` and corresponding with the output
    of :meth:`transform`).

target_type_ : str
    Type of target.

target_mean_ : float
    The overall mean of the target. This value is only used in :meth:`transform`
    to encode categories.

n_features_in_ : int
    Number of features seen during :term:`fit`.

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

classes_ : ndarray or None
    If `target_type_` is 'binary' or 'multiclass', holds the label for each class,
    otherwise `None`.

See Also
--------
OrdinalEncoder : Performs an ordinal (integer) encoding of the categorical features.
    Contrary to TargetEncoder, this encoding is not supervised. Treating the
    resulting encoding as a numerical features therefore lead arbitrarily
    ordered values and therefore typically lead to lower predictive performance
    when used as preprocessing for a classifier or regressor.
OneHotEncoder : Performs a one-hot encoding of categorical features. This
    unsupervised encoding is better suited for low cardinality categorical
    variables as it generate one new feature per unique category.

References
----------
.. [MIC] :doi:`Micci-Barreca, Daniele. \"A preprocessing scheme for high-cardinality
   categorical attributes in classification and prediction problems\"
   SIGKDD Explor. Newsl. 3, 1 (July 2001), 2732. <10.1145/507533.507538>`

Examples
--------
With `smooth=\"auto\"`, the smoothing parameter is set to an empirical Bayes estimate:

>>> import numpy as np
>>> from sklearn.preprocessing import TargetEncoder
>>> X = np.array([[\"dog\"] * 20 + [\"cat\"] * 30 + [\"snake\"] * 38], dtype=object).T
>>> y = [90.3] * 5 + [80.1] * 15 + [20.4] * 5 + [20.1] * 25 + [21.2] * 8 + [49] * 30
>>> enc_auto = TargetEncoder(smooth=\"auto\")
>>> X_trans = enc_auto.fit_transform(X, y)

>>> # A high `smooth` parameter puts more weight on global mean on the categorical
>>> # encodings:
>>> enc_high_smooth = TargetEncoder(smooth=5000.0).fit(X, y)
>>> enc_high_smooth.target_mean_
44...
>>> enc_high_smooth.encodings_
[array([44..., 44..., 44...])]

>>> # On the other hand, a low `smooth` parameter puts more weight on target
>>> # conditioned on the value of the categorical:
>>> enc_low_smooth = TargetEncoder(smooth=1.0).fit(X, y)
>>> enc_low_smooth.encodings_
[array([20..., 80..., 43...])]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                             rdfs:comment """Theil-Sen Estimator: robust multivariate regression model.

The algorithm calculates least square solutions on subsets with size
n_subsamples of the samples in X. Any value of n_subsamples between the
number of features and samples leads to an estimator with a compromise
between robustness and efficiency. Since the number of least square
solutions is \"n_samples choose n_subsamples\", it can be extremely large
and can therefore be limited with max_subpopulation. If this limit is
reached, the subsets are chosen randomly. In a final step, the spatial
median (or L1 median) is calculated of all least square solutions.

Read more in the :ref:`User Guide <theil_sen_regression>`.

Parameters
----------
fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations.

copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.

max_subpopulation : int, default=1e4
    Instead of computing with a set of cardinality 'n choose k', where n is
    the number of samples and k is the number of subsamples (at least
    number of features), consider only a stochastic subpopulation of a
    given maximal size if 'n choose k' is larger than max_subpopulation.
    For other than small problem sizes this parameter will determine
    memory usage and runtime if n_subsamples is not changed. Note that the
    data type should be int but floats such as 1e4 can be accepted too.

n_subsamples : int, default=None
    Number of samples to calculate the parameters. This is at least the
    number of features (plus 1 if fit_intercept=True) and the number of
    samples as a maximum. A lower number leads to a higher breakdown
    point and a low efficiency while a high number leads to a low
    breakdown point and a high efficiency. If None, take the
    minimum number of subsamples leading to maximal robustness.
    If n_subsamples is set to n_samples, Theil-Sen is identical to least
    squares.

max_iter : int, default=300
    Maximum number of iterations for the calculation of spatial median.

tol : float, default=1e-3
    Tolerance when calculating spatial median.

random_state : int, RandomState instance or None, default=None
    A random number generator instance to define the state of the random
    permutations generator. Pass an int for reproducible output across
    multiple function calls.
    See :term:`Glossary <random_state>`.

n_jobs : int, default=None
    Number of CPUs to use during the cross validation.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : bool, default=False
    Verbose mode when fitting the model.

Attributes
----------
coef_ : ndarray of shape (n_features,)
    Coefficients of the regression model (median of distribution).

intercept_ : float
    Estimated intercept of regression model.

breakdown_ : float
    Approximated breakdown point.

n_iter_ : int
    Number of iterations needed for the spatial median.

n_subpopulation_ : int
    Number of combinations taken into account from 'n choose k', where n is
    the number of samples and k is the number of subsamples.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
HuberRegressor : Linear regression model that is robust to outliers.
RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.
SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.

References
----------
- Theil-Sen Estimators in a Multiple Linear Regression Model, 2009
  Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang
  http://home.olemiss.edu/~xdang/papers/MTSE.pdf

Examples
--------
>>> from sklearn.linear_model import TheilSenRegressor
>>> from sklearn.datasets import make_regression
>>> X, y = make_regression(
...     n_samples=200, n_features=2, noise=4.0, random_state=0)
>>> reg = TheilSenRegressor(random_state=0).fit(X, y)
>>> reg.score(X, y)
0.9884...
>>> reg.predict(X[:1,])
array([-31.5871...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                           rdfs:comment """Time Series cross-validator.

Provides train/test indices to split time series data samples
that are observed at fixed time intervals, in train/test sets.
In each split, test indices must be higher than before, and thus shuffling
in cross validator is inappropriate.

This cross-validation object is a variation of :class:`KFold`.
In the kth split, it returns first k folds as train set and the
(k+1)th fold as test set.

Note that unlike standard cross-validation methods, successive
training sets are supersets of those that come before them.

Read more in the :ref:`User Guide <time_series_split>`.

For visualisation of cross-validation behaviour and
comparison between common scikit-learn split methods
refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`

.. versionadded:: 0.18

Parameters
----------
n_splits : int, default=5
    Number of splits. Must be at least 2.

    .. versionchanged:: 0.22
        ``n_splits`` default value changed from 3 to 5.

max_train_size : int, default=None
    Maximum size for a single training set.

test_size : int, default=None
    Used to limit the size of the test set. Defaults to
    ``n_samples // (n_splits + 1)``, which is the maximum allowed value
    with ``gap=0``.

    .. versionadded:: 0.24

gap : int, default=0
    Number of samples to exclude from the end of each train set before
    the test set.

    .. versionadded:: 0.24

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import TimeSeriesSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> tscv = TimeSeriesSplit()
>>> print(tscv)
TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)
>>> for i, (train_index, test_index) in enumerate(tscv.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[0]
  Test:  index=[1]
Fold 1:
  Train: index=[0 1]
  Test:  index=[2]
Fold 2:
  Train: index=[0 1 2]
  Test:  index=[3]
Fold 3:
  Train: index=[0 1 2 3]
  Test:  index=[4]
Fold 4:
  Train: index=[0 1 2 3 4]
  Test:  index=[5]
>>> # Fix test_size to 2 with 12 samples
>>> X = np.random.randn(12, 2)
>>> y = np.random.randint(0, 2, 12)
>>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)
>>> for i, (train_index, test_index) in enumerate(tscv.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[0 1 2 3 4 5]
  Test:  index=[6 7]
Fold 1:
  Train: index=[0 1 2 3 4 5 6 7]
  Test:  index=[8 9]
Fold 2:
  Train: index=[0 1 2 3 4 5 6 7 8 9]
  Test:  index=[10 11]
>>> # Add in a 2 period gap
>>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)
>>> for i, (train_index, test_index) in enumerate(tscv.split(X)):
...     print(f\"Fold {i}:\")
...     print(f\"  Train: index={train_index}\")
...     print(f\"  Test:  index={test_index}\")
Fold 0:
  Train: index=[0 1 2 3]
  Test:  index=[6 7]
Fold 1:
  Train: index=[0 1 2 3 4 5]
  Test:  index=[8 9]
Fold 2:
  Train: index=[0 1 2 3 4 5 6 7]
  Test:  index=[10 11]

For a more extended example see
:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`.

Notes
-----
The training set has size ``i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)`` in the ``i`` th split,
with a test set of size ``n_samples//(n_splits + 1)`` by default,
where ``n_samples`` is the number of samples.""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                             rdfs:comment """Top-k Accuracy classification score.

This metric computes the number of times where the correct label is among
the top `k` labels predicted (ranked by predicted scores). Note that the
multilabel case isn't covered here.

Read more in the :ref:`User Guide <top_k_accuracy_score>`

Parameters
----------
y_true : array-like of shape (n_samples,)
    True labels.

y_score : array-like of shape (n_samples,) or (n_samples, n_classes)
    Target scores. These can be either probability estimates or
    non-thresholded decision values (as returned by
    :term:`decision_function` on some classifiers).
    The binary case expects scores with shape (n_samples,) while the
    multiclass case expects scores with shape (n_samples, n_classes).
    In the multiclass case, the order of the class scores must
    correspond to the order of ``labels``, if provided, or else to
    the numerical or lexicographical order of the labels in ``y_true``.
    If ``y_true`` does not contain all the labels, ``labels`` must be
    provided.

k : int, default=2
    Number of most likely outcomes considered to find the correct label.

normalize : bool, default=True
    If `True`, return the fraction of correctly classified samples.
    Otherwise, return the number of correctly classified samples.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If `None`, all samples are given the same weight.

labels : array-like of shape (n_classes,), default=None
    Multiclass only. List of labels that index the classes in ``y_score``.
    If ``None``, the numerical or lexicographical order of the labels in
    ``y_true`` is used. If ``y_true`` does not contain all the labels,
    ``labels`` must be provided.

Returns
-------
score : float
    The top-k accuracy score. The best performance is 1 with
    `normalize == True` and the number of samples with
    `normalize == False`.

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.

Notes
-----
In cases where two or more labels are assigned equal predicted scores,
the labels with the highest indices will be chosen first. This might
impact the result if the correct label falls after the threshold because
of that.

Examples
--------
>>> import numpy as np
>>> from sklearn.metrics import top_k_accuracy_score
>>> y_true = np.array([0, 1, 2, 2])
>>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2
...                     [0.3, 0.4, 0.2],  # 1 is in top 2
...                     [0.2, 0.4, 0.3],  # 2 is in top 2
...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2
>>> top_k_accuracy_score(y_true, y_score, k=2)
0.75
>>> # Not normalizing gives the number of \"correctly\" classified samples
>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)
3""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> rdf:type owl:Class .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod> rdf:type owl:Class ;
                                                                                                          rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                          <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                          rdfs:comment """Split arrays or matrices into random train and test subsets.

Quick utility that wraps input validation,
``next(ShuffleSplit().split(X, y))``, and application to input data
into a single call for splitting (and optionally subsampling) data into a
one-liner.

Read more in the :ref:`User Guide <cross_validation>`.

Parameters
----------
*arrays : sequence of indexables with same length / shape[0]
    Allowed inputs are lists, numpy arrays, scipy-sparse
    matrices or pandas dataframes.

test_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the proportion
    of the dataset to include in the test split. If int, represents the
    absolute number of test samples. If None, the value is set to the
    complement of the train size. If ``train_size`` is also None, it will
    be set to 0.25.

train_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the
    proportion of the dataset to include in the train split. If
    int, represents the absolute number of train samples. If None,
    the value is automatically set to the complement of the test size.

random_state : int, RandomState instance or None, default=None
    Controls the shuffling applied to the data before applying the split.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.

shuffle : bool, default=True
    Whether or not to shuffle the data before splitting. If shuffle=False
    then stratify must be None.

stratify : array-like, default=None
    If not None, data is split in a stratified fashion, using this as
    the class labels.
    Read more in the :ref:`User Guide <stratification>`.

Returns
-------
splitting : list, length=2 * len(arrays)
    List containing train-test split of inputs.

    .. versionadded:: 0.16
        If the input is sparse, the output will be a
        ``scipy.sparse.csr_matrix``. Else, output type is the same as the
        input type.

Examples
--------
>>> import numpy as np
>>> from sklearn.model_selection import train_test_split
>>> X, y = np.arange(10).reshape((5, 2)), range(5)
>>> X
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])
>>> list(y)
[0, 1, 2, 3, 4]

>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, random_state=42)
...
>>> X_train
array([[4, 5],
       [0, 1],
       [6, 7]])
>>> y_train
[2, 0, 3]
>>> X_test
array([[2, 3],
       [8, 9]])
>>> y_test
[1, 4]

>>> train_test_split(y, shuffle=False)
[[0, 1, 2], [3, 4]]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule> rdf:type owl:Class ;
                                                                                                rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule> .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod> rdf:type owl:Class ;
                                                                                                        rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule> ,
                                                                                                                        <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                        rdfs:comment """Dimensionality reduction using truncated SVD (aka LSA).

This transformer performs linear dimensionality reduction by means of
truncated singular value decomposition (SVD). Contrary to PCA, this
estimator does not center the data before computing the singular value
decomposition. This means it can work with sparse matrices
efficiently.

In particular, truncated SVD works on term count/tf-idf matrices as
returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In
that context, it is known as latent semantic analysis (LSA).

This estimator supports two algorithms: a fast randomized SVD solver, and
a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or
`X.T * X`, whichever is more efficient.

Read more in the :ref:`User Guide <LSA>`.

Parameters
----------
n_components : int, default=2
    Desired dimensionality of output data.
    If algorithm='arpack', must be strictly less than the number of features.
    If algorithm='randomized', must be less than or equal to the number of features.
    The default value is useful for visualisation. For LSA, a value of
    100 is recommended.

algorithm : {'arpack', 'randomized'}, default='randomized'
    SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy
    (scipy.sparse.linalg.svds), or \"randomized\" for the randomized
    algorithm due to Halko (2009).

n_iter : int, default=5
    Number of iterations for randomized SVD solver. Not used by ARPACK. The
    default is larger than the default in
    :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse
    matrices that may have large slowly decaying spectrum.

n_oversamples : int, default=10
    Number of oversamples for randomized SVD solver. Not used by ARPACK.
    See :func:`~sklearn.utils.extmath.randomized_svd` for a complete
    description.

    .. versionadded:: 1.1

power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
    Power iteration normalizer for randomized SVD solver.
    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`
    for more details.

    .. versionadded:: 1.1

random_state : int, RandomState instance or None, default=None
    Used during randomized svd. Pass an int for reproducible results across
    multiple function calls.
    See :term:`Glossary <random_state>`.

tol : float, default=0.0
    Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
    SVD solver.

Attributes
----------
components_ : ndarray of shape (n_components, n_features)
    The right singular vectors of the input data.

explained_variance_ : ndarray of shape (n_components,)
    The variance of the training samples transformed by a projection to
    each component.

explained_variance_ratio_ : ndarray of shape (n_components,)
    Percentage of variance explained by each of the selected components.

singular_values_ : ndarray of shape (n_components,)
    The singular values corresponding to each of the selected components.
    The singular values are equal to the 2-norms of the ``n_components``
    variables in the lower-dimensional space.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
DictionaryLearning : Find a dictionary that sparsely encodes data.
FactorAnalysis : A simple linear generative model with
    Gaussian latent variables.
IncrementalPCA : Incremental principal components analysis.
KernelPCA : Kernel Principal component analysis.
NMF : Non-Negative Matrix Factorization.
PCA : Principal component analysis.

Notes
-----
SVD suffers from a problem called \"sign indeterminacy\", which means the
sign of the ``components_`` and the output from transform depend on the
algorithm and random state. To work around this, fit instances of this
class to data once, then keep the instance around to do transformations.

References
----------
:arxiv:`Halko, et al. (2009). \"Finding structure with randomness:
Stochastic algorithms for constructing approximate matrix decompositions\"
<0909.4061>`

Examples
--------
>>> from sklearn.decomposition import TruncatedSVD
>>> from scipy.sparse import csr_matrix
>>> import numpy as np
>>> np.random.seed(0)
>>> X_dense = np.random.rand(100, 100)
>>> X_dense[:, 2 * np.arange(50)] = 0
>>> X = csr_matrix(X_dense)
>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> svd.fit(X)
TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> print(svd.explained_variance_ratio_)
[0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]
>>> print(svd.explained_variance_ratio_.sum())
0.2102...
>>> print(svd.singular_values_)
[35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """Generalized Linear Model with a Tweedie distribution.

This estimator can be used to model different GLMs depending on the
``power`` parameter, which determines the underlying distribution.

Read more in the :ref:`User Guide <Generalized_linear_models>`.

.. versionadded:: 0.23

Parameters
----------
power : float, default=0
        The power determines the underlying target distribution according
        to the following table:

        +-------+------------------------+
        | Power | Distribution           |
        +=======+========================+
        | 0     | Normal                 |
        +-------+------------------------+
        | 1     | Poisson                |
        +-------+------------------------+
        | (1,2) | Compound Poisson Gamma |
        +-------+------------------------+
        | 2     | Gamma                  |
        +-------+------------------------+
        | 3     | Inverse Gaussian       |
        +-------+------------------------+

        For ``0 < power < 1``, no distribution exists.

alpha : float, default=1
    Constant that multiplies the L2 penalty term and determines the
    regularization strength. ``alpha = 0`` is equivalent to unpenalized
    GLMs. In this case, the design matrix `X` must have full column rank
    (no collinearities).
    Values of `alpha` must be in the range `[0.0, inf)`.

fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the linear predictor (`X @ coef + intercept`).

link : {'auto', 'identity', 'log'}, default='auto'
    The link function of the GLM, i.e. mapping from linear predictor
    `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets
    the link depending on the chosen `power` parameter as follows:

    - 'identity' for ``power <= 0``, e.g. for the Normal distribution
    - 'log' for ``power > 0``, e.g. for Poisson, Gamma and Inverse Gaussian
      distributions

solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'
    Algorithm to use in the optimization problem:

    'lbfgs'
        Calls scipy's L-BFGS-B optimizer.

    'newton-cholesky'
        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to
        iterated reweighted least squares) with an inner Cholesky based solver.
        This solver is a good choice for `n_samples` >> `n_features`, especially
        with one-hot encoded categorical features with rare categories. Be aware
        that the memory usage of this solver has a quadratic dependency on
        `n_features` because it explicitly computes the Hessian matrix.

        .. versionadded:: 1.2

max_iter : int, default=100
    The maximal number of iterations for the solver.
    Values must be in the range `[1, inf)`.

tol : float, default=1e-4
    Stopping criterion. For the lbfgs solver,
    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``
    where ``g_j`` is the j-th component of the gradient (derivative) of
    the objective function.
    Values must be in the range `(0.0, inf)`.

warm_start : bool, default=False
    If set to ``True``, reuse the solution of the previous call to ``fit``
    as initialization for ``coef_`` and ``intercept_`` .

verbose : int, default=0
    For the lbfgs solver set verbose to any positive number for verbosity.
    Values must be in the range `[0, inf)`.

Attributes
----------
coef_ : array of shape (n_features,)
    Estimated coefficients for the linear predictor (`X @ coef_ +
    intercept_`) in the GLM.

intercept_ : float
    Intercept (a.k.a. bias) added to linear predictor.

n_iter_ : int
    Actual number of iterations used in the solver.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
PoissonRegressor : Generalized Linear Model with a Poisson distribution.
GammaRegressor : Generalized Linear Model with a Gamma distribution.

Examples
--------
>>> from sklearn import linear_model
>>> clf = linear_model.TweedieRegressor()
>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]
>>> y = [2, 3.5, 5, 5.5]
>>> clf.fit(X, y)
TweedieRegressor()
>>> clf.score(X, y)
0.839...
>>> clf.coef_
array([0.599..., 0.299...])
>>> clf.intercept_
1.600...
>>> clf.predict([[1, 1], [3, 4]])
array([2.500..., 4.599...])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod> rdf:type owl:Class ;
                                                                                                         rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                         <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                         rdfs:comment """V-measure cluster labeling given a ground truth.

This score is identical to :func:`normalized_mutual_info_score` with
the ``'arithmetic'`` option for averaging.

The V-measure is the harmonic mean between homogeneity and completeness::

    v = (1 + beta) * homogeneity * completeness
         / (beta * homogeneity + completeness)

This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.

This metric is furthermore symmetric: switching ``label_true`` with
``label_pred`` will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.

Read more in the :ref:`User Guide <homogeneity_completeness>`.

Parameters
----------
labels_true : array-like of shape (n_samples,)
    Ground truth class labels to be used as a reference.

labels_pred : array-like of shape (n_samples,)
    Cluster labels to evaluate.

beta : float, default=1.0
    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
    If ``beta`` is greater than 1, ``completeness`` is weighted more
    strongly in the calculation. If ``beta`` is less than 1,
    ``homogeneity`` is weighted more strongly.

Returns
-------
v_measure : float
   Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.

See Also
--------
homogeneity_score : Homogeneity metric of cluster labeling.
completeness_score : Completeness metric of cluster labeling.
normalized_mutual_info_score : Normalized Mutual Information.

References
----------

.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
   conditional entropy-based external cluster evaluation measure
   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_

Examples
--------
Perfect labelings are both homogeneous and complete, hence have score 1.0::

  >>> from sklearn.metrics.cluster import v_measure_score
  >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])
  1.0
  >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])
  1.0

Labelings that assign all classes members to the same clusters
are complete but not homogeneous, hence penalized::

  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))
  0.8...
  >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))
  0.66...

Labelings that have pure clusters with members coming from the same
classes are homogeneous but un-necessary splits harm completeness
and thus penalize V-measure as well::

  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))
  0.8...
  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))
  0.66...

If classes members are completely split across different clusters,
the assignment is totally incomplete, hence the V-Measure is null::

  >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))
  0.0...

Clusters that include samples from totally different classes totally
destroy the homogeneity of the labeling, hence::

  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))
  0.0...""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod> rdf:type owl:Class ;
                                                                                                                  rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod> ,
                                                                                                                                  <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule> ;
                                                                                                                  rdfs:comment """Validation Curve visualization.

It is recommended to use
:meth:`~sklearn.model_selection.ValidationCurveDisplay.from_estimator` to
create a :class:`~sklearn.model_selection.ValidationCurveDisplay` instance.
All parameters are stored as attributes.

Read more in the :ref:`User Guide <visualizations>` for general information
about the visualization API and :ref:`detailed documentation
<validation_curve>` regarding the validation curve visualization.

.. versionadded:: 1.3

Parameters
----------
param_name : str
    Name of the parameter that has been varied.

param_range : array-like of shape (n_ticks,)
    The values of the parameter that have been evaluated.

train_scores : ndarray of shape (n_ticks, n_cv_folds)
    Scores on training sets.

test_scores : ndarray of shape (n_ticks, n_cv_folds)
    Scores on test set.

score_name : str, default=None
    The name of the score used in `validation_curve`. It will override the name
    inferred from the `scoring` parameter. If `score` is `None`, we use `\"Score\"` if
    `negate_score` is `False` and `\"Negative score\"` otherwise. If `scoring` is a
    string or a callable, we infer the name. We replace `_` by spaces and capitalize
    the first letter. We remove `neg_` and replace it by `\"Negative\"` if
    `negate_score` is `False` or just remove it otherwise.

Attributes
----------
ax_ : matplotlib Axes
    Axes with the validation curve.

figure_ : matplotlib Figure
    Figure containing the validation curve.

errorbar_ : list of matplotlib Artist or None
    When the `std_display_style` is `\"errorbar\"`, this is a list of
    `matplotlib.container.ErrorbarContainer` objects. If another style is
    used, `errorbar_` is `None`.

lines_ : list of matplotlib Artist or None
    When the `std_display_style` is `\"fill_between\"`, this is a list of
    `matplotlib.lines.Line2D` objects corresponding to the mean train and
    test scores. If another style is used, `line_` is `None`.

fill_between_ : list of matplotlib Artist or None
    When the `std_display_style` is `\"fill_between\"`, this is a list of
    `matplotlib.collections.PolyCollection` objects. If another style is
    used, `fill_between_` is `None`.

See Also
--------
sklearn.model_selection.validation_curve : Compute the validation curve.

Examples
--------
>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> from sklearn.datasets import make_classification
>>> from sklearn.model_selection import ValidationCurveDisplay, validation_curve
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = make_classification(n_samples=1_000, random_state=0)
>>> logistic_regression = LogisticRegression()
>>> param_name, param_range = \"C\", np.logspace(-8, 3, 10)
>>> train_scores, test_scores = validation_curve(
...     logistic_regression, X, y, param_name=param_name, param_range=param_range
... )
>>> display = ValidationCurveDisplay(
...     param_name=param_name, param_range=param_range,
...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\"
... )
>>> display.plot()
<...>
>>> plt.show()""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod> rdf:type owl:Class ;
                                                                                                             rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule> ,
                                                                                                                             <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod> ;
                                                                                                             rdfs:comment """Feature selector that removes all low-variance features.

This feature selection algorithm looks only at the features (X), not the
desired outputs (y), and can thus be used for unsupervised learning.

Read more in the :ref:`User Guide <variance_threshold>`.

Parameters
----------
threshold : float, default=0
    Features with a training-set variance lower than this threshold will
    be removed. The default is to keep all features with non-zero variance,
    i.e. remove the features that have the same value in all samples.

Attributes
----------
variances_ : array, shape (n_features,)
    Variances of individual features.

n_features_in_ : int
    Number of features seen during :term:`fit`.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.

    .. versionadded:: 1.0

See Also
--------
SelectFromModel: Meta-transformer for selecting features based on
    importance weights.
SelectPercentile : Select features according to a percentile of the highest
    scores.
SequentialFeatureSelector : Transformer that performs Sequential Feature
    Selection.

Notes
-----
Allows NaN in the input.
Raises ValueError if no feature in X meets the variance threshold.

Examples
--------
The following dataset has integer features, two of which are the same
in every sample. These are removed with the default setting for threshold::

    >>> from sklearn.feature_selection import VarianceThreshold
    >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]
    >>> selector = VarianceThreshold()
    >>> selector.fit_transform(X)
    array([[2, 0],
           [1, 4],
           [1, 1]])""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod> rdf:type owl:Class ;
                                                                                                            rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                            <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                            rdfs:comment """Soft Voting/Majority Rule classifier for unfitted estimators.

Read more in the :ref:`User Guide <voting_classifier>`.

.. versionadded:: 0.17

Parameters
----------
estimators : list of (str, estimator) tuples
    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones
    of those original estimators that will be stored in the class attribute
    ``self.estimators_``. An estimator can be set to ``'drop'`` using
    :meth:`set_params`.

    .. versionchanged:: 0.21
        ``'drop'`` is accepted. Using None was deprecated in 0.22 and
        support was removed in 0.24.

voting : {'hard', 'soft'}, default='hard'
    If 'hard', uses predicted class labels for majority rule voting.
    Else if 'soft', predicts the class label based on the argmax of
    the sums of the predicted probabilities, which is recommended for
    an ensemble of well-calibrated classifiers.

weights : array-like of shape (n_classifiers,), default=None
    Sequence of weights (`float` or `int`) to weight the occurrences of
    predicted class labels (`hard` voting) or class probabilities
    before averaging (`soft` voting). Uses uniform weights if `None`.

n_jobs : int, default=None
    The number of jobs to run in parallel for ``fit``.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

    .. versionadded:: 0.18

flatten_transform : bool, default=True
    Affects shape of transform output only when voting='soft'
    If voting='soft' and flatten_transform=True, transform method returns
    matrix with shape (n_samples, n_classifiers * n_classes). If
    flatten_transform=False, it returns
    (n_classifiers, n_samples, n_classes).

verbose : bool, default=False
    If True, the time elapsed while fitting will be printed as it
    is completed.

    .. versionadded:: 0.23

Attributes
----------
estimators_ : list of classifiers
    The collection of fitted sub-estimators as defined in ``estimators``
    that are not 'drop'.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

    .. versionadded:: 0.20

le_ : :class:`~sklearn.preprocessing.LabelEncoder`
    Transformer used to encode the labels during fit and decode during
    prediction.

classes_ : ndarray of shape (n_classes,)
    The classes labels.

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying classifier exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

See Also
--------
VotingRegressor : Prediction voting regressor.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)
>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
>>> clf3 = GaussianNB()
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> eclf1 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
>>> eclf1 = eclf1.fit(X, y)
>>> print(eclf1.predict(X))
[1 1 1 2 2 2]
>>> np.array_equal(eclf1.named_estimators_.lr.predict(X),
...                eclf1.named_estimators_['lr'].predict(X))
True
>>> eclf2 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...         voting='soft')
>>> eclf2 = eclf2.fit(X, y)
>>> print(eclf2.predict(X))
[1 1 1 2 2 2]

To drop an estimator, :meth:`set_params` can be used to remove it. Here we
dropped one of the estimators, resulting in 2 fitted estimators:

>>> eclf2 = eclf2.set_params(lr='drop')
>>> eclf2 = eclf2.fit(X, y)
>>> len(eclf2.estimators_)
2

Setting `flatten_transform=True` with `voting='soft'` flattens output shape of
`transform`:

>>> eclf3 = VotingClassifier(estimators=[
...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...        voting='soft', weights=[2,1,1],
...        flatten_transform=True)
>>> eclf3 = eclf3.fit(X, y)
>>> print(eclf3.predict(X))
[1 1 1 2 2 2]
>>> print(eclf3.transform(X).shape)
(6, 6)""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod> rdf:type owl:Class ;
                                                                                                           rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule> ,
                                                                                                                           <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod> ;
                                                                                                           rdfs:comment """Prediction voting regressor for unfitted estimators.

A voting regressor is an ensemble meta-estimator that fits several base
regressors, each on the whole dataset. Then it averages the individual
predictions to form a final prediction.

Read more in the :ref:`User Guide <voting_regressor>`.

.. versionadded:: 0.21

Parameters
----------
estimators : list of (str, estimator) tuples
    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones
    of those original estimators that will be stored in the class attribute
    ``self.estimators_``. An estimator can be set to ``'drop'`` using
    :meth:`set_params`.

    .. versionchanged:: 0.21
        ``'drop'`` is accepted. Using None was deprecated in 0.22 and
        support was removed in 0.24.

weights : array-like of shape (n_regressors,), default=None
    Sequence of weights (`float` or `int`) to weight the occurrences of
    predicted values before averaging. Uses uniform weights if `None`.

n_jobs : int, default=None
    The number of jobs to run in parallel for ``fit``.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

verbose : bool, default=False
    If True, the time elapsed while fitting will be printed as it
    is completed.

    .. versionadded:: 0.23

Attributes
----------
estimators_ : list of regressors
    The collection of fitted sub-estimators as defined in ``estimators``
    that are not 'drop'.

named_estimators_ : :class:`~sklearn.utils.Bunch`
    Attribute to access any fitted sub-estimators by name.

    .. versionadded:: 0.20

n_features_in_ : int
    Number of features seen during :term:`fit`. Only defined if the
    underlying regressor exposes such an attribute when fit.

    .. versionadded:: 0.24

feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Only defined if the
    underlying estimators expose such an attribute when fit.

    .. versionadded:: 1.0

See Also
--------
VotingClassifier : Soft Voting/Majority Rule classifier.

Examples
--------
>>> import numpy as np
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.ensemble import VotingRegressor
>>> from sklearn.neighbors import KNeighborsRegressor
>>> r1 = LinearRegression()
>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)
>>> r3 = KNeighborsRegressor()
>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])
>>> y = np.array([2, 6, 12, 20, 30, 42])
>>> er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])
>>> print(er.fit(X, y).predict(X))
[ 6.8...  8.4... 12.5... 17.8... 26...  34...]

In the following example, we drop the `'lr'` estimator with
:meth:`~VotingRegressor.set_params` and fit the remaining two estimators:

>>> er = er.set_params(lr='drop')
>>> er = er.fit(X, y)
>>> len(er.estimators_)
2""" .


###  https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod
<https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod> rdf:type owl:Class ;
                                                                                                       rdfs:subClassOf <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule> ,
                                                                                                                       <https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod> ;
                                                                                                       rdfs:comment """Zero-one classification loss.

If normalize is ``True``, return the fraction of misclassifications
(float), else it returns the number of misclassifications (int). The best
performance is 0.

Read more in the :ref:`User Guide <zero_one_loss>`.

Parameters
----------
y_true : 1d array-like, or label indicator array / sparse matrix
    Ground truth (correct) labels.

y_pred : 1d array-like, or label indicator array / sparse matrix
    Predicted labels, as returned by a classifier.

normalize : bool, default=True
    If ``False``, return the number of misclassifications.
    Otherwise, return the fraction of misclassifications.

sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.

Returns
-------
loss : float or int,
    If ``normalize == True``, return the fraction of misclassifications
    (float), else it returns the number of misclassifications (int).

See Also
--------
accuracy_score : Compute the accuracy score. By default, the function will
    return the fraction of correct predictions divided by the total number
    of predictions.
hamming_loss : Compute the average Hamming loss or Hamming distance between
    two sets of samples.
jaccard_score : Compute the Jaccard similarity coefficient score.

Notes
-----
In multilabel classification, the zero_one_loss function corresponds to
the subset zero-one loss: for each sample, the entire set of labels must be
correctly predicted, otherwise the loss for that sample is equal to one.

Examples
--------
>>> from sklearn.metrics import zero_one_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> zero_one_loss(y_true, y_pred)
0.25
>>> zero_one_loss(y_true, y_pred, normalize=False)
1.0

In the multilabel case with binary label indicators:

>>> import numpy as np
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5""" .


###  Generated by the OWL API (version 5.1.18) https://github.com/owlcs/owlapi/
