[ {
  "@id" : "_:genid1",
  "@type" : [ "http://www.w3.org/2002/07/owl#Ontology" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Bayesian ARD regression.\n\nFit the weights of a regression model, using an ARD prior. The weights of\nthe regression model are assumed to be in Gaussian distributions.\nAlso estimate the parameters lambda (precisions of the distributions of the\nweights) and alpha (precision of the distribution of the noise).\nThe estimation is done by an iterative procedures (Evidence Maximization)\n\nRead more in the :ref:`User Guide <bayesian_regression>`.\n\nParameters\n----------\nmax_iter : int, default=None\n    Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.\n\n    .. versionchanged:: 1.3\n\ntol : float, default=1e-3\n    Stop the algorithm if w has converged.\n\nalpha_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the alpha parameter.\n\nalpha_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the alpha parameter.\n\nlambda_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the lambda parameter.\n\nlambda_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the lambda parameter.\n\ncompute_score : bool, default=False\n    If True, compute the objective function at each step of the model.\n\nthreshold_lambda : float, default=10 000\n    Threshold for removing (pruning) weights with high precision from\n    the computation.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nverbose : bool, default=False\n    Verbose mode when fitting the model.\n\nn_iter : int\n    Maximum number of iterations.\n\n    .. deprecated:: 1.3\n       `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use\n       `max_iter` instead.\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    Coefficients of the regression model (mean of distribution)\n\nalpha_ : float\n   estimated precision of the noise.\n\nlambda_ : array-like of shape (n_features,)\n   estimated precisions of the weights.\n\nsigma_ : array-like of shape (n_features, n_features)\n    estimated variance-covariance matrix of the weights\n\nscores_ : float\n    if computed, value of the objective function (to be maximized)\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n\n    .. versionadded:: 1.3\n\nintercept_ : float\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nX_offset_ : float\n    If `fit_intercept=True`, offset subtracted for centering data to a\n    zero mean. Set to np.zeros(n_features) otherwise.\n\nX_scale_ : float\n    Set to np.ones(n_features).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nBayesianRidge : Bayesian ridge regression.\n\nNotes\n-----\nFor an example, see :ref:`examples/linear_model/plot_ard.py\n<sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\nReferences\n----------\nD. J. C. MacKay, Bayesian nonlinear modeling for the prediction\ncompetition, ASHRAE Transactions, 1994.\n\nR. Salakhutdinov, Lecture notes on Statistical Machine Learning,\nhttp://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\nTheir beta is our ``self.alpha_``\nTheir alpha is our ``self.lambda_``\nARD is a little different than the slide: only dimensions/features for\nwhich ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\ndiscarded.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.ARDRegression()\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\nARDRegression()\n>>> clf.predict([[1, 1]])\narray([1.])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Accuracy classification score.\n\nIn multilabel classification, this function computes subset accuracy:\nthe set of labels predicted for a sample must *exactly* match the\ncorresponding set of labels in y_true.\n\nRead more in the :ref:`User Guide <accuracy_score>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nnormalize : bool, default=True\n    If ``False``, return the number of correctly classified samples.\n    Otherwise, return the fraction of correctly classified samples.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    If ``normalize == True``, return the fraction of correctly\n    classified samples (float), else returns the number of correctly\n    classified samples (int).\n\n    The best performance is 1 with ``normalize == True`` and the number\n    of samples with ``normalize == False``.\n\nSee Also\n--------\nbalanced_accuracy_score : Compute the balanced accuracy to deal with\n    imbalanced datasets.\njaccard_score : Compute the Jaccard similarity coefficient score.\nhamming_loss : Compute the average Hamming loss or Hamming distance between\n    two sets of samples.\nzero_one_loss : Compute the Zero-one classification loss. By default, the\n    function will return the percentage of imperfectly predicted subsets.\n\nNotes\n-----\nIn binary classification, this function is equal to the `jaccard_score`\nfunction.\n\nExamples\n--------\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred = [0, 2, 1, 3]\n>>> y_true = [0, 1, 2, 3]\n>>> accuracy_score(y_true, y_pred)\n0.5\n>>> accuracy_score(y_true, y_pred, normalize=False)\n2.0\n\nIn the multilabel case with binary label indicators:\n\n>>> import numpy as np\n>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An AdaBoost classifier.\n\nAn AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm based on [2]_.\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nestimator : object, default=None\n    The base estimator from which the boosted ensemble is built.\n    Support for sample weighting is required, as well as proper\n    ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n    the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n    initialized with `max_depth=1`.\n\n    .. versionadded:: 1.2\n       `base_estimator` was renamed to `estimator`.\n\nn_estimators : int, default=50\n    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n    Values must be in the range `[1, inf)`.\n\nlearning_rate : float, default=1.0\n    Weight applied to each classifier at each boosting iteration. A higher\n    learning rate increases the contribution of each classifier. There is\n    a trade-off between the `learning_rate` and `n_estimators` parameters.\n    Values must be in the range `(0.0, inf)`.\n\nalgorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n    If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n    ``estimator`` must support calculation of class probabilities.\n    If 'SAMME' then use the SAMME discrete boosting algorithm.\n    The SAMME.R algorithm typically converges faster than SAMME,\n    achieving a lower test error with fewer boosting iterations.\n\n    .. deprecated:: 1.4\n        `\"SAMME.R\"` is deprecated and will be removed in version 1.6.\n        '\"SAMME\"' will become the default.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given at each `estimator` at each\n    boosting iteration.\n    Thus, it is only used when `estimator` exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nestimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_classes_ : int\n    The number of classes.\n\nestimator_weights_ : ndarray of floats\n    Weights for each estimator in the boosted ensemble.\n\nestimator_errors_ : ndarray of floats\n    Classification error for each estimator in the boosted\n    ensemble.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances if supported by the\n    ``estimator`` (when based on decision trees).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n    regressor on the original dataset and then fits additional copies of\n    the regressor on the same dataset but where the weights of instances\n    are adjusted according to the error of the current prediction.\n\nGradientBoostingClassifier : GB builds an additive model in a forward\n    stage-wise fashion. Regression trees are fit on the negative gradient\n    of the binomial or multinomial deviance loss function. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\nsklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n    method used for classification.\n    Creates a model that predicts the value of a target variable by\n    learning simple decision rules inferred from the data features.\n\nReferences\n----------\n.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n       on-Line Learning and an Application to Boosting\", 1995.\n\n.. [2] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"\n       Statistics and its Interface 2.3 (2009): 349-360.\n       <10.4310/SII.2009.v2.n3.a8>`\n\nExamples\n--------\n>>> from sklearn.ensemble import AdaBoostClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n>>> clf.fit(X, y)\nAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])\n>>> clf.score(X, y)\n0.96..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An AdaBoost regressor.\n\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost.R2 [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nestimator : object, default=None\n    The base estimator from which the boosted ensemble is built.\n    If ``None``, then the base estimator is\n    :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n    `max_depth=3`.\n\n    .. versionadded:: 1.2\n       `base_estimator` was renamed to `estimator`.\n\nn_estimators : int, default=50\n    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n    Values must be in the range `[1, inf)`.\n\nlearning_rate : float, default=1.0\n    Weight applied to each regressor at each boosting iteration. A higher\n    learning rate increases the contribution of each regressor. There is\n    a trade-off between the `learning_rate` and `n_estimators` parameters.\n    Values must be in the range `(0.0, inf)`.\n\nloss : {'linear', 'square', 'exponential'}, default='linear'\n    The loss function to use when updating the weights after each\n    boosting iteration.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given at each `estimator` at each\n    boosting iteration.\n    Thus, it is only used when `estimator` exposes a `random_state`.\n    In addition, it controls the bootstrap of the weights used to train the\n    `estimator` at each boosting iteration.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nestimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of regressors\n    The collection of fitted sub-estimators.\n\nestimator_weights_ : ndarray of floats\n    Weights for each estimator in the boosted ensemble.\n\nestimator_errors_ : ndarray of floats\n    Regression error for each estimator in the boosted ensemble.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances if supported by the\n    ``estimator`` (when based on decision trees).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdaBoostClassifier : An AdaBoost classifier.\nGradientBoostingRegressor : Gradient Boosting Classification Tree.\nsklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n\nReferences\n----------\n.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n       on-Line Learning and an Application to Boosting\", 1995.\n\n.. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n\nExamples\n--------\n>>> from sklearn.ensemble import AdaBoostRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, n_informative=2,\n...                        random_state=0, shuffle=False)\n>>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n>>> regr.fit(X, y)\nAdaBoostRegressor(n_estimators=100, random_state=0)\n>>> regr.predict([[0, 0, 0, 0]])\narray([4.7972...])\n>>> regr.score(X, y)\n0.9771..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Adjusted Mutual Information between two clusterings.\n\nAdjusted Mutual Information (AMI) is an adjustment of the Mutual\nInformation (MI) score to account for chance. It accounts for the fact that\nthe MI is generally higher for two clusterings with a larger number of\nclusters, regardless of whether there is actually more information shared.\nFor two clusterings :math:`U` and :math:`V`, the AMI is given as::\n\n    AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching :math:`U` (``label_true``)\nwith :math:`V` (``labels_pred``) will return the same score value. This can\nbe useful to measure the agreement of two independent label assignments\nstrategies on the same dataset when the real ground truth is not known.\n\nBe mindful that this function is an order of magnitude slower than other\nmetrics, such as the Adjusted Rand Index.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n\nParameters\n----------\nlabels_true : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets, called :math:`U` in\n    the above formula.\n\nlabels_pred : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets, called :math:`V` in\n    the above formula.\n\naverage_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'\n    How to compute the normalizer in the denominator.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 0.22\n       The default value of ``average_method`` changed from 'max' to\n       'arithmetic'.\n\nReturns\n-------\nami: float (upperlimited by 1.0)\n   The AMI returns a value of 1 when the two partitions are identical\n   (ie perfectly matched). Random partitions (independent labellings) have\n   an expected AMI around 0 on average hence can be negative. The value is\n   in adjusted nats (based on the natural logarithm).\n\nSee Also\n--------\nadjusted_rand_score : Adjusted Rand Index.\nmutual_info_score : Mutual Information (not adjusted for chance).\n\nReferences\n----------\n.. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n   Clusterings Comparison: Variants, Properties, Normalization and\n   Correction for Chance, JMLR\n   <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n\n.. [2] `Wikipedia entry for the Adjusted Mutual Information\n   <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have\nscore 1.0::\n\n  >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n  >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n  ... # doctest: +SKIP\n  1.0\n  >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n  ... # doctest: +SKIP\n  1.0\n\nIf classes members are completely split across different clusters,\nthe assignment is totally in-complete, hence the AMI is null::\n\n  >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n  ... # doctest: +SKIP\n  0.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedRandScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Rand index adjusted for chance.\n\nThe Rand Index computes a similarity measure between two clusterings\nby considering all pairs of samples and counting pairs that are\nassigned in the same or different clusters in the predicted and\ntrue clusterings.\n\nThe raw RI score is then \"adjusted for chance\" into the ARI score\nusing the following scheme::\n\n    ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n\nThe adjusted Rand index is thus ensured to have a value close to\n0.0 for random labeling independently of the number of clusters and\nsamples and exactly 1.0 when the clusterings are identical (up to\na permutation). The adjusted Rand index is bounded below by -0.5 for\nespecially discordant clusterings.\n\nARI is a symmetric measure::\n\n    adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n\nRead more in the :ref:`User Guide <adjusted_rand_score>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=int\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,), dtype=int\n    Cluster labels to evaluate.\n\nReturns\n-------\nARI : float\n   Similarity score between -0.5 and 1.0. Random labelings have an ARI\n   close to 0.0. 1.0 stands for perfect match.\n\nSee Also\n--------\nadjusted_mutual_info_score : Adjusted Mutual Information.\n\nReferences\n----------\n.. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n  Journal of Classification 1985\n  https://link.springer.com/article/10.1007%2FBF01908075\n\n.. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n  adjusted Rand index, Psychological Methods 2004\n\n.. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n\n.. [Chacon] :doi:`Minimum adjusted Rand index for two clusterings of a given size,\n  2022, J. E. Chac√≥n and A. I. Rastrojo <10.1007/s11634-022-00491-w>`\n\nExamples\n--------\nPerfectly matching labelings have a score of 1 even\n\n  >>> from sklearn.metrics.cluster import adjusted_rand_score\n  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n  1.0\n  >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nLabelings that assign all classes members to the same clusters\nare complete but may not always be pure, hence penalized::\n\n  >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n  0.57...\n\nARI is symmetric, so labelings that have pure clusters with members\ncoming from the same classes but unnecessary splits are penalized::\n\n  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n  0.57...\n\nIf classes members are completely split across different clusters, the\nassignment is totally incomplete, hence the ARI is very low::\n\n  >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n  0.0\n\nARI may take a negative value for especially discordant labelings that\nare a worse choice than the expected value of random labels::\n\n  >>> adjusted_rand_score([0, 0, 1, 1], [0, 1, 0, 1])\n  -0.5"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\ndamping : float, default=0.5\n    Damping factor in the range `[0.5, 1.0)` is the extent to\n    which the current value is maintained relative to\n    incoming values (weighted 1 - damping). This in order\n    to avoid numerical oscillations when updating these\n    values (messages).\n\nmax_iter : int, default=200\n    Maximum number of iterations.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\ncopy : bool, default=True\n    Make a copy of input data.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number\n    of exemplars, ie of clusters, is influenced by the input\n    preferences value. If the preferences are not passed as arguments,\n    they will be set to the median of the input similarities.\n\naffinity : {'euclidean', 'precomputed'}, default='euclidean'\n    Which affinity to use. At the moment 'precomputed' and\n    ``euclidean`` are supported. 'euclidean' uses the\n    negative squared euclidean distance between points.\n\nverbose : bool, default=False\n    Whether to be verbose.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nAttributes\n----------\ncluster_centers_indices_ : ndarray of shape (n_clusters,)\n    Indices of cluster centers.\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Cluster centers (if affinity != ``precomputed``).\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\naffinity_matrix_ : ndarray of shape (n_samples, n_samples)\n    Stores the affinity matrix used in ``fit``.\n\nn_iter_ : int\n    Number of iterations taken to converge.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAgglomerativeClustering : Recursively merges the pair of\n    clusters that minimally increases a given linkage distance.\nFeatureAgglomeration : Similar to AgglomerativeClustering,\n    but recursively merges features instead of samples.\nKMeans : K-Means clustering.\nMiniBatchKMeans : Mini-Batch K-Means clustering.\nMeanShift : Mean shift clustering using a flat kernel.\nSpectralClustering : Apply clustering to a projection\n    of the normalized Laplacian.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nThe algorithmic complexity of affinity propagation is quadratic\nin the number of points.\n\nWhen the algorithm does not converge, it will still return a arrays of\n``cluster_center_indices`` and labels if there are any exemplars/clusters,\nhowever they may be degenerate and should be used with caution.\n\nWhen ``fit`` does not converge, ``cluster_centers_`` is still populated\nhowever it may be degenerate. In such a case, proceed with caution.\nIf ``fit`` does not converge and fails to produce any ``cluster_centers_``\nthen ``predict`` will label every sample as ``-1``.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, ``fit`` will result in\na single cluster center and label ``0`` for every sample. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\n\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007\n\nExamples\n--------\n>>> from sklearn.cluster import AffinityPropagation\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AffinityPropagation(random_state=5).fit(X)\n>>> clustering\nAffinityPropagation(random_state=5)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])\n>>> clustering.predict([[0, 0], [4, 4]])\narray([0, 1])\n>>> clustering.cluster_centers_\narray([[1, 2],\n       [4, 2]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Agglomerative Clustering.\n\nRecursively merges pair of clusters of sample data; uses linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int or None, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\nmetric : str or callable, default=\"euclidean\"\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n    \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed\n    as input for the fit method.\n\n    .. versionadded:: 1.2\n\n    .. deprecated:: 1.4\n       `metric=None` is deprecated in 1.4 and will be removed in 1.6.\n       Let `metric` be the default value (i.e. `\"euclidean\"`) instead.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each sample the neighboring\n    samples following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    `kneighbors_graph`. Default is ``None``, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at ``n_clusters``. This is\n    useful to decrease computation time if the number of clusters is not\n    small compared to the number of samples. This option is useful only\n    when specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of observation. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - 'ward' minimizes the variance of the clusters being merged.\n    - 'average' uses the average of the distances of each observation of\n      the two sets.\n    - 'complete' or 'maximum' linkage uses the maximum distances between\n      all observations of the two sets.\n    - 'single' uses the minimum of the distances between all observations\n      of the two sets.\n\n    .. versionadded:: 0.20\n        Added the 'single' option\n\ndistance_threshold : float, default=None\n    The linkage distance threshold at or above which clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : ndarray of shape (n_samples)\n    Cluster labels for each point.\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nchildren_ : array-like of shape (n_samples-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`.\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nSee Also\n--------\nFeatureAgglomeration : Agglomerative clustering but for features instead of\n    samples.\nward_tree : Hierarchical clustering with ward linkage.\n\nExamples\n--------\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AgglomerativeClustering().fit(X)\n>>> clustering\nAgglomerativeClustering()\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AucMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Area Under the Curve (AUC) using the trapezoidal rule.\n\nThis is a general function, given points on a curve.  For computing the\narea under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\nway to summarize a precision-recall curve, see\n:func:`average_precision_score`.\n\nParameters\n----------\nx : array-like of shape (n,)\n    X coordinates. These must be either monotonic increasing or monotonic\n    decreasing.\ny : array-like of shape (n,)\n    Y coordinates.\n\nReturns\n-------\nauc : float\n    Area Under the Curve.\n\nSee Also\n--------\nroc_auc_score : Compute the area under the ROC curve.\naverage_precision_score : Compute average precision from prediction scores.\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n>>> metrics.auc(fpr, tpr)\n0.75"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute average precision (AP) from prediction scores.\n\nAP summarizes a precision-recall curve as the weighted mean of precisions\nachieved at each threshold, with the increase in recall from the previous\nthreshold used as the weight:\n\n.. math::\n    \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the nth\nthreshold [1]_. This implementation is not interpolated and is different\nfrom computing the area under the precision-recall curve with the\ntrapezoidal rule, which uses linear interpolation and can be too\noptimistic.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n    True binary labels or binary label indicators.\n\ny_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by :term:`decision_function` on some classifiers).\n\naverage : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n    If ``None``, the scores for each class are returned. Otherwise,\n    this determines the type of averaging performed on the data:\n\n    ``'micro'``:\n        Calculate metrics globally by considering each element of the label\n        indicator matrix as a label.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average, weighted\n        by support (the number of true instances for each label).\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average.\n\n    Will be ignored when ``y_true`` is binary.\n\npos_label : int, float, bool or str, default=1\n    The label of the positive class. Only applied to binary ``y_true``.\n    For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\naverage_precision : float\n    Average precision score.\n\nSee Also\n--------\nroc_auc_score : Compute the area under the ROC curve.\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\n\nNotes\n-----\n.. versionchanged:: 0.19\n  Instead of linearly interpolating between operating points, precisions\n  are weighted by the change in recall since the last operating point.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Average precision\n       <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n       oldid=793358396#Average_precision>`_\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import average_precision_score\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> average_precision_score(y_true, y_scores)\n0.83...\n>>> y_true = np.array([0, 0, 1, 1, 2, 2])\n>>> y_scores = np.array([\n...     [0.7, 0.2, 0.1],\n...     [0.4, 0.3, 0.3],\n...     [0.1, 0.8, 0.1],\n...     [0.2, 0.3, 0.5],\n...     [0.4, 0.4, 0.2],\n...     [0.1, 0.2, 0.7],\n... ])\n>>> average_precision_score(y_true, y_scores)\n0.77..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "A Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base\nclassifiers each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nestimator : object, default=None\n    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a\n    :class:`~sklearn.tree.DecisionTreeClassifier`.\n\n    .. versionadded:: 1.2\n       `base_estimator` was renamed to `estimator`.\n\nn_estimators : int, default=10\n    The number of base estimators in the ensemble.\n\nmax_samples : int or float, default=1.0\n    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\nbootstrap : bool, default=True\n    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n\nbootstrap_features : bool, default=False\n    Whether features are drawn with replacement.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization error. Only available if bootstrap=True.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.17\n       *warm_start* constructor parameter.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nAttributes\n----------\nestimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nestimators_ : list of estimators\n    The collection of fitted base estimators.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\nestimators_features_ : list of arrays\n    The subset of drawn features for each base estimator.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_classes_ : int or list\n    The number of classes.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nSee Also\n--------\nBaggingRegressor : A Bagging regressor.\n\nReferences\n----------\n\n.. [1] L. Breiman, \"Pasting small votes for classification in large\n       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n       1996.\n\n.. [3] T. Ho, \"The random subspace method for constructing decision\n       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n       1998.\n\n.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n       Learning and Knowledge Discovery in Databases, 346-361, 2012.\n\nExamples\n--------\n>>> from sklearn.svm import SVC\n>>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=100, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = BaggingClassifier(estimator=SVC(),\n...                         n_estimators=10, random_state=0).fit(X, y)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "A Bagging regressor.\n\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nestimator : object, default=None\n    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a\n    :class:`~sklearn.tree.DecisionTreeRegressor`.\n\n    .. versionadded:: 1.2\n       `base_estimator` was renamed to `estimator`.\n\nn_estimators : int, default=10\n    The number of base estimators in the ensemble.\n\nmax_samples : int or float, default=1.0\n    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\nbootstrap : bool, default=True\n    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n\nbootstrap_features : bool, default=False\n    Whether features are drawn with replacement.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization error. Only available if bootstrap=True.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nAttributes\n----------\nestimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nestimators_ : list of estimators\n    The collection of fitted sub-estimators.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\nestimators_features_ : list of arrays\n    The subset of drawn features for each base estimator.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_prediction_` might contain NaN. This attribute exists only\n    when ``oob_score`` is True.\n\nSee Also\n--------\nBaggingClassifier : A Bagging classifier.\n\nReferences\n----------\n\n.. [1] L. Breiman, \"Pasting small votes for classification in large\n       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n       1996.\n\n.. [3] T. Ho, \"The random subspace method for constructing decision\n       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n       1998.\n\n.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n       Learning and Knowledge Discovery in Databases, 346-361, 2012.\n\nExamples\n--------\n>>> from sklearn.svm import SVR\n>>> from sklearn.ensemble import BaggingRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_samples=100, n_features=4,\n...                        n_informative=2, n_targets=1,\n...                        random_state=0, shuffle=False)\n>>> regr = BaggingRegressor(estimator=SVR(),\n...                         n_estimators=10, random_state=0).fit(X, y)\n>>> regr.predict([[0, 0, 0, 0]])\narray([-2.8720...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the balanced accuracy.\n\nThe balanced accuracy in binary and multiclass classification problems to\ndeal with imbalanced datasets. It is defined as the average of recall\nobtained on each class.\n\nThe best value is 1 and the worst value is 0 when ``adjusted=False``.\n\nRead more in the :ref:`User Guide <balanced_accuracy_score>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated targets as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nadjusted : bool, default=False\n    When true, the result is adjusted for chance, so that random\n    performance would score 0, while keeping perfect performance at a score\n    of 1.\n\nReturns\n-------\nbalanced_accuracy : float\n    Balanced accuracy score.\n\nSee Also\n--------\naverage_precision_score : Compute average precision (AP) from prediction\n    scores.\nprecision_score : Compute the precision score.\nrecall_score : Compute the recall score.\nroc_auc_score : Compute Area Under the Receiver Operating Characteristic\n    Curve (ROC AUC) from prediction scores.\n\nNotes\n-----\nSome literature promotes alternative definitions of balanced accuracy. Our\ndefinition is equivalent to :func:`accuracy_score` with class-balanced\nsample weights, and shares desirable properties with the binary case.\nSee the :ref:`User Guide <balanced_accuracy_score>`.\n\nReferences\n----------\n.. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n       The balanced accuracy and its posterior distribution.\n       Proceedings of the 20th International Conference on Pattern\n       Recognition, 3121-24.\n.. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n       `Fundamentals of Machine Learning for Predictive Data Analytics:\n       Algorithms, Worked Examples, and Case Studies\n       <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import balanced_accuracy_score\n>>> y_true = [0, 1, 0, 0, 1, 0]\n>>> y_pred = [0, 1, 0, 0, 0, 1]\n>>> balanced_accuracy_score(y_true, y_pred)\n0.625"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BallTreeMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Bayesian ridge regression.\n\nFit a Bayesian ridge model. See the Notes section for details on this\nimplementation and the optimization of the regularization parameters\nlambda (precision of the weights) and alpha (precision of the noise).\n\nRead more in the :ref:`User Guide <bayesian_regression>`.\n\nParameters\n----------\nmax_iter : int, default=None\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion. If `None`, it\n    corresponds to `max_iter=300`.\n\n    .. versionchanged:: 1.3\n\ntol : float, default=1e-3\n    Stop the algorithm if w has converged.\n\nalpha_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the alpha parameter.\n\nalpha_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the alpha parameter.\n\nlambda_1 : float, default=1e-6\n    Hyper-parameter : shape parameter for the Gamma distribution prior\n    over the lambda parameter.\n\nlambda_2 : float, default=1e-6\n    Hyper-parameter : inverse scale parameter (rate parameter) for the\n    Gamma distribution prior over the lambda parameter.\n\nalpha_init : float, default=None\n    Initial value for alpha (precision of the noise).\n    If not set, alpha_init is 1/Var(y).\n\n        .. versionadded:: 0.22\n\nlambda_init : float, default=None\n    Initial value for lambda (precision of the weights).\n    If not set, lambda_init is 1.\n\n        .. versionadded:: 0.22\n\ncompute_score : bool, default=False\n    If True, compute the log marginal likelihood at each iteration of the\n    optimization.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model.\n    The intercept is not treated as a probabilistic parameter\n    and thus has no associated variance. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nverbose : bool, default=False\n    Verbose mode when fitting the model.\n\nn_iter : int\n    Maximum number of iterations. Should be greater than or equal to 1.\n\n    .. deprecated:: 1.3\n       `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use\n       `max_iter` instead.\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    Coefficients of the regression model (mean of distribution)\n\nintercept_ : float\n    Independent term in decision function. Set to 0.0 if\n    `fit_intercept = False`.\n\nalpha_ : float\n   Estimated precision of the noise.\n\nlambda_ : float\n   Estimated precision of the weights.\n\nsigma_ : array-like of shape (n_features, n_features)\n    Estimated variance-covariance matrix of the weights\n\nscores_ : array-like of shape (n_iter_+1,)\n    If computed_score is True, value of the log marginal likelihood (to be\n    maximized) at each iteration of the optimization. The array starts\n    with the value of the log marginal likelihood obtained for the initial\n    values of alpha and lambda and ends with the value obtained for the\n    estimated alpha and lambda.\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n\nX_offset_ : ndarray of shape (n_features,)\n    If `fit_intercept=True`, offset subtracted for centering data to a\n    zero mean. Set to np.zeros(n_features) otherwise.\n\nX_scale_ : ndarray of shape (n_features,)\n    Set to np.ones(n_features).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nARDRegression : Bayesian ARD regression.\n\nNotes\n-----\nThere exist several strategies to perform Bayesian ridge regression. This\nimplementation is based on the algorithm described in Appendix A of\n(Tipping, 2001) where updates of the regularization parameters are done as\nsuggested in (MacKay, 1992). Note that according to A New\nView of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\nupdate rules do not guarantee that the marginal likelihood is increasing\nbetween two consecutive iterations of the optimization.\n\nReferences\n----------\nD. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\nVol. 4, No. 3, 1992.\n\nM. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\nJournal of Machine Learning Research, Vol. 1, 2001.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.BayesianRidge()\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\nBayesianRidge()\n>>> clf.predict([[1, 1]])\narray([1.])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Naive Bayes classifier for multivariate Bernoulli models.\n\nLike MultinomialNB, this classifier is suitable for discrete data. The\ndifference is that while MultinomialNB works with occurrence counts,\nBernoulliNB is designed for binary/boolean features.\n\nRead more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n\nParameters\n----------\nalpha : float or array-like of shape (n_features,), default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing).\n\nforce_alpha : bool, default=True\n    If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`.\n\nbinarize : float or None, default=0.0\n    Threshold for binarizing (mapping to booleans) of sample features.\n    If None, input is presumed to already consist of binary vectors.\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes,)\n    Log probability of each class (smoothed).\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical log probability of features given a class, P(x_i|y).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : The Complement Naive Bayes classifier\n    described in Rennie et al. (2003).\nGaussianNB : Gaussian Naive Bayes (GaussianNB).\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n\nA. McCallum and K. Nigam (1998). A comparison of event models for naive\nBayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\nText Categorization, pp. 41-48.\n\nV. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\nnaive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> Y = np.array([1, 2, 3, 4, 4, 5])\n>>> from sklearn.naive_bayes import BernoulliNB\n>>> clf = BernoulliNB()\n>>> clf.fit(X, Y)\nBernoulliNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Bernoulli Restricted Boltzmann Machine (RBM).\n\nA Restricted Boltzmann Machine with binary visible units and\nbinary hidden units. Parameters are estimated using Stochastic Maximum\nLikelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n[2].\n\nThe time complexity of this implementation is ``O(d ** 2)`` assuming\nd ~ n_features ~ n_components.\n\nRead more in the :ref:`User Guide <rbm>`.\n\nParameters\n----------\nn_components : int, default=256\n    Number of binary hidden units.\n\nlearning_rate : float, default=0.1\n    The learning rate for weight updates. It is *highly* recommended\n    to tune this hyper-parameter. Reasonable values are in the\n    10**[0., -3.] range.\n\nbatch_size : int, default=10\n    Number of examples per minibatch.\n\nn_iter : int, default=10\n    Number of iterations/sweeps over the training dataset to perform\n    during training.\n\nverbose : int, default=0\n    The verbosity level. The default, zero, means silent mode. Range\n    of values is [0, inf].\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for:\n\n    - Gibbs sampling from visible and hidden layers.\n\n    - Initializing components, sampling from layers during fit.\n\n    - Corrupting the data when scoring samples.\n\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nintercept_hidden_ : array-like of shape (n_components,)\n    Biases of the hidden units.\n\nintercept_visible_ : array-like of shape (n_features,)\n    Biases of the visible units.\n\ncomponents_ : array-like of shape (n_components, n_features)\n    Weight matrix, where `n_features` is the number of\n    visible units and `n_components` is the number of hidden units.\n\nh_samples_ : array-like of shape (batch_size, n_components)\n    Hidden Activation sampled from the model distribution,\n    where `batch_size` is the number of examples per minibatch and\n    `n_components` is the number of hidden units.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.neural_network.MLPRegressor : Multi-layer Perceptron regressor.\nsklearn.neural_network.MLPClassifier : Multi-layer Perceptron classifier.\nsklearn.decomposition.PCA : An unsupervised linear dimensionality\n    reduction model.\n\nReferences\n----------\n\n[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n    deep belief nets. Neural Computation 18, pp 1527-1554.\n    https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n\n[2] Tieleman, T. Training Restricted Boltzmann Machines using\n    Approximations to the Likelihood Gradient. International Conference\n    on Machine Learning (ICML) 2008\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from sklearn.neural_network import BernoulliRBM\n>>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n>>> model = BernoulliRBM(n_components=2)\n>>> model.fit(X)\nBernoulliRBM(n_components=2)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Binarize data (set feature values to 0 or 1) according to a threshold.\n\nValues greater than the threshold map to 1, while values less than\nor equal to the threshold map to 0. With the default threshold of 0,\nonly positive values map to 1.\n\nBinarization is a common operation on text count data where the\nanalyst can decide to only consider the presence or absence of a\nfeature rather than a quantified number of occurrences for instance.\n\nIt can also be used as a pre-processing step for estimators that\nconsider boolean random variables (e.g. modelled using the Bernoulli\ndistribution in a Bayesian setting).\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.\n\nParameters\n----------\nthreshold : float, default=0.0\n    Feature values below or equal to this are replaced by 0, above it by 1.\n    Threshold may not be less than 0 for operations on sparse matrices.\n\ncopy : bool, default=True\n    Set to False to perform inplace binarization and avoid a copy (if\n    the input is already a numpy array or a scipy.sparse CSR matrix).\n\nAttributes\n----------\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nbinarize : Equivalent function without the estimator API.\nKBinsDiscretizer : Bin continuous data into intervals.\nOneHotEncoder : Encode categorical features as a one-hot numeric array.\n\nNotes\n-----\nIf the input is a sparse matrix, only the non-zero values are subject\nto update by the :class:`Binarizer` class.\n\nThis estimator is :term:`stateless` and does not need to be fitted.\nHowever, we recommend to call :meth:`fit_transform` instead of\n:meth:`transform`, as parameter validation is only performed in\n:meth:`fit`.\n\nExamples\n--------\n>>> from sklearn.preprocessing import Binarizer\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> transformer = Binarizer().fit(X)  # fit does nothing.\n>>> transformer\nBinarizer()\n>>> transformer.transform(X)\narray([[1., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nthreshold : float, default=0.5\n    The radius of the subcluster obtained by merging a new sample and the\n    closest subcluster should be lesser than the threshold. Otherwise a new\n    subcluster is started. Setting this value to be very low promotes\n    splitting and vice-versa.\n\nbranching_factor : int, default=50\n    Maximum number of CF subclusters in each node. If a new samples enters\n    such that the number of subclusters exceed the branching_factor then\n    that node is split into two nodes with the subclusters redistributed\n    in each. The parent subcluster of that node is removed and two new\n    subclusters are added as parents of the 2 split nodes.\n\nn_clusters : int, instance of sklearn.cluster model or None, default=3\n    Number of clusters after the final clustering step, which treats the\n    subclusters from the leaves as new samples.\n\n    - `None` : the final clustering step is not performed and the\n      subclusters are returned as they are.\n\n    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n      is fit treating the subclusters as new samples and the initial data\n      is mapped to the label of the closest subcluster.\n\n    - `int` : the model fit is :class:`AgglomerativeClustering` with\n      `n_clusters` set to be equal to the int.\n\ncompute_labels : bool, default=True\n    Whether or not to compute labels for each fit.\n\ncopy : bool, default=True\n    Whether or not to make a copy of the given data. If set to False,\n    the initial data will be overwritten.\n\nAttributes\n----------\nroot_ : _CFNode\n    Root of the CFTree.\n\ndummy_leaf_ : _CFNode\n    Start pointer to all the leaves.\n\nsubcluster_centers_ : ndarray\n    Centroids of all subclusters read directly from the leaves.\n\nsubcluster_labels_ : ndarray\n    Labels assigned to the centroids of the subclusters after\n    they are clustered globally.\n\nlabels_ : ndarray of shape (n_samples,)\n    Array of labels assigned to the input data.\n    if partial_fit is used instead of fit, they are assigned to the\n    last batch of data.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative implementation that does incremental updates\n    of the centers' positions using mini-batches.\n\nNotes\n-----\nThe tree data structure consists of nodes with each node consisting of\na number of subclusters. The maximum number of subclusters in a node\nis determined by the branching factor. Each subcluster maintains a\nlinear sum, squared sum and the number of samples in that subcluster.\nIn addition, each subcluster can also have a node as its child, if the\nsubcluster is not a member of a leaf node.\n\nFor a new point entering the root, it is merged with the subcluster closest\nto it and the linear sum, squared sum and the number of samples of that\nsubcluster are updated. This is done recursively till the properties of\nthe leaf node are updated.\n\nReferences\n----------\n* Tian Zhang, Raghu Ramakrishnan, Maron Livny\n  BIRCH: An efficient data clustering method for large databases.\n  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n* Roberto Perdisci\n  JBirch - Java implementation of BIRCH clustering algorithm\n  https://code.google.com/archive/p/jbirch\n\nExamples\n--------\n>>> from sklearn.cluster import Birch\n>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n>>> brc = Birch(n_clusters=None)\n>>> brc.fit(X)\nBirch(n_clusters=None)\n>>> brc.predict(X)\narray([0, 0, 0, 1, 1, 1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Bisecting K-Means clustering.\n\nRead more in the :ref:`User Guide <bisect_k_means>`.\n\n.. versionadded:: 1.1\n\nParameters\n----------\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'} or callable, default='random'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nn_init : int, default=1\n    Number of time the inner k-means algorithm will be run with different\n    centroid seeds in each bisection.\n    That will result producing for each bisection best output of n_init\n    consecutive runs in terms of inertia.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization\n    in inner K-Means. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the inner k-means algorithm at each\n    bisection.\n\nverbose : int, default=0\n    Verbosity mode.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations  to declare\n    convergence. Used in inner k-means algorithm at each bisection to pick\n    best possible clusters.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    Inner K-means algorithm used in bisection.\n    The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\nbisecting_strategy : {\"biggest_inertia\", \"largest_cluster\"},            default=\"biggest_inertia\"\n    Defines how bisection should be performed:\n\n     - \"biggest_inertia\" means that BisectingKMeans will always check\n        all calculated cluster for cluster with biggest SSE\n        (Sum of squared errors) and bisect it. This approach concentrates on\n        precision, but may be costly in terms of execution time (especially for\n        larger amount of data points).\n\n     - \"largest_cluster\" - BisectingKMeans will always split cluster with\n        largest amount of points assigned to it from all clusters\n        previously calculated. That should work faster than picking by SSE\n        ('biggest_inertia') and may produce similar results in most cases.\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\nSee Also\n--------\nKMeans : Original implementation of K-Means algorithm.\n\nNotes\n-----\nIt might be inefficient when n_cluster is less than 3, due to unnecessary\ncalculations for that case.\n\nExamples\n--------\n>>> from sklearn.cluster import BisectingKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 1], [10, 1], [3, 1],\n...               [10, 0], [2, 1], [10, 2],\n...               [10, 8], [10, 9], [10, 10]])\n>>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)\n>>> bisect_means.labels_\narray([0, 2, 0, 2, 0, 2, 1, 1, 1], dtype=int32)\n>>> bisect_means.predict([[0, 0], [12, 3]])\narray([0, 2], dtype=int32)\n>>> bisect_means.cluster_centers_\narray([[ 2., 1.],\n       [10., 9.],\n       [10., 1.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BoostingRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the Brier score loss.\n\nThe smaller the Brier score loss, the better, hence the naming with \"loss\".\nThe Brier score measures the mean squared difference between the predicted\nprobability and the actual outcome. The Brier score always\ntakes on a value between zero and one, since this is the largest\npossible difference between a predicted probability (which must be\nbetween zero and one) and the actual outcome (which can take on values\nof only 0 and 1). It can be decomposed as the sum of refinement loss and\ncalibration loss.\n\nThe Brier score is appropriate for binary and categorical outcomes that\ncan be structured as true or false, but is inappropriate for ordinal\nvariables which can take on three or more values (this is because the\nBrier score assumes that all possible outcomes are equivalently\n\"distant\" from one another). Which label is considered to be the positive\nlabel is controlled via the parameter `pos_label`, which defaults to\nthe greater label unless `y_true` is all 0 or all -1, in which case\n`pos_label` defaults to 1.\n\nRead more in the :ref:`User Guide <brier_score_loss>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True targets.\n\ny_prob : array-like of shape (n_samples,)\n    Probabilities of the positive class.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\npos_label : int, float, bool or str, default=None\n    Label of the positive class. `pos_label` will be inferred in the\n    following manner:\n\n    * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n    * else if `y_true` contains string, an error will be raised and\n      `pos_label` should be explicitly specified;\n    * otherwise, `pos_label` defaults to the greater label,\n      i.e. `np.unique(y_true)[-1]`.\n\nReturns\n-------\nscore : float\n    Brier score loss.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Brier score\n        <https://en.wikipedia.org/wiki/Brier_score>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import brier_score_loss\n>>> y_true = np.array([0, 1, 1, 0])\n>>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n>>> brier_score_loss(y_true, y_prob)\n0.037...\n>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n0.037...\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n0.037...\n>>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n0.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CalinskiHarabaszScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the Calinski and Harabasz score.\n\nIt is also known as the Variance Ratio Criterion.\n\nThe score is defined as ratio of the sum of between-cluster dispersion and\nof within-cluster dispersion.\n\nRead more in the :ref:`User Guide <calinski_harabasz_index>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    A list of ``n_features``-dimensional data points. Each row corresponds\n    to a single data point.\n\nlabels : array-like of shape (n_samples,)\n    Predicted labels for each sample.\n\nReturns\n-------\nscore : float\n    The resulting Calinski-Harabasz score.\n\nReferences\n----------\n.. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n   analysis\". Communications in Statistics\n   <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n\nExamples\n--------\n>>> from sklearn.datasets import make_blobs\n>>> from sklearn.cluster import KMeans\n>>> from sklearn.metrics import calinski_harabasz_score\n>>> X, _ = make_blobs(random_state=0)\n>>> kmeans = KMeans(n_clusters=3, random_state=0,).fit(X)\n>>> calinski_harabasz_score(X, kmeans.labels_)\n114.8..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Naive Bayes classifier for categorical features.\n\nThe categorical Naive Bayes classifier is suitable for classification with\ndiscrete features that are categorically distributed. The categories of\neach feature are drawn from a categorical distribution.\n\nRead more in the :ref:`User Guide <categorical_naive_bayes>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing).\n\nforce_alpha : bool, default=True\n    If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`.\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nmin_categories : int or array-like of shape (n_features,), default=None\n    Minimum number of categories per feature.\n\n    - integer: Sets the minimum number of categories per feature to\n      `n_categories` for each features.\n    - array-like: shape (n_features,) where `n_categories[i]` holds the\n      minimum number of categories for the ith column of the input.\n    - None (default): Determines the number of categories automatically\n      from the training data.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncategory_count_ : list of arrays of shape (n_features,)\n    Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the number of samples\n    encountered for each class and category of the specific feature.\n\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes,)\n    Smoothed empirical log probability for each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\nfeature_log_prob_ : list of arrays of shape (n_features,)\n    Holds arrays of shape (n_classes, n_categories of respective feature)\n    for each feature. Each array provides the empirical log probability\n    of categories given the respective feature and class, ``P(x_i|y)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_categories_ : ndarray of shape (n_features,), dtype=np.int64\n    Number of categories for each feature. This value is\n    inferred from the data or set by the minimum number of categories.\n\n    .. versionadded:: 0.24\n\nSee Also\n--------\nBernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nComplementNB : Complement Naive Bayes classifier.\nGaussianNB : Gaussian Naive Bayes.\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import CategoricalNB\n>>> clf = CategoricalNB()\n>>> clf.fit(X, y)\nCategoricalNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Determine scorer from user options.\n\nA TypeError will be thrown if the estimator cannot be scored.\n\nParameters\n----------\nestimator : estimator object implementing 'fit'\n    The object to use to fit the data.\n\nscoring : str or callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n    If None, the provided estimator object's `score` method is used.\n\nallow_none : bool, default=False\n    If no scoring is specified and the estimator has no score function, we\n    can either return None or raise an exception.\n\nReturns\n-------\nscoring : callable\n    A scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.metrics import check_scoring\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)\n>>> scorer = check_scoring(classifier, scoring='accuracy')\n>>> scorer(classifier, X, y)\n0.96..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Chi2Method",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute chi-squared stats between each non-negative feature and class.\n\nThis score can be used to select the `n_features` features with the\nhighest values for the test chi-squared statistic from X, which must\ncontain only **non-negative features** such as booleans or frequencies\n(e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic\nvariables, so using this function \"weeds out\" the features that are the\nmost likely to be independent of class and therefore irrelevant for\nclassification.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Sample vectors.\n\ny : array-like of shape (n_samples,)\n    Target vector (class labels).\n\nReturns\n-------\nchi2 : ndarray of shape (n_features,)\n    Chi2 statistics for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values for each feature.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\n\nNotes\n-----\nComplexity of this algorithm is O(n_classes * n_features).\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.feature_selection import chi2\n>>> X = np.array([[1, 1, 3],\n...               [0, 1, 5],\n...               [5, 4, 1],\n...               [6, 6, 2],\n...               [1, 4, 0],\n...               [0, 0, 0]])\n>>> y = np.array([1, 1, 0, 0, 2, 2])\n>>> chi2_stats, p_values = chi2(X, y)\n>>> chi2_stats\narray([15.3...,  6.5       ,  8.9...])\n>>> p_values\narray([0.0004..., 0.0387..., 0.0116... ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute binary classification positive and negative likelihood ratios.\n\nThe positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`\nwhere the sensitivity or recall is the ratio `tp / (tp + fn)` and the\nspecificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1\n- sensitivity) / specificity`. Here `tp` is the number of true positives,\n`fp` the number of false positives, `tn` is the number of true negatives and\n`fn` the number of false negatives. Both class likelihood ratios can be used\nto obtain post-test probabilities given a pre-test probability.\n\n`LR+` ranges from 1 to infinity. A `LR+` of 1 indicates that the probability\nof predicting the positive class is the same for samples belonging to either\nclass; therefore, the test is useless. The greater `LR+` is, the more a\npositive prediction is likely to be a true positive when compared with the\npre-test probability. A value of `LR+` lower than 1 is invalid as it would\nindicate that the odds of a sample being a true positive decrease with\nrespect to the pre-test odds.\n\n`LR-` ranges from 0 to 1. The closer it is to 0, the lower the probability\nof a given sample to be a false negative. A `LR-` of 1 means the test is\nuseless because the odds of having the condition did not change after the\ntest. A value of `LR-` greater than 1 invalidates the classifier as it\nindicates an increase in the odds of a sample belonging to the positive\nclass after being classified as negative. This is the case when the\nclassifier systematically predicts the opposite of the true label.\n\nA typical application in medicine is to identify the positive/negative class\nto the presence/absence of a disease, respectively; the classifier being a\ndiagnostic test; the pre-test probability of an individual having the\ndisease can be the prevalence of such disease (proportion of a particular\npopulation found to be affected by a medical condition); and the post-test\nprobabilities would be the probability that the condition is truly present\ngiven a positive test result.\n\nRead more in the :ref:`User Guide <class_likelihood_ratios>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    List of labels to index the matrix. This may be used to select the\n    positive and negative classes with the ordering `labels=[negative_class,\n    positive_class]`. If `None` is given, those that appear at least once in\n    `y_true` or `y_pred` are used in sorted order.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nraise_warning : bool, default=True\n    Whether or not a case-specific warning message is raised when there is a\n    zero division. Even if the error is not raised, the function will return\n    nan in such cases.\n\nReturns\n-------\n(positive_likelihood_ratio, negative_likelihood_ratio) : tuple\n    A tuple of two float, the first containing the Positive likelihood ratio\n    and the second the Negative likelihood ratio.\n\nWarns\n-----\nWhen `false positive == 0`, the positive likelihood ratio is undefined.\nWhen `true negative == 0`, the negative likelihood ratio is undefined.\nWhen `true positive + false negative == 0` both ratios are undefined.\nIn such cases, `UserWarning` will be raised if raise_warning=True.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing\n       <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import class_likelihood_ratios\n>>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])\n(1.5, 0.75)\n>>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n>>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n>>> class_likelihood_ratios(y_true, y_pred)\n(1.33..., 0.66...)\n>>> y_true = np.array([\"non-zebra\", \"zebra\", \"non-zebra\", \"zebra\", \"non-zebra\"])\n>>> y_pred = np.array([\"zebra\", \"zebra\", \"non-zebra\", \"non-zebra\", \"non-zebra\"])\n>>> class_likelihood_ratios(y_true, y_pred)\n(1.5, 0.75)\n\nTo avoid ambiguities, use the notation `labels=[negative_class,\npositive_class]`\n\n>>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n>>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n>>> class_likelihood_ratios(y_true, y_pred, labels=[\"non-cat\", \"cat\"])\n(1.5, 0.75)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Build a text report showing the main classification metrics.\n\nRead more in the :ref:`User Guide <classification_report>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like of shape (n_labels,), default=None\n    Optional list of label indices to include in the report.\n\ntarget_names : array-like of shape (n_labels,), default=None\n    Optional display names matching the labels (same order).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndigits : int, default=2\n    Number of digits for formatting output floating point values.\n    When ``output_dict`` is ``True``, this will be ignored and the\n    returned values will not be rounded.\n\noutput_dict : bool, default=False\n    If True, return output as dict.\n\n    .. versionadded:: 0.20\n\nzero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n    Sets the value to return when there is a zero division. If set to\n    \"warn\", this acts as 0, but warnings are also raised.\n\n    .. versionadded:: 1.3\n       `np.nan` option was added.\n\nReturns\n-------\nreport : str or dict\n    Text summary of the precision, recall, F1 score for each class.\n    Dictionary returned if output_dict is True. Dictionary has the\n    following structure::\n\n        {'label 1': {'precision':0.5,\n                     'recall':1.0,\n                     'f1-score':0.67,\n                     'support':1},\n         'label 2': { ... },\n          ...\n        }\n\n    The reported averages include macro average (averaging the unweighted\n    mean per label), weighted average (averaging the support-weighted mean\n    per label), and sample average (only for multilabel classification).\n    Micro average (averaging the total true positives, false negatives and\n    false positives) is only shown for multi-label or multi-class\n    with a subset of classes, because it corresponds to accuracy\n    otherwise and would be the same for all metrics.\n    See also :func:`precision_recall_fscore_support` for more details\n    on averages.\n\n    Note that in binary classification, recall of the positive class\n    is also known as \"sensitivity\"; recall of the negative class is\n    \"specificity\".\n\nSee Also\n--------\nprecision_recall_fscore_support: Compute precision, recall, F-measure and\n    support for each class.\nconfusion_matrix: Compute confusion matrix to evaluate the accuracy of a\n    classification.\nmultilabel_confusion_matrix: Compute a confusion matrix for each class or sample.\n\nExamples\n--------\n>>> from sklearn.metrics import classification_report\n>>> y_true = [0, 1, 2, 2, 2]\n>>> y_pred = [0, 0, 2, 2, 1]\n>>> target_names = ['class 0', 'class 1', 'class 2']\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\n              precision    recall  f1-score   support\n<BLANKLINE>\n     class 0       0.50      1.00      0.67         1\n     class 1       0.00      0.00      0.00         1\n     class 2       1.00      0.67      0.80         3\n<BLANKLINE>\n    accuracy                           0.60         5\n   macro avg       0.50      0.56      0.49         5\nweighted avg       0.70      0.60      0.61         5\n<BLANKLINE>\n>>> y_pred = [1, 1, 0]\n>>> y_true = [1, 1, 1]\n>>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n              precision    recall  f1-score   support\n<BLANKLINE>\n           1       1.00      0.67      0.80         3\n           2       0.00      0.00      0.00         0\n           3       0.00      0.00      0.00         0\n<BLANKLINE>\n   micro avg       1.00      0.67      0.80         3\n   macro avg       0.33      0.22      0.27         3\nweighted avg       1.00      0.67      0.80         3\n<BLANKLINE>"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Clustering",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\n\nThis function computes Cohen's kappa [1]_, a score that expresses the level\nof agreement between two annotators on a classification problem. It is\ndefined as\n\n.. math::\n    \\kappa = (p_o - p_e) / (1 - p_e)\n\nwhere :math:`p_o` is the empirical probability of agreement on the label\nassigned to any sample (the observed agreement ratio), and :math:`p_e` is\nthe expected agreement when both annotators assign labels randomly.\n:math:`p_e` is estimated using a per-annotator empirical prior over the\nclass labels [2]_.\n\nRead more in the :ref:`User Guide <cohen_kappa>`.\n\nParameters\n----------\ny1 : array-like of shape (n_samples,)\n    Labels assigned by the first annotator.\n\ny2 : array-like of shape (n_samples,)\n    Labels assigned by the second annotator. The kappa statistic is\n    symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n\nlabels : array-like of shape (n_classes,), default=None\n    List of labels to index the matrix. This may be used to select a\n    subset of labels. If `None`, all labels that appear at least once in\n    ``y1`` or ``y2`` are used.\n\nweights : {'linear', 'quadratic'}, default=None\n    Weighting type to calculate the score. `None` means no weighted;\n    \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nkappa : float\n    The kappa statistic, which is a number between -1 and 1. The maximum\n    value means complete agreement; zero or lower means chance agreement.\n\nReferences\n----------\n.. [1] :doi:`J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n       Educational and Psychological Measurement 20(1):37-46.\n       <10.1177/001316446002000104>`\n.. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n       computational linguistics\". Computational Linguistics 34(4):555-596\n       <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n.. [3] `Wikipedia entry for the Cohen's kappa\n        <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import cohen_kappa_score\n>>> y1 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n>>> y2 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"negative\"]\n>>> cohen_kappa_score(y1, y2)\n0.6875"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "The Complement Naive Bayes classifier described in Rennie et al. (2003).\n\nThe Complement Naive Bayes classifier was designed to correct the \"severe\nassumptions\" made by the standard Multinomial Naive Bayes classifier. It is\nparticularly suited for imbalanced data sets.\n\nRead more in the :ref:`User Guide <complement_naive_bayes>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nalpha : float or array-like of shape (n_features,), default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing).\n\nforce_alpha : bool, default=True\n    If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`.\n\nfit_prior : bool, default=True\n    Only used in edge case with a single class in the training set.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. Not used.\n\nnorm : bool, default=False\n    Whether or not a second normalization of the weights is performed. The\n    default behavior mirrors the implementations found in Mahout and Weka,\n    which do not follow the full algorithm described in Table 9 of the\n    paper.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes,)\n    Smoothed empirical log probability for each class. Only used in edge\n    case with a single class in the training set.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\nfeature_all_ : ndarray of shape (n_features,)\n    Number of samples encountered for each feature during fitting. This\n    value is weighted by the sample weight when provided.\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature) during fitting.\n    This value is weighted by the sample weight when provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical weights for class complements.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nBernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nGaussianNB : Gaussian Naive Bayes.\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nReferences\n----------\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\nTackling the poor assumptions of naive bayes text classifiers. In ICML\n(Vol. 3, pp. 616-623).\nhttps://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import ComplementNB\n>>> clf = ComplementNB()\n>>> clf.fit(X, y)\nComplementNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CompletenessScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute completeness metric of a cluster labeling given a ground truth.\n\nA clustering result satisfies completeness if all the data points\nthat are members of a given class are elements of the same cluster.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is not symmetric: switching ``label_true`` with ``label_pred``\nwill return the :func:`homogeneity_score` which will be different in\ngeneral.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,)\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,)\n    Cluster labels to evaluate.\n\nReturns\n-------\ncompleteness : float\n   Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n\nSee Also\n--------\nhomogeneity_score : Homogeneity metric of cluster labeling.\nv_measure_score : V-Measure (NMI with arithmetic mean option).\n\nReferences\n----------\n\n.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n   conditional entropy-based external cluster evaluation measure\n   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n\nExamples\n--------\n\nPerfect labelings are complete::\n\n  >>> from sklearn.metrics.cluster import completeness_score\n  >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nNon-perfect labelings that assign all classes members to the same clusters\nare still complete::\n\n  >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n  1.0\n  >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n  0.999...\n\nIf classes members are split across different clusters, the\nassignment cannot be complete::\n\n  >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n  0.0\n  >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n  0.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute confusion matrix to evaluate the accuracy of a classification.\n\nBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\nis equal to the number of observations known to be in group :math:`i` and\npredicted to be in group :math:`j`.\n\nThus in binary classification, the count of true negatives is\n:math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n:math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n\nRead more in the :ref:`User Guide <confusion_matrix>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated targets as returned by a classifier.\n\nlabels : array-like of shape (n_classes), default=None\n    List of labels to index the matrix. This may be used to reorder\n    or select a subset of labels.\n    If ``None`` is given, those that appear at least once\n    in ``y_true`` or ``y_pred`` are used in sorted order.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.18\n\nnormalize : {'true', 'pred', 'all'}, default=None\n    Normalizes confusion matrix over the true (rows), predicted (columns)\n    conditions or all the population. If None, confusion matrix will not be\n    normalized.\n\nReturns\n-------\nC : ndarray of shape (n_classes, n_classes)\n    Confusion matrix whose i-th row and j-th\n    column entry indicates the number of\n    samples with true label being i-th class\n    and predicted label being j-th class.\n\nSee Also\n--------\nConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n    given an estimator, the data, and the label.\nConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n    given the true and predicted labels.\nConfusionMatrixDisplay : Confusion Matrix visualization.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Confusion matrix\n       <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n       (Wikipedia and other references may use a different\n       convention for axes).\n\nExamples\n--------\n>>> from sklearn.metrics import confusion_matrix\n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> confusion_matrix(y_true, y_pred)\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n\n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n\nIn the binary case, we can extract true positives, etc. as follows:\n\n>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n>>> (tn, fp, fn, tp)\n(0, 2, 1, 1)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "The similarity of two sets of biclusters.\n\nSimilarity between individual biclusters is computed. Then the\nbest matching between sets is found using the Hungarian algorithm.\nThe final score is the sum of similarities divided by the size of\nthe larger set.\n\nRead more in the :ref:`User Guide <biclustering>`.\n\nParameters\n----------\na : tuple (rows, columns)\n    Tuple of row and column indicators for a set of biclusters.\n\nb : tuple (rows, columns)\n    Another set of biclusters like ``a``.\n\nsimilarity : 'jaccard' or callable, default='jaccard'\n    May be the string \"jaccard\" to use the Jaccard coefficient, or\n    any function that takes four arguments, each of which is a 1d\n    indicator vector: (a_rows, a_columns, b_rows, b_columns).\n\nReturns\n-------\nconsensus_score : float\n   Consensus score, a non-negative value, sum of similarities\n   divided by size of larger set.\n\nReferences\n----------\n\n* Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n  for bicluster acquisition\n  <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n\nExamples\n--------\n>>> from sklearn.metrics import consensus_score\n>>> a = ([[True, False], [False, True]], [[False, True], [True, False]])\n>>> b = ([[False, True], [True, False]], [[True, False], [False, True]])\n>>> consensus_score(a, b, similarity='jaccard')\n1.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CoverageErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Coverage error measure.\n\nCompute how far we need to go through the ranked scores to cover all\ntrue labels. The best value is equal to the average number\nof labels in ``y_true`` per sample.\n\nTies in ``y_scores`` are broken by giving maximal rank that would have\nbeen assigned to all tied values.\n\nNote: Our implementation's score is 1 greater than the one given in\nTsoumakas et al., 2010. This extends it to handle the degenerate case\nin which an instance has 0 true labels.\n\nRead more in the :ref:`User Guide <coverage_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples, n_labels)\n    True binary labels in binary indicator format.\n\ny_score : array-like of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\ncoverage_error : float\n    The coverage error.\n\nReferences\n----------\n.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n       Mining multi-label data. In Data mining and knowledge discovery\n       handbook (pp. 667-685). Springer US.\n\nExamples\n--------\n>>> from sklearn.metrics import coverage_error\n>>> y_true = [[1, 0, 0], [0, 1, 1]]\n>>> y_score = [[1, 0, 0], [0, 1, 1]]\n>>> coverage_error(y_true, y_score)\n1.5"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ":math:`D^2` regression score function, fraction of absolute error explained.\n\nBest possible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A model that always uses the empirical median of `y_true`\nas constant prediction, disregarding the input features,\ngets a :math:`D^2` score of 0.0.\n\nRead more in the :ref:`User Guide <d2_score>`.\n\n.. versionadded:: 1.1\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average scores.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Scores of all outputs are averaged with uniform weight.\n\nReturns\n-------\nscore : float or ndarray of floats\n    The :math:`D^2` score with an absolute error deviance\n    or ndarray of scores if 'multioutput' is 'raw_values'.\n\nNotes\n-----\nLike :math:`R^2`, :math:`D^2` score may be negative\n(it need not actually be the square of a quantity D).\n\nThis metric is not well-defined for single samples and will return a NaN\nvalue if n_samples is less than two.\n\n References\n----------\n.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n       Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n       Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n\nExamples\n--------\n>>> from sklearn.metrics import d2_absolute_error_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> d2_absolute_error_score(y_true, y_pred)\n0.764...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')\n0.691...\n>>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')\narray([0.8125    , 0.57142857])\n>>> y_true = [1, 2, 3]\n>>> y_pred = [1, 2, 3]\n>>> d2_absolute_error_score(y_true, y_pred)\n1.0\n>>> y_true = [1, 2, 3]\n>>> y_pred = [2, 2, 2]\n>>> d2_absolute_error_score(y_true, y_pred)\n0.0\n>>> y_true = [1, 2, 3]\n>>> y_pred = [3, 2, 1]\n>>> d2_absolute_error_score(y_true, y_pred)\n-1.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ":math:`D^2` regression score function, fraction of pinball loss explained.\n\nBest possible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A model that always uses the empirical alpha-quantile of\n`y_true` as constant prediction, disregarding the input features,\ngets a :math:`D^2` score of 0.0.\n\nRead more in the :ref:`User Guide <d2_score>`.\n\n.. versionadded:: 1.1\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nalpha : float, default=0.5\n    Slope of the pinball deviance. It determines the quantile level alpha\n    for which the pinball deviance and also D2 are optimal.\n    The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average scores.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Scores of all outputs are averaged with uniform weight.\n\nReturns\n-------\nscore : float or ndarray of floats\n    The :math:`D^2` score with a pinball deviance\n    or ndarray of scores if `multioutput='raw_values'`.\n\nNotes\n-----\nLike :math:`R^2`, :math:`D^2` score may be negative\n(it need not actually be the square of a quantity D).\n\nThis metric is not well-defined for a single point and will return a NaN\nvalue if n_samples is less than two.\n\n References\n----------\n.. [1] Eq. (7) of `Koenker, Roger; Machado, Jos√© A. F. (1999).\n       \"Goodness of Fit and Related Inference Processes for Quantile Regression\"\n       <https://doi.org/10.1080/01621459.1999.10473882>`_\n.. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n       Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n       Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n\nExamples\n--------\n>>> from sklearn.metrics import d2_pinball_score\n>>> y_true = [1, 2, 3]\n>>> y_pred = [1, 3, 3]\n>>> d2_pinball_score(y_true, y_pred)\n0.5\n>>> d2_pinball_score(y_true, y_pred, alpha=0.9)\n0.772...\n>>> d2_pinball_score(y_true, y_pred, alpha=0.1)\n-1.045...\n>>> d2_pinball_score(y_true, y_true, alpha=0.1)\n1.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ":math:`D^2` regression score function, fraction of Tweedie deviance explained.\n\nBest possible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A model that always uses the empirical mean of `y_true` as\nconstant prediction, disregarding the input features, gets a D^2 score of 0.0.\n\nRead more in the :ref:`User Guide <d2_score>`.\n\n.. versionadded:: 1.0\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\npower : float, default=0\n    Tweedie power parameter. Either power <= 0 or power >= 1.\n\n    The higher `p` the less weight is given to extreme\n    deviations between true and predicted targets.\n\n    - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n    - power = 0 : Normal distribution, output corresponds to r2_score.\n      y_true and y_pred can be any real numbers.\n    - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n      y_pred > 0.\n    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n      and y_pred > 0.\n    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n      and y_pred > 0.\n    - otherwise : Positive stable distribution. Requires: y_true > 0\n      and y_pred > 0.\n\nReturns\n-------\nz : float or ndarray of floats\n    The D^2 score.\n\nNotes\n-----\nThis is not a symmetric function.\n\nLike R^2, D^2 score may be negative (it need not actually be the square of\na quantity D).\n\nThis metric is not well-defined for single samples and will return a NaN\nvalue if n_samples is less than two.\n\nReferences\n----------\n.. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n       Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n       Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n\nExamples\n--------\n>>> from sklearn.metrics import d2_tweedie_score\n>>> y_true = [0.5, 1, 2.5, 7]\n>>> y_pred = [1, 1, 5, 3.5]\n>>> d2_tweedie_score(y_true, y_pred)\n0.285...\n>>> d2_tweedie_score(y_true, y_pred, power=1)\n0.487...\n>>> d2_tweedie_score(y_true, y_pred, power=2)\n0.630...\n>>> d2_tweedie_score(y_true, y_true, power=2)\n1.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nThe worst case memory complexity of DBSCAN is :math:`O({n}^2)`, which can\noccur when the `eps` param is large and `min_samples` is low.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point to\n    be considered as a core point. This includes the point itself. If\n    `min_samples` is set to a higher value, DBSCAN will find denser clusters,\n    whereas if it is set to a lower value, the found clusters will be more\n    sparse.\n\nmetric : str, or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n    .. versionadded:: 0.17\n       metric *precomputed* to accept precomputed sparse matrix.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=None\n    The power of the Minkowski metric to be used to calculate distance\n    between points. If None, then ``p=2`` (equivalent to the Euclidean\n    distance).\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\ncore_sample_indices_ : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\ncomponents_ : ndarray of shape (n_core_samples, n_features)\n    Copy of each core sample found by training.\n\nlabels_ : ndarray of shape (n_samples)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples are given the label -1.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nOPTICS : A similar clustering at multiple values of eps. Our implementation\n    is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory\nusage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n<https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n:doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n<10.1145/3068335>`\nACM Transactions on Database Systems (TODS), 42(3), 19.\n\nExamples\n--------\n>>> from sklearn.cluster import DBSCAN\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 2], [2, 3],\n...               [8, 7], [8, 8], [25, 80]])\n>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n>>> clustering.labels_\narray([ 0,  0,  0,  1,  1, -1])\n>>> clustering\nDBSCAN(eps=3, min_samples=2)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataProcessing",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplitting",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DaviesBouldinScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the Davies-Bouldin score.\n\nThe score is defined as the average similarity measure of each cluster with\nits most similar cluster, where similarity is the ratio of within-cluster\ndistances to between-cluster distances. Thus, clusters which are farther\napart and less dispersed will result in a better score.\n\nThe minimum score is zero, with lower values indicating better clustering.\n\nRead more in the :ref:`User Guide <davies-bouldin_index>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    A list of ``n_features``-dimensional data points. Each row corresponds\n    to a single data point.\n\nlabels : array-like of shape (n_samples,)\n    Predicted labels for each sample.\n\nReturns\n-------\nscore: float\n    The resulting Davies-Bouldin score.\n\nReferences\n----------\n.. [1] Davies, David L.; Bouldin, Donald W. (1979).\n   `\"A Cluster Separation Measure\"\n   <https://ieeexplore.ieee.org/document/4766909>`__.\n   IEEE Transactions on Pattern Analysis and Machine Intelligence.\n   PAMI-1 (2): 224-227\n\nExamples\n--------\n>>> from sklearn.metrics import davies_bouldin_score\n>>> X = [[0, 1], [1, 1], [3, 4]]\n>>> labels = [0, 0, 1]\n>>> davies_bouldin_score(X, labels)\n0.12..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Discounted Cumulative Gain.\n\nSum the true scores ranked in the order induced by the predicted scores,\nafter applying a logarithmic discount.\n\nThis ranking metric yields a high value if true labels are ranked high by\n``y_score``.\n\nUsually the Normalized Discounted Cumulative Gain (NDCG, computed by\nndcg_score) is preferred.\n\nParameters\n----------\ny_true : array-like of shape (n_samples, n_labels)\n    True targets of multilabel classification, or true scores of entities\n    to be ranked.\n\ny_score : array-like of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates, confidence values,\n    or non-thresholded measure of decisions (as returned by\n    \"decision_function\" on some classifiers).\n\nk : int, default=None\n    Only consider the highest k scores in the ranking. If None, use all\n    outputs.\n\nlog_base : float, default=2\n    Base of the logarithm used for the discount. A low value means a\n    sharper discount (top results are more important).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If `None`, all samples are given the same weight.\n\nignore_ties : bool, default=False\n    Assume that there are no ties in y_score (which is likely to be the\n    case if y_score is continuous) for efficiency gains.\n\nReturns\n-------\ndiscounted_cumulative_gain : float\n    The averaged sample DCG scores.\n\nSee Also\n--------\nndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n    Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n    have a score between 0 and 1.\n\nReferences\n----------\n`Wikipedia entry for Discounted Cumulative Gain\n<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n\nJarvelin, K., & Kekalainen, J. (2002).\nCumulated gain-based evaluation of IR techniques. ACM Transactions on\nInformation Systems (TOIS), 20(4), 422-446.\n\nWang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\nA theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\nAnnual Conference on Learning Theory (COLT 2013).\n\nMcSherry, F., & Najork, M. (2008, March). Computing information retrieval\nperformance measures efficiently in the presence of tied scores. In\nEuropean conference on information retrieval (pp. 414-421). Springer,\nBerlin, Heidelberg.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import dcg_score\n>>> # we have groud-truth relevance of some answers to a query:\n>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n>>> # we predict scores for the answers\n>>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n>>> dcg_score(true_relevance, scores)\n9.49...\n>>> # we can set k to truncate the sum; only top k answers contribute\n>>> dcg_score(true_relevance, scores, k=2)\n5.63...\n>>> # now we have some ties in our prediction\n>>> scores = np.asarray([[1, 0, 0, 0, 1]])\n>>> # by default ties are averaged, so here we get the average true\n>>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n>>> dcg_score(true_relevance, scores, k=1)\n7.5\n>>> # we can choose to ignore ties for faster results, but only\n>>> # if we know there aren't ties in our scores, otherwise we get\n>>> # wrong results:\n>>> dcg_score(true_relevance,\n...           scores, k=1, ignore_ties=True)\n5.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "A decision tree classifier.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at\n          each split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes > 2`),\n      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeRegressor : A decision tree regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe :meth:`predict` method operates using the :func:`numpy.argmax`\nfunction on the outputs of :meth:`predict_proba`. This means that in\ncase the highest predicted probabilities are tied, the classifier will\npredict the tied class with the lowest index in :term:`classes_`.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> clf = DecisionTreeClassifier(random_state=0)\n>>> iris = load_iris()\n>>> cross_val_score(clf, iris.data, iris.target, cv=10)\n...                             # doctest: +SKIP\n...\narray([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "A decision tree regressor.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\",             \"poisson\"}, default=\"squared_error\"\n    The function to measure the quality of a split. Supported criteria\n    are \"squared_error\" for the mean squared error, which is equal to\n    variance reduction as feature selection criterion and minimizes the L2\n    loss using the mean of each terminal node, \"friedman_mse\", which uses\n    mean squared error with Friedman's improvement score for potential\n    splits, \"absolute_error\" for the mean absolute error, which minimizes\n    the L1 loss using the median of each terminal node, and \"poisson\" which\n    uses reduction in Poisson deviance to find splits.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\n    .. versionadded:: 0.24\n        Poisson deviance criterion.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary <random_state>` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multioutput regressions (i.e. when `n_outputs_ > 1`),\n      - regressions trained on data with missing values.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nfeature_importances_ : ndarray of shape (n_features,)\n    The feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the\n    (normalized) total reduction of the criterion brought\n    by that feature. It is also known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeClassifier : A decision tree classifier.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> regressor = DecisionTreeRegressor(random_state=0)\n>>> cross_val_score(regressor, X, y, cv=10)\n...                    # doctest: +SKIP\n...\narray([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Decomposition",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute error rates for different probability thresholds.\n\n.. note::\n   This metric is used for evaluation of ranking and error tradeoffs of\n   a binary classification task.\n\nRead more in the :ref:`User Guide <det_curve>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\ny_true : ndarray of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\ny_score : ndarray of shape of (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\npos_label : int, float, bool or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nfpr : ndarray of shape (n_thresholds,)\n    False positive rate (FPR) such that element i is the false positive\n    rate of predictions with score >= thresholds[i]. This is occasionally\n    referred to as false acceptance probability or fall-out.\n\nfnr : ndarray of shape (n_thresholds,)\n    False negative rate (FNR) such that element i is the false negative\n    rate of predictions with score >= thresholds[i]. This is occasionally\n    referred to as false rejection or miss rate.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Decreasing score values.\n\nSee Also\n--------\nDetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n    some data.\nDetCurveDisplay.from_predictions : Plot DET curve given the true and\n    predicted labels.\nDetCurveDisplay : DET curve visualization.\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\nprecision_recall_curve : Compute precision-recall curve.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import det_curve\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n>>> fpr\narray([0.5, 0.5, 0. ])\n>>> fnr\narray([0. , 0.5, 0.5])\n>>> thresholds\narray([0.35, 0.4 , 0.8 ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transforms lists of feature-value mappings to vectors.\n\nThis transformer turns lists of mappings (dict-like objects) of feature\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\nwith scikit-learn estimators.\n\nWhen feature values are strings, this transformer will do a binary one-hot\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\nof the possible string values that the feature can take on. For instance,\na feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\nfeatures in the output, one signifying \"f=ham\", the other \"f=spam\".\n\nIf a feature value is a sequence or set of strings, this transformer\nwill iterate over the values and will count the occurrences of each string\nvalue.\n\nHowever, note that this transformer will only do a binary one-hot encoding\nwhen feature values are of type string. If categorical features are\nrepresented as numeric values such as int or iterables of strings, the\nDictVectorizer can be followed by\n:class:`~sklearn.preprocessing.OneHotEncoder` to complete\nbinary one-hot encoding.\n\nFeatures that do not occur in a sample (mapping) will have a zero value\nin the resulting array/matrix.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <dict_feature_extraction>`.\n\nParameters\n----------\ndtype : dtype, default=np.float64\n    The type of feature values. Passed to Numpy array/scipy.sparse matrix\n    constructors as the dtype argument.\nseparator : str, default=\"=\"\n    Separator string used when constructing new features for one-hot\n    coding.\nsparse : bool, default=True\n    Whether transform should produce scipy.sparse matrices.\nsort : bool, default=True\n    Whether ``feature_names_`` and ``vocabulary_`` should be\n    sorted when fitting.\n\nAttributes\n----------\nvocabulary_ : dict\n    A dictionary mapping feature names to feature indices.\n\nfeature_names_ : list\n    A list of length n_features containing the feature names (e.g., \"f=ham\"\n    and \"f=spam\").\n\nSee Also\n--------\nFeatureHasher : Performs vectorization using only a hash function.\nsklearn.preprocessing.OrdinalEncoder : Handles nominal/categorical\n    features encoded as columns of arbitrary data types.\n\nExamples\n--------\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> v = DictVectorizer(sparse=False)\n>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n>>> X = v.fit_transform(D)\n>>> X\narray([[2., 0., 1.],\n       [0., 1., 3.]])\n>>> v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},\n...                            {'baz': 1.0, 'foo': 3.0}]\nTrue\n>>> v.transform({'foo': 4, 'unseen_feature': 3})\narray([[0., 0., 4.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Dictionary learning.\n\nFinds a dictionary (a set of atoms) that performs well at sparsely\nencoding the fitted data.\n\nSolves the optimization problem::\n\n    (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                (U,V)\n                with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\nthe entry-wise matrix norm which is the sum of the absolute values\nof all the entries in the matrix.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of dictionary elements to extract. If None, then ``n_components``\n    is set to ``n_features``.\n\nalpha : float, default=1.0\n    Sparsity controlling parameter.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for numerical error.\n\nfit_algorithm : {'lars', 'cd'}, default='lars'\n    * `'lars'`: uses the least angle regression method to solve the lasso\n      problem (:func:`~sklearn.linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n      faster if the estimated components are sparse.\n\n    .. versionadded:: 0.17\n       *cd* coordinate descent method to improve speed.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (:func:`~sklearn.linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n      will be faster if the estimated components are sparse.\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution.\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\n    .. versionadded:: 0.17\n       *lasso_cd* coordinate descent method to improve speed.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and\n    `algorithm='omp'`. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `None`, defaults to `alpha`.\n\n    .. versionchanged:: 1.2\n        When None, default value changed from 1.0 to `alpha`.\n\nn_jobs : int or None, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ncode_init : ndarray of shape (n_samples, n_components), default=None\n    Initial value for the code, for warm restart. Only used if `code_init`\n    and `dict_init` are not None.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial values for the dictionary, for warm restart. Only used if\n    `code_init` and `dict_init` are not None.\n\ncallback : callable, default=None\n    Callable that gets invoked every five iterations.\n\n    .. versionadded:: 1.3\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    dictionary atoms extracted from the data\n\nerror_ : array\n    vector of errors at each iteration\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of iterations run.\n\nSee Also\n--------\nMiniBatchDictionaryLearning: A faster, less accurate, version of the\n    dictionary learning algorithm.\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nSparseCoder : Find a sparse representation of data from a fixed,\n    precomputed dictionary.\nSparsePCA : Sparse Principal Components Analysis.\n\nReferences\n----------\n\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\nfor sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import DictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42,\n... )\n>>> dict_learner = DictionaryLearning(\n...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n...     random_state=42,\n... )\n>>> X_transformed = dict_learner.fit(X).transform(X)\n\nWe can check the level of sparsity of `X_transformed`:\n\n>>> np.mean(X_transformed == 0)\n0.52...\n\nWe can compare the average squared euclidean norm of the reconstruction\nerror of the sparse coded signal relative to the squared euclidean norm of\nthe original signal:\n\n>>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n0.05..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Elastic Net model with iterative fitting along a regularization path.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n\nParameters\n----------\nl1_ratio : float or list of float, default=0.5\n    Float between 0 and 1 passed to ElasticNet (scaling between\n    l1 and l2 penalties). For ``l1_ratio = 0``\n    the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n    For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n    This parameter can be a list, in which case the different\n    values are tested by cross-validation and the one giving the best\n    prediction score is used. Note that a good choice of list of\n    values for l1_ratio is often to put more values close to 1\n    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n    .9, .95, .99, 1]``.\n\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path, used for each l1_ratio.\n\nalphas : array-like, default=None\n    List of alphas where to compute the models.\n    If None alphas are set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nprecompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nverbose : bool or int, default=0\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\nl1_ratio_ : float\n    The compromise between l1 and l2 penalization chosen by\n    cross validation.\n\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\nintercept_ : float or ndarray of shape (n_targets, n_features)\n    Independent term in the decision function.\n\nmse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n    Mean square error for the test set on each fold, varying l1_ratio and\n    alpha.\n\nalphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n    The grid of alphas used for fitting, for each l1_ratio.\n\ndual_gap_ : float\n    The dual gaps at the end of the optimization for the optimal alpha.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nenet_path : Compute elastic net path with coordinate descent.\nElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n\nNotes\n-----\nIn `fit`, once the best parameters `l1_ratio` and `alpha` are found through\ncross-validation, the model is fit again using the entire training set.\n\nTo avoid unnecessary memory duplication the `X` argument of the `fit`\nmethod should be directly passed as a Fortran-contiguous numpy array.\n\nThe parameter `l1_ratio` corresponds to alpha in the glmnet R package\nwhile alpha corresponds to the lambda parameter in glmnet.\nMore specifically, the optimization objective is::\n\n    1 / (2 * n_samples) * ||y - Xw||^2_2\n    + alpha * l1_ratio * ||w||_1\n    + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nIf you are interested in controlling the L1 and L2 penalty\nseparately, keep in mind that this is equivalent to::\n\n    a * L1 + b * L2\n\nfor::\n\n    alpha = a + b and l1_ratio = a / (a + b).\n\nFor an example, see\n:ref:`examples/linear_model/plot_lasso_model_selection.py\n<sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\nExamples\n--------\n>>> from sklearn.linear_model import ElasticNetCV\n>>> from sklearn.datasets import make_regression\n\n>>> X, y = make_regression(n_features=2, random_state=0)\n>>> regr = ElasticNetCV(cv=5, random_state=0)\n>>> regr.fit(X, y)\nElasticNetCV(cv=5, random_state=0)\n>>> print(regr.alpha_)\n0.199...\n>>> print(regr.intercept_)\n0.398...\n>>> print(regr.predict([[0, 0]]))\n[0.398...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear regression with combined L1 and L2 priors as regularizer.\n\nMinimizes the objective function::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nIf you are interested in controlling the L1 and L2 penalty\nseparately, keep in mind that this is equivalent to::\n\n        a * ||w||_1 + 0.5 * b * ||w||_2^2\n\nwhere::\n\n        alpha = a + b and l1_ratio = a / (a + b)\n\nThe parameter l1_ratio corresponds to alpha in the glmnet R package while\nalpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\nunless you supply your own sequence of alpha.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the penalty terms. Defaults to 1.0.\n    See the notes for the exact mathematical meaning of this\n    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n    solved by the :class:`LinearRegression` object. For numerical\n    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n    Given this, you should use the :class:`LinearRegression` object.\n\nl1_ratio : float, default=0.5\n    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n    combination of L1 and L2.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If ``False``, the\n    data is assumed to be already centered.\n\nprecompute : bool or array-like of shape (n_features, n_features),                 default=False\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. The Gram matrix can also be passed as argument.\n    For sparse input this option is always ``False`` to preserve sparsity.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``, see Notes below.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\nsparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n    Sparse representation of the `coef_`.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : list of int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\ndual_gap_ : float or ndarray of shape (n_targets,)\n    Given param alpha, the dual gaps at the end of the optimization,\n    same shape as each observation of y.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nElasticNetCV : Elastic net model with best model selection by\n    cross-validation.\nSGDRegressor : Implements elastic net regression with incremental training.\nSGDClassifier : Implements logistic regression with elastic net penalty\n    (``SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\")``).\n\nNotes\n-----\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array.\n\nThe precise stopping criteria based on `tol` are the following: First, check that\nthat maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\nis smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\nIf so, then additionally check whether the dual gap is smaller than `tol` times\n:math:`||y||_2^2 / n_{      ext{samples}}`.\n\nExamples\n--------\n>>> from sklearn.linear_model import ElasticNet\n>>> from sklearn.datasets import make_regression\n\n>>> X, y = make_regression(n_features=2, random_state=0)\n>>> regr = ElasticNet(random_state=0)\n>>> regr.fit(X, y)\nElasticNet(random_state=0)\n>>> print(regr.coef_)\n[18.83816048 64.55968825]\n>>> print(regr.intercept_)\n1.451...\n>>> print(regr.predict([[0, 0]]))\n[1.451...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the distance matrix between each pair from a vector array X and Y.\n\nFor efficiency reasons, the euclidean distance between a pair of row\nvector x and y is computed as::\n\n    dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n\nThis formulation has two advantages over other ways of computing distances.\nFirst, it is computationally efficient when dealing with sparse data.\nSecond, if one argument varies but the other remains unchanged, then\n`dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n\nHowever, this is not the most precise way of doing this computation,\nbecause this equation potentially suffers from \"catastrophic cancellation\".\nAlso, the distance matrix returned by this function may not be exactly\nsymmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    An array where each row is a sample and each column is a feature.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n    An array where each row is a sample and each column is a feature.\n    If `None`, method uses `Y=X`.\n\nY_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None\n    Pre-computed dot-products of vectors in Y (e.g.,\n    ``(Y**2).sum(axis=1)``)\n    May be ignored in some cases, see the note below.\n\nsquared : bool, default=False\n    Return squared Euclidean distances.\n\nX_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None\n    Pre-computed dot-products of vectors in X (e.g.,\n    ``(X**2).sum(axis=1)``)\n    May be ignored in some cases, see the note below.\n\nReturns\n-------\ndistances : ndarray of shape (n_samples_X, n_samples_Y)\n    Returns the distances between the row vectors of `X`\n    and the row vectors of `Y`.\n\nSee Also\n--------\npaired_distances : Distances between pairs of elements of X and Y.\n\nNotes\n-----\nTo achieve a better accuracy, `X_norm_squared`¬†and `Y_norm_squared` may be\nunused if they are passed as `np.float32`.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import euclidean_distances\n>>> X = [[0, 1], [1, 1]]\n>>> # distance between rows of X\n>>> euclidean_distances(X, X)\narray([[0., 1.],\n       [1., 0.]])\n>>> # get distance to origin\n>>> euclidean_distances(X, [[0, 0]])\narray([[1.        ],\n       [1.41421356]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Explained variance regression score function.\n\nBest possible score is 1.0, lower values are worse.\n\nIn the particular case when ``y_true`` is constant, the explained variance\nscore is not finite: it is either ``NaN`` (perfect predictions) or\n``-Inf`` (imperfect predictions). To prevent such non-finite numbers to\npollute higher-level experiments such as a grid search cross-validation,\nby default these cases are replaced with 1.0 (perfect predictions) or 0.0\n(imperfect predictions) respectively. If ``force_finite``\nis set to ``False``, this score falls back on the original :math:`R^2`\ndefinition.\n\n.. note::\n   The Explained Variance score is similar to the\n   :func:`R^2 score <r2_score>`, with the notable difference that it\n   does not account for systematic offsets in the prediction. Most often\n   the :func:`R^2 score <r2_score>` should be preferred.\n\nRead more in the :ref:`User Guide <explained_variance_score>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output scores.\n    Array-like value defines weights used to average scores.\n\n    'raw_values' :\n        Returns a full set of scores in case of multioutput input.\n\n    'uniform_average' :\n        Scores of all outputs are averaged with uniform weight.\n\n    'variance_weighted' :\n        Scores of all outputs are averaged, weighted by the variances\n        of each individual output.\n\nforce_finite : bool, default=True\n    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n    data should be replaced with real numbers (``1.0`` if prediction is\n    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n    for hyperparameters' search procedures (e.g. grid search\n    cross-validation).\n\n    .. versionadded:: 1.1\n\nReturns\n-------\nscore : float or ndarray of floats\n    The explained variance or ndarray if 'multioutput' is 'raw_values'.\n\nSee Also\n--------\nr2_score :\n    Similar metric, but accounting for systematic offsets in\n    prediction.\n\nNotes\n-----\nThis is not a symmetric function.\n\nExamples\n--------\n>>> from sklearn.metrics import explained_variance_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> explained_variance_score(y_true, y_pred)\n0.957...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n0.983...\n>>> y_true = [-2, -2, -2]\n>>> y_pred = [-2, -2, -2]\n>>> explained_variance_score(y_true, y_pred)\n1.0\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\nnan\n>>> y_true = [-2, -2, -2]\n>>> y_pred = [-2, -2, -2 + 1e-8]\n>>> explained_variance_score(y_true, y_pred)\n0.0\n>>> explained_variance_score(y_true, y_pred, force_finite=False)\n-inf"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n\nsplitter : {\"random\", \"best\"}, default=\"random\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float, {\"sqrt\", \"log2\"} or None, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at\n      each split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to pick randomly the `max_features` used at each split.\n    See :term:`Glossary <random_state>` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes > 2`),\n      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nExtraTreeRegressor : An extremely randomized tree regressor.\nsklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\nsklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\nsklearn.ensemble.RandomForestClassifier : A random forest classifier.\nsklearn.ensemble.RandomForestRegressor : A random forest regressor.\nsklearn.ensemble.RandomTreesEmbedding : An ensemble of\n    totally random trees.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.tree import ExtraTreeClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...    X, y, random_state=0)\n>>> extra_tree = ExtraTreeClassifier(random_state=0)\n>>> cls = BaggingClassifier(extra_tree, random_state=0).fit(\n...    X_train, y_train)\n>>> cls.score(X_test, y_test)\n0.8947..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An extremely randomized tree regressor.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n\nParameters\n----------\ncriterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"},             default=\"squared_error\"\n    The function to measure the quality of a split. Supported criteria\n    are \"squared_error\" for the mean squared error, which is equal to\n    variance reduction as feature selection criterion and minimizes the L2\n    loss using the mean of each terminal node, \"friedman_mse\", which uses\n    mean squared error with Friedman's improvement score for potential\n    splits, \"absolute_error\" for the mean absolute error, which minimizes\n    the L1 loss using the median of each terminal node, and \"poisson\" which\n    uses reduction in Poisson deviance to find splits.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\n    .. versionadded:: 0.24\n        Poisson deviance criterion.\n\nsplitter : {\"random\", \"best\"}, default=\"random\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float, {\"sqrt\", \"log2\"} or None, default=1.0\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `1.0`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to pick randomly the `max_features` used at each split.\n    See :term:`Glossary <random_state>` for details.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multioutput regressions (i.e. when `n_outputs_ > 1`),\n      - regressions trained on data with missing values.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nmax_features_ : int\n    The inferred value of max_features.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nfeature_importances_ : ndarray of shape (n_features,)\n    Return impurity-based feature importances (the higher, the more\n    important the feature).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nExtraTreeClassifier : An extremely randomized tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\nsklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import BaggingRegressor\n>>> from sklearn.tree import ExtraTreeRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> extra_tree = ExtraTreeRegressor(random_state=0)\n>>> reg = BaggingRegressor(extra_tree, random_state=0).fit(\n...     X_train, y_train)\n>>> reg.score(X_test, y_test)\n0.33..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=False\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls 3 sources of randomness:\n\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonically increasing\n      - 0: no constraint\n      - -1: monotonically decreasing\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes > 2`),\n      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.ExtraTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nExtraTreesRegressor : An extra-trees regressor with random splits.\nRandomForestClassifier : A random forest classifier with optimal splits.\nRandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n       trees\", Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n>>> clf.fit(X, y)\nExtraTreesClassifier(random_state=0)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An extra-trees regressor.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n    The function to measure the quality of a split. Supported criteria\n    are \"squared_error\" for the mean squared error, which is equal to\n    variance reduction as feature selection criterion and minimizes the L2\n    loss using the mean of each terminal node, \"friedman_mse\", which uses\n    mean squared error with Friedman's improvement score for potential\n    splits, \"absolute_error\" for the mean absolute error, which minimizes\n    the L1 loss using the median of each terminal node, and \"poisson\" which\n    uses reduction in Poisson deviance to find splits.\n    Training using \"absolute_error\" is significantly slower\n    than when using \"squared_error\".\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None or 1.0, then `max_features=n_features`.\n\n    .. note::\n        The default of 1.0 is equivalent to bagged trees and more\n        randomness can be achieved by setting smaller values, e.g. 0.3.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to 1.0.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=False\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.r2_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls 3 sources of randomness:\n\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonically increasing\n      - 0: no constraint\n      - -1: monotonically decreasing\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multioutput regressions (i.e. when `n_outputs_ > 1`),\n      - regressions trained on data with missing values.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of DecisionTreeRegressor\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n    Prediction computed with out-of-bag estimate on the training set.\n    This attribute exists only when ``oob_score`` is True.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nExtraTreesClassifier : An extra-trees classifier with random splits.\nRandomForestClassifier : A random forest classifier with optimal splits.\nRandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import ExtraTreesRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n...    X_train, y_train)\n>>> reg.score(X_test, y_test)\n0.2727..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the F1 score, also known as balanced F-score or F-measure.\n\nThe F1 score can be interpreted as a harmonic mean of the precision and\nrecall, where an F1 score reaches its best value at 1 and worst score at 0.\nThe relative contribution of precision and recall to the F1 score are\nequal. The formula for the F1 score is:\n\n.. math::\n    \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}}\n\nWhere :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the\nnumber of false negatives, and :math:`\\text{FP}` is the number of false positives.\nF1 is by default\ncalculated as 0.0 when there are no true positives, false negatives, or\nfalse positives.\n\nSupport beyond :term:`binary` targets is achieved by treating :term:`multiclass`\nand :term:`multilabel` data as a collection of binary problems, one for each\nlabel. For the :term:`binary` case, setting `average='binary'` will return\nF1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\nand F1 score for both classes are computed, then averaged or both returned (when\n`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\nF1 score for all `labels` are either returned or averaged depending on the\n`average` parameter. Use `labels` specify the set of labels to calculate F1 score\nfor.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    The set of labels to include when `average != 'binary'`, and their\n    order if `average is None`. Labels present in the data can be\n    excluded, for example in multiclass classification to exclude a \"negative\n    class\". Labels not present in the data can be included and will be\n    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : int, float, bool or str, default=1\n    The class to report if `average='binary'` and the data is binary,\n    otherwise this parameter is ignored.\n    For multiclass or multilabel targets, set `labels=[pos_label]` and\n    `average != 'binary'` to report metrics for one label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n    Sets the value to return when there is a zero division, i.e. when all\n    predictions and labels are negative.\n\n    Notes:\n    - If set to \"warn\", this acts like 0, but a warning is also raised.\n    - If set to `np.nan`, such values will be excluded from the average.\n\n    .. versionadded:: 1.3\n       `np.nan` option was added.\n\nReturns\n-------\nf1_score : float or array of float, shape = [n_unique_labels]\n    F1 score of the positive class in binary classification or weighted\n    average of the F1 scores of each class for the multiclass task.\n\nSee Also\n--------\nfbeta_score : Compute the F-beta score.\nprecision_recall_fscore_support : Compute the precision, recall, F-score,\n    and support.\njaccard_score : Compute the Jaccard similarity coefficient score.\nmultilabel_confusion_matrix : Compute a confusion matrix for each class or\n    sample.\n\nNotes\n-----\nWhen ``true positive + false positive + false negative == 0`` (i.e. a class\nis completely absent from both ``y_true`` or ``y_pred``), f-score is\nundefined. In such cases, by default f-score will be set to 0.0, and\n``UndefinedMetricWarning`` will be raised. This behavior can be modified by\nsetting the ``zero_division`` parameter.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the F1-score\n       <https://en.wikipedia.org/wiki/F1_score>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import f1_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> f1_score(y_true, y_pred, average='macro')\n0.26...\n>>> f1_score(y_true, y_pred, average='micro')\n0.33...\n>>> f1_score(y_true, y_pred, average='weighted')\n0.26...\n>>> f1_score(y_true, y_pred, average=None)\narray([0.8, 0. , 0. ])\n\n>>> # binary classification\n>>> y_true_empty = [0, 0, 0, 0, 0, 0]\n>>> y_pred_empty = [0, 0, 0, 0, 0, 0]\n>>> f1_score(y_true_empty, y_pred_empty)\n0.0...\n>>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)\n1.0...\n>>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)\nnan...\n\n>>> # multilabel classification\n>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n>>> f1_score(y_true, y_pred, average=None)\narray([0.66666667, 1.        , 0.66666667])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FClassifMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the ANOVA F-value for the provided sample.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The set of regressors that will be tested sequentially.\n\ny : array-like of shape (n_samples,)\n    The target vector.\n\nReturns\n-------\nf_statistic : ndarray of shape (n_features,)\n    F-statistic for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values associated with the F-statistic.\n\nSee Also\n--------\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\n\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.feature_selection import f_classif\n>>> X, y = make_classification(\n...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,\n...     shuffle=False, random_state=42\n... )\n>>> f_statistic, p_values = f_classif(X, y)\n>>> f_statistic\narray([2.2...e+02, 7.0...e-01, 1.6...e+00, 9.3...e-01,\n       5.4...e+00, 3.2...e-01, 4.7...e-02, 5.7...e-01,\n       7.5...e-01, 8.9...e-02])\n>>> p_values\narray([7.1...e-27, 4.0...e-01, 1.9...e-01, 3.3...e-01,\n       2.2...e-02, 5.7...e-01, 8.2...e-01, 4.5...e-01,\n       3.8...e-01, 7.6...e-01])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Univariate linear regression tests returning F-statistic and p-values.\n\nQuick linear model for testing the effect of a single regressor,\nsequentially for many regressors.\n\nThis is done in 2 steps:\n\n1. The cross correlation between each regressor and the target is computed\n   using :func:`r_regression` as::\n\n       E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))\n\n2. It is converted to an F score and then to a p-value.\n\n:func:`f_regression` is derived from :func:`r_regression` and will rank\nfeatures in the same order if all the features are positively correlated\nwith the target.\n\nNote however that contrary to :func:`f_regression`, :func:`r_regression`\nvalues lie in [-1, 1] and can thus be negative. :func:`f_regression` is\ntherefore recommended as a feature selection criterion to identify\npotentially predictive feature for a downstream classifier, irrespective of\nthe sign of the association with the target variable.\n\nFurthermore :func:`f_regression` returns p-values while\n:func:`r_regression` does not.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data matrix.\n\ny : array-like of shape (n_samples,)\n    The target vector.\n\ncenter : bool, default=True\n    Whether or not to center the data matrix `X` and the target vector `y`.\n    By default, `X` and `y` will be centered.\n\nforce_finite : bool, default=True\n    Whether or not to force the F-statistics and associated p-values to\n    be finite. There are two cases where the F-statistic is expected to not\n    be finite:\n\n    - when the target `y` or some features in `X` are constant. In this\n      case, the Pearson's R correlation is not defined leading to obtain\n      `np.nan` values in the F-statistic and p-value. When\n      `force_finite=True`, the F-statistic is set to `0.0` and the\n      associated p-value is set to `1.0`.\n    - when a feature in `X` is perfectly correlated (or\n      anti-correlated) with the target `y`. In this case, the F-statistic\n      is expected to be `np.inf`. When `force_finite=True`, the F-statistic\n      is set to `np.finfo(dtype).max` and the associated p-value is set to\n      `0.0`.\n\n    .. versionadded:: 1.1\n\nReturns\n-------\nf_statistic : ndarray of shape (n_features,)\n    F-statistic for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values associated with the F-statistic.\n\nSee Also\n--------\nr_regression: Pearson's R between label/feature for regression tasks.\nf_classif: ANOVA F-value between label/feature for classification tasks.\nchi2: Chi-squared stats of non-negative features for classification tasks.\nSelectKBest: Select features based on the k highest scores.\nSelectFpr: Select features based on a false positive rate test.\nSelectFdr: Select features based on an estimated false discovery rate.\nSelectFwe: Select features based on family-wise error rate.\nSelectPercentile: Select features based on percentile of the highest\n    scores.\n\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.feature_selection import f_regression\n>>> X, y = make_regression(\n...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42\n... )\n>>> f_statistic, p_values = f_regression(X, y)\n>>> f_statistic\narray([1.2...+00, 2.6...+13, 2.6...+00])\n>>> p_values\narray([2.7..., 1.5..., 1.0...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Factor Analysis (FA).\n\nA simple linear generative model with Gaussian latent variables.\n\nThe observations are assumed to be caused by a linear transformation of\nlower dimensional latent factors and added Gaussian noise.\nWithout loss of generality the factors are distributed according to a\nGaussian with zero mean and unit covariance. The noise is also zero mean\nand has an arbitrary diagonal covariance matrix.\n\nIf we would restrict the model further, by assuming that the Gaussian\nnoise is even isotropic (all diagonal entries are the same) we would obtain\n:class:`PCA`.\n\nFactorAnalysis performs a maximum likelihood estimate of the so-called\n`loading` matrix, the transformation of the latent variables to the\nobserved ones, using SVD based approach.\n\nRead more in the :ref:`User Guide <FA>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_components : int, default=None\n    Dimensionality of latent space, the number of components\n    of ``X`` that are obtained after ``transform``.\n    If None, n_components is set to the number of features.\n\ntol : float, default=1e-2\n    Stopping tolerance for log-likelihood increase.\n\ncopy : bool, default=True\n    Whether to make a copy of X. If ``False``, the input X gets overwritten\n    during fitting.\n\nmax_iter : int, default=1000\n    Maximum number of iterations.\n\nnoise_variance_init : array-like of shape (n_features,), default=None\n    The initial guess of the noise variance for each feature.\n    If None, it defaults to np.ones(n_features).\n\nsvd_method : {'lapack', 'randomized'}, default='randomized'\n    Which SVD method to use. If 'lapack' use standard SVD from\n    scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n    Defaults to 'randomized'. For most applications 'randomized' will\n    be sufficiently precise while providing significant speed gains.\n    Accuracy can also be improved by setting higher values for\n    `iterated_power`. If this is not sufficient, for maximum precision\n    you should choose 'lapack'.\n\niterated_power : int, default=3\n    Number of iterations for the power method. 3 by default. Only used\n    if ``svd_method`` equals 'randomized'.\n\nrotation : {'varimax', 'quartimax'}, default=None\n    If not None, apply the indicated rotation. Currently, varimax and\n    quartimax are implemented. See\n    `\"The varimax criterion for analytic rotation in factor analysis\"\n    <https://link.springer.com/article/10.1007%2FBF02289233>`_\n    H. F. Kaiser, 1958.\n\n    .. versionadded:: 0.24\n\nrandom_state : int or RandomState instance, default=0\n    Only used when ``svd_method`` equals 'randomized'. Pass an int for\n    reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components with maximum variance.\n\nloglike_ : list of shape (n_iterations,)\n    The log likelihood at each iteration.\n\nnoise_variance_ : ndarray of shape (n_features,)\n    The estimated noise variance for each feature.\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nPCA: Principal component analysis is also a latent linear variable model\n    which however assumes equal noise variance for each feature.\n    This extra assumption makes probabilistic PCA faster as it can be\n    computed in closed form.\nFastICA: Independent component analysis, a latent variable model with\n    non-Gaussian latent variables.\n\nReferences\n----------\n- David Barber, Bayesian Reasoning and Machine Learning,\n  Algorithm 21.1.\n\n- Christopher M. Bishop: Pattern Recognition and Machine Learning,\n  Chapter 12.2.4.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FactorAnalysis\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FactorAnalysis(n_components=7, random_state=0)\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "FastICA: a fast algorithm for Independent Component Analysis.\n\nThe implementation is based on [1]_.\n\nRead more in the :ref:`User Guide <ICA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components to use. If None is passed, all are used.\n\nalgorithm : {'parallel', 'deflation'}, default='parallel'\n    Specify which algorithm to use for FastICA.\n\nwhiten : str or bool, default='unit-variance'\n    Specify the whitening strategy to use.\n\n    - If 'arbitrary-variance', a whitening with variance\n      arbitrary is used.\n    - If 'unit-variance', the whitening matrix is rescaled to ensure that\n      each recovered source has unit variance.\n    - If False, the data is already considered to be whitened, and no\n      whitening is performed.\n\n    .. versionchanged:: 1.3\n        The default value of `whiten` changed to 'unit-variance' in 1.3.\n\nfun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n    The functional form of the G function used in the\n    approximation to neg-entropy. Could be either 'logcosh', 'exp',\n    or 'cube'.\n    You can also provide your own function. It should return a tuple\n    containing the value of the function, and of its derivative, in the\n    point. The derivative should be averaged along its last dimension.\n    Example::\n\n        def my_g(x):\n            return x ** 3, (3 * x ** 2).mean(axis=-1)\n\nfun_args : dict, default=None\n    Arguments to send to the functional form.\n    If empty or None and if fun='logcosh', fun_args will take value\n    {'alpha' : 1.0}.\n\nmax_iter : int, default=200\n    Maximum number of iterations during fit.\n\ntol : float, default=1e-4\n    A positive scalar giving the tolerance at which the\n    un-mixing matrix is considered to have converged.\n\nw_init : array-like of shape (n_components, n_components), default=None\n    Initial un-mixing array. If `w_init=None`, then an array of values\n    drawn from a normal distribution is used.\n\nwhiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n    The solver to use for whitening.\n\n    - \"svd\" is more stable numerically if the problem is degenerate, and\n      often faster when `n_samples <= n_features`.\n\n    - \"eigh\" is generally more memory efficient when\n      `n_samples >= n_features`, and can be faster when\n      `n_samples >= 50 * n_features`.\n\n    .. versionadded:: 1.2\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to initialize ``w_init`` when not specified, with a\n    normal distribution. Pass an int, for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The linear operator to apply to the data to get the independent\n    sources. This is equal to the unmixing matrix when ``whiten`` is\n    False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n    ``whiten`` is True.\n\nmixing_ : ndarray of shape (n_features, n_components)\n    The pseudo-inverse of ``components_``. It is the linear operator\n    that maps independent sources to the data.\n\nmean_ : ndarray of shape(n_features,)\n    The mean over features. Only set if `self.whiten` is True.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    If the algorithm is \"deflation\", n_iter is the\n    maximum number of iterations run across all components. Else\n    they are just the number of iterations taken to converge.\n\nwhitening_ : ndarray of shape (n_components, n_features)\n    Only set if whiten is 'True'. This is the pre-whitening matrix\n    that projects data onto the first `n_components` principal components.\n\nSee Also\n--------\nPCA : Principal component analysis (PCA).\nIncrementalPCA : Incremental principal components analysis (IPCA).\nKernelPCA : Kernel Principal component analysis (KPCA).\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nSparsePCA : Sparse Principal Components Analysis (SparsePCA).\n\nReferences\n----------\n.. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:\n       Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n       pp. 411-430.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FastICA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FastICA(n_components=7,\n...         random_state=0,\n...         whiten='unit-variance')\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the F-beta score.\n\nThe F-beta score is the weighted harmonic mean of precision and recall,\nreaching its optimal value at 1 and its worst value at 0.\n\nThe `beta` parameter represents the ratio of recall importance to\nprecision importance. `beta > 1` gives more weight to recall, while\n`beta < 1` favors precision. For example, `beta = 2` makes recall twice\nas important as precision, while `beta = 0.5` does the opposite.\nAsymptotically, `beta -> +inf` considers only recall, and `beta -> 0`\nonly precision.\n\nThe formula for F-beta score is:\n\n.. math::\n\n   F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}\n                    {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}\n\nWhere :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the\nnumber of false positives, and :math:`\\text{fn}` is the number of false negatives.\n\nSupport beyond term:`binary` targets is achieved by treating :term:`multiclass`\nand :term:`multilabel` data as a collection of binary problems, one for each\nlabel. For the :term:`binary` case, setting `average='binary'` will return\nF-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is\nignored and F-beta score for both classes are computed, then averaged or both\nreturned (when `average=None`). Similarly, for :term:`multiclass` and\n:term:`multilabel` targets, F-beta score for all `labels` are either returned or\naveraged depending on the `average` parameter. Use `labels` specify the set of\nlabels to calculate F-beta score for.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nbeta : float\n    Determines the weight of recall in the combined score.\n\nlabels : array-like, default=None\n    The set of labels to include when `average != 'binary'`, and their\n    order if `average is None`. Labels present in the data can be\n    excluded, for example in multiclass classification to exclude a \"negative\n    class\". Labels not present in the data can be included and will be\n    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : int, float, bool or str, default=1\n    The class to report if `average='binary'` and the data is binary,\n    otherwise this parameter is ignored.\n    For multiclass or multilabel targets, set `labels=[pos_label]` and\n    `average != 'binary'` to report metrics for one label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n    Sets the value to return when there is a zero division, i.e. when all\n    predictions and labels are negative.\n\n    Notes:\n    - If set to \"warn\", this acts like 0, but a warning is also raised.\n    - If set to `np.nan`, such values will be excluded from the average.\n\n    .. versionadded:: 1.3\n       `np.nan` option was added.\n\nReturns\n-------\nfbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n    F-beta score of the positive class in binary classification or weighted\n    average of the F-beta score of each class for the multiclass task.\n\nSee Also\n--------\nprecision_recall_fscore_support : Compute the precision, recall, F-score,\n    and support.\nmultilabel_confusion_matrix : Compute a confusion matrix for each class or\n    sample.\n\nNotes\n-----\nWhen ``true positive + false positive + false negative == 0``, f-score\nreturns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be\nmodified by setting ``zero_division``.\n\nReferences\n----------\n.. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n       Modern Information Retrieval. Addison Wesley, pp. 327-328.\n\n.. [2] `Wikipedia entry for the F1-score\n       <https://en.wikipedia.org/wiki/F1_score>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import fbeta_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n0.23...\n>>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n0.33...\n>>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n0.23...\n>>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\narray([0.71..., 0.        , 0.        ])\n>>> y_pred_empty = [0, 0, 0, 0, 0, 0]\n>>> fbeta_score(y_true, y_pred_empty,\n...             average=\"macro\", zero_division=np.nan, beta=0.5)\n0.12..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Agglomerate features.\n\nRecursively merges pair of clusters of features.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int or None, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\nmetric : str or callable, default=\"euclidean\"\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or \"precomputed\". If linkage is \"ward\", only\n    \"euclidean\" is accepted. If \"precomputed\", a distance matrix is needed\n    as input for the fit method.\n\n    .. versionadded:: 1.2\n\n    .. deprecated:: 1.4\n       `metric=None` is deprecated in 1.4 and will be removed in 1.6.\n       Let `metric` be the default value (i.e. `\"euclidean\"`) instead.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each feature the neighboring\n    features following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    `kneighbors_graph`. Default is `None`, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at `n_clusters`. This is useful\n    to decrease computation time if the number of clusters is not small\n    compared to the number of features. This option is useful only when\n    specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {\"ward\", \"complete\", \"average\", \"single\"}, default=\"ward\"\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of features. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - \"ward\" minimizes the variance of the clusters being merged.\n    - \"complete\" or maximum linkage uses the maximum distances between\n      all features of the two sets.\n    - \"average\" uses the average of the distances of each feature of\n      the two sets.\n    - \"single\" uses the minimum of the distances between all features\n      of the two sets.\n\npooling_func : callable, default=np.mean\n    This combines the values of agglomerated features into a single\n    value, and should accept an array of shape [M, N] and the keyword\n    argument `axis=1`, and reduce it to an array of size [M].\n\ndistance_threshold : float, default=None\n    The linkage distance threshold at or above which clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : array-like of (n_features,)\n    Cluster labels for each feature.\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nchildren_ : array-like of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_features`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_features` is a non-leaf\n    node and has children `children_[i - n_features]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_features + i`.\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nSee Also\n--------\nAgglomerativeClustering : Agglomerative clustering samples instead of\n    features.\nward_tree : Hierarchical clustering with ward linkage.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets, cluster\n>>> digits = datasets.load_digits()\n>>> images = digits.images\n>>> X = np.reshape(images, (len(images), -1))\n>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n>>> agglo.fit(X)\nFeatureAgglomeration(n_clusters=32)\n>>> X_reduced = agglo.transform(X)\n>>> X_reduced.shape\n(1797, 32)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Implements feature hashing, aka the hashing trick.\n\nThis class turns sequences of symbolic feature names (strings) into\nscipy.sparse matrices, using a hash function to compute the matrix column\ncorresponding to a name. The hash function employed is the signed 32-bit\nversion of Murmurhash3.\n\nFeature names of type byte string are used as-is. Unicode strings are\nconverted to UTF-8 first, but no Unicode normalization is done.\nFeature values must be (finite) numbers.\n\nThis class is a low-memory alternative to DictVectorizer and\nCountVectorizer, intended for large-scale (online) learning and situations\nwhere memory is tight, e.g. when running prediction code on embedded\ndevices.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <feature_hashing>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_features : int, default=2**20\n    The number of features (columns) in the output matrices. Small numbers\n    of features are likely to cause hash collisions, but large numbers\n    will cause larger coefficient dimensions in linear learners.\ninput_type : str, default='dict'\n    Choose a string from {'dict', 'pair', 'string'}.\n    Either \"dict\" (the default) to accept dictionaries over\n    (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n    or \"string\" to accept single strings.\n    feature_name should be a string, while value should be a number.\n    In the case of \"string\", a value of 1 is implied.\n    The feature_name is hashed to find the appropriate column for the\n    feature. The value's sign might be flipped in the output (but see\n    non_negative, below).\ndtype : numpy dtype, default=np.float64\n    The type of feature values. Passed to scipy.sparse matrix constructors\n    as the dtype argument. Do not set this to bool, np.boolean or any\n    unsigned integer type.\nalternate_sign : bool, default=True\n    When True, an alternating sign is added to the features as to\n    approximately conserve the inner product in the hashed space even for\n    small n_features. This approach is similar to sparse random projection.\n\n    .. versionchanged:: 0.19\n        ``alternate_sign`` replaces the now deprecated ``non_negative``\n        parameter.\n\nSee Also\n--------\nDictVectorizer : Vectorizes string-valued features using a hash table.\nsklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.\n\nNotes\n-----\nThis estimator is :term:`stateless` and does not need to be fitted.\nHowever, we recommend to call :meth:`fit_transform` instead of\n:meth:`transform`, as parameter validation is only performed in\n:meth:`fit`.\n\nExamples\n--------\n>>> from sklearn.feature_extraction import FeatureHasher\n>>> h = FeatureHasher(n_features=10)\n>>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n>>> f = h.transform(D)\n>>> f.toarray()\narray([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\nWith `input_type=\"string\"`, the input must be an iterable over iterables of\nstrings:\n\n>>> h = FeatureHasher(n_features=8, input_type=\"string\")\n>>> raw_X = [[\"dog\", \"cat\", \"snake\"], [\"snake\", \"dog\"], [\"cat\", \"bird\"]]\n>>> f = h.transform(raw_X)\n>>> f.toarray()\narray([[ 0.,  0.,  0., -1.,  0., -1.,  0.,  1.],\n       [ 0.,  0.,  0., -1.,  0., -1.,  0.,  0.],\n       [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  1.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureExtractionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelection",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Measure the similarity of two clusterings of a set of points.\n\n.. versionadded:: 0.18\n\nThe Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\nthe precision and recall::\n\n    FMI = TP / sqrt((TP + FP) * (TP + FN))\n\nWhere ``TP`` is the number of **True Positive** (i.e. the number of pair of\npoints that belongs in the same clusters in both ``labels_true`` and\n``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\nnumber of pair of points that belongs in the same clusters in\n``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n**False Negative** (i.e. the number of pair of points that belongs in the\nsame clusters in ``labels_pred`` and not in ``labels_True``).\n\nThe score ranges from 0 to 1. A high value indicates a good similarity\nbetween two clusters.\n\nRead more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=int\n    A clustering of the data into disjoint subsets.\n\nlabels_pred : array-like of shape (n_samples,), dtype=int\n    A clustering of the data into disjoint subsets.\n\nsparse : bool, default=False\n    Compute contingency matrix internally with sparse matrix.\n\nReturns\n-------\nscore : float\n   The resulting Fowlkes-Mallows score.\n\nReferences\n----------\n.. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n   hierarchical clusterings\". Journal of the American Statistical\n   Association\n   <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_\n\n.. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n       <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have\nscore 1.0::\n\n  >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n  >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n  1.0\n  >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nIf classes members are completely split across different clusters,\nthe assignment is totally random, hence the FMI is null::\n\n  >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n  0.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Constructs a transformer from an arbitrary callable.\n\nA FunctionTransformer forwards its X (and optionally y) arguments to a\nuser-defined function or function object and returns the result of this\nfunction. This is useful for stateless transformations such as taking the\nlog of frequencies, doing custom scaling, etc.\n\nNote: If a lambda is used as the function, then the resulting\ntransformer will not be pickleable.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <function_transformer>`.\n\nParameters\n----------\nfunc : callable, default=None\n    The callable to use for the transformation. This will be passed\n    the same arguments as transform, with args and kwargs forwarded.\n    If func is None, then func will be the identity function.\n\ninverse_func : callable, default=None\n    The callable to use for the inverse transformation. This will be\n    passed the same arguments as inverse transform, with args and\n    kwargs forwarded. If inverse_func is None, then inverse_func\n    will be the identity function.\n\nvalidate : bool, default=False\n    Indicate that the input X array should be checked before calling\n    ``func``. The possibilities are:\n\n    - If False, there is no input validation.\n    - If True, then X will be converted to a 2-dimensional NumPy array or\n      sparse matrix. If the conversion is not possible an exception is\n      raised.\n\n    .. versionchanged:: 0.22\n       The default of ``validate`` changed from True to False.\n\naccept_sparse : bool, default=False\n    Indicate that func accepts a sparse matrix as input. If validate is\n    False, this has no effect. Otherwise, if accept_sparse is false,\n    sparse matrix inputs will cause an exception to be raised.\n\ncheck_inverse : bool, default=True\n   Whether to check that or ``func`` followed by ``inverse_func`` leads to\n   the original inputs. It can be used for a sanity check, raising a\n   warning when the condition is not fulfilled.\n\n   .. versionadded:: 0.20\n\nfeature_names_out : callable, 'one-to-one' or None, default=None\n    Determines the list of feature names that will be returned by the\n    `get_feature_names_out` method. If it is 'one-to-one', then the output\n    feature names will be equal to the input feature names. If it is a\n    callable, then it must take two positional arguments: this\n    `FunctionTransformer` (`self`) and an array-like of input feature names\n    (`input_features`). It must return an array-like of output feature\n    names. The `get_feature_names_out` method is only defined if\n    `feature_names_out` is not None.\n\n    See ``get_feature_names_out`` for more details.\n\n    .. versionadded:: 1.1\n\nkw_args : dict, default=None\n    Dictionary of additional keyword arguments to pass to func.\n\n    .. versionadded:: 0.18\n\ninv_kw_args : dict, default=None\n    Dictionary of additional keyword arguments to pass to inverse_func.\n\n    .. versionadded:: 0.18\n\nAttributes\n----------\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X` has feature\n    names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMaxAbsScaler : Scale each feature by its maximum absolute value.\nStandardScaler : Standardize features by removing the mean and\n    scaling to unit variance.\nLabelBinarizer : Binarize labels in a one-vs-all fashion.\nMultiLabelBinarizer : Transform between iterable of iterables\n    and a multilabel format.\n\nNotes\n-----\nIf `func` returns an output with a `columns` attribute, then the columns is enforced\nto be consistent with the output of `get_feature_names_out`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> transformer = FunctionTransformer(np.log1p)\n>>> X = np.array([[0, 1], [2, 3]])\n>>> transformer.transform(X)\narray([[0.       , 0.6931...],\n       [1.0986..., 1.3862...]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Generalized Linear Model with a Gamma distribution.\n\nThis regressor uses the 'log' link function.\n\nRead more in the :ref:`User Guide <Generalized_linear_models>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\nalpha : float, default=1\n    Constant that multiplies the L2 penalty term and determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n    Values of `alpha` must be in the range `[0.0, inf)`.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor `X @ coef_ + intercept_`.\n\nsolver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n    Algorithm to use in the optimization problem:\n\n    'lbfgs'\n        Calls scipy's L-BFGS-B optimizer.\n\n    'newton-cholesky'\n        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n        iterated reweighted least squares) with an inner Cholesky based solver.\n        This solver is a good choice for `n_samples` >> `n_features`, especially\n        with one-hot encoded categorical features with rare categories. Be aware\n        that the memory usage of this solver has a quadratic dependency on\n        `n_features` because it explicitly computes the Hessian matrix.\n\n        .. versionadded:: 1.2\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n    Values must be in the range `[1, inf)`.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n    Values must be in the range `(0.0, inf)`.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for `coef_` and `intercept_`.\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n    Values must be in the range `[0, inf)`.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X @ coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nn_iter_ : int\n    Actual number of iterations used in the solver.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nPoissonRegressor : Generalized Linear Model with a Poisson distribution.\nTweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.GammaRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [19, 26, 33, 30]\n>>> clf.fit(X, y)\nGammaRegressor()\n>>> clf.score(X, y)\n0.773...\n>>> clf.coef_\narray([0.072..., 0.066...])\n>>> clf.intercept_\n2.896...\n>>> clf.predict([[1, 0], [2, 8]])\narray([19.483..., 35.795...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Gaussian Naive Bayes (GaussianNB).\n\nCan perform online updates to model parameters via :meth:`partial_fit`.\nFor details on algorithm used to update feature means and variance online,\nsee Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n\n    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n\nRead more in the :ref:`User Guide <gaussian_naive_bayes>`.\n\nParameters\n----------\npriors : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nvar_smoothing : float, default=1e-9\n    Portion of the largest variance of all features that is added to\n    variances for calculation stability.\n\n    .. versionadded:: 0.20\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    number of training samples observed in each class.\n\nclass_prior_ : ndarray of shape (n_classes,)\n    probability of each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    class labels known to the classifier.\n\nepsilon_ : float\n    absolute additive value to variances.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nvar_ : ndarray of shape (n_classes, n_features)\n    Variance of each feature per class.\n\n    .. versionadded:: 1.0\n\ntheta_ : ndarray of shape (n_classes, n_features)\n    mean of each feature per class.\n\nSee Also\n--------\nBernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : Complement Naive Bayes classifier.\nMultinomialNB : Naive Bayes classifier for multinomial models.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> Y = np.array([1, 1, 1, 2, 2, 2])\n>>> from sklearn.naive_bayes import GaussianNB\n>>> clf = GaussianNB()\n>>> clf.fit(X, Y)\nGaussianNB()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]\n>>> clf_pf = GaussianNB()\n>>> clf_pf.partial_fit(X, Y, np.unique(Y))\nGaussianNB()\n>>> print(clf_pf.predict([[-0.8, -1]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n    a single array scores.\n\nmode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n    Feature selection mode. Note that the `'percentile'` and `'kbest'`\n    modes are supporting unsupervised feature selection (when `y` is `None`).\n\nparam : \"all\", float or int, default=1e-5\n    Parameter of the corresponding mode.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned scores only.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n>>> X_new = transformer.fit_transform(X, y)\n>>> X_new.shape\n(569, 20)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Get a scorer from string.\n\nRead more in the :ref:`User Guide <scoring_parameter>`.\n:func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names\nof all available scorers.\n\nParameters\n----------\nscoring : str, callable or None\n    Scoring method as string. If callable it is returned as is.\n    If None, returns None.\n\nReturns\n-------\nscorer : callable\n    The scorer.\n\nNotes\n-----\nWhen passed a string, this function always returns a copy of the scorer\nobject. Calling `get_scorer` twice for the same scorer results in two\nseparate scorer objects.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.dummy import DummyClassifier\n>>> from sklearn.metrics import get_scorer\n>>> X = np.reshape([0, 1, -1, -0.5, 2], (-1, 1))\n>>> y = np.array([0, 1, 1, 0, 1])\n>>> classifier = DummyClassifier(strategy=\"constant\", constant=0).fit(X, y)\n>>> accuracy = get_scorer(\"accuracy\")\n>>> accuracy(classifier, X, y)\n0.4"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerNamesMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Get the names of all available scorers.\n\nThese names can be passed to :func:`~sklearn.metrics.get_scorer` to\nretrieve the scorer object.\n\nReturns\n-------\nlist of str\n    Names of all available scorers.\n\nExamples\n--------\n>>> from sklearn.metrics import get_scorer_names\n>>> all_scorers = get_scorer_names()\n>>> type(all_scorers)\n<class 'list'>\n>>> all_scorers[:3]\n['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score']\n>>> \"roc_auc\" in all_scorers\nTrue"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Gradient Boosting for classification.\n\nThis algorithm builds an additive model in a forward stage-wise fashion; it\nallows for the optimization of arbitrary differentiable loss functions. In\neach stage ``n_classes_`` regression trees are fit on the negative gradient\nof the loss function, e.g. binary or multiclass log loss. Binary\nclassification is a special case where only a single regression tree is\ninduced.\n\n:class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\nvariant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n\nParameters\n----------\nloss : {'log_loss', 'exponential'}, default='log_loss'\n    The loss function to be optimized. 'log_loss' refers to binomial and\n    multinomial deviance, the same as used in logistic regression.\n    It is a good choice for classification with probabilistic outputs.\n    For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n    Values must be in the range `[0.0, inf)`.\n\nn_estimators : int, default=100\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n    Values must be in the range `[1, inf)`.\n\nsubsample : float, default=1.0\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n    Values must be in the range `(0.0, 1.0]`.\n\ncriterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n    The function to measure the quality of a split. Supported criteria are\n    'friedman_mse' for the mean squared error with improvement score by\n    Friedman, 'squared_error' for mean squared error. The default value of\n    'friedman_mse' is generally the best as it can provide a better\n    approximation in some cases.\n\n    .. versionadded:: 0.18\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, values must be in the range `[2, inf)`.\n    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n      will be `ceil(min_samples_split * n_samples)`.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, values must be in the range `[1, inf)`.\n    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n      will be `ceil(min_samples_leaf * n_samples)`.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n    Values must be in the range `[0.0, 0.5]`.\n\nmax_depth : int or None, default=3\n    Maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n    If int, values must be in the range `[1, inf)`.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    Values must be in the range `[0.0, inf)`.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\ninit : estimator or 'zero', default=None\n    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n    'zero', the initial raw predictions are set to zero. By default, a\n    ``DummyEstimator`` predicting the classes priors is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random splitting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_features : {'sqrt', 'log2'}, int or float, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, values must be in the range `[1, inf)`.\n    - If float, values must be in the range `(0.0, 1.0]` and the features\n      considered at each split will be `max(1, int(max_features * n_features_in_))`.\n    - If 'sqrt', then `max_features=sqrt(n_features)`.\n    - If 'log2', then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n    Values must be in the range `[0, inf)`.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    Values must be in the range `[2, inf)`.\n    If `None`, then unlimited number of leaf nodes.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Values must be in the range `(0.0, 1.0)`.\n    Only used if ``n_iter_no_change`` is set to an integer.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=None\n    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations. The split is stratified.\n    Values must be in the range `[1, inf)`.\n    See\n    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.\n\n    .. versionadded:: 0.20\n\ntol : float, default=1e-4\n    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n    Values must be in the range `[0.0, inf)`.\n\n    .. versionadded:: 0.20\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n    Values must be in the range `[0.0, inf)`.\n    See :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nn_estimators_ : int\n    The number of estimators as selected by early stopping (if\n    ``n_iter_no_change`` is specified). Otherwise it is set to\n    ``n_estimators``.\n\n    .. versionadded:: 0.20\n\nn_trees_per_iteration_ : int\n    The number of trees that are built at each iteration. For binary classifiers,\n    this is always 1.\n\n    .. versionadded:: 1.4.0\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_improvement_ : ndarray of shape (n_estimators,)\n    The improvement in loss on the out-of-bag samples\n    relative to the previous iteration.\n    ``oob_improvement_[0]`` is the improvement in\n    loss of the first stage over the ``init`` estimator.\n    Only available if ``subsample < 1.0``.\n\noob_scores_ : ndarray of shape (n_estimators,)\n    The full history of the loss values on the out-of-bag\n    samples. Only available if `subsample < 1.0`.\n\n    .. versionadded:: 1.3\n\noob_score_ : float\n    The last value of the loss on the out-of-bag samples. It is\n    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n    .. versionadded:: 1.3\n\ntrain_score_ : ndarray of shape (n_estimators,)\n    The i-th score ``train_score_[i]`` is the loss of the\n    model at iteration ``i`` on the in-bag sample.\n    If ``subsample == 1`` this is the loss on the training data.\n\ninit_ : estimator\n    The estimator that provides the initial predictions. Set via the ``init``\n    argument.\n\nestimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``n_trees_per_iteration_``)\n    The collection of fitted sub-estimators. ``n_trees_per_iteration_`` is 1 for\n    binary classification, otherwise ``n_classes``.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_classes_ : int\n    The number of classes.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nSee Also\n--------\nHistGradientBoostingClassifier : Histogram-based Gradient Boosting\n    Classification Tree.\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nRandomForestClassifier : A meta-estimator that fits a number of decision\n    tree classifiers on various sub-samples of the dataset and uses\n    averaging to improve the predictive accuracy and control over-fitting.\nAdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n    on the original dataset and then fits additional copies of the\n    classifier on the same dataset where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers\n    focus more on difficult cases.\n\nNotes\n-----\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data and\n``max_features=n_features``, if the improvement of the criterion is\nidentical for several splits enumerated during the search of the best\nsplit. To obtain a deterministic behaviour during fitting,\n``random_state`` has to be fixed.\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\nJ. Friedman, Stochastic Gradient Boosting, 1999\n\nT. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009.\n\nExamples\n--------\nThe following example shows how to fit a gradient boosting classifier with\n100 decision stumps as weak learners.\n\n>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)\n0.913..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Gradient Boosting for regression.\n\nThis estimator builds an additive model in a forward stage-wise fashion; it\nallows for the optimization of arbitrary differentiable loss functions. In\neach stage a regression tree is fit on the negative gradient of the given\nloss function.\n\n:class:`sklearn.ensemble.HistGradientBoostingRegressor` is a much faster\nvariant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n\nParameters\n----------\nloss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\n    Loss function to be optimized. 'squared_error' refers to the squared\n    error for regression. 'absolute_error' refers to the absolute error of\n    regression and is a robust loss function. 'huber' is a\n    combination of the two. 'quantile' allows quantile regression (use\n    `alpha` to specify the quantile).\n\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n    Values must be in the range `[0.0, inf)`.\n\nn_estimators : int, default=100\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n    Values must be in the range `[1, inf)`.\n\nsubsample : float, default=1.0\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n    Values must be in the range `(0.0, 1.0]`.\n\ncriterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n    The function to measure the quality of a split. Supported criteria are\n    \"friedman_mse\" for the mean squared error with improvement score by\n    Friedman, \"squared_error\" for mean squared error. The default value of\n    \"friedman_mse\" is generally the best as it can provide a better\n    approximation in some cases.\n\n    .. versionadded:: 0.18\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, values must be in the range `[2, inf)`.\n    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n      will be `ceil(min_samples_split * n_samples)`.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, values must be in the range `[1, inf)`.\n    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n      will be `ceil(min_samples_leaf * n_samples)`.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n    Values must be in the range `[0.0, 0.5]`.\n\nmax_depth : int or None, default=3\n    Maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n    If int, values must be in the range `[1, inf)`.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    Values must be in the range `[0.0, inf)`.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\ninit : estimator or 'zero', default=None\n    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n    initial raw predictions are set to zero. By default a\n    ``DummyEstimator`` is used, predicting either the average target value\n    (for loss='squared_error'), or a quantile for the other losses.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random splitting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_features : {'sqrt', 'log2'}, int or float, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, values must be in the range `[1, inf)`.\n    - If float, values must be in the range `(0.0, 1.0]` and the features\n      considered at each split will be `max(1, int(max_features * n_features_in_))`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nalpha : float, default=0.9\n    The alpha-quantile of the huber loss function and the quantile\n    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n    Values must be in the range `(0.0, 1.0)`.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n    Values must be in the range `[0, inf)`.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    Values must be in the range `[2, inf)`.\n    If None, then unlimited number of leaf nodes.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Values must be in the range `(0.0, 1.0)`.\n    Only used if ``n_iter_no_change`` is set to an integer.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=None\n    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations.\n    Values must be in the range `[1, inf)`.\n    See\n    :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.\n\n    .. versionadded:: 0.20\n\ntol : float, default=1e-4\n    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n    Values must be in the range `[0.0, inf)`.\n\n    .. versionadded:: 0.20\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n    Values must be in the range `[0.0, inf)`.\n    See :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nn_estimators_ : int\n    The number of estimators as selected by early stopping (if\n    ``n_iter_no_change`` is specified). Otherwise it is set to\n    ``n_estimators``.\n\nn_trees_per_iteration_ : int\n    The number of trees that are built at each iteration. For regressors, this is\n    always 1.\n\n    .. versionadded:: 1.4.0\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_improvement_ : ndarray of shape (n_estimators,)\n    The improvement in loss on the out-of-bag samples\n    relative to the previous iteration.\n    ``oob_improvement_[0]`` is the improvement in\n    loss of the first stage over the ``init`` estimator.\n    Only available if ``subsample < 1.0``.\n\noob_scores_ : ndarray of shape (n_estimators,)\n    The full history of the loss values on the out-of-bag\n    samples. Only available if `subsample < 1.0`.\n\n    .. versionadded:: 1.3\n\noob_score_ : float\n    The last value of the loss on the out-of-bag samples. It is\n    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n    .. versionadded:: 1.3\n\ntrain_score_ : ndarray of shape (n_estimators,)\n    The i-th score ``train_score_[i]`` is the loss of the\n    model at iteration ``i`` on the in-bag sample.\n    If ``subsample == 1`` this is the loss on the training data.\n\ninit_ : estimator\n    The estimator that provides the initial predictions. Set via the ``init``\n    argument.\n\nestimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n    The collection of fitted sub-estimators.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nmax_features_ : int\n    The inferred value of max_features.\n\nSee Also\n--------\nHistGradientBoostingRegressor : Histogram-based Gradient Boosting\n    Classification Tree.\nsklearn.tree.DecisionTreeRegressor : A decision tree regressor.\nsklearn.ensemble.RandomForestRegressor : A random forest regressor.\n\nNotes\n-----\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data and\n``max_features=n_features``, if the improvement of the criterion is\nidentical for several splits enumerated during the search of the best\nsplit. To obtain a deterministic behaviour during fitting,\n``random_state`` has to be fixed.\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\nJ. Friedman, Stochastic Gradient Boosting, 1999\n\nT. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009.\n\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_regression(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> reg = GradientBoostingRegressor(random_state=0)\n>>> reg.fit(X_train, y_train)\nGradientBoostingRegressor(random_state=0)\n>>> reg.predict(X_test[1:2])\narray([-61...])\n>>> reg.score(X_test, y_test)\n0.4..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Exhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"score_samples\", \"predict\", \"predict_proba\",\n\"decision_function\", \"transform\" and \"inverse_transform\" if they are\nimplemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated grid-search over a parameter grid.\n\nRead more in the :ref:`User Guide <grid_search>`.\n\nParameters\n----------\nestimator : estimator object\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_grid : dict or list of dictionaries\n    Dictionary with parameters names (`str`) as keys and lists of\n    parameter settings to try as values, or a list of such\n    dictionaries, in which case the grids spanned by each dictionary\n    in the list are explored. This enables searching over any sequence\n    of parameter settings.\n\nscoring : str, callable, list, tuple or dict, default=None\n    Strategy to evaluate the performance of the cross-validated model on\n    the test set.\n\n    If `scoring` represents a single score, one can use:\n\n    - a single string (see :ref:`scoring_parameter`);\n    - a callable (see :ref:`scoring`) that returns a single value.\n\n    If `scoring` represents multiple scores, one can use:\n\n    - a list or tuple of unique strings;\n    - a callable returning a dictionary where the keys are the metric\n      names and the values are the metric scores;\n    - a dictionary with metric names as keys and callables a values.\n\n    See :ref:`multimetric_grid_search` for an example.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nrefit : bool, str, or callable, default=True\n    Refit an estimator using the best found parameters on the whole\n    dataset.\n\n    For multiple metric evaluation, this needs to be a `str` denoting the\n    scorer that would be used to find the best parameters for refitting\n    the estimator at the end.\n\n    Where there are considerations other than maximum score in\n    choosing a best estimator, ``refit`` can be set to a function which\n    returns the selected ``best_index_`` given ``cv_results_``. In that\n    case, the ``best_estimator_`` and ``best_params_`` will be set\n    according to the returned ``best_index_`` while the ``best_score_``\n    attribute will not be available.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``GridSearchCV`` instance.\n\n    Also for multiple metric evaluation, the attributes ``best_index_``,\n    ``best_score_`` and ``best_params_`` will only be available if\n    ``refit`` is set and all of them will be determined w.r.t this specific\n    scorer.\n\n    See ``scoring`` parameter to know more about multiple metric\n    evaluation.\n\n    See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n    to see how to design a custom selection strategy using a callable\n    via `refit`.\n\n    .. versionchanged:: 0.20\n        Support for callable added.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used. These splitters are instantiated\n    with `shuffle=False` so the splits will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\n    - >1 : the computation time for each fold and parameter candidate is\n      displayed;\n    - >2 : the score is also displayed;\n    - >3 : the fold and candidate parameter indexes are also displayed\n      together with the starting time of the computation.\n\npre_dispatch : int, or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\n    .. versionadded:: 0.19\n\n    .. versionchanged:: 0.21\n        Default value was changed from ``True`` to ``False``\n\nAttributes\n----------\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``.\n\n    For instance the below given table\n\n    +------------+-----------+------------+-----------------+---+---------+\n    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n    +============+===========+============+=================+===+=========+\n    |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n    +------------+-----------+------------+-----------------+---+---------+\n\n    will be represented by a ``cv_results_`` dict of::\n\n        {\n        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                     mask = [False False False False]...)\n        'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                    mask = [ True  True False False]...),\n        'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                     mask = [False False  True  True]...),\n        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n        'rank_test_score'    : [2, 4, 3, 1],\n        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n        }\n\n    NOTE\n\n    The key ``'params'`` is used to store a list of parameter\n    settings dicts for all the parameter candidates.\n\n    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n    ``std_score_time`` are all in seconds.\n\n    For multi-metric evaluation, the scores for all the scorers are\n    available in the ``cv_results_`` dict at the keys ending with that\n    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n    above. ('split0_test_precision', 'mean_train_precision' etc.)\n\nbest_estimator_ : estimator\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\n    See ``refit`` parameter for more information on allowed values.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\n    This attribute is not available if ``refit`` is a function.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\n    For multi-metric evaluation, this attribute holds the validated\n    ``scoring`` dict which maps the scorer key to the scorer callable.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\n    .. versionadded:: 0.20\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. This is present only if ``refit`` is specified and\n    the underlying estimator is a classifier.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `n_features_in_` when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `feature_names_in_` when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nParameterGrid : Generates all the combinations of a hyperparameter grid.\ntrain_test_split : Utility function to split the data into a development\n    set usable for fitting a GridSearchCV instance and an evaluation set\n    for its final evaluation.\nsklearn.metrics.make_scorer : Make a scorer from a performance metric or\n    loss function.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the left out\ndata, unless an explicit score is passed in which case it is used instead.\n\nIf `n_jobs` was set to a value higher than one, the data is copied for each\npoint in the grid (and not `n_jobs` times). This is done for efficiency\nreasons if individual jobs take very little time, but may raise errors if\nthe dataset is large and not enough memory is available.  A workaround in\nthis case is to set `pre_dispatch`. Then, the memory is copied only\n`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\nn_jobs`.\n\nExamples\n--------\n>>> from sklearn import svm, datasets\n>>> from sklearn.model_selection import GridSearchCV\n>>> iris = datasets.load_iris()\n>>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n>>> svc = svm.SVC()\n>>> clf = GridSearchCV(svc, parameters)\n>>> clf.fit(iris.data, iris.target)\nGridSearchCV(estimator=SVC(),\n             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n>>> sorted(clf.cv_results_.keys())\n['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n 'param_C', 'param_kernel', 'params',...\n 'rank_test_score', 'split0_test_score',...\n 'split2_test_score', ...\n 'std_fit_time', 'std_score_time', 'std_test_score']"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "K-fold iterator variant with non-overlapping groups.\n\nEach group will appear exactly once in the test set across all folds (the\nnumber of distinct groups has to be at least equal to the number of folds).\n\nThe folds are approximately balanced in the sense that the number of\ndistinct groups is approximately the same in each fold.\n\nRead more in the :ref:`User Guide <group_k_fold>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nNotes\n-----\nGroups appear in an arbitrary order throughout the folds.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import GroupKFold\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> groups = np.array([0, 0, 2, 2, 3, 3])\n>>> group_kfold = GroupKFold(n_splits=2)\n>>> group_kfold.get_n_splits(X, y, groups)\n2\n>>> print(group_kfold)\nGroupKFold(n_splits=2)\n>>> for i, (train_index, test_index) in enumerate(group_kfold.split(X, y, groups)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\nFold 0:\n  Train: index=[2 3], group=[2 2]\n  Test:  index=[0 1 4 5], group=[0 0 3 3]\nFold 1:\n  Train: index=[0 1 4 5], group=[0 0 3 3]\n  Test:  index=[2 3], group=[2 2]\n\nSee Also\n--------\nLeaveOneGroupOut : For splitting the data according to explicit\n    domain-specific stratification of the dataset.\n\nStratifiedKFold : Takes class information into account to avoid building\n    folds with imbalanced class proportions (for binary or multiclass\n    classification tasks)."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Shuffle-Group(s)-Out cross-validation iterator.\n\nProvides randomized train/test indices to split data according to a\nthird-party provided group. This group information can be used to encode\narbitrary domain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePGroupsOut and GroupShuffleSplit is that\nthe former generates splits using all subsets of size ``p`` unique groups,\nwhereas GroupShuffleSplit generates a user-determined number of random\ntest splits, each with a user-determined fraction of unique groups.\n\nFor example, a less computationally intensive alternative to\n``LeavePGroupsOut(p=10)`` would be\n``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\nNote: The parameters ``test_size`` and ``train_size`` refer to groups, and\nnot to samples, as in ShuffleSplit.\n\nRead more in the :ref:`User Guide <group_shuffle_split>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=5\n    Number of re-shuffling & splitting iterations.\n\ntest_size : float, int, default=0.2\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of groups to include in the test split (rounded up). If int,\n    represents the absolute number of test groups. If None, the value is\n    set to the complement of the train size.\n    The default will change in version 0.21. It will remain 0.2 only\n    if ``train_size`` is unspecified, otherwise it will complement\n    the specified ``train_size``.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the groups to include in the train split. If\n    int, represents the absolute number of train groups. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the training and testing indices produced.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import GroupShuffleSplit\n>>> X = np.ones(shape=(8, 2))\n>>> y = np.ones(shape=(8, 1))\n>>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n>>> print(groups.shape)\n(8,)\n>>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n>>> gss.get_n_splits()\n2\n>>> print(gss)\nGroupShuffleSplit(n_splits=2, random_state=42, test_size=None, train_size=0.7)\n>>> for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\nFold 0:\n  Train: index=[2 3 4 5 6 7], group=[2 2 2 3 3 3]\n  Test:  index=[0 1], group=[1 1]\nFold 1:\n  Train: index=[0 1 5 6 7], group=[1 1 3 3 3]\n  Test:  index=[2 3 4], group=[2 2 2]\n\nSee Also\n--------\nShuffleSplit : Shuffles samples to create independent test/train sets.\n\nLeavePGroupsOut : Train set leaves out all possible subsets of `p` groups."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Cluster data using hierarchical density-based clustering.\n\nHDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\nwith Noise. Performs :class:`~sklearn.cluster.DBSCAN` over varying epsilon\nvalues and integrates the result to find a clustering that gives the best\nstability over epsilon.\nThis allows HDBSCAN to find clusters of varying densities (unlike\n:class:`~sklearn.cluster.DBSCAN`), and be more robust to parameter selection.\nRead more in the :ref:`User Guide <hdbscan>`.\n\nFor an example of how to use HDBSCAN, as well as a comparison to\n:class:`~sklearn.cluster.DBSCAN`, please see the :ref:`plotting demo\n<sphx_glr_auto_examples_cluster_plot_hdbscan.py>`.\n\n.. versionadded:: 1.3\n\nParameters\n----------\nmin_cluster_size : int, default=5\n    The minimum number of samples in a group for that group to be\n    considered a cluster; groupings smaller than this size will be left\n    as noise.\n\nmin_samples : int, default=None\n    The number of samples in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n    When `None`, defaults to `min_cluster_size`.\n\ncluster_selection_epsilon : float, default=0.0\n    A distance threshold. Clusters below this value will be merged.\n    See [5]_ for more information.\n\nmax_cluster_size : int, default=None\n    A limit to the size of clusters returned by the `\"eom\"` cluster\n    selection algorithm. There is no limit when `max_cluster_size=None`.\n    Has no effect if `cluster_selection_method=\"leaf\"`.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array.\n\n    - If metric is a string or callable, it must be one of\n      the options allowed by :func:`~sklearn.metrics.pairwise_distances`\n      for its metric parameter.\n\n    - If metric is \"precomputed\", X is assumed to be a distance matrix and\n      must be square.\n\nmetric_params : dict, default=None\n    Arguments passed to the distance metric.\n\nalpha : float, default=1.0\n    A distance scaling parameter as used in robust single linkage.\n    See [3]_ for more information.\n\nalgorithm : {\"auto\", \"brute\", \"kd_tree\", \"ball_tree\"}, default=\"auto\"\n    Exactly which algorithm to use for computing core distances; By default\n    this is set to `\"auto\"` which attempts to use a\n    :class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses\n    a :class:`~sklearn.neighbors.BallTree` tree. Both `\"kd_tree\"` and\n    `\"ball_tree\"` algorithms use the\n    :class:`~sklearn.neighbors.NearestNeighbors` estimator.\n\n    If the `X` passed during `fit` is sparse or `metric` is invalid for\n    both :class:`~sklearn.neighbors.KDTree` and\n    :class:`~sklearn.neighbors.BallTree`, then it resolves to use the\n    `\"brute\"` algorithm.\n\n    .. deprecated:: 1.4\n       The `'kdtree'` option was deprecated in version 1.4,\n       and will be renamed to `'kd_tree'` in 1.6.\n\n    .. deprecated:: 1.4\n       The `'balltree'` option was deprecated in version 1.4,\n       and will be renamed to `'ball_tree'` in 1.6.\n\nleaf_size : int, default=40\n    Leaf size for trees responsible for fast nearest neighbour queries when\n    a KDTree or a BallTree are used as core-distance algorithms. A large\n    dataset size and small `leaf_size` may induce excessive memory usage.\n    If you are running out of memory consider increasing the `leaf_size`\n    parameter. Ignored for `algorithm=\"brute\"`.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel to calculate distances.\n    `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    `-1` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ncluster_selection_method : {\"eom\", \"leaf\"}, default=\"eom\"\n    The method used to select clusters from the condensed tree. The\n    standard approach for HDBSCAN* is to use an Excess of Mass (`\"eom\"`)\n    algorithm to find the most persistent clusters. Alternatively you can\n    instead select the clusters at the leaves of the tree -- this provides\n    the most fine grained and homogeneous clusters.\n\nallow_single_cluster : bool, default=False\n    By default HDBSCAN* will not produce a single cluster, setting this\n    to True will override this and allow single cluster results in\n    the case that you feel this is a valid result for your dataset.\n\nstore_centers : str, default=None\n    Which, if any, cluster centers to compute and store. The options are:\n\n    - `None` which does not compute nor store any centers.\n    - `\"centroid\"` which calculates the center by taking the weighted\n      average of their positions. Note that the algorithm uses the\n      euclidean metric and does not guarantee that the output will be\n      an observed data point.\n    - `\"medoid\"` which calculates the center by taking the point in the\n      fitted data which minimizes the distance to all other points in\n      the cluster. This is slower than \"centroid\" since it requires\n      computing additional pairwise distances between points of the\n      same cluster but guarantees the output is an observed data point.\n      The medoid is also well-defined for arbitrary metrics, and does not\n      depend on a euclidean metric.\n    - `\"both\"` which computes and stores both forms of centers.\n\ncopy : bool, default=False\n    If `copy=True` then any time an in-place modifications would be made\n    that would overwrite data passed to :term:`fit`, a copy will first be\n    made, guaranteeing that the original data will be unchanged.\n    Currently, it only applies when `metric=\"precomputed\"`, when passing\n    a dense array or a CSR sparse matrix and when `algorithm=\"brute\"`.\n\nAttributes\n----------\nlabels_ : ndarray of shape (n_samples,)\n    Cluster labels for each point in the dataset given to :term:`fit`.\n    Outliers are labeled as follows:\n\n    - Noisy samples are given the label -1.\n    - Samples with infinite elements (+/- np.inf) are given the label -2.\n    - Samples with missing data are given the label -3, even if they\n      also have infinite elements.\n\nprobabilities_ : ndarray of shape (n_samples,)\n    The strength with which each sample is a member of its assigned\n    cluster.\n\n    - Clustered samples have probabilities proportional to the degree that\n      they persist as part of the cluster.\n    - Noisy samples have probability zero.\n    - Samples with infinite elements (+/- np.inf) have probability 0.\n    - Samples with missing data have probability `np.nan`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\ncentroids_ : ndarray of shape (n_clusters, n_features)\n    A collection containing the centroid of each cluster calculated under\n    the standard euclidean metric. The centroids may fall \"outside\" their\n    respective clusters if the clusters themselves are non-convex.\n\n    Note that `n_clusters` only counts non-outlier clusters. That is to\n    say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\nmedoids_ : ndarray of shape (n_clusters, n_features)\n    A collection containing the medoid of each cluster calculated under\n    the whichever metric was passed to the `metric` parameter. The\n    medoids are points in the original cluster which minimize the average\n    distance to all other points in that cluster under the chosen metric.\n    These can be thought of as the result of projecting the `metric`-based\n    centroid back onto the cluster.\n\n    Note that `n_clusters` only counts non-outlier clusters. That is to\n    say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\nSee Also\n--------\nDBSCAN : Density-Based Spatial Clustering of Applications\n    with Noise.\nOPTICS : Ordering Points To Identify the Clustering Structure.\nBirch : Memory-efficient, online-learning algorithm.\n\nReferences\n----------\n\n.. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering\n  based on hierarchical density estimates.\n  <10.1007/978-3-642-37456-2_14>`\n.. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.\n   Hierarchical density estimates for data clustering, visualization,\n   and outlier detection.<10.1145/2733381>`\n\n.. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the\n   cluster tree.\n   <https://papers.nips.cc/paper/2010/hash/\n   b534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_\n\n.. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n   Sander, J. Density-Based Clustering Validation.\n   <https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_\n\n.. [5] :arxiv:`Malzer, C., & Baum, M. \"A Hybrid Approach To Hierarchical\n   Density-based Cluster Selection.\"<1911.02282>`.\n\nExamples\n--------\n>>> from sklearn.cluster import HDBSCAN\n>>> from sklearn.datasets import load_digits\n>>> X, _ = load_digits(return_X_y=True)\n>>> hdb = HDBSCAN(min_cluster_size=20)\n>>> hdb.fit(X)\nHDBSCAN(min_cluster_size=20)\n>>> hdb.labels_\narray([ 2,  6, -1, ..., -1, -1, -1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HdbscanModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Search over specified parameter values with successive halving.\n\nThe search strategy starts evaluating all the candidates with a small\namount of resources and iteratively selects the best candidates, using\nmore and more resources.\n\nRead more in the :ref:`User guide <successive_halving_user_guide>`.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_halving_search_cv``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_halving_search_cv # noqa\n    >>> # now you can import normally from model_selection\n    >>> from sklearn.model_selection import HalvingGridSearchCV\n\nParameters\n----------\nestimator : estimator object\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_grid : dict or list of dictionaries\n    Dictionary with parameters names (string) as keys and lists of\n    parameter settings to try as values, or a list of such\n    dictionaries, in which case the grids spanned by each dictionary\n    in the list are explored. This enables searching over any sequence\n    of parameter settings.\n\nfactor : int or float, default=3\n    The 'halving' parameter, which determines the proportion of candidates\n    that are selected for each subsequent iteration. For example,\n    ``factor=3`` means that only one third of the candidates are selected.\n\nresource : ``'n_samples'`` or str, default='n_samples'\n    Defines the resource that increases with each iteration. By default,\n    the resource is the number of samples. It can also be set to any\n    parameter of the base estimator that accepts positive integer\n    values, e.g. 'n_iterations' or 'n_estimators' for a gradient\n    boosting estimator. In this case ``max_resources`` cannot be 'auto'\n    and must be set explicitly.\n\nmax_resources : int, default='auto'\n    The maximum amount of resource that any candidate is allowed to use\n    for a given iteration. By default, this is set to ``n_samples`` when\n    ``resource='n_samples'`` (default), else an error is raised.\n\nmin_resources : {'exhaust', 'smallest'} or int, default='exhaust'\n    The minimum amount of resource that any candidate is allowed to use\n    for a given iteration. Equivalently, this defines the amount of\n    resources `r0` that are allocated for each candidate at the first\n    iteration.\n\n    - 'smallest' is a heuristic that sets `r0` to a small value:\n\n        - ``n_splits * 2`` when ``resource='n_samples'`` for a regression\n          problem\n        - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a\n          classification problem\n        - ``1`` when ``resource != 'n_samples'``\n\n    - 'exhaust' will set `r0` such that the **last** iteration uses as\n      much resources as possible. Namely, the last iteration will use the\n      highest value smaller than ``max_resources`` that is a multiple of\n      both ``min_resources`` and ``factor``. In general, using 'exhaust'\n      leads to a more accurate estimator, but is slightly more time\n      consuming.\n\n    Note that the amount of resources used at each iteration is always a\n    multiple of ``min_resources``.\n\naggressive_elimination : bool, default=False\n    This is only relevant in cases where there isn't enough resources to\n    reduce the remaining candidates to at most `factor` after the last\n    iteration. If ``True``, then the search process will 'replay' the\n    first iteration for as long as needed until the number of candidates\n    is small enough. This is ``False`` by default, which means that the\n    last iteration may evaluate more than ``factor`` candidates. See\n    :ref:`aggressive_elimination` for more details.\n\ncv : int, cross-validation generator or iterable, default=5\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used. These splitters are instantiated\n    with `shuffle=False` so the splits will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n        Due to implementation details, the folds produced by `cv` must be\n        the same across multiple calls to `cv.split()`. For\n        built-in `scikit-learn` iterators, this can be achieved by\n        deactivating shuffling (`shuffle=False`), or by setting the\n        `cv`'s `random_state` parameter to an integer.\n\nscoring : str, callable, or None, default=None\n    A single string (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n    If None, the estimator's score method is used.\n\nrefit : bool, default=True\n    If True, refit an estimator using the best found parameters on the\n    whole dataset.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``HalvingGridSearchCV`` instance.\n\nerror_score : 'raise' or numeric\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error. Default is ``np.nan``.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for subsampling the dataset\n    when `resources != 'n_samples'`. Ignored otherwise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int or None, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\nAttributes\n----------\nn_resources_ : list of int\n    The amount of resources used at each iteration.\n\nn_candidates_ : list of int\n    The number of candidate parameters that were evaluated at each\n    iteration.\n\nn_remaining_candidates_ : int\n    The number of candidate parameters that are left after the last\n    iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\nmax_resources_ : int\n    The maximum number of resources that any candidate is allowed to use\n    for a given iteration. Note that since the number of resources used\n    at each iteration must be a multiple of ``min_resources_``, the\n    actual number of resources used at the last iteration may be smaller\n    than ``max_resources_``.\n\nmin_resources_ : int\n    The amount of resources that are allocated for each candidate at the\n    first iteration.\n\nn_iterations_ : int\n    The actual number of iterations that were run. This is equal to\n    ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.\n    Else, this is equal to ``min(n_possible_iterations_,\n    n_required_iterations_)``.\n\nn_possible_iterations_ : int\n    The number of iterations that are possible starting with\n    ``min_resources_`` resources and without exceeding\n    ``max_resources_``.\n\nn_required_iterations_ : int\n    The number of iterations that are required to end up with less than\n    ``factor`` candidates at the last iteration, starting with\n    ``min_resources_`` resources. This will be smaller than\n    ``n_possible_iterations_`` when there isn't enough resources.\n\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``. It contains lots of information\n    for analysing the results of a search.\n    Please refer to the :ref:`User guide<successive_halving_cv_results>`\n    for details.\n\nbest_estimator_ : estimator or dict\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. This is present only if ``refit`` is specified and\n    the underlying estimator is a classifier.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `n_features_in_` when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `feature_names_in_` when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\n:class:`HalvingRandomSearchCV`:\n    Random search over a set of parameters using successive halving.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the held-out\ndata, according to the scoring parameter.\n\nAll parameter combinations scored with a NaN will share the lowest rank.\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingGridSearchCV\n...\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = RandomForestClassifier(random_state=0)\n...\n>>> param_grid = {\"max_depth\": [3, None],\n...               \"min_samples_split\": [5, 10]}\n>>> search = HalvingGridSearchCV(clf, param_grid, resource='n_estimators',\n...                              max_resources=10,\n...                              random_state=0).fit(X, y)\n>>> search.best_params_  # doctest: +SKIP\n{'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Randomized search on hyper parameters.\n\nThe search strategy starts evaluating all the candidates with a small\namount of resources and iteratively selects the best candidates, using more\nand more resources.\n\nThe candidates are sampled at random from the parameter space and the\nnumber of sampled candidates is determined by ``n_candidates``.\n\nRead more in the :ref:`User guide<successive_halving_user_guide>`.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_halving_search_cv``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_halving_search_cv # noqa\n    >>> # now you can import normally from model_selection\n    >>> from sklearn.model_selection import HalvingRandomSearchCV\n\nParameters\n----------\nestimator : estimator object\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_distributions : dict or list of dicts\n    Dictionary with parameters names (`str`) as keys and distributions\n    or lists of parameters to try. Distributions must provide a ``rvs``\n    method for sampling (such as those from scipy.stats.distributions).\n    If a list is given, it is sampled uniformly.\n    If a list of dicts is given, first a dict is sampled uniformly, and\n    then a parameter is sampled using that dict as above.\n\nn_candidates : \"exhaust\" or int, default=\"exhaust\"\n    The number of candidate parameters to sample, at the first\n    iteration. Using 'exhaust' will sample enough candidates so that the\n    last iteration uses as many resources as possible, based on\n    `min_resources`, `max_resources` and `factor`. In this case,\n    `min_resources` cannot be 'exhaust'.\n\nfactor : int or float, default=3\n    The 'halving' parameter, which determines the proportion of candidates\n    that are selected for each subsequent iteration. For example,\n    ``factor=3`` means that only one third of the candidates are selected.\n\nresource : ``'n_samples'`` or str, default='n_samples'\n    Defines the resource that increases with each iteration. By default,\n    the resource is the number of samples. It can also be set to any\n    parameter of the base estimator that accepts positive integer\n    values, e.g. 'n_iterations' or 'n_estimators' for a gradient\n    boosting estimator. In this case ``max_resources`` cannot be 'auto'\n    and must be set explicitly.\n\nmax_resources : int, default='auto'\n    The maximum number of resources that any candidate is allowed to use\n    for a given iteration. By default, this is set ``n_samples`` when\n    ``resource='n_samples'`` (default), else an error is raised.\n\nmin_resources : {'exhaust', 'smallest'} or int, default='smallest'\n    The minimum amount of resource that any candidate is allowed to use\n    for a given iteration. Equivalently, this defines the amount of\n    resources `r0` that are allocated for each candidate at the first\n    iteration.\n\n    - 'smallest' is a heuristic that sets `r0` to a small value:\n\n        - ``n_splits * 2`` when ``resource='n_samples'`` for a regression\n          problem\n        - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a\n          classification problem\n        - ``1`` when ``resource != 'n_samples'``\n\n    - 'exhaust' will set `r0` such that the **last** iteration uses as\n      much resources as possible. Namely, the last iteration will use the\n      highest value smaller than ``max_resources`` that is a multiple of\n      both ``min_resources`` and ``factor``. In general, using 'exhaust'\n      leads to a more accurate estimator, but is slightly more time\n      consuming. 'exhaust' isn't available when `n_candidates='exhaust'`.\n\n    Note that the amount of resources used at each iteration is always a\n    multiple of ``min_resources``.\n\naggressive_elimination : bool, default=False\n    This is only relevant in cases where there isn't enough resources to\n    reduce the remaining candidates to at most `factor` after the last\n    iteration. If ``True``, then the search process will 'replay' the\n    first iteration for as long as needed until the number of candidates\n    is small enough. This is ``False`` by default, which means that the\n    last iteration may evaluate more than ``factor`` candidates. See\n    :ref:`aggressive_elimination` for more details.\n\ncv : int, cross-validation generator or an iterable, default=5\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used. These splitters are instantiated\n    with `shuffle=False` so the splits will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n        Due to implementation details, the folds produced by `cv` must be\n        the same across multiple calls to `cv.split()`. For\n        built-in `scikit-learn` iterators, this can be achieved by\n        deactivating shuffling (`shuffle=False`), or by setting the\n        `cv`'s `random_state` parameter to an integer.\n\nscoring : str, callable, or None, default=None\n    A single string (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n    If None, the estimator's score method is used.\n\nrefit : bool, default=True\n    If True, refit an estimator using the best found parameters on the\n    whole dataset.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``HalvingRandomSearchCV`` instance.\n\nerror_score : 'raise' or numeric\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error. Default is ``np.nan``.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for subsampling the dataset\n    when `resources != 'n_samples'`. Also used for random uniform\n    sampling from lists of possible values instead of scipy.stats\n    distributions.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int or None, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\nAttributes\n----------\nn_resources_ : list of int\n    The amount of resources used at each iteration.\n\nn_candidates_ : list of int\n    The number of candidate parameters that were evaluated at each\n    iteration.\n\nn_remaining_candidates_ : int\n    The number of candidate parameters that are left after the last\n    iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\nmax_resources_ : int\n    The maximum number of resources that any candidate is allowed to use\n    for a given iteration. Note that since the number of resources used at\n    each iteration must be a multiple of ``min_resources_``, the actual\n    number of resources used at the last iteration may be smaller than\n    ``max_resources_``.\n\nmin_resources_ : int\n    The amount of resources that are allocated for each candidate at the\n    first iteration.\n\nn_iterations_ : int\n    The actual number of iterations that were run. This is equal to\n    ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.\n    Else, this is equal to ``min(n_possible_iterations_,\n    n_required_iterations_)``.\n\nn_possible_iterations_ : int\n    The number of iterations that are possible starting with\n    ``min_resources_`` resources and without exceeding\n    ``max_resources_``.\n\nn_required_iterations_ : int\n    The number of iterations that are required to end up with less than\n    ``factor`` candidates at the last iteration, starting with\n    ``min_resources_`` resources. This will be smaller than\n    ``n_possible_iterations_`` when there isn't enough resources.\n\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``. It contains lots of information\n    for analysing the results of a search.\n    Please refer to the :ref:`User guide<successive_halving_cv_results>`\n    for details.\n\nbest_estimator_ : estimator or dict\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. This is present only if ``refit`` is specified and\n    the underlying estimator is a classifier.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `n_features_in_` when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `feature_names_in_` when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\n:class:`HalvingGridSearchCV`:\n    Search over a grid of parameters using successive halving.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the held-out\ndata, according to the scoring parameter.\n\nAll parameter combinations scored with a NaN will share the lowest rank.\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.experimental import enable_halving_search_cv  # noqa\n>>> from sklearn.model_selection import HalvingRandomSearchCV\n>>> from scipy.stats import randint\n>>> import numpy as np\n...\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = RandomForestClassifier(random_state=0)\n>>> np.random.seed(0)\n...\n>>> param_distributions = {\"max_depth\": [3, None],\n...                        \"min_samples_split\": randint(2, 11)}\n>>> search = HalvingRandomSearchCV(clf, param_distributions,\n...                                resource='n_estimators',\n...                                max_resources=10,\n...                                random_state=0).fit(X, y)\n>>> search.best_params_  # doctest: +SKIP\n{'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HammingLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the average Hamming loss.\n\nThe Hamming loss is the fraction of labels that are incorrectly predicted.\n\nRead more in the :ref:`User Guide <hamming_loss>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nloss : float or int\n    Return the average Hamming loss between element of ``y_true`` and\n    ``y_pred``.\n\nSee Also\n--------\naccuracy_score : Compute the accuracy score. By default, the function will\n    return the fraction of correct predictions divided by the total number\n    of predictions.\njaccard_score : Compute the Jaccard similarity coefficient score.\nzero_one_loss : Compute the Zero-one classification loss. By default, the\n    function will return the percentage of imperfectly predicted subsets.\n\nNotes\n-----\nIn multiclass classification, the Hamming loss corresponds to the Hamming\ndistance between ``y_true`` and ``y_pred`` which is equivalent to the\nsubset ``zero_one_loss`` function, when `normalize` parameter is set to\nTrue.\n\nIn multilabel classification, the Hamming loss is different from the\nsubset zero-one loss. The zero-one loss considers the entire set of labels\nfor a given sample incorrect if it does not entirely match the true set of\nlabels. Hamming loss is more forgiving in that it penalizes only the\nindividual labels.\n\nThe Hamming loss is upperbounded by the subset zero-one loss, when\n`normalize` parameter is set to True. It is always between 0 and 1,\nlower being better.\n\nReferences\n----------\n.. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n       An Overview. International Journal of Data Warehousing & Mining,\n       3(3), 1-13, July-September 2007.\n\n.. [2] `Wikipedia entry on the Hamming distance\n       <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import hamming_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> hamming_loss(y_true, y_pred)\n0.25\n\nIn the multilabel case with binary label indicators:\n\n>>> import numpy as np\n>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HdbscanModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Average hinge loss (non-regularized).\n\nIn binary class case, assuming labels in y_true are encoded with +1 and -1,\nwhen a prediction mistake is made, ``margin = y_true * pred_decision`` is\nalways negative (since the signs disagree), implying ``1 - margin`` is\nalways greater than 1.  The cumulated hinge loss is therefore an upper\nbound of the number of mistakes made by the classifier.\n\nIn multiclass case, the function expects that either all the labels are\nincluded in y_true or an optional labels argument is provided which\ncontains all the labels. The multilabel margin is calculated according\nto Crammer-Singer's method. As in the binary case, the cumulated hinge loss\nis an upper bound of the number of mistakes made by the classifier.\n\nRead more in the :ref:`User Guide <hinge_loss>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True target, consisting of integers of two values. The positive label\n    must be greater than the negative label.\n\npred_decision : array-like of shape (n_samples,) or (n_samples, n_classes)\n    Predicted decisions, as output by decision_function (floats).\n\nlabels : array-like, default=None\n    Contains all the labels for the problem. Used in multiclass hinge loss.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n    Average hinge loss.\n\nReferences\n----------\n.. [1] `Wikipedia entry on the Hinge loss\n       <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n\n.. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n       Implementation of Multiclass Kernel-based Vector\n       Machines. Journal of Machine Learning Research 2,\n       (2001), 265-292.\n\n.. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n       by Robert C. Moore, John DeNero\n       <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.\n\nExamples\n--------\n>>> from sklearn import svm\n>>> from sklearn.metrics import hinge_loss\n>>> X = [[0], [1]]\n>>> y = [-1, 1]\n>>> est = svm.LinearSVC(dual=\"auto\", random_state=0)\n>>> est.fit(X, y)\nLinearSVC(dual='auto', random_state=0)\n>>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n>>> pred_decision\narray([-2.18...,  2.36...,  0.09...])\n>>> hinge_loss([-1, 1, 1], pred_decision)\n0.30...\n\nIn the multiclass case:\n\n>>> import numpy as np\n>>> X = np.array([[0], [1], [2], [3]])\n>>> Y = np.array([0, 1, 2, 3])\n>>> labels = np.array([0, 1, 2, 3])\n>>> est = svm.LinearSVC(dual=\"auto\")\n>>> est.fit(X, Y)\nLinearSVC(dual='auto')\n>>> pred_decision = est.decision_function([[-1], [2], [3]])\n>>> y_true = [0, 2, 3]\n>>> hinge_loss(y_true, pred_decision, labels=labels)\n0.56..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Histogram-based Gradient Boosting Classification Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nloss : {'log_loss'}, default='log_loss'\n    The loss function to use in the boosting process.\n\n    For binary classification problems, 'log_loss' is also known as logistic loss,\n    binomial deviance or binary crossentropy. Internally, the model fits one tree\n    per boosting iteration and uses the logistic sigmoid function (expit) as\n    inverse link function to compute the predicted positive class probability.\n\n    For multiclass classification problems, 'log_loss' is also known as multinomial\n    deviance or categorical crossentropy. Internally, the model fits one tree per\n    boosting iteration and per class and uses the softmax function as inverse link\n    function to compute the predicted probabilities of the classes.\n\nlearning_rate : float, default=0.1\n    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\nmax_iter : int, default=100\n    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees for binary classification. For multiclass\n    classification, `n_classes` trees per iteration are built.\nmax_leaf_nodes : int or None, default=31\n    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\nmax_depth : int or None, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\nl2_regularization : float, default=0\n    The L2 regularization parameter. Use ``0`` for no regularization (default).\nmax_features : float, default=1.0\n    Proportion of randomly chosen features in each and every node split.\n    This is a form of regularization, smaller values make the trees weaker\n    learners and might prevent overfitting.\n    If interaction constraints from `interaction_cst` are present, only allowed\n    features are taken into account for the subsampling.\n\n    .. versionadded:: 1.4\n\nmax_bins : int, default=255\n    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\ncategorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None\n    Indicates the categorical features.\n\n    - None : no feature will be considered categorical.\n    - boolean array-like : boolean mask indicating categorical features.\n    - integer array-like : integer indices indicating categorical\n      features.\n    - str array-like: names of categorical features (assuming the training\n      data has feature names).\n    - `\"from_dtype\"`: dataframe columns with dtype \"category\" are\n      considered to be categorical features. The input must be an object\n      exposing a ``__dataframe__`` method such as pandas or polars\n      DataFrames to use this feature.\n\n    For each categorical feature, there must be at most `max_bins` unique\n    categories. Negative values for categorical features encoded as numeric\n    dtypes are treated as missing values. All categorical values are\n    converted to floating point numbers. This means that categorical values\n    of 1.0 and 1 are treated as the same category.\n\n    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n    .. versionadded:: 0.24\n\n    .. versionchanged:: 1.2\n       Added support for feature names.\n\n    .. versionchanged:: 1.4\n       Added `\"from_dtype\"` option. The default will change to `\"from_dtype\"` in\n       v1.6.\n\nmonotonic_cst : array-like of int of shape (n_features) or dict, default=None\n    Monotonic constraint to enforce on each feature are specified using the\n    following integer values:\n\n    - 1: monotonic increase\n    - 0: no constraint\n    - -1: monotonic decrease\n\n    If a dict with str keys, map feature to monotonic constraints by name.\n    If an array, the features are mapped to constraints by position. See\n    :ref:`monotonic_cst_features_names` for a usage example.\n\n    The constraints are only valid for binary classifications and hold\n    over the probability of the positive class.\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 0.23\n\n    .. versionchanged:: 1.2\n       Accept dict of constraints with feature names as keys.\n\ninteraction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None\n    Specify interaction constraints, the sets of features which can\n    interact with each other in child node splits.\n\n    Each item specifies the set of feature indices that are allowed\n    to interact with each other. If there are more features than\n    specified in these constraints, they are treated as if they were\n    specified as an additional set.\n\n    The strings \"pairwise\" and \"no_interactions\" are shorthands for\n    allowing only pairwise or no interactions, respectively.\n\n    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n    is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n    and specifies that each branch of a tree will either only split\n    on features 0 and 1 or only split on features 2, 3 and 4.\n\n    .. versionadded:: 1.2\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\nearly_stopping : 'auto' or bool, default='auto'\n    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n\n    .. versionadded:: 0.23\n\nscoring : str or callable or None, default='loss'\n    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer\n    is used. If ``scoring='loss'``, early stopping is checked\n    w.r.t the loss value. Only used if early stopping is performed.\nvalidation_fraction : int or float or None, default=0.1\n    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\nn_iter_no_change : int, default=10\n    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\ntol : float, default=1e-7\n    The absolute tolerance to use when comparing scores. The higher the\n    tolerance, the more likely we are to early stop: higher tolerance\n    means that it will be harder for subsequent iterations to be\n    considered an improvement upon the reference score.\nverbose : int, default=0\n    The verbosity level. If not zero, print some information about the\n    fitting process.\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form `{class_label: weight}`.\n    If not given, all classes are supposed to have weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as `n_samples / (n_classes * np.bincount(y))`.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if `sample_weight` is specified.\n\n    .. versionadded:: 1.2\n\nAttributes\n----------\nclasses_ : array, shape = (n_classes,)\n    Class labels.\ndo_early_stopping_ : bool\n    Indicates whether early stopping is used during training.\nn_iter_ : int\n    The number of iterations as selected by early stopping, depending on\n    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\nn_trees_per_iteration_ : int\n    The number of tree that are built at each iteration. This is equal to 1\n    for binary classification, and to ``n_classes`` for multiclass\n    classification.\ntrain_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the training data. The first entry\n    is the score of the ensemble before the first iteration. Scores are\n    computed according to the ``scoring`` parameter. If ``scoring`` is\n    not 'loss', scores are computed on a subset of at most 10 000\n    samples. Empty if no early stopping.\nvalidation_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the held-out validation data. The\n    first entry is the score of the ensemble before the first iteration.\n    Scores are computed according to the ``scoring`` parameter. Empty if\n    no early stopping or if ``validation_fraction`` is None.\nis_categorical_ : ndarray, shape (n_features, ) or None\n    Boolean mask for the categorical features. ``None`` if there are no\n    categorical features.\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nGradientBoostingClassifier : Exact gradient boosting method that does not\n    scale as good on datasets with a large number of samples.\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nRandomForestClassifier : A meta-estimator that fits a number of decision\n    tree classifiers on various sub-samples of the dataset and uses\n    averaging to improve the predictive accuracy and control over-fitting.\nAdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n    on the original dataset and then fits additional copies of the\n    classifier on the same dataset where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers\n    focus more on difficult cases.\n\nExamples\n--------\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = HistGradientBoostingClassifier().fit(X, y)\n>>> clf.score(X, y)\n1.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Histogram-based Gradient Boosting Regression Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nloss : {'squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile'},             default='squared_error'\n    The loss function to use in the boosting process. Note that the\n    \"squared error\", \"gamma\" and \"poisson\" losses actually implement\n    \"half least squares loss\", \"half gamma deviance\" and \"half poisson\n    deviance\" to simplify the computation of the gradient. Furthermore,\n    \"gamma\" and \"poisson\" losses internally use a log-link, \"gamma\"\n    requires ``y > 0`` and \"poisson\" requires ``y >= 0``.\n    \"quantile\" uses the pinball loss.\n\n    .. versionchanged:: 0.23\n       Added option 'poisson'.\n\n    .. versionchanged:: 1.1\n       Added option 'quantile'.\n\n    .. versionchanged:: 1.3\n       Added option 'gamma'.\n\nquantile : float, default=None\n    If loss is \"quantile\", this parameter specifies which quantile to be estimated\n    and must be between 0 and 1.\nlearning_rate : float, default=0.1\n    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\nmax_iter : int, default=100\n    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees.\nmax_leaf_nodes : int or None, default=31\n    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\nmax_depth : int or None, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\nl2_regularization : float, default=0\n    The L2 regularization parameter. Use ``0`` for no regularization (default).\nmax_features : float, default=1.0\n    Proportion of randomly chosen features in each and every node split.\n    This is a form of regularization, smaller values make the trees weaker\n    learners and might prevent overfitting.\n    If interaction constraints from `interaction_cst` are present, only allowed\n    features are taken into account for the subsampling.\n\n    .. versionadded:: 1.4\n\nmax_bins : int, default=255\n    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\ncategorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default=None\n    Indicates the categorical features.\n\n    - None : no feature will be considered categorical.\n    - boolean array-like : boolean mask indicating categorical features.\n    - integer array-like : integer indices indicating categorical\n      features.\n    - str array-like: names of categorical features (assuming the training\n      data has feature names).\n    - `\"from_dtype\"`: dataframe columns with dtype \"category\" are\n      considered to be categorical features. The input must be an object\n      exposing a ``__dataframe__`` method such as pandas or polars\n      DataFrames to use this feature.\n\n    For each categorical feature, there must be at most `max_bins` unique\n    categories. Negative values for categorical features encoded as numeric\n    dtypes are treated as missing values. All categorical values are\n    converted to floating point numbers. This means that categorical values\n    of 1.0 and 1 are treated as the same category.\n\n    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n    .. versionadded:: 0.24\n\n    .. versionchanged:: 1.2\n       Added support for feature names.\n\n    .. versionchanged:: 1.4\n       Added `\"from_dtype\"` option. The default will change to `\"from_dtype\"` in\n       v1.6.\n\nmonotonic_cst : array-like of int of shape (n_features) or dict, default=None\n    Monotonic constraint to enforce on each feature are specified using the\n    following integer values:\n\n    - 1: monotonic increase\n    - 0: no constraint\n    - -1: monotonic decrease\n\n    If a dict with str keys, map feature to monotonic constraints by name.\n    If an array, the features are mapped to constraints by position. See\n    :ref:`monotonic_cst_features_names` for a usage example.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 0.23\n\n    .. versionchanged:: 1.2\n       Accept dict of constraints with feature names as keys.\n\ninteraction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None\n    Specify interaction constraints, the sets of features which can\n    interact with each other in child node splits.\n\n    Each item specifies the set of feature indices that are allowed\n    to interact with each other. If there are more features than\n    specified in these constraints, they are treated as if they were\n    specified as an additional set.\n\n    The strings \"pairwise\" and \"no_interactions\" are shorthands for\n    allowing only pairwise or no interactions, respectively.\n\n    For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n    is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n    and specifies that each branch of a tree will either only split\n    on features 0 and 1 or only split on features 2, 3 and 4.\n\n    .. versionadded:: 1.2\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\nearly_stopping : 'auto' or bool, default='auto'\n    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n\n    .. versionadded:: 0.23\n\nscoring : str or callable or None, default='loss'\n    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer is used. If\n    ``scoring='loss'``, early stopping is checked w.r.t the loss value.\n    Only used if early stopping is performed.\nvalidation_fraction : int or float or None, default=0.1\n    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\nn_iter_no_change : int, default=10\n    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\ntol : float, default=1e-7\n    The absolute tolerance to use when comparing scores during early\n    stopping. The higher the tolerance, the more likely we are to early\n    stop: higher tolerance means that it will be harder for subsequent\n    iterations to be considered an improvement upon the reference score.\nverbose : int, default=0\n    The verbosity level. If not zero, print some information about the\n    fitting process.\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ndo_early_stopping_ : bool\n    Indicates whether early stopping is used during training.\nn_iter_ : int\n    The number of iterations as selected by early stopping, depending on\n    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\nn_trees_per_iteration_ : int\n    The number of tree that are built at each iteration. For regressors,\n    this is always 1.\ntrain_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the training data. The first entry\n    is the score of the ensemble before the first iteration. Scores are\n    computed according to the ``scoring`` parameter. If ``scoring`` is\n    not 'loss', scores are computed on a subset of at most 10 000\n    samples. Empty if no early stopping.\nvalidation_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the held-out validation data. The\n    first entry is the score of the ensemble before the first iteration.\n    Scores are computed according to the ``scoring`` parameter. Empty if\n    no early stopping or if ``validation_fraction`` is None.\nis_categorical_ : ndarray, shape (n_features, ) or None\n    Boolean mask for the categorical features. ``None`` if there are no\n    categorical features.\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nGradientBoostingRegressor : Exact gradient boosting method that does not\n    scale as good on datasets with a large number of samples.\nsklearn.tree.DecisionTreeRegressor : A decision tree regressor.\nRandomForestRegressor : A meta-estimator that fits a number of decision\n    tree regressors on various sub-samples of the dataset and uses\n    averaging to improve the statistical performance and control\n    over-fitting.\nAdaBoostRegressor : A meta-estimator that begins by fitting a regressor\n    on the original dataset and then fits additional copies of the\n    regressor on the same dataset but where the weights of instances are\n    adjusted according to the error of the current prediction. As such,\n    subsequent regressors focus more on difficult cases.\n\nExamples\n--------\n>>> from sklearn.ensemble import HistGradientBoostingRegressor\n>>> from sklearn.datasets import load_diabetes\n>>> X, y = load_diabetes(return_X_y=True)\n>>> est = HistGradientBoostingRegressor().fit(X, y)\n>>> est.score(X, y)\n0.92..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the homogeneity and completeness and V-Measure scores at once.\n\nThose metrics are based on normalized conditional entropy measures of\nthe clustering labeling to evaluate given the knowledge of a Ground\nTruth class labels of the same samples.\n\nA clustering result satisfies homogeneity if all of its clusters\ncontain only data points which are members of a single class.\n\nA clustering result satisfies completeness if all the data points\nthat are members of a given class are elements of the same cluster.\n\nBoth scores have positive values between 0.0 and 1.0, larger values\nbeing desirable.\n\nThose 3 metrics are independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore values in any way.\n\nV-Measure is furthermore symmetric: swapping ``labels_true`` and\n``label_pred`` will give the same score. This does not hold for\nhomogeneity and completeness. V-Measure is identical to\n:func:`normalized_mutual_info_score` with the arithmetic averaging\nmethod.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,)\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,)\n    Gluster labels to evaluate.\n\nbeta : float, default=1.0\n    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n    If ``beta`` is greater than 1, ``completeness`` is weighted more\n    strongly in the calculation. If ``beta`` is less than 1,\n    ``homogeneity`` is weighted more strongly.\n\nReturns\n-------\nhomogeneity : float\n    Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n\ncompleteness : float\n    Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n\nv_measure : float\n    Harmonic mean of the first two.\n\nSee Also\n--------\nhomogeneity_score : Homogeneity metric of cluster labeling.\ncompleteness_score : Completeness metric of cluster labeling.\nv_measure_score : V-Measure (NMI with arithmetic mean option).\n\nExamples\n--------\n>>> from sklearn.metrics import homogeneity_completeness_v_measure\n>>> y_true, y_pred = [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 2, 2]\n>>> homogeneity_completeness_v_measure(y_true, y_pred)\n(0.71..., 0.77..., 0.73...)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Homogeneity metric of a cluster labeling given a ground truth.\n\nA clustering result satisfies homogeneity if all of its clusters\ncontain only data points which are members of a single class.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is not symmetric: switching ``label_true`` with ``label_pred``\nwill return the :func:`completeness_score` which will be different in\ngeneral.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,)\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,)\n    Cluster labels to evaluate.\n\nReturns\n-------\nhomogeneity : float\n   Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n\nSee Also\n--------\ncompleteness_score : Completeness metric of cluster labeling.\nv_measure_score : V-Measure (NMI with arithmetic mean option).\n\nReferences\n----------\n\n.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n   conditional entropy-based external cluster evaluation measure\n   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n\nExamples\n--------\n\nPerfect labelings are homogeneous::\n\n  >>> from sklearn.metrics.cluster import homogeneity_score\n  >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nNon-perfect labelings that further split classes into more clusters can be\nperfectly homogeneous::\n\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n  1.000000\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n  1.000000\n\nClusters that include samples from different classes do not make for an\nhomogeneous labeling::\n\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n  0.0...\n  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n  0.0..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "L2-regularized linear regression model that is robust to outliers.\n\nThe Huber Regressor optimizes the squared loss for the samples where\n``|(y - Xw - c) / sigma| < epsilon`` and the absolute loss for the samples\nwhere ``|(y - Xw - c) / sigma| > epsilon``, where the model coefficients\n``w``, the intercept ``c`` and the scale ``sigma`` are parameters\nto be optimized. The parameter sigma makes sure that if y is scaled up\nor down by a certain factor, one does not need to rescale epsilon to\nachieve the same robustness. Note that this does not take into account\nthe fact that the different features of X may be of different scales.\n\nThe Huber loss function has the advantage of not being heavily influenced\nby the outliers while not completely ignoring their effect.\n\nRead more in the :ref:`User Guide <huber_regression>`\n\n.. versionadded:: 0.18\n\nParameters\n----------\nepsilon : float, default=1.35\n    The parameter epsilon controls the number of samples that should be\n    classified as outliers. The smaller the epsilon, the more robust it is\n    to outliers. Epsilon must be in the range `[1, inf)`.\n\nmax_iter : int, default=100\n    Maximum number of iterations that\n    ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n\nalpha : float, default=0.0001\n    Strength of the squared L2 regularization. Note that the penalty is\n    equal to ``alpha * ||w||^2``.\n    Must be in the range `[0, inf)`.\n\nwarm_start : bool, default=False\n    This is useful if the stored attributes of a previously used model\n    has to be reused. If set to False, then the coefficients will\n    be rewritten for every call to fit.\n    See :term:`the Glossary <warm_start>`.\n\nfit_intercept : bool, default=True\n    Whether or not to fit the intercept. This can be set to False\n    if the data is already centered around the origin.\n\ntol : float, default=1e-05\n    The iteration will stop when\n    ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n    where pg_i is the i-th component of the projected gradient.\n\nAttributes\n----------\ncoef_ : array, shape (n_features,)\n    Features got by optimizing the L2-regularized Huber loss.\n\nintercept_ : float\n    Bias.\n\nscale_ : float\n    The value by which ``|y - Xw - c|`` is scaled down.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of iterations that\n    ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n\n    .. versionchanged:: 0.20\n\n        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\noutliers_ : array, shape (n_samples,)\n    A boolean mask which is set to True where the samples are identified\n    as outliers.\n\nSee Also\n--------\nRANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\nTheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\nSGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\nReferences\n----------\n.. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n       Concomitant scale estimates, pg 172\n.. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n       https://statweb.stanford.edu/~owen/reports/hhu.pdf\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import HuberRegressor, LinearRegression\n>>> from sklearn.datasets import make_regression\n>>> rng = np.random.RandomState(0)\n>>> X, y, coef = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n>>> X[:4] = rng.uniform(10, 20, (4, 2))\n>>> y[:4] = rng.uniform(10, 20, 4)\n>>> huber = HuberRegressor().fit(X, y)\n>>> huber.score(X, y)\n-7.284...\n>>> huber.predict(X[:1,])\narray([806.7200...])\n>>> linear = LinearRegression().fit(X, y)\n>>> print(\"True coefficients:\", coef)\nTrue coefficients: [20.4923...  34.1698...]\n>>> print(\"Huber coefficients:\", huber.coef_)\nHuber coefficients: [17.7906... 31.0106...]\n>>> print(\"Linear Regression coefficients:\", linear.coef_)\nLinear Regression coefficients: [-1.9221...  7.0226...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Incremental principal components analysis (IPCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of\nthe data, keeping only the most significant singular vectors to\nproject the data to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nDepending on the size of the input data, this algorithm can be much more\nmemory efficient than a PCA, and allows sparse input.\n\nThis algorithm has constant memory complexity, on the order\nof ``batch_size * n_features``, enabling use of np.memmap files without\nloading the entire file into memory. For sparse matrices, the input\nis converted to dense in batches (in order to be able to subtract the\nmean) which avoids storing the entire dense matrix at any one time.\n\nThe computational overhead of each SVD is\n``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\nremain in memory at a time. There will be ``n_samples / batch_size`` SVD\ncomputations to get the principal components, versus 1 large SVD of\ncomplexity ``O(n_samples * n_features ** 2)`` for PCA.\n\nFor a usage example, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`.\n\nRead more in the :ref:`User Guide <IncrementalPCA>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nn_components : int, default=None\n    Number of components to keep. If ``n_components`` is ``None``,\n    then ``n_components`` is set to ``min(n_samples, n_features)``.\n\nwhiten : bool, default=False\n    When True (False by default) the ``components_`` vectors are divided\n    by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n    with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometimes\n    improve the predictive accuracy of the downstream estimators by\n    making data respect some hard-wired assumptions.\n\ncopy : bool, default=True\n    If False, X will be overwritten. ``copy=False`` can be used to\n    save memory but is unsafe for general use.\n\nbatch_size : int, default=None\n    The number of samples to use for each batch. Only used when calling\n    ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n    is inferred from the data and set to ``5 * n_features``, to provide a\n    balance between approximation accuracy and memory consumption.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. Equivalently, the right singular\n    vectors of the centered input data, parallel to its eigenvectors.\n    The components are sorted by decreasing ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    Variance explained by each of the selected components.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n    If all components are stored, the sum of explained variances is equal\n    to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\nvar_ : ndarray of shape (n_features,)\n    Per-feature empirical variance, aggregate over calls to\n    ``partial_fit``.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf.\n\nn_components_ : int\n    The estimated number of components. Relevant when\n    ``n_components=None``.\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator. Will be reset on\n    new calls to fit, but increments across ``partial_fit`` calls.\n\nbatch_size_ : int\n    Inferred batch size from ``batch_size``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nPCA : Principal component analysis (PCA).\nKernelPCA : Kernel Principal component analysis (KPCA).\nSparsePCA : Sparse Principal Components Analysis (SparsePCA).\nTruncatedSVD : Dimensionality reduction using truncated SVD.\n\nNotes\n-----\nImplements the incremental PCA model from:\n*D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\npp. 125-141, May 2008.*\nSee https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\nThis model is an extension of the Sequential Karhunen-Loeve Transform from:\n:doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\nits Application to Images, IEEE Transactions on Image Processing, Volume 9,\nNumber 8, pp. 1371-1374, August 2000. <10.1109/83.855432>`\n\nWe have specifically abstained from an optimization used by authors of both\npapers, a QR decomposition used in specific situations to reduce the\nalgorithmic complexity of the SVD. The source for this technique is\n*Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\nsection 5.4.4, pp 252-253.*. This technique has been omitted because it is\nadvantageous only when decomposing a matrix with ``n_samples`` (rows)\n>= 5/3 * ``n_features`` (columns), and hurts the readability of the\nimplemented algorithm. This would be a good opportunity for future\noptimization, if it is deemed necessary.\n\nReferences\n----------\nD. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77,\nIssue 1-3, pp. 125-141, May 2008.\n\nG. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\nSection 5.4.4, pp. 252-253.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import IncrementalPCA\n>>> from scipy import sparse\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n>>> # either partially fit on smaller batches of data\n>>> transformer.partial_fit(X[:100, :])\nIncrementalPCA(batch_size=200, n_components=7)\n>>> # or let the fit function itself divide the data into batches\n>>> X_sparse = sparse.csr_matrix(X)\n>>> X_transformed = transformer.fit_transform(X_sparse)\n>>> X_transformed.shape\n(1797, 7)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#InstanceBasedRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Isolation Forest Algorithm.\n\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest 'isolates' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nRead more in the :ref:`User Guide <isolation_forest>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of base estimators in the ensemble.\n\nmax_samples : \"auto\", int or float, default=\"auto\"\n    The number of samples to draw from X to train each base estimator.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n        - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n    If max_samples is larger than the number of samples provided,\n    all samples will be used for all trees (no sampling).\n\ncontamination : 'auto' or float, default='auto'\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. Used when fitting to define the threshold\n    on the scores of the samples.\n\n        - If 'auto', the threshold is determined as in the\n          original paper.\n        - If float, the contamination should be in the range (0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator.\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\n    Note: using a float number less than 1.0 or integer less than number of\n    features will enable feature subsampling and leads to a longer runtime.\n\nbootstrap : bool, default=False\n    If True, individual trees are fit on random subsets of the training\n    data sampled with replacement. If False, sampling without replacement\n    is performed.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo-randomness of the selection of the feature\n    and split values for each branching step and each tree in the forest.\n\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity of the tree building process.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.21\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n    The child estimator template used to create the collection of\n    fitted sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of ExtraTreeRegressor instances\n    The collection of fitted sub-estimators.\n\nestimators_features_ : list of ndarray\n    The subset of drawn features for each base estimator.\n\nestimators_samples_ : list of ndarray\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator.\n\nmax_samples_ : int\n    The actual number of samples.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores. We\n    have the relation: ``decision_function = score_samples - offset_``.\n    ``offset_`` is defined as follows. When the contamination parameter is\n    set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n    close to 0 and the scores of outliers are close to -1. When a\n    contamination parameter different than \"auto\" is provided, the offset\n    is defined in such a way we obtain the expected number of outliers\n    (samples with decision function < 0) in training.\n\n    .. versionadded:: 0.20\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n    Gaussian distributed dataset.\nsklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n    Estimate the support of a high-dimensional distribution.\n    The implementation is based on libsvm.\nsklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n    using Local Outlier Factor (LOF).\n\nNotes\n-----\nThe implementation is based on an ensemble of ExtraTreeRegressor. The\nmaximum depth of each tree is set to ``ceil(log_2(n))`` where\n:math:`n` is the number of samples used to build the tree\n(see (Liu et al., 2008) for more details).\n\nReferences\n----------\n.. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n.. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n       anomaly detection.\" ACM Transactions on Knowledge Discovery from\n       Data (TKDD) 6.1 (2012): 3.\n\nExamples\n--------\n>>> from sklearn.ensemble import IsolationForest\n>>> X = [[-1.1], [0.3], [0.5], [100]]\n>>> clf = IsolationForest(random_state=0).fit(X)\n>>> clf.predict([[0.1], [0], [90]])\narray([ 1,  1, -1])\n\nFor an example of using isolation forest for anomaly detection see\n:ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py`."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Jaccard similarity coefficient score.\n\nThe Jaccard index [1], or Jaccard similarity coefficient, defined as\nthe size of the intersection divided by the size of the union of two label\nsets, is used to compare set of predicted labels for a sample to the\ncorresponding set of labels in ``y_true``.\n\nSupport beyond term:`binary` targets is achieved by treating :term:`multiclass`\nand :term:`multilabel` data as a collection of binary problems, one for each\nlabel. For the :term:`binary` case, setting `average='binary'` will return the\nJaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`,\n`pos_label` is ignored and scores for both classes are computed, then averaged or\nboth returned (when `average=None`). Similarly, for :term:`multiclass` and\n:term:`multilabel` targets, scores for all `labels` are either returned or\naveraged depending on the `average` parameter. Use `labels` specify the set of\nlabels to calculate the score for.\n\nRead more in the :ref:`User Guide <jaccard_similarity_score>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nlabels : array-like of shape (n_classes,), default=None\n    The set of labels to include when `average != 'binary'`, and their\n    order if `average is None`. Labels present in the data can be\n    excluded, for example in multiclass classification to exclude a \"negative\n    class\". Labels not present in the data can be included and will be\n    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\npos_label : int, float, bool or str, default=1\n    The class to report if `average='binary'` and the data is binary,\n    otherwise this parameter is ignored.\n    For multiclass or multilabel targets, set `labels=[pos_label]` and\n    `average != 'binary'` to report metrics for one label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average, weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n    Sets the value to return when there is a zero division, i.e. when there\n    there are no negative values in predictions and labels. If set to\n    \"warn\", this acts like 0, but a warning is also raised.\n\nReturns\n-------\nscore : float or ndarray of shape (n_unique_labels,), dtype=np.float64\n    The Jaccard score. When `average` is not `None`, a single scalar is\n    returned.\n\nSee Also\n--------\naccuracy_score : Function for calculating the accuracy score.\nf1_score : Function for calculating the F1 score.\nmultilabel_confusion_matrix : Function for computing a confusion matrix                                  for each class or sample.\n\nNotes\n-----\n:func:`jaccard_score` may be a poor metric if there are no\npositives for some samples or classes. Jaccard is undefined if there are\nno true or predicted labels, and our implementation will return a score\nof 0 with a warning.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Jaccard index\n       <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import jaccard_score\n>>> y_true = np.array([[0, 1, 1],\n...                    [1, 1, 0]])\n>>> y_pred = np.array([[1, 1, 1],\n...                    [1, 0, 0]])\n\nIn the binary case:\n\n>>> jaccard_score(y_true[0], y_pred[0])\n0.6666...\n\nIn the 2D comparison case (e.g. image similarity):\n\n>>> jaccard_score(y_true, y_pred, average=\"micro\")\n0.6\n\nIn the multilabel case:\n\n>>> jaccard_score(y_true, y_pred, average='samples')\n0.5833...\n>>> jaccard_score(y_true, y_pred, average='macro')\n0.6666...\n>>> jaccard_score(y_true, y_pred, average=None)\narray([0.5, 0.5, 1. ])\n\nIn the multiclass case:\n\n>>> y_pred = [0, 2, 1, 2]\n>>> y_true = [0, 1, 2, 2]\n>>> jaccard_score(y_true, y_pred, average=None)\narray([1. , 0. , 0.33...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Bin continuous data into intervals.\n\nRead more in the :ref:`User Guide <preprocessing_discretization>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nn_bins : int or array-like of shape (n_features,), default=5\n    The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n\nencode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'\n    Method used to encode the transformed result.\n\n    - 'onehot': Encode the transformed result with one-hot encoding\n      and return a sparse matrix. Ignored features are always\n      stacked to the right.\n    - 'onehot-dense': Encode the transformed result with one-hot encoding\n      and return a dense array. Ignored features are always\n      stacked to the right.\n    - 'ordinal': Return the bin identifier encoded as an integer value.\n\nstrategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'\n    Strategy used to define the widths of the bins.\n\n    - 'uniform': All bins in each feature have identical widths.\n    - 'quantile': All bins in each feature have the same number of points.\n    - 'kmeans': Values in each bin have the same nearest center of a 1D\n      k-means cluster.\n\n    For an example of the different strategies see:\n    :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`.\n\ndtype : {np.float32, np.float64}, default=None\n    The desired data-type for the output. If None, output dtype is\n    consistent with input dtype. Only np.float32 and np.float64 are\n    supported.\n\n    .. versionadded:: 0.24\n\nsubsample : int or None, default='warn'\n    Maximum number of samples, used to fit the model, for computational\n    efficiency. Defaults to 200_000 when `strategy='quantile'` and to `None`\n    when `strategy='uniform'` or `strategy='kmeans'`.\n    `subsample=None` means that all the training samples are used when\n    computing the quantiles that determine the binning thresholds.\n    Since quantile computation relies on sorting each column of `X` and\n    that sorting has an `n log(n)` time complexity,\n    it is recommended to use subsampling on datasets with a\n    very large number of samples.\n\n    .. versionchanged:: 1.3\n        The default value of `subsample` changed from `None` to `200_000` when\n        `strategy=\"quantile\"`.\n\n    .. versionchanged:: 1.5\n        The default value of `subsample` changed from `None` to `200_000` when\n        `strategy=\"uniform\"` or `strategy=\"kmeans\"`.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for subsampling.\n    Pass an int for reproducible results across multiple function calls.\n    See the `subsample` parameter for more details.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 1.1\n\nAttributes\n----------\nbin_edges_ : ndarray of ndarray of shape (n_features,)\n    The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n    Ignored features will have empty arrays.\n\nn_bins_ : ndarray of shape (n_features,), dtype=np.int64\n    Number of bins per feature. Bins whose width are too small\n    (i.e., <= 1e-8) are removed with a warning.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nBinarizer : Class used to bin values as ``0`` or\n    ``1`` based on a parameter ``threshold``.\n\nNotes\n-----\n\nFor a visualization of discretization on different datasets refer to\n:ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`.\nOn the effect of discretization on linear models see:\n:ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`.\n\nIn bin edges for feature ``i``, the first and last values are used only for\n``inverse_transform``. During transform, bin edges are extended to::\n\n  np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n\nYou can combine ``KBinsDiscretizer`` with\n:class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess\npart of the features.\n\n``KBinsDiscretizer`` might produce constant features (e.g., when\n``encode = 'onehot'`` and certain bins do not contain any data).\nThese features can be removed with feature selection algorithms\n(e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).\n\nExamples\n--------\n>>> from sklearn.preprocessing import KBinsDiscretizer\n>>> X = [[-2, 1, -4,   -1],\n...      [-1, 2, -3, -0.5],\n...      [ 0, 3, -2,  0.5],\n...      [ 1, 4, -1,    2]]\n>>> est = KBinsDiscretizer(\n...     n_bins=3, encode='ordinal', strategy='uniform', subsample=None\n... )\n>>> est.fit(X)\nKBinsDiscretizer(...)\n>>> Xt = est.transform(X)\n>>> Xt  # doctest: +SKIP\narray([[ 0., 0., 0., 0.],\n       [ 1., 1., 1., 0.],\n       [ 2., 2., 2., 1.],\n       [ 2., 2., 2., 2.]])\n\nSometimes it may be useful to convert the data back into the original\nfeature space. The ``inverse_transform`` function converts the binned\ndata into the original feature space. Each value will be equal to the mean\nof the two bin edges.\n\n>>> est.bin_edges_[0]\narray([-2., -1.,  0.,  1.])\n>>> est.inverse_transform(Xt)\narray([[-1.5,  1.5, -3.5, -0.5],\n       [-0.5,  2.5, -2.5, -0.5],\n       [ 0.5,  3.5, -1.5,  0.5],\n       [ 0.5,  3.5, -1.5,  1.5]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KDTreeMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "K-Fold cross-validator.\n\nProvides train/test indices to split data in train/test sets. Split\ndataset into k consecutive folds (without shuffling by default).\n\nEach fold is then used once as a validation while the k - 1 remaining\nfolds form the training set.\n\nRead more in the :ref:`User Guide <k_fold>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nshuffle : bool, default=False\n    Whether to shuffle the data before splitting into batches.\n    Note that the samples within each split will not be shuffled.\n\nrandom_state : int, RandomState instance or None, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold. Otherwise, this\n    parameter has no effect.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import KFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([1, 2, 3, 4])\n>>> kf = KFold(n_splits=2)\n>>> kf.get_n_splits(X)\n2\n>>> print(kf)\nKFold(n_splits=2, random_state=None, shuffle=False)\n>>> for i, (train_index, test_index) in enumerate(kf.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[2 3]\n  Test:  index=[0 1]\nFold 1:\n  Train: index=[0 1]\n  Test:  index=[2 3]\n\nNotes\n-----\nThe first ``n_samples % n_splits`` folds have size\n``n_samples // n_splits + 1``, other folds have size\n``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nStratifiedKFold : Takes class information into account to avoid building\n    folds with imbalanced class distributions (for binary or multiclass\n    classification tasks).\n\nGroupKFold : K-fold iterator variant with non-overlapping groups.\n\nRepeatedKFold : Repeats K-Fold n times."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Imputation for completing missing values using k-Nearest Neighbors.\n\nEach sample's missing values are imputed using the mean value from\n`n_neighbors` nearest neighbors found in the training set. Two samples are\nclose if the features that neither is missing are close.\n\nRead more in the :ref:`User Guide <knnimpute>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nmissing_values : int, float, str, np.nan or None, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to np.nan, since `pd.NA` will be converted to np.nan.\n\nn_neighbors : int, default=5\n    Number of neighboring samples to use for imputation.\n\nweights : {'uniform', 'distance'} or callable, default='uniform'\n    Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights. All points in each neighborhood are\n      weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - callable : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\nmetric : {'nan_euclidean'} or callable, default='nan_euclidean'\n    Distance metric for searching neighbors. Possible values:\n\n    - 'nan_euclidean'\n    - callable : a user-defined function which conforms to the definition\n      of ``_pairwise_callable(X, Y, metric, **kwds)``. The function\n      accepts two arrays, X and Y, and a `missing_values` keyword in\n      `kwds` and returns a scalar distance value.\n\ncopy : bool, default=True\n    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible.\n\nadd_indicator : bool, default=False\n    If True, a :class:`MissingIndicator` transform will stack onto the\n    output of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on the\n    missing indicator even if there are missing values at transform/test\n    time.\n\nkeep_empty_features : bool, default=False\n    If True, features that consist exclusively of missing values when\n    `fit` is called are returned in results when `transform` is called.\n    The imputed value is always `0`.\n\n    .. versionadded:: 1.2\n\nAttributes\n----------\nindicator_ : :class:`~sklearn.impute.MissingIndicator`\n    Indicator used to add binary indicators for missing values.\n    ``None`` if add_indicator is False.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nSimpleImputer : Univariate imputer for completing missing values\n    with simple strategies.\nIterativeImputer : Multivariate imputer that estimates values to impute for\n    each feature with missing values from all the others.\n\nReferences\n----------\n* `Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor\n  Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing\n  value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17\n  no. 6, 2001 Pages 520-525.\n  <https://academic.oup.com/bioinformatics/article/17/6/520/272365>`_\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.impute import KNNImputer\n>>> X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n>>> imputer = KNNImputer(n_neighbors=2)\n>>> imputer.fit_transform(X)\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\nFor a more detailed example see\n:ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Classifier implementing the k-nearest neighbors vote.\n\nRead more in the :ref:`User Guide <classification>`.\n\nParameters\n----------\nn_neighbors : int, default=5\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nweights : {'uniform', 'distance'}, callable or None, default='uniform'\n    Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Refer to the example entitled\n    :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n    showing the impact of the `weights` parameter on the decision\n    boundary.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : float, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is equivalent\n    to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n    For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n    to be positive.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------\nclasses_ : array of shape (n_classes,)\n    Class labels known to the classifier\n\neffective_metric_ : str or callble\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\noutputs_2d_ : bool\n    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True.\n\nSee Also\n--------\nRadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\nKNeighborsRegressor: Regression based on k-nearest neighbors.\nRadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\nNearestNeighbors: Unsupervised learner for implementing neighbor searches.\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances\n   but different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> neigh = KNeighborsClassifier(n_neighbors=3)\n>>> neigh.fit(X, y)\nKNeighborsClassifier(...)\n>>> print(neigh.predict([[1.1]]))\n[0]\n>>> print(neigh.predict_proba([[0.9]]))\n[[0.666... 0.333...]]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Regression based on k-nearest neighbors.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\nn_neighbors : int, default=5\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nweights : {'uniform', 'distance'}, callable or None, default='uniform'\n    Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : float, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric : str, DistanceMetric object or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    If metric is a DistanceMetric object, it will be passed directly to\n    the underlying computation routines.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n    Doesn't affect :meth:`fit` method.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nSee Also\n--------\nNearestNeighbors : Unsupervised learner for implementing neighbor searches.\nRadiusNeighborsRegressor : Regression based on neighbors within a fixed radius.\nKNeighborsClassifier : Classifier implementing the k-nearest neighbors vote.\nRadiusNeighborsClassifier : Classifier implementing\n    a vote among neighbors within a given radius.\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n.. warning::\n\n   Regarding the Nearest Neighbors algorithms, if it is found that two\n   neighbors, neighbor `k+1` and `k`, have identical distances but\n   different labels, the results will depend on the ordering of the\n   training data.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>> neigh = KNeighborsRegressor(n_neighbors=2)\n>>> neigh.fit(X, y)\nKNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transform X into a (weighted) graph of k nearest neighbors.\n\nThe transformed data is a sparse graph as returned by kneighbors_graph.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nmode : {'distance', 'connectivity'}, default='distance'\n    Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric.\n\nn_neighbors : int, default=5\n    Number of neighbors for each sample in the transformed sparse graph.\n    For compatibility reasons, as each sample is considered as its own\n    neighbor, one extra neighbor will be computed when mode == 'distance'.\n    In this case, the sparse graph contains (n_neighbors + 1) neighbors.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\np : float, default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nSee Also\n--------\nkneighbors_graph : Compute the weighted graph of k-neighbors for\n    points in X.\nRadiusNeighborsTransformer : Transform X into a weighted graph of\n    neighbors nearer than a radius.\n\nNotes\n-----\nFor an example of using :class:`~sklearn.neighbors.KNeighborsTransformer`\nin combination with :class:`~sklearn.manifold.TSNE` see\n:ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.\n\nExamples\n--------\n>>> from sklearn.datasets import load_wine\n>>> from sklearn.neighbors import KNeighborsTransformer\n>>> X, _ = load_wine(return_X_y=True)\n>>> X.shape\n(178, 13)\n>>> transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n>>> X_dist_graph = transformer.fit_transform(X)\n>>> X_dist_graph.shape\n(178, 178)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelCentererMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Center an arbitrary kernel matrix :math:`K`.\n\nLet define a kernel :math:`K` such that:\n\n.. math::\n    K(X, Y) = \\phi(X) . \\phi(Y)^{T}\n\n:math:`\\phi(X)` is a function mapping of rows of :math:`X` to a\nHilbert space and :math:`K` is of shape `(n_samples, n_samples)`.\n\nThis class allows to compute :math:`\\tilde{K}(X, Y)` such that:\n\n.. math::\n    \\tilde{K(X, Y)} = \\tilde{\\phi}(X) . \\tilde{\\phi}(Y)^{T}\n\n:math:`\\tilde{\\phi}(X)` is the centered mapped data in the Hilbert\nspace.\n\n`KernelCenterer` centers the features without explicitly computing the\nmapping :math:`\\phi(\\cdot)`. Working with centered kernels is sometime\nexpected when dealing with algebra computation such as eigendecomposition\nfor :class:`~sklearn.decomposition.KernelPCA` for instance.\n\nRead more in the :ref:`User Guide <kernel_centering>`.\n\nAttributes\n----------\nK_fit_rows_ : ndarray of shape (n_samples,)\n    Average of each column of kernel matrix.\n\nK_fit_all_ : float\n    Average of kernel matrix.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.kernel_approximation.Nystroem : Approximate a kernel map\n    using a subset of the training data.\n\nReferences\n----------\n.. [1] `Sch√∂lkopf, Bernhard, Alexander Smola, and Klaus-Robert M√ºller.\n   \"Nonlinear component analysis as a kernel eigenvalue problem.\"\n   Neural computation 10.5 (1998): 1299-1319.\n   <https://www.mlpack.org/papers/kpca.pdf>`_\n\nExamples\n--------\n>>> from sklearn.preprocessing import KernelCenterer\n>>> from sklearn.metrics.pairwise import pairwise_kernels\n>>> X = [[ 1., -2.,  2.],\n...      [ -2.,  1.,  3.],\n...      [ 4.,  1., -2.]]\n>>> K = pairwise_kernels(X, metric='linear')\n>>> K\narray([[  9.,   2.,  -2.],\n       [  2.,  14., -13.],\n       [ -2., -13.,  21.]])\n>>> transformer = KernelCenterer().fit(K)\n>>> transformer\nKernelCenterer()\n>>> transformer.transform(K)\narray([[  5.,   0.,  -5.],\n       [  0.,  14., -14.],\n       [ -5., -14.,  19.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Kernel Density Estimation.\n\nRead more in the :ref:`User Guide <kernel_density>`.\n\nParameters\n----------\nbandwidth : float or {\"scott\", \"silverman\"}, default=1.0\n    The bandwidth of the kernel. If bandwidth is a float, it defines the\n    bandwidth of the kernel. If bandwidth is a string, one of the estimation\n    methods is implemented.\n\nalgorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto'\n    The tree algorithm to use.\n\nkernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'\n    The kernel to use.\n\nmetric : str, default='euclidean'\n    Metric to use for distance computation. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    Not all metrics are valid with all algorithms: refer to the\n    documentation of :class:`BallTree` and :class:`KDTree`. Note that the\n    normalization of the density output is correct only for the Euclidean\n    distance metric.\n\natol : float, default=0\n    The desired absolute tolerance of the result.  A larger tolerance will\n    generally lead to faster execution.\n\nrtol : float, default=0\n    The desired relative tolerance of the result.  A larger tolerance will\n    generally lead to faster execution.\n\nbreadth_first : bool, default=True\n    If true (default), use a breadth-first approach to the problem.\n    Otherwise use a depth-first approach.\n\nleaf_size : int, default=40\n    Specify the leaf size of the underlying tree.  See :class:`BallTree`\n    or :class:`KDTree` for details.\n\nmetric_params : dict, default=None\n    Additional parameters to be passed to the tree for use with the\n    metric.  For more information, see the documentation of\n    :class:`BallTree` or :class:`KDTree`.\n\nAttributes\n----------\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\ntree_ : ``BinaryTree`` instance\n    The tree algorithm for fast generalized N-point problems.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\nbandwidth_ : float\n    Value of the bandwidth, given directly by the bandwidth parameter or\n    estimated using the 'scott' or 'silverman' method.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n    problems.\nsklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n    problems.\n\nExamples\n--------\nCompute a gaussian kernel density estimate with a fixed bandwidth.\n\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> rng = np.random.RandomState(42)\n>>> X = rng.random_sample((100, 3))\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n>>> log_density = kde.score_samples(X[:3])\n>>> log_density\narray([-1.52955942, -1.51462041, -1.60244657])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Kernel Principal component analysis (KPCA) [1]_.\n\nNon-linear dimensionality reduction through the use of kernels (see\n:ref:`metrics`).\n\nIt uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD\nor the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the\ntruncated SVD, depending on the shape of the input data and the number of\ncomponents to extract. It can also use a randomized truncated SVD by the\nmethod proposed in [3]_, see `eigen_solver`.\n\nFor a usage example, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.\n\nRead more in the :ref:`User Guide <kernel_PCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components. If None, all non-zero components are kept.\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear'\n    Kernel used for PCA.\n\ngamma : float, default=None\n    Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n    kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\ndegree : float, default=3\n    Degree for poly kernels. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Independent term in poly and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict, default=None\n    Parameters (keyword arguments) and\n    values for kernel passed as callable object.\n    Ignored by other kernels.\n\nalpha : float, default=1.0\n    Hyperparameter of the ridge regression that learns the\n    inverse transform (when fit_inverse_transform=True).\n\nfit_inverse_transform : bool, default=False\n    Learn the inverse transform for non-precomputed kernels\n    (i.e. learn to find the pre-image of a point). This method is based\n    on [2]_.\n\neigen_solver : {'auto', 'dense', 'arpack', 'randomized'},             default='auto'\n    Select eigensolver to use. If `n_components` is much\n    less than the number of training samples, randomized (or arpack to a\n    smaller extent) may be more efficient than the dense eigensolver.\n    Randomized SVD is performed according to the method of Halko et al\n    [3]_.\n\n    auto :\n        the solver is selected by a default policy based on n_samples\n        (the number of training samples) and `n_components`:\n        if the number of components to extract is less than 10 (strict) and\n        the number of samples is more than 200 (strict), the 'arpack'\n        method is enabled. Otherwise the exact full eigenvalue\n        decomposition is computed and optionally truncated afterwards\n        ('dense' method).\n    dense :\n        run exact full eigenvalue decomposition calling the standard\n        LAPACK solver via `scipy.linalg.eigh`, and select the components\n        by postprocessing\n    arpack :\n        run SVD truncated to n_components calling ARPACK solver using\n        `scipy.sparse.linalg.eigsh`. It requires strictly\n        0 < n_components < n_samples\n    randomized :\n        run randomized SVD by the method of Halko et al. [3]_. The current\n        implementation selects eigenvalues based on their module; therefore\n        using this method can lead to unexpected results if the kernel is\n        not positive semi-definite. See also [4]_.\n\n    .. versionchanged:: 1.0\n       `'randomized'` was added.\n\ntol : float, default=0\n    Convergence tolerance for arpack.\n    If 0, optimal value will be chosen by arpack.\n\nmax_iter : int, default=None\n    Maximum number of iterations for arpack.\n    If None, optimal value will be chosen by arpack.\n\niterated_power : int >= 0, or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'. When 'auto', it is set to 7 when\n    `n_components < 0.1 * min(X.shape)`, other it is set to 4.\n\n    .. versionadded:: 1.0\n\nremove_zero_eig : bool, default=False\n    If True, then all components with zero eigenvalues are removed, so\n    that the number of components in the output may be < n_components\n    (and sometimes even zero due to numerical instability).\n    When n_components is None, this parameter is ignored and components\n    with zero eigenvalues are removed regardless.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18\n\ncopy_X : bool, default=True\n    If True, input X is copied and stored by the model in the `X_fit_`\n    attribute. If no further changes will be done to X, setting\n    `copy_X=False` saves memory by storing a reference.\n\n    .. versionadded:: 0.18\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nAttributes\n----------\neigenvalues_ : ndarray of shape (n_components,)\n    Eigenvalues of the centered kernel matrix in decreasing order.\n    If `n_components` and `remove_zero_eig` are not set,\n    then all values are stored.\n\neigenvectors_ : ndarray of shape (n_samples, n_components)\n    Eigenvectors of the centered kernel matrix. If `n_components` and\n    `remove_zero_eig` are not set, then all components are stored.\n\ndual_coef_ : ndarray of shape (n_samples, n_features)\n    Inverse transform matrix. Only available when\n    ``fit_inverse_transform`` is True.\n\nX_transformed_fit_ : ndarray of shape (n_samples, n_components)\n    Projection of the fitted data on the kernel principal components.\n    Only available when ``fit_inverse_transform`` is True.\n\nX_fit_ : ndarray of shape (n_samples, n_features)\n    The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n    a reference. This attribute is used for the calls to transform.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\ngamma_ : float\n    Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma`\n    is explicitly provided, this is just the same as `gamma`. When `gamma`\n    is `None`, this is the actual value of kernel coefficient.\n\n    .. versionadded:: 1.3\n\nSee Also\n--------\nFastICA : A fast algorithm for Independent Component Analysis.\nIncrementalPCA : Incremental Principal Component Analysis.\nNMF : Non-Negative Matrix Factorization.\nPCA : Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\n\nReferences\n----------\n.. [1] `Sch√∂lkopf, Bernhard, Alexander Smola, and Klaus-Robert M√ºller.\n   \"Kernel principal component analysis.\"\n   International conference on artificial neural networks.\n   Springer, Berlin, Heidelberg, 1997.\n   <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n\n.. [2] `Bakƒ±r, G√∂khan H., Jason Weston, and Bernhard Sch√∂lkopf.\n   \"Learning to find pre-images.\"\n   Advances in neural information processing systems 16 (2004): 449-456.\n   <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n\n.. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.\n   \"Finding structure with randomness: Probabilistic algorithms for\n   constructing approximate matrix decompositions.\"\n   SIAM review 53.2 (2011): 217-288. <0909.4061>`\n\n.. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.\n   \"A randomized algorithm for the decomposition of matrices.\"\n   Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.\n   <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import KernelPCA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = KernelPCA(n_components=7, kernel='linear')\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Binarize labels in a one-vs-all fashion.\n\nSeveral regression and binary classification algorithms are\navailable in scikit-learn. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nAt learning time, this simply consists in learning one regressor\nor binary classifier per class. In doing so, one needs to convert\nmulti-class labels to binary labels (belong or does not belong\nto the class). `LabelBinarizer` makes this process easy with the\ntransform method.\n\nAt prediction time, one assigns the class for which the corresponding\nmodel gave the greatest confidence. `LabelBinarizer` makes this easy\nwith the :meth:`inverse_transform` method.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.\n\nParameters\n----------\nneg_label : int, default=0\n    Value with which negative labels must be encoded.\n\npos_label : int, default=1\n    Value with which positive labels must be encoded.\n\nsparse_output : bool, default=False\n    True if the returned array from transform is desired to be in sparse\n    CSR format.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Holds the label for each class.\n\ny_type_ : str\n    Represents the type of the target data as evaluated by\n    :func:`~sklearn.utils.multiclass.type_of_target`. Possible type are\n    'continuous', 'continuous-multioutput', 'binary', 'multiclass',\n    'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.\n\nsparse_input_ : bool\n    `True` if the input data to transform is given as a sparse matrix,\n     `False` otherwise.\n\nSee Also\n--------\nlabel_binarize : Function to perform the transform operation of\n    LabelBinarizer with fixed classes.\nOneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n    scheme.\n\nExamples\n--------\n>>> from sklearn.preprocessing import LabelBinarizer\n>>> lb = LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2])\nLabelBinarizer()\n>>> lb.classes_\narray([1, 2, 4, 6])\n>>> lb.transform([1, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])\n\nBinary targets transform to a column vector\n\n>>> lb = LabelBinarizer()\n>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\narray([[1],\n       [0],\n       [0],\n       [1]])\n\nPassing a 2D matrix for multilabel classification\n\n>>> import numpy as np\n>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\nLabelBinarizer()\n>>> lb.classes_\narray([0, 1, 2])\n>>> lb.transform([0, 1, 2, 1])\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelEncoderMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingAveragePrecisionScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute ranking-based average precision.\n\nLabel ranking average precision (LRAP) is the average over each ground\ntruth label assigned to each sample, of the ratio of true vs. total\nlabels with lower score.\n\nThis metric is used in multilabel ranking problem, where the goal\nis to give better rank to the labels associated to each sample.\n\nThe obtained score is always strictly greater than 0 and\nthe best value is 1.\n\nRead more in the :ref:`User Guide <label_ranking_average_precision>`.\n\nParameters\n----------\ny_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n    True binary labels in binary indicator format.\n\ny_score : array-like of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nscore : float\n    Ranking-based average precision score.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_average_precision_score\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_average_precision_score(y_true, y_score)\n0.416..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Ranking loss measure.\n\nCompute the average number of label pairs that are incorrectly ordered\ngiven y_score weighted by the size of the label set and the number of\nlabels not in the label set.\n\nThis is similar to the error set size, but weighted by the number of\nrelevant and irrelevant labels. The best performance is achieved with\na ranking loss of zero.\n\nRead more in the :ref:`User Guide <label_ranking_loss>`.\n\n.. versionadded:: 0.17\n   A function *label_ranking_loss*\n\nParameters\n----------\ny_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n    True binary labels in binary indicator format.\n\ny_score : array-like of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n    Average number of label pairs that are incorrectly ordered given\n    y_score weighted by the size of the label set and the number of labels not\n    in the label set.\n\nReferences\n----------\n.. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n       Mining multi-label data. In Data mining and knowledge discovery\n       handbook (pp. 667-685). Springer US.\n\nExamples\n--------\n>>> from sklearn.metrics import label_ranking_loss\n>>> y_true = [[1, 0, 0], [0, 0, 1]]\n>>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]\n>>> label_ranking_loss(y_true, y_score)\n0.75..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Cross-validated Least Angle Regression model.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform.\n\nprecompute : bool, 'auto' or array-like , default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram matrix\n    cannot be passed as argument since we will use only subsets of X.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nmax_n_alphas : int, default=1000\n    The maximum number of points on the path used to compute the\n    residuals in the cross-validation.\n\nn_jobs : int or None, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nAttributes\n----------\nactive_ : list of length n_alphas or list of such lists\n    Indices of active variables at the end of the path.\n    If this is a list of lists, the outer list length is `n_targets`.\n\ncoef_ : array-like of shape (n_features,)\n    parameter vector (w in the formulation formula)\n\nintercept_ : float\n    independent term in decision function\n\ncoef_path_ : array-like of shape (n_features, n_alphas)\n    the varying values of the coefficients along the path\n\nalpha_ : float\n    the estimated regularization parameter alpha\n\nalphas_ : array-like of shape (n_alphas,)\n    the different values of alpha along the path\n\ncv_alphas_ : array-like of shape (n_cv_alphas,)\n    all the values of alpha along the path for the different folds\n\nmse_path_ : array-like of shape (n_folds, n_cv_alphas)\n    the mean square error on left-out for each fold along the path\n    (alpha values given by ``cv_alphas``)\n\nn_iter_ : array-like or int\n    the number of iterations run by Lars with the optimal alpha.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path : Compute Least Angle Regression or Lasso\n    path using LARS algorithm.\nlasso_path : Compute Lasso path with coordinate descent.\nLasso : Linear Model trained with L1 prior as\n    regularizer (aka the Lasso).\nLassoCV : Lasso linear model with iterative fitting\n    along a regularization path.\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\nLassoLarsIC : Lasso model fit with Lars using BIC\n    or AIC for model selection.\nsklearn.decomposition.sparse_encode : Sparse coding.\n\nNotes\n-----\nIn `fit`, once the best parameter `alpha` is found through\ncross-validation, the model is fit again using the entire training set.\n\nExamples\n--------\n>>> from sklearn.linear_model import LarsCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n>>> reg = LarsCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9996...\n>>> reg.alpha_\n0.2961...\n>>> reg.predict(X[:1,])\narray([154.3996...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Least Angle Regression model a.k.a. LAR.\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nprecompute : bool, 'auto' or array-like , default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nn_nonzero_coefs : int, default=500\n    Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nfit_path : bool, default=True\n    If True the full path is stored in the ``coef_path_`` attribute.\n    If you compute the solution for a large problem or many targets,\n    setting ``fit_path`` to ``False`` will lead to a speedup, especially\n    with a small alpha.\n\njitter : float, default=None\n    Upper bound on a uniform noise parameter to be added to the\n    `y` values, to satisfy the model's assumption of\n    one-at-a-time computations. Might help with stability.\n\n    .. versionadded:: 0.23\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for jittering. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller. If this is a list of array-like, the length of the outer\n    list is `n_targets`.\n\nactive_ : list of shape (n_alphas,) or list of such lists\n    Indices of active variables at the end of the path.\n    If this is a list of list, the length of the outer list is `n_targets`.\n\ncoef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n    The varying values of the coefficients along the path. It is not\n    present if the ``fit_path`` parameter is ``False``. If this is a list\n    of array-like, the length of the outer list is `n_targets`.\n\ncoef_ : array-like of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the formulation formula).\n\nintercept_ : float or array-like of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : array-like or int\n    The number of iterations taken by lars_path to find the\n    grid of alphas for each target.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path: Compute Least Angle Regression or Lasso\n    path using LARS algorithm.\nLarsCV : Cross-validated Least Angle Regression model.\nsklearn.decomposition.sparse_encode : Sparse coding.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.Lars(n_nonzero_coefs=1)\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\nLars(n_nonzero_coefs=1)\n>>> print(reg.coef_)\n[ 0. -1.11...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Lasso linear model with iterative fitting along a regularization path.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe best model is selected by cross-validation.\n\nThe optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <lasso>`.\n\nParameters\n----------\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : array-like, default=None\n    List of alphas where to compute the models.\n    If ``None`` alphas are set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nprecompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : bool or int, default=False\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive : bool, default=False\n    If positive, restrict regression coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nmse_path_ : ndarray of shape (n_alphas, n_folds)\n    Mean square error for the test set on each fold, varying alpha.\n\nalphas_ : ndarray of shape (n_alphas,)\n    The grid of alphas used for fitting.\n\ndual_gap_ : float or ndarray of shape (n_targets,)\n    The dual gap at the end of the optimization for the optimal alpha\n    (``alpha_``).\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path : Compute Least Angle Regression or Lasso path using LARS\n    algorithm.\nlasso_path : Compute Lasso path with coordinate descent.\nLasso : The Lasso is a linear model that estimates sparse coefficients.\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\nLassoCV : Lasso linear model with iterative fitting along a regularization\n    path.\nLassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n\nNotes\n-----\nIn `fit`, once the best parameter `alpha` is found through\ncross-validation, the model is fit again using the entire training set.\n\nTo avoid unnecessary memory duplication the `X` argument of the `fit`\nmethod should be directly passed as a Fortran-contiguous numpy array.\n\n For an example, see\n :ref:`examples/linear_model/plot_lasso_model_selection.py\n <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n:class:`LassoCV` leads to different results than a hyperparameter\nsearch using :class:`~sklearn.model_selection.GridSearchCV` with a\n:class:`Lasso` model. In :class:`LassoCV`, a model for a given\npenalty `alpha` is warm started using the coefficients of the\nclosest model (trained at the previous iteration) on the\nregularization path. It tends to speed up the hyperparameter\nsearch.\n\nExamples\n--------\n>>> from sklearn.linear_model import LassoCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9993...\n>>> reg.predict(X[:1,])\narray([-78.4951...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Cross-validated Lasso, using the LARS algorithm.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform.\n\nprecompute : bool or 'auto' , default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram matrix\n    cannot be passed as argument since we will use only subsets of X.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nmax_n_alphas : int, default=1000\n    The maximum number of points on the path used to compute the\n    residuals in the cross-validation.\n\nn_jobs : int or None, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0. Be aware that you might want to\n    remove fit_intercept which is set True by default.\n    Under the positive restriction the model coefficients do not converge\n    to the ordinary-least-squares solution for small values of alpha.\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n    algorithm are typically in congruence with the solution of the\n    coordinate descent Lasso estimator.\n    As a consequence using LassoLarsCV only makes sense for problems where\n    a sparse solution is expected and/or reached.\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    parameter vector (w in the formulation formula)\n\nintercept_ : float\n    independent term in decision function.\n\ncoef_path_ : array-like of shape (n_features, n_alphas)\n    the varying values of the coefficients along the path\n\nalpha_ : float\n    the estimated regularization parameter alpha\n\nalphas_ : array-like of shape (n_alphas,)\n    the different values of alpha along the path\n\ncv_alphas_ : array-like of shape (n_cv_alphas,)\n    all the values of alpha along the path for the different folds\n\nmse_path_ : array-like of shape (n_folds, n_cv_alphas)\n    the mean square error on left-out for each fold along the path\n    (alpha values given by ``cv_alphas``)\n\nn_iter_ : array-like or int\n    the number of iterations run by Lars with the optimal alpha.\n\nactive_ : list of int\n    Indices of active variables at the end of the path.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path : Compute Least Angle Regression or Lasso\n    path using LARS algorithm.\nlasso_path : Compute Lasso path with coordinate descent.\nLasso : Linear Model trained with L1 prior as\n    regularizer (aka the Lasso).\nLassoCV : Lasso linear model with iterative fitting\n    along a regularization path.\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\nLassoLarsIC : Lasso model fit with Lars using BIC\n    or AIC for model selection.\nsklearn.decomposition.sparse_encode : Sparse coding.\n\nNotes\n-----\nThe object solves the same problem as the\n:class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n:class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\nby itself. In general, because of this property, it will be more stable.\nHowever, it is more fragile to heavily multicollinear datasets.\n\nIt is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\nonly a small number of features are selected compared to the total number,\nfor instance if there are very few samples compared to the number of\nfeatures.\n\nIn `fit`, once the best parameter `alpha` is found through\ncross-validation, the model is fit again using the entire training set.\n\nExamples\n--------\n>>> from sklearn.linear_model import LassoLarsCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4.0, random_state=0)\n>>> reg = LassoLarsCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9993...\n>>> reg.alpha_\n0.3972...\n>>> reg.predict(X[:1,])\narray([-78.4831...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Lasso model fit with Lars using BIC or AIC for model selection.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nAIC is the Akaike information criterion [2]_ and BIC is the Bayes\nInformation criterion [3]_. Such criteria are useful to select the value\nof the regularization parameter by making a trade-off between the\ngoodness of fit and the complexity of the model. A good model should\nexplain well the data while being simple.\n\nRead more in the :ref:`User Guide <lasso_lars_ic>`.\n\nParameters\n----------\ncriterion : {'aic', 'bic'}, default='aic'\n    The type of criterion to use.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nprecompute : bool, 'auto' or array-like, default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform. Can be used for\n    early stopping.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0. Be aware that you might want to\n    remove fit_intercept which is set True by default.\n    Under the positive restriction the model coefficients do not converge\n    to the ordinary-least-squares solution for small values of alpha.\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n    algorithm are typically in congruence with the solution of the\n    coordinate descent Lasso estimator.\n    As a consequence using LassoLarsIC only makes sense for problems where\n    a sparse solution is expected and/or reached.\n\nnoise_variance : float, default=None\n    The estimated noise variance of the data. If `None`, an unbiased\n    estimate is computed by an OLS model. However, it is only possible\n    in the case where `n_samples > n_features + fit_intercept`.\n\n    .. versionadded:: 1.1\n\nAttributes\n----------\ncoef_ : array-like of shape (n_features,)\n    parameter vector (w in the formulation formula)\n\nintercept_ : float\n    independent term in decision function.\n\nalpha_ : float\n    the alpha parameter chosen by the information criterion\n\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller. If a list, it will be of length `n_targets`.\n\nn_iter_ : int\n    number of iterations run by lars_path to find the grid of\n    alphas.\n\ncriterion_ : array-like of shape (n_alphas,)\n    The value of the information criteria ('aic', 'bic') across all\n    alphas. The alpha which has the smallest information criterion is\n    chosen, as specified in [1]_.\n\nnoise_variance_ : float\n    The estimated noise variance from the data used to compute the\n    criterion.\n\n    .. versionadded:: 1.1\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path : Compute Least Angle Regression or Lasso\n    path using LARS algorithm.\nlasso_path : Compute Lasso path with coordinate descent.\nLasso : Linear Model trained with L1 prior as\n    regularizer (aka the Lasso).\nLassoCV : Lasso linear model with iterative fitting\n    along a regularization path.\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\nLassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\nsklearn.decomposition.sparse_encode : Sparse coding.\n\nNotes\n-----\nThe number of degrees of freedom is computed as in [1]_.\n\nTo have more details regarding the mathematical formulation of the\nAIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\n\nReferences\n----------\n.. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n        \"On the degrees of freedom of the lasso.\"\n        The Annals of Statistics 35.5 (2007): 2173-2192.\n        <0712.0881>`\n\n.. [2] `Wikipedia entry on the Akaike information criterion\n        <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n\n.. [3] `Wikipedia entry on the Bayesian information criterion\n        <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLarsIC(criterion='bic')\n>>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n>>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n>>> reg.fit(X, y)\nLassoLarsIC(criterion='bic')\n>>> print(reg.coef_)\n[ 0.  -1.11...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Lasso model fit with Least Angle Regression a.k.a. Lars.\n\nIt is a Linear Model trained with an L1 prior as regularizer.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the penalty term. Defaults to 1.0.\n    ``alpha = 0`` is equivalent to an ordinary least square, solved\n    by :class:`LinearRegression`. For numerical reasons, using\n    ``alpha = 0`` with the LassoLars object is not advised and you\n    should prefer the LinearRegression object.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nprecompute : bool, 'auto' or array-like, default='auto'\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. If set to ``'auto'`` let us decide. The Gram\n    matrix can also be passed as argument.\n\nmax_iter : int, default=500\n    Maximum number of iterations to perform.\n\neps : float, default=np.finfo(float).eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Unlike the ``tol`` parameter in some iterative\n    optimization-based algorithms, this parameter does not control\n    the tolerance of the optimization.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nfit_path : bool, default=True\n    If ``True`` the full path is stored in the ``coef_path_`` attribute.\n    If you compute the solution for a large problem or many targets,\n    setting ``fit_path`` to ``False`` will lead to a speedup, especially\n    with a small alpha.\n\npositive : bool, default=False\n    Restrict coefficients to be >= 0. Be aware that you might want to\n    remove fit_intercept which is set True by default.\n    Under the positive restriction the model coefficients will not converge\n    to the ordinary-least-squares solution for small values of alpha.\n    Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n    0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n    algorithm are typically in congruence with the solution of the\n    coordinate descent Lasso estimator.\n\njitter : float, default=None\n    Upper bound on a uniform noise parameter to be added to the\n    `y` values, to satisfy the model's assumption of\n    one-at-a-time computations. Might help with stability.\n\n    .. versionadded:: 0.23\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for jittering. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nalphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n    number of nodes in the path with ``alpha >= alpha_min``, whichever\n    is smaller. If this is a list of array-like, the length of the outer\n    list is `n_targets`.\n\nactive_ : list of length n_alphas or list of such lists\n    Indices of active variables at the end of the path.\n    If this is a list of list, the length of the outer list is `n_targets`.\n\ncoef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n    If a list is passed it's expected to be one of n_targets such arrays.\n    The varying values of the coefficients along the path. It is not\n    present if the ``fit_path`` parameter is ``False``. If this is a list\n    of array-like, the length of the outer list is `n_targets`.\n\ncoef_ : array-like of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the formulation formula).\n\nintercept_ : float or array-like of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : array-like or int\n    The number of iterations taken by lars_path to find the\n    grid of alphas for each target.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path : Compute Least Angle Regression or Lasso\n    path using LARS algorithm.\nlasso_path : Compute Lasso path with coordinate descent.\nLasso : Linear Model trained with L1 prior as\n    regularizer (aka the Lasso).\nLassoCV : Lasso linear model with iterative fitting\n    along a regularization path.\nLassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\nLassoLarsIC : Lasso model fit with Lars using BIC\n    or AIC for model selection.\nsklearn.decomposition.sparse_encode : Sparse coding.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> reg = linear_model.LassoLars(alpha=0.01)\n>>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\nLassoLars(alpha=0.01)\n>>> print(reg.coef_)\n[ 0.         -0.955...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\nThe optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nTechnically the Lasso model is optimizing the same objective function as\nthe Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n\nRead more in the :ref:`User Guide <lasso>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the L1 term, controlling regularization\n    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n\n    When `alpha = 0`, the objective is equivalent to ordinary least\n    squares, solved by the :class:`LinearRegression` object. For numerical\n    reasons, using `alpha = 0` with the `Lasso` object is not advised.\n    Instead, you should use the :class:`LinearRegression` object.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nprecompute : bool or array-like of shape (n_features, n_features),                 default=False\n    Whether to use a precomputed Gram matrix to speed up\n    calculations. The Gram matrix can also be passed as argument.\n    For sparse input this option is always ``False`` to preserve sparsity.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``, see Notes below.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the cost function formula).\n\ndual_gap_ : float or ndarray of shape (n_targets,)\n    Given param alpha, the dual gaps at the end of the optimization,\n    same shape as each observation of y.\n\nsparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n    Readonly property derived from ``coef_``.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : int or list of int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nlars_path : Regularization path using LARS.\nlasso_path : Regularization path using Lasso.\nLassoLars : Lasso Path along the regularization parameter using LARS algorithm.\nLassoCV : Lasso alpha parameter by cross-validation.\nLassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\nsklearn.decomposition.sparse_encode : Sparse coding array estimator.\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X argument of the fit method\nshould be directly passed as a Fortran-contiguous numpy array.\n\nRegularization improves the conditioning of the problem and\nreduces the variance of the estimates. Larger values specify stronger\nregularization. Alpha corresponds to `1 / (2C)` in other linear\nmodels such as :class:`~sklearn.linear_model.LogisticRegression` or\n:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\nassumed to be specific to the targets. Hence they must correspond in\nnumber.\n\nThe precise stopping criteria based on `tol` are the following: First, check that\nthat maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\nis smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\nIf so, then additionally check whether the dual gap is smaller than `tol` times\n:math:`||y||_2^2 / n_{\\text{samples}}`.\n\nThe target can be a 2-dimensional array, resulting in the optimization of the\nfollowing objective::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n\nwhere :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\nIt should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\ninstead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\nsparsity in the coefficients.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.Lasso(alpha=0.1)\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\nLasso(alpha=0.1)\n>>> print(clf.coef_)\n[0.85 0.  ]\n>>> print(clf.intercept_)\n0.15..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Latent Dirichlet Allocation with online variational Bayes algorithm.\n\nThe implementation is based on [1]_ and [2]_.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\nParameters\n----------\nn_components : int, default=10\n    Number of topics.\n\n    .. versionchanged:: 0.19\n        ``n_topics`` was renamed to ``n_components``\n\ndoc_topic_prior : float, default=None\n    Prior of document topic distribution `theta`. If the value is None,\n    defaults to `1 / n_components`.\n    In [1]_, this is called `alpha`.\n\ntopic_word_prior : float, default=None\n    Prior of topic word distribution `beta`. If the value is None, defaults\n    to `1 / n_components`.\n    In [1]_, this is called `eta`.\n\nlearning_method : {'batch', 'online'}, default='batch'\n    Method used to update `_component`. Only used in :meth:`fit` method.\n    In general, if the data size is large, the online update will be much\n    faster than the batch update.\n\n    Valid options::\n\n        'batch': Batch variational Bayes method. Use all training data in\n            each EM update.\n            Old `components_` will be overwritten in each iteration.\n        'online': Online variational Bayes method. In each EM update, use\n            mini-batch of training data to update the ``components_``\n            variable incrementally. The learning rate is controlled by the\n            ``learning_decay`` and the ``learning_offset`` parameters.\n\n    .. versionchanged:: 0.20\n        The default learning method is now ``\"batch\"``.\n\nlearning_decay : float, default=0.7\n    It is a parameter that control learning rate in the online learning\n    method. The value should be set between (0.5, 1.0] to guarantee\n    asymptotic convergence. When the value is 0.0 and batch_size is\n    ``n_samples``, the update method is same as batch learning. In the\n    literature, this is called kappa.\n\nlearning_offset : float, default=10.0\n    A (positive) parameter that downweights early iterations in online\n    learning.  It should be greater than 1.0. In the literature, this is\n    called tau_0.\n\nmax_iter : int, default=10\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the :meth:`fit` method, and not the\n    :meth:`partial_fit` method.\n\nbatch_size : int, default=128\n    Number of documents to use in each EM iteration. Only used in online\n    learning.\n\nevaluate_every : int, default=-1\n    How often to evaluate perplexity. Only used in `fit` method.\n    set it to 0 or negative number to not evaluate perplexity in\n    training at all. Evaluating perplexity can help you check convergence\n    in training process, but it will also increase total training time.\n    Evaluating perplexity in every iteration might increase training time\n    up to two-fold.\n\ntotal_samples : int, default=1e6\n    Total number of documents. Only used in the :meth:`partial_fit` method.\n\nperp_tol : float, default=1e-1\n    Perplexity tolerance in batch learning. Only used when\n    ``evaluate_every`` is greater than 0.\n\nmean_change_tol : float, default=1e-3\n    Stopping tolerance for updating document topic distribution in E-step.\n\nmax_doc_update_iter : int, default=100\n    Max number of iterations for updating document topic distribution in\n    the E-step.\n\nn_jobs : int, default=None\n    The number of jobs to use in the E-step.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    Verbosity level.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Variational parameters for topic word distribution. Since the complete\n    conditional for topic word distribution is a Dirichlet,\n    ``components_[i, j]`` can be viewed as pseudocount that represents the\n    number of times word `j` was assigned to topic `i`.\n    It can also be viewed as distribution over the words for each topic\n    after normalization:\n    ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\nexp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n    Exponential value of expectation of log topic word distribution.\n    In the literature, this is `exp(E[log(beta)])`.\n\nn_batch_iter_ : int\n    Number of iterations of the EM step.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of passes over the dataset.\n\nbound_ : float\n    Final perplexity score on training set.\n\ndoc_topic_prior_ : float\n    Prior of document topic distribution `theta`. If the value is None,\n    it is `1 / n_components`.\n\nrandom_state_ : RandomState instance\n    RandomState instance that is generated either from a seed, the random\n    number generator or by `np.random`.\n\ntopic_word_prior_ : float\n    Prior of topic word distribution `beta`. If the value is None, it is\n    `1 / n_components`.\n\nSee Also\n--------\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis:\n    A classifier with a linear decision boundary, generated by fitting\n    class conditional densities to the data and using Bayes' rule.\n\nReferences\n----------\n.. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n       Hoffman, David M. Blei, Francis Bach, 2010\n       https://github.com/blei-lab/onlineldavb\n\n.. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,\n       David M. Blei, Chong Wang, John Paisley, 2013\n\nExamples\n--------\n>>> from sklearn.decomposition import LatentDirichletAllocation\n>>> from sklearn.datasets import make_multilabel_classification\n>>> # This produces a feature matrix of token counts, similar to what\n>>> # CountVectorizer would produce on text.\n>>> X, _ = make_multilabel_classification(random_state=0)\n>>> lda = LatentDirichletAllocation(n_components=5,\n...     random_state=0)\n>>> lda.fit(X)\nLatentDirichletAllocation(...)\n>>> # get topics for some given samples:\n>>> lda.transform(X[-2:])\narray([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Learning Curve visualization.\n\nIt is recommended to use\n:meth:`~sklearn.model_selection.LearningCurveDisplay.from_estimator` to\ncreate a :class:`~sklearn.model_selection.LearningCurveDisplay` instance.\nAll parameters are stored as attributes.\n\nRead more in the :ref:`User Guide <visualizations>` for general information\nabout the visualization API and\n:ref:`detailed documentation <learning_curve>` regarding the learning\ncurve visualization.\n\n.. versionadded:: 1.2\n\nParameters\n----------\ntrain_sizes : ndarray of shape (n_unique_ticks,)\n    Numbers of training examples that has been used to generate the\n    learning curve.\n\ntrain_scores : ndarray of shape (n_ticks, n_cv_folds)\n    Scores on training sets.\n\ntest_scores : ndarray of shape (n_ticks, n_cv_folds)\n    Scores on test set.\n\nscore_name : str, default=None\n    The name of the score used in `learning_curve`. It will override the name\n    inferred from the `scoring` parameter. If `score` is `None`, we use `\"Score\"` if\n    `negate_score` is `False` and `\"Negative score\"` otherwise. If `scoring` is a\n    string or a callable, we infer the name. We replace `_` by spaces and capitalize\n    the first letter. We remove `neg_` and replace it by `\"Negative\"` if\n    `negate_score` is `False` or just remove it otherwise.\n\nAttributes\n----------\nax_ : matplotlib Axes\n    Axes with the learning curve.\n\nfigure_ : matplotlib Figure\n    Figure containing the learning curve.\n\nerrorbar_ : list of matplotlib Artist or None\n    When the `std_display_style` is `\"errorbar\"`, this is a list of\n    `matplotlib.container.ErrorbarContainer` objects. If another style is\n    used, `errorbar_` is `None`.\n\nlines_ : list of matplotlib Artist or None\n    When the `std_display_style` is `\"fill_between\"`, this is a list of\n    `matplotlib.lines.Line2D` objects corresponding to the mean train and\n    test scores. If another style is used, `line_` is `None`.\n\nfill_between_ : list of matplotlib Artist or None\n    When the `std_display_style` is `\"fill_between\"`, this is a list of\n    `matplotlib.collections.PolyCollection` objects. If another style is\n    used, `fill_between_` is `None`.\n\nSee Also\n--------\nsklearn.model_selection.learning_curve : Compute the learning curve.\n\nExamples\n--------\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import LearningCurveDisplay, learning_curve\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> tree = DecisionTreeClassifier(random_state=0)\n>>> train_sizes, train_scores, test_scores = learning_curve(\n...     tree, X, y)\n>>> display = LearningCurveDisplay(train_sizes=train_sizes,\n...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n>>> display.plot()\n<...>\n>>> plt.show()"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneGroupOutMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneOutMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Leave P Group(s) Out cross-validator.\n\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePGroupsOut and LeaveOneGroupOut is that\nthe former builds the test sets with all the samples assigned to\n``p`` different values of the groups while the latter uses samples\nall assigned the same groups.\n\nRead more in the :ref:`User Guide <leave_p_groups_out>`.\n\nParameters\n----------\nn_groups : int\n    Number of groups (``p``) to leave out in the test split.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import LeavePGroupsOut\n>>> X = np.array([[1, 2], [3, 4], [5, 6]])\n>>> y = np.array([1, 2, 1])\n>>> groups = np.array([1, 2, 3])\n>>> lpgo = LeavePGroupsOut(n_groups=2)\n>>> lpgo.get_n_splits(X, y, groups)\n3\n>>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n3\n>>> print(lpgo)\nLeavePGroupsOut(n_groups=2)\n>>> for i, (train_index, test_index) in enumerate(lpgo.split(X, y, groups)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\nFold 0:\n  Train: index=[2], group=[3]\n  Test:  index=[0 1], group=[1 2]\nFold 1:\n  Train: index=[1], group=[2]\n  Test:  index=[0 2], group=[1 3]\nFold 2:\n  Train: index=[0], group=[1]\n  Test:  index=[1 2], group=[2 3]\n\nSee Also\n--------\nGroupKFold : K-fold iterator variant with non-overlapping groups."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Leave-P-Out cross-validator.\n\nProvides train/test indices to split data in train/test sets. This results\nin testing on all distinct samples of size p, while the remaining n - p\nsamples form the training set in each iteration.\n\nNote: ``LeavePOut(p)`` is NOT equivalent to\n``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n\nDue to the high number of iterations which grows combinatorically with the\nnumber of samples this cross-validation method can be very costly. For\nlarge datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\nor :class:`ShuffleSplit`.\n\nRead more in the :ref:`User Guide <leave_p_out>`.\n\nParameters\n----------\np : int\n    Size of the test sets. Must be strictly less than the number of\n    samples.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import LeavePOut\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n>>> y = np.array([1, 2, 3, 4])\n>>> lpo = LeavePOut(2)\n>>> lpo.get_n_splits(X)\n6\n>>> print(lpo)\nLeavePOut(p=2)\n>>> for i, (train_index, test_index) in enumerate(lpo.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[2 3]\n  Test:  index=[0 1]\nFold 1:\n  Train: index=[1 3]\n  Test:  index=[0 2]\nFold 2:\n  Train: index=[1 2]\n  Test:  index=[0 3]\nFold 3:\n  Train: index=[0 3]\n  Test:  index=[1 2]\nFold 4:\n  Train: index=[0 2]\n  Test:  index=[1 3]\nFold 5:\n  Train: index=[0 1]\n  Test:  index=[2 3]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Ordinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup in case of sufficiently large problems, that is if firstly\n    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n    to `True`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_\narray([1., 2.])\n>>> reg.intercept_\n3.0...\n>>> reg.predict(np.array([[3, 5]]))\narray([16.])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear Support Vector Classification.\n\nSimilar to SVC with parameter kernel='linear', but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\n\nThe main differences between :class:`~sklearn.svm.LinearSVC` and\n:class:`~sklearn.svm.SVC` lie in the loss function used by default, and in\nthe handling of intercept regularization between those two implementations.\n\nThis class supports both dense and sparse input and the multiclass support\nis handled according to a one-vs-the-rest scheme.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n\nParameters\n----------\npenalty : {'l1', 'l2'}, default='l2'\n    Specifies the norm used in the penalization. The 'l2'\n    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n    vectors that are sparse.\n\nloss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n    Specifies the loss function. 'hinge' is the standard SVM loss\n    (used e.g. by the SVC class) while 'squared_hinge' is the\n    square of the hinge loss. The combination of ``penalty='l1'``\n    and ``loss='hinge'`` is not supported.\n\ndual : \"auto\" or bool, default=True\n    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n    `dual=\"auto\"` will choose the value of the parameter automatically,\n    based on the values of `n_samples`, `n_features`, `loss`, `multi_class`\n    and `penalty`. If `n_samples` < `n_features` and optimizer supports\n    chosen `loss`, `multi_class` and `penalty`, then dual will be set to True,\n    otherwise it will be set to False.\n\n    .. versionchanged:: 1.3\n       The `\"auto\"` option is added in version 1.3 and will be the default\n       in version 1.5.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n\nmulti_class : {'ovr', 'crammer_singer'}, default='ovr'\n    Determines the multi-class strategy if `y` contains more than\n    two classes.\n    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n    While `crammer_singer` is interesting from a theoretical perspective\n    as it is consistent, it is seldom used in practice as it rarely leads\n    to better accuracy and is more expensive to compute.\n    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n    will be ignored.\n\nfit_intercept : bool, default=True\n    Whether or not to fit an intercept. If set to True, the feature vector\n    is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where\n    1 corresponds to the intercept. If set to False, no intercept will be\n    used in calculations (i.e. data is expected to be already centered).\n\nintercept_scaling : float, default=1.0\n    When `fit_intercept` is True, the instance vector x becomes ``[x_1,\n    ..., x_n, intercept_scaling]``, i.e. a \"synthetic\" feature with a\n    constant value equal to `intercept_scaling` is appended to the instance\n    vector. The intercept becomes intercept_scaling * synthetic feature\n    weight. Note that liblinear internally penalizes the intercept,\n    treating it like any other term in the feature vector. To reduce the\n    impact of the regularization on the intercept, the `intercept_scaling`\n    parameter can be set to a value greater than 1; the higher the value of\n    `intercept_scaling`, the lower the impact of regularization on it.\n    Then, the weights become `[w_x_1, ..., w_x_n,\n    w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent\n    the feature weights and the intercept weight is scaled by\n    `intercept_scaling`. This scaling allows the intercept term to have a\n    different regularization behavior compared to the other features.\n\nclass_weight : dict or 'balanced', default=None\n    Set the parameter C of class i to ``class_weight[i]*C`` for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nverbose : int, default=0\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data for\n    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n    underlying implementation of :class:`LinearSVC` is not random and\n    ``random_state`` has no effect on the results.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_iter : int, default=1000\n    The maximum number of iterations to be run.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem).\n\n    ``coef_`` is a readonly property derived from ``raw_coef_`` that\n    follows the internal memory layout of liblinear.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Maximum number of iterations run across all classes.\n\nSee Also\n--------\nSVC : Implementation of Support Vector Machine classifier using libsvm:\n    the kernel can be non-linear but its SMO algorithm does not\n    scale to large number of samples as LinearSVC does.\n\n    Furthermore SVC multi-class mode is implemented using one\n    vs one scheme while LinearSVC uses one vs the rest. It is\n    possible to implement one vs the rest with SVC by using the\n    :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n\n    Finally SVC can fit dense data without memory copy if the input\n    is C-contiguous. Sparse data will still incur memory copy though.\n\nsklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n    cost function as LinearSVC\n    by adjusting the penalty and loss parameters. In addition it requires\n    less memory, allows incremental (online) learning, and implements\n    various loss functions and regularization regimes.\n\nNotes\n-----\nThe underlying C implementation uses a random number generator to\nselect features when fitting the model. It is thus not uncommon\nto have slightly different results for the same input data. If\nthat happens, try with a smaller ``tol`` parameter.\n\nThe underlying implementation, liblinear, uses a sparse internal\nrepresentation for the data that will incur a memory copy.\n\nPredict output may not match that of standalone liblinear in certain\ncases. See :ref:`differences from liblinear <liblinear_differences>`\nin the narrative documentation.\n\nReferences\n----------\n`LIBLINEAR: A Library for Large Linear Classification\n<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n\nExamples\n--------\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = make_pipeline(StandardScaler(),\n...                     LinearSVC(dual=\"auto\", random_state=0, tol=1e-5))\n>>> clf.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvc', LinearSVC(dual='auto', random_state=0, tol=1e-05))])\n\n>>> print(clf.named_steps['linearsvc'].coef_)\n[[0.141...   0.526... 0.679... 0.493...]]\n\n>>> print(clf.named_steps['linearsvc'].intercept_)\n[0.1693...]\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear Support Vector Regression.\n\nSimilar to SVR with parameter kernel='linear', but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\n\nThe main differences between :class:`~sklearn.svm.LinearSVR` and\n:class:`~sklearn.svm.SVR` lie in the loss function used by default, and in\nthe handling of intercept regularization between those two implementations.\n\nThis class supports both dense and sparse input.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nepsilon : float, default=0.0\n    Epsilon parameter in the epsilon-insensitive loss function. Note\n    that the value of this parameter depends on the scale of the target\n    variable y. If unsure, set ``epsilon=0``.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n\nloss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\n    Specifies the loss function. The epsilon-insensitive loss\n    (standard SVR) is the L1 loss, while the squared epsilon-insensitive\n    loss ('squared_epsilon_insensitive') is the L2 loss.\n\nfit_intercept : bool, default=True\n    Whether or not to fit an intercept. If set to True, the feature vector\n    is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where\n    1 corresponds to the intercept. If set to False, no intercept will be\n    used in calculations (i.e. data is expected to be already centered).\n\nintercept_scaling : float, default=1.0\n    When `fit_intercept` is True, the instance vector x becomes `[x_1, ...,\n    x_n, intercept_scaling]`, i.e. a \"synthetic\" feature with a constant\n    value equal to `intercept_scaling` is appended to the instance vector.\n    The intercept becomes intercept_scaling * synthetic feature weight.\n    Note that liblinear internally penalizes the intercept, treating it\n    like any other term in the feature vector. To reduce the impact of the\n    regularization on the intercept, the `intercept_scaling` parameter can\n    be set to a value greater than 1; the higher the value of\n    `intercept_scaling`, the lower the impact of regularization on it.\n    Then, the weights become `[w_x_1, ..., w_x_n,\n    w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent\n    the feature weights and the intercept weight is scaled by\n    `intercept_scaling`. This scaling allows the intercept term to have a\n    different regularization behavior compared to the other features.\n\ndual : \"auto\" or bool, default=True\n    Select the algorithm to either solve the dual or primal\n    optimization problem. Prefer dual=False when n_samples > n_features.\n    `dual=\"auto\"` will choose the value of the parameter automatically,\n    based on the values of `n_samples`, `n_features` and `loss`. If\n    `n_samples` < `n_features` and optimizer supports chosen `loss`,\n    then dual will be set to True, otherwise it will be set to False.\n\n    .. versionchanged:: 1.3\n       The `\"auto\"` option is added in version 1.3 and will be the default\n       in version 1.5.\n\nverbose : int, default=0\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in liblinear that, if enabled, may not work\n    properly in a multithreaded context.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_iter : int, default=1000\n    The maximum number of iterations to be run.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem).\n\n    `coef_` is a readonly property derived from `raw_coef_` that\n    follows the internal memory layout of liblinear.\n\nintercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Maximum number of iterations run across all classes.\n\nSee Also\n--------\nLinearSVC : Implementation of Support Vector Machine classifier using the\n    same library as this class (liblinear).\n\nSVR : Implementation of Support Vector Machine regression using libsvm:\n    the kernel can be non-linear but its SMO algorithm does not scale to\n    large number of samples as :class:`~sklearn.svm.LinearSVR` does.\n\nsklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost\n    function as LinearSVR\n    by adjusting the penalty and loss parameters. In addition it requires\n    less memory, allows incremental (online) learning, and implements\n    various loss functions and regularization regimes.\n\nExamples\n--------\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, random_state=0)\n>>> regr = make_pipeline(StandardScaler(),\n...                      LinearSVR(dual=\"auto\", random_state=0, tol=1e-5))\n>>> regr.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearsvr', LinearSVR(dual='auto', random_state=0, tol=1e-05))])\n\n>>> print(regr.named_steps['linearsvr'].coef_)\n[18.582... 27.023... 44.357... 64.522...]\n>>> print(regr.named_steps['linearsvr'].intercept_)\n[-4...]\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-2.384...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n\nThe anomaly score of each sample is called the Local Outlier Factor.\nIt measures the local deviation of the density of a given sample with respect\nto its neighbors.\nIt is local in that the anomaly score depends on how isolated the object\nis with respect to the surrounding neighborhood.\nMore precisely, locality is given by k-nearest neighbors, whose distance\nis used to estimate the local density.\nBy comparing the local density of a sample to the local densities of its\nneighbors, one can identify samples that have a substantially lower density\nthan their neighbors. These are considered outliers.\n\n.. versionadded:: 0.19\n\nParameters\n----------\nn_neighbors : int, default=20\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n    If n_neighbors is larger than the number of samples provided,\n    all samples will be used.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\np : float, default=2\n    Parameter for the Minkowski metric from\n    :func:`sklearn.metrics.pairwise_distances`. When p = 1, this\n    is equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncontamination : 'auto' or float, default='auto'\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. When fitting this is used to define the\n    threshold on the scores of the samples.\n\n    - if 'auto', the threshold is determined as in the\n      original paper,\n    - if a float, the contamination should be in the range (0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``.\n\nnovelty : bool, default=False\n    By default, LocalOutlierFactor is only meant to be used for outlier\n    detection (novelty=False). Set novelty to True if you want to use\n    LocalOutlierFactor for novelty detection. In this case be aware that\n    you should only use predict, decision_function and score_samples\n    on new unseen data and not on the training set; and note that the\n    results obtained this way may differ from the standard LOF results.\n\n    .. versionadded:: 0.20\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nnegative_outlier_factor_ : ndarray of shape (n_samples,)\n    The opposite LOF of the training samples. The higher, the more normal.\n    Inliers tend to have a LOF score close to 1\n    (``negative_outlier_factor_`` close to -1), while outliers tend to have\n    a larger LOF score.\n\n    The local outlier factor (LOF) of a sample captures its\n    supposed 'degree of abnormality'.\n    It is the average of the ratio of the local reachability density of\n    a sample and those of its k-nearest neighbors.\n\nn_neighbors_ : int\n    The actual number of neighbors used for :meth:`kneighbors` queries.\n\noffset_ : float\n    Offset used to obtain binary labels from the raw scores.\n    Observations having a negative_outlier_factor smaller than `offset_`\n    are detected as abnormal.\n    The offset is set to -1.5 (inliers score around -1), except when a\n    contamination parameter different than \"auto\" is provided. In that\n    case, the offset is defined in such a way we obtain the expected\n    number of outliers in training.\n\n    .. versionadded:: 0.20\n\neffective_metric_ : str\n    The effective metric used for the distance computation.\n\neffective_metric_params_ : dict\n    The effective additional keyword arguments for the metric function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    It is the number of samples in the fitted data.\n\nSee Also\n--------\nsklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n    Support Vector Machine.\n\nReferences\n----------\n.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n       LOF: identifying density-based local outliers. In ACM sigmod record.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.neighbors import LocalOutlierFactor\n>>> X = [[-1.1], [0.2], [101.1], [0.3]]\n>>> clf = LocalOutlierFactor(n_neighbors=2)\n>>> clf.fit_predict(X)\narray([ 1,  1, -1,  1])\n>>> clf.negative_outlier_factor_\narray([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Log loss, aka logistic loss or cross-entropy loss.\n\nThis is the loss function used in (multinomial) logistic regression\nand extensions of it such as neural networks, defined as the negative\nlog-likelihood of a logistic model that returns ``y_pred`` probabilities\nfor its training data ``y_true``.\nThe log loss is only defined for two or more labels.\nFor a single sample with true label :math:`y \\in \\{0,1\\}` and\na probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\nloss is:\n\n.. math::\n    L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n\nRead more in the :ref:`User Guide <log_loss>`.\n\nParameters\n----------\ny_true : array-like or label indicator matrix\n    Ground truth (correct) labels for n_samples samples.\n\ny_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n    Predicted probabilities, as returned by a classifier's\n    predict_proba method. If ``y_pred.shape = (n_samples,)``\n    the probabilities provided are assumed to be that of the\n    positive class. The labels in ``y_pred`` are assumed to be\n    ordered alphabetically, as done by\n    :class:`~sklearn.preprocessing.LabelBinarizer`.\n\neps : float or \"auto\", default=\"auto\"\n    Log loss is undefined for p=0 or p=1, so probabilities are\n    clipped to `max(eps, min(1 - eps, p))`. The default will depend on the\n    data type of `y_pred` and is set to `np.finfo(y_pred.dtype).eps`.\n\n    .. versionadded:: 1.2\n\n    .. versionchanged:: 1.2\n       The default value changed from `1e-15` to `\"auto\"` that is\n       equivalent to `np.finfo(y_pred.dtype).eps`.\n\n    .. deprecated:: 1.3\n       `eps` is deprecated in 1.3 and will be removed in 1.5.\n\nnormalize : bool, default=True\n    If true, return the mean loss per sample.\n    Otherwise, return the sum of the per-sample losses.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nlabels : array-like, default=None\n    If not provided, labels will be inferred from y_true. If ``labels``\n    is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n    assumed to be binary and are inferred from ``y_true``.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nloss : float\n    Log loss, aka logistic loss or cross-entropy loss.\n\nNotes\n-----\nThe logarithm used is the natural logarithm (base-e).\n\nReferences\n----------\nC.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\np. 209.\n\nExamples\n--------\n>>> from sklearn.metrics import log_loss\n>>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n0.21616..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Logistic Regression CV (aka logit, MaxEnt) classifier.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThis class implements logistic regression using liblinear, newton-cg, sag\nof lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\nregularization with primal formulation. The liblinear solver supports both\nL1 and L2 regularization, with a dual formulation only for the L2 penalty.\nElastic-Net penalty is only supported by the saga solver.\n\nFor the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\nis selected by the cross-validator\n:class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\nusing the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\nsolvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n\nParameters\n----------\nCs : int or list of floats, default=10\n    Each of the values in Cs describes the inverse of regularization\n    strength. If Cs is as an int, then a grid of Cs values are chosen\n    in a logarithmic scale between 1e-4 and 1e4.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n\ncv : int or cross-validation generator, default=None\n    The default cross-validation generator used is Stratified K-Folds.\n    If an integer is provided, then it is the number of folds used.\n    See the module :mod:`sklearn.model_selection` module for the\n    list of possible cross-validation objects.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ndual : bool, default=False\n    Dual (constrained) or primal (regularized, see also\n    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n\npenalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n    Specify the norm of the penalty:\n\n    - `'l2'`: add a L2 penalty term (used by default);\n    - `'l1'`: add a L1 penalty term;\n    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n    .. warning::\n       Some penalties may not work with some solvers. See the parameter\n       `solver` below, to know the compatibility between the penalty and\n       solver.\n\nscoring : str or callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``. For a list of scoring functions\n    that can be used, look at :mod:`sklearn.metrics`. The\n    default scoring option used is 'accuracy'.\n\nsolver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n\n    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n    To choose a solver, you might want to consider the following aspects:\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n          and 'saga' are faster for large ones;\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n          'lbfgs' handle multinomial loss;\n        - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n          because it does not handle warm-starting. 'liblinear' is\n          limited to one-versus-rest schemes.\n        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n          especially with one-hot encoded categorical features with rare\n          categories. Note that it is limited to binary classification and the\n          one-versus-rest reduction for multiclass classification. Be aware that\n          the memory usage of this solver has a quadratic dependency on\n          `n_features` because it explicitly computes the Hessian matrix.\n\n    .. warning::\n       The choice of the algorithm depends on the penalty chosen.\n       Supported penalties by solver:\n\n       - 'lbfgs'           -   ['l2']\n       - 'liblinear'       -   ['l1', 'l2']\n       - 'newton-cg'       -   ['l2']\n       - 'newton-cholesky' -   ['l2']\n       - 'sag'             -   ['l2']\n       - 'saga'            -   ['elasticnet', 'l1', 'l2']\n\n    .. note::\n       'sag' and 'saga' fast convergence is only guaranteed on features\n       with approximately the same scale. You can preprocess the data with\n       a scaler from :mod:`sklearn.preprocessing`.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionadded:: 1.2\n       newton-cholesky solver.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nmax_iter : int, default=100\n    Maximum number of iterations of the optimization algorithm.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\n    .. versionadded:: 0.17\n       class_weight == 'balanced'\n\nn_jobs : int, default=None\n    Number of CPU cores used during the cross-validation loop.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n    positive number for verbosity.\n\nrefit : bool, default=True\n    If set to True, the scores are averaged across all folds, and the\n    coefs and the C that corresponds to the best score is taken, and a\n    final refit is done using these parameters.\n    Otherwise the coefs, intercepts and C that correspond to the\n    best scores across folds are averaged.\n\nintercept_scaling : float, default=1\n    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n\nmulti_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n\nrandom_state : int, RandomState instance, default=None\n    Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n    Note that this only applies to the solver and not the cross-validation\n    generator. See :term:`Glossary <random_state>` for details.\n\nl1_ratios : list of float, default=None\n    The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n    Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n    using ``penalty='l2'``, while 1 is equivalent to using\n    ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n    of L1 and L2.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes, )\n    A list of class labels known to the classifier.\n\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function.\n\n    `coef_` is of shape (1, n_features) when the given problem\n    is binary.\n\nintercept_ : ndarray of shape (1,) or (n_classes,)\n    Intercept (a.k.a. bias) added to the decision function.\n\n    If `fit_intercept` is set to False, the intercept is set to zero.\n    `intercept_` is of shape(1,) when the problem is binary.\n\nCs_ : ndarray of shape (n_cs)\n    Array of C i.e. inverse of regularization parameter values used\n    for cross-validation.\n\nl1_ratios_ : ndarray of shape (n_l1_ratios)\n    Array of l1_ratios used for cross-validation. If no l1_ratio is used\n    (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n\ncoefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n    dict with classes as the keys, and the path of coefficients obtained\n    during cross-validating across each fold and then across each Cs\n    after doing an OvR for the corresponding class as values.\n    If the 'multi_class' option is set to 'multinomial', then\n    the coefs_paths are the coefficients corresponding to each class.\n    Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n    ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n    intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n    ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n    ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n\nscores_ : dict\n    dict with classes as the keys, and the values as the\n    grid of scores obtained during cross-validating each fold, after doing\n    an OvR for the corresponding class. If the 'multi_class' option\n    given is 'multinomial' then the same scores are repeated across\n    all classes, since this is the multinomial class. Each dict value\n    has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n    ``penalty='elasticnet'``.\n\nC_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n    Array of C that maps to the best scores across every class. If refit is\n    set to False, then for each class, the best C is the average of the\n    C's that correspond to the best scores for each fold.\n    `C_` is of shape(n_classes,) when the problem is binary.\n\nl1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n    Array of l1_ratio that maps to the best scores across every class. If\n    refit is set to False, then for each class, the best l1_ratio is the\n    average of the l1_ratio's that correspond to the best scores for each\n    fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n\nn_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n    Actual number of iterations for all classes, folds and Cs.\n    In the binary or multinomial cases, the first dimension is equal to 1.\n    If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n    n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nLogisticRegression : Logistic regression without tuning the\n    hyperparameter `C`.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegressionCV\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])\narray([0, 0])\n>>> clf.predict_proba(X[:2, :]).shape\n(2, 3)\n>>> clf.score(X, y)\n0.98..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Logistic Regression (aka logit, MaxEnt) classifier.\n\nIn the multiclass case, the training algorithm uses the one-vs-rest (OvR)\nscheme if the 'multi_class' option is set to 'ovr', and uses the\ncross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n(Currently the 'multinomial' option is supported only by the 'lbfgs',\n'sag', 'saga' and 'newton-cg' solvers.)\n\nThis class implements regularized logistic regression using the\n'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\nthat regularization is applied by default**. It can handle both dense\nand sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\nfloats for optimal performance; any other input format will be converted\n(and copied).\n\nThe 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\nwith primal formulation, or no regularization. The 'liblinear' solver\nsupports both L1 and L2 regularization, with a dual formulation only for\nthe L2 penalty. The Elastic-Net regularization is only supported by the\n'saga' solver.\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n\nParameters\n----------\npenalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n    Specify the norm of the penalty:\n\n    - `None`: no penalty is added;\n    - `'l2'`: add a L2 penalty term and it is the default choice;\n    - `'l1'`: add a L1 penalty term;\n    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n    .. warning::\n       Some penalties may not work with some solvers. See the parameter\n       `solver` below, to know the compatibility between the penalty and\n       solver.\n\n    .. versionadded:: 0.19\n       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\ndual : bool, default=False\n    Dual (constrained) or primal (regularized, see also\n    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n    n_samples > n_features.\n\ntol : float, default=1e-4\n    Tolerance for stopping criteria.\n\nC : float, default=1.0\n    Inverse of regularization strength; must be a positive float.\n    Like in support vector machines, smaller values specify stronger\n    regularization.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the decision function.\n\nintercept_scaling : float, default=1\n    Useful only when the solver 'liblinear' is used\n    and self.fit_intercept is set to True. In this case, x becomes\n    [x, self.intercept_scaling],\n    i.e. a \"synthetic\" feature with constant value equal to\n    intercept_scaling is appended to the instance vector.\n    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n    Note! the synthetic feature weight is subject to l1/l2 regularization\n    as all other features.\n    To lessen the effect of regularization on synthetic feature weight\n    (and therefore on the intercept) intercept_scaling has to be increased.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\n    .. versionadded:: 0.17\n       *class_weight='balanced'*\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n    data. See :term:`Glossary <random_state>` for details.\n\nsolver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n\n    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n    To choose a solver, you might want to consider the following aspects:\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n          and 'saga' are faster for large ones;\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n          'lbfgs' handle multinomial loss;\n        - 'liblinear' is limited to one-versus-rest schemes.\n        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n          especially with one-hot encoded categorical features with rare\n          categories. Note that it is limited to binary classification and the\n          one-versus-rest reduction for multiclass classification. Be aware that\n          the memory usage of this solver has a quadratic dependency on\n          `n_features` because it explicitly computes the Hessian matrix.\n\n    .. warning::\n       The choice of the algorithm depends on the penalty chosen.\n       Supported penalties by solver:\n\n       - 'lbfgs'           -   ['l2', None]\n       - 'liblinear'       -   ['l1', 'l2']\n       - 'newton-cg'       -   ['l2', None]\n       - 'newton-cholesky' -   ['l2', None]\n       - 'sag'             -   ['l2', None]\n       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n\n    .. note::\n       'sag' and 'saga' fast convergence is only guaranteed on features\n       with approximately the same scale. You can preprocess the data with\n       a scaler from :mod:`sklearn.preprocessing`.\n\n    .. seealso::\n       Refer to the User Guide for more information regarding\n       :class:`LogisticRegression` and more specifically the\n       :ref:`Table <Logistic_regression>`\n       summarizing solver/penalty supports.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n    .. versionchanged:: 0.22\n        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n    .. versionadded:: 1.2\n       newton-cholesky solver.\n\nmax_iter : int, default=100\n    Maximum number of iterations taken for the solvers to converge.\n\nmulti_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n\n    .. versionadded:: 0.18\n       Stochastic Average Gradient descent solver for 'multinomial' case.\n    .. versionchanged:: 0.22\n        Default changed from 'ovr' to 'auto' in 0.22.\n\nverbose : int, default=0\n    For the liblinear and lbfgs solvers set verbose to any positive\n    number for verbosity.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.17\n       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\nn_jobs : int, default=None\n    Number of CPU cores used when parallelizing over classes if\n    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n    set to 'liblinear' regardless of whether 'multi_class' is specified or\n    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors.\n    See :term:`Glossary <n_jobs>` for more details.\n\nl1_ratio : float, default=None\n    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n    combination of L1 and L2.\n\nAttributes\n----------\n\nclasses_ : ndarray of shape (n_classes, )\n    A list of class labels known to the classifier.\n\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function.\n\n    `coef_` is of shape (1, n_features) when the given problem is binary.\n    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\nintercept_ : ndarray of shape (1,) or (n_classes,)\n    Intercept (a.k.a. bias) added to the decision function.\n\n    If `fit_intercept` is set to False, the intercept is set to zero.\n    `intercept_` is of shape (1,) when the given problem is binary.\n    In particular, when `multi_class='multinomial'`, `intercept_`\n    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n    outcome 0 (False).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : ndarray of shape (n_classes,) or (1, )\n    Actual number of iterations for all classes. If binary or multinomial,\n    it returns only 1 element. For liblinear solver, only the maximum\n    number of iteration across all classes is given.\n\n    .. versionchanged:: 0.20\n\n        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\nSee Also\n--------\nSGDClassifier : Incrementally trained logistic regression (when given\n    the parameter ``loss=\"log_loss\"``).\nLogisticRegressionCV : Logistic regression with built-in cross validation.\n\nNotes\n-----\nThe underlying C implementation uses a random number generator to\nselect features when fitting the model. It is thus not uncommon,\nto have slightly different results for the same input data. If\nthat happens, try with a smaller tol parameter.\n\nPredict output may not match that of standalone liblinear in certain\ncases. See :ref:`differences from liblinear <liblinear_differences>`\nin the narrative documentation.\n\nReferences\n----------\n\nL-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\nLIBLINEAR -- A Library for Large Linear Classification\n    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\nSAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n    Minimizing Finite Sums with the Stochastic Average Gradient\n    https://hal.inria.fr/hal-00860051/document\n\nSAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n\nHsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n    methods for logistic regression and maximum entropy models.\n    Machine Learning 85(1-2):41-75.\n    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegression(random_state=0).fit(X, y)\n>>> clf.predict(X[:2, :])\narray([0, 0])\n>>> clf.predict_proba(X[:2, :])\narray([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n       [9.7...e-01, 2.8...e-02, ...e-08]])\n>>> clf.score(X, y)\n0.97..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Multi-layer Perceptron classifier.\n\nThis model optimizes the log-loss function using LBFGS or stochastic\ngradient descent.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nhidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)\n    The ith element represents the number of neurons in the ith\n    hidden layer.\n\nactivation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n    Activation function for the hidden layer.\n\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n\nsolver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n    The solver for weight optimization.\n\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n\n    - 'sgd' refers to stochastic gradient descent.\n\n    - 'adam' refers to a stochastic gradient-based optimizer proposed\n      by Kingma, Diederik, and Jimmy Ba\n\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n\nalpha : float, default=0.0001\n    Strength of the L2 regularization term. The L2 regularization term\n    is divided by the sample size when added to the loss.\n\nbatch_size : int, default='auto'\n    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`.\n\nlearning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n    Learning rate schedule for weight updates.\n\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n\n    - 'invscaling' gradually decreases the learning rate at each\n      time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n\n    Only used when ``solver='sgd'``.\n\nlearning_rate_init : float, default=0.001\n    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n\npower_t : float, default=0.5\n    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n\nmax_iter : int, default=200\n    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n\nshuffle : bool, default=True\n    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n\nrandom_state : int, RandomState instance, default=None\n    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=1e-4\n    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n\nverbose : bool, default=False\n    Whether to print progress messages to stdout.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nmomentum : float, default=0.9\n    Momentum for gradient descent update. Should be between 0 and 1. Only\n    used when solver='sgd'.\n\nnesterovs_momentum : bool, default=True\n    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to true, it will automatically set\n    aside 10% of training data as validation and terminate training when\n    validation score is not improving by at least ``tol`` for\n    ``n_iter_no_change`` consecutive epochs. The split is stratified,\n    except in a multilabel setting.\n    If early stopping is False, then the training stops when the training\n    loss does not improve by more than tol for n_iter_no_change consecutive\n    passes over the training set.\n    Only effective when solver='sgd' or 'adam'.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\nbeta_1 : float, default=0.9\n    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'.\n\nbeta_2 : float, default=0.999\n    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'.\n\nepsilon : float, default=1e-8\n    Value for numerical stability in adam. Only used when solver='adam'.\n\nn_iter_no_change : int, default=10\n    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'.\n\n    .. versionadded:: 0.20\n\nmax_fun : int, default=15000\n    Only used when solver='lbfgs'. Maximum number of loss function calls.\n    The solver iterates until convergence (determined by 'tol'), number\n    of iterations reaches max_iter, or this number of loss function calls.\n    Note that number of loss function calls will be greater than or equal\n    to the number of iterations for the `MLPClassifier`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nclasses_ : ndarray or list of ndarray of shape (n_classes,)\n    Class labels for each output.\n\nloss_ : float\n    The current loss computed with the loss function.\n\nbest_loss_ : float or None\n    The minimum loss reached by the solver throughout fitting.\n    If `early_stopping=True`, this attribute is set to `None`. Refer to\n    the `best_validation_score_` fitted attribute instead.\n\nloss_curve_ : list of shape (`n_iter_`,)\n    The ith element in the list represents the loss at the ith iteration.\n\nvalidation_scores_ : list of shape (`n_iter_`,) or None\n    The score at each iteration on a held-out validation set. The score\n    reported is the accuracy score. Only available if `early_stopping=True`,\n    otherwise the attribute is set to `None`.\n\nbest_validation_score_ : float or None\n    The best validation score (i.e. accuracy score) that triggered the\n    early stopping. Only available if `early_stopping=True`, otherwise the\n    attribute is set to `None`.\n\nt_ : int\n    The number of training samples seen by the solver during fitting.\n\ncoefs_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the weight matrix corresponding\n    to layer i.\n\nintercepts_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the bias vector corresponding to\n    layer i + 1.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    The number of iterations the solver has run.\n\nn_layers_ : int\n    Number of layers.\n\nn_outputs_ : int\n    Number of outputs.\n\nout_activation_ : str\n    Name of the output activation function.\n\nSee Also\n--------\nMLPRegressor : Multi-layer Perceptron regressor.\nBernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\n\nNotes\n-----\nMLPClassifier trains iteratively since at each time step\nthe partial derivatives of the loss function with respect to the model\nparameters are computed to update the parameters.\n\nIt can also have a regularization term added to the loss function\nthat shrinks model parameters to prevent overfitting.\n\nThis implementation works with data represented as dense numpy arrays or\nsparse scipy arrays of floating point values.\n\nReferences\n----------\nHinton, Geoffrey E. \"Connectionist learning procedures.\"\nArtificial intelligence 40.1 (1989): 185-234.\n\nGlorot, Xavier, and Yoshua Bengio.\n\"Understanding the difficulty of training deep feedforward neural networks.\"\nInternational Conference on Artificial Intelligence and Statistics. 2010.\n\n:arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification.\" <1502.01852>`\n\n:arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\n\"Adam: A method for stochastic optimization.\" <1412.6980>`\n\nExamples\n--------\n>>> from sklearn.neural_network import MLPClassifier\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_classification(n_samples=100, random_state=1)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n...                                                     random_state=1)\n>>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n>>> clf.predict_proba(X_test[:1])\narray([[0.038..., 0.961...]])\n>>> clf.predict(X_test[:5, :])\narray([1, 0, 1, 0, 1])\n>>> clf.score(X_test, y_test)\n0.8..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Multi-layer Perceptron regressor.\n\nThis model optimizes the squared error using LBFGS or stochastic gradient\ndescent.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nhidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)\n    The ith element represents the number of neurons in the ith\n    hidden layer.\n\nactivation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n    Activation function for the hidden layer.\n\n    - 'identity', no-op activation, useful to implement linear bottleneck,\n      returns f(x) = x\n\n    - 'logistic', the logistic sigmoid function,\n      returns f(x) = 1 / (1 + exp(-x)).\n\n    - 'tanh', the hyperbolic tan function,\n      returns f(x) = tanh(x).\n\n    - 'relu', the rectified linear unit function,\n      returns f(x) = max(0, x)\n\nsolver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n    The solver for weight optimization.\n\n    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n\n    - 'sgd' refers to stochastic gradient descent.\n\n    - 'adam' refers to a stochastic gradient-based optimizer proposed by\n      Kingma, Diederik, and Jimmy Ba\n\n    Note: The default solver 'adam' works pretty well on relatively\n    large datasets (with thousands of training samples or more) in terms of\n    both training time and validation score.\n    For small datasets, however, 'lbfgs' can converge faster and perform\n    better.\n\nalpha : float, default=0.0001\n    Strength of the L2 regularization term. The L2 regularization term\n    is divided by the sample size when added to the loss.\n\nbatch_size : int, default='auto'\n    Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the regressor will not use minibatch.\n    When set to \"auto\", `batch_size=min(200, n_samples)`.\n\nlearning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n    Learning rate schedule for weight updates.\n\n    - 'constant' is a constant learning rate given by\n      'learning_rate_init'.\n\n    - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n      at each time step 't' using an inverse scaling exponent of 'power_t'.\n      effective_learning_rate = learning_rate_init / pow(t, power_t)\n\n    - 'adaptive' keeps the learning rate constant to\n      'learning_rate_init' as long as training loss keeps decreasing.\n      Each time two consecutive epochs fail to decrease training loss by at\n      least tol, or fail to increase validation score by at least tol if\n      'early_stopping' is on, the current learning rate is divided by 5.\n\n    Only used when solver='sgd'.\n\nlearning_rate_init : float, default=0.001\n    The initial learning rate used. It controls the step-size\n    in updating the weights. Only used when solver='sgd' or 'adam'.\n\npower_t : float, default=0.5\n    The exponent for inverse scaling learning rate.\n    It is used in updating effective learning rate when the learning_rate\n    is set to 'invscaling'. Only used when solver='sgd'.\n\nmax_iter : int, default=200\n    Maximum number of iterations. The solver iterates until convergence\n    (determined by 'tol') or this number of iterations. For stochastic\n    solvers ('sgd', 'adam'), note that this determines the number of epochs\n    (how many times each data point will be used), not the number of\n    gradient steps.\n\nshuffle : bool, default=True\n    Whether to shuffle samples in each iteration. Only used when\n    solver='sgd' or 'adam'.\n\nrandom_state : int, RandomState instance, default=None\n    Determines random number generation for weights and bias\n    initialization, train-test split if early stopping is used, and batch\n    sampling when solver='sgd' or 'adam'.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=1e-4\n    Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n\nverbose : bool, default=False\n    Whether to print progress messages to stdout.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous\n    call to fit as initialization, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nmomentum : float, default=0.9\n    Momentum for gradient descent update. Should be between 0 and 1. Only\n    used when solver='sgd'.\n\nnesterovs_momentum : bool, default=True\n    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n    momentum > 0.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set\n    aside ``validation_fraction`` of training data as validation and\n    terminate training when validation score is not improving by at\n    least ``tol`` for ``n_iter_no_change`` consecutive epochs.\n    Only effective when solver='sgd' or 'adam'.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\nbeta_1 : float, default=0.9\n    Exponential decay rate for estimates of first moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'.\n\nbeta_2 : float, default=0.999\n    Exponential decay rate for estimates of second moment vector in adam,\n    should be in [0, 1). Only used when solver='adam'.\n\nepsilon : float, default=1e-8\n    Value for numerical stability in adam. Only used when solver='adam'.\n\nn_iter_no_change : int, default=10\n    Maximum number of epochs to not meet ``tol`` improvement.\n    Only effective when solver='sgd' or 'adam'.\n\n    .. versionadded:: 0.20\n\nmax_fun : int, default=15000\n    Only used when solver='lbfgs'. Maximum number of function calls.\n    The solver iterates until convergence (determined by ``tol``), number\n    of iterations reaches max_iter, or this number of function calls.\n    Note that number of function calls will be greater than or equal to\n    the number of iterations for the MLPRegressor.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nloss_ : float\n    The current loss computed with the loss function.\n\nbest_loss_ : float\n    The minimum loss reached by the solver throughout fitting.\n    If `early_stopping=True`, this attribute is set to `None`. Refer to\n    the `best_validation_score_` fitted attribute instead.\n    Only accessible when solver='sgd' or 'adam'.\n\nloss_curve_ : list of shape (`n_iter_`,)\n    Loss value evaluated at the end of each training step.\n    The ith element in the list represents the loss at the ith iteration.\n    Only accessible when solver='sgd' or 'adam'.\n\nvalidation_scores_ : list of shape (`n_iter_`,) or None\n    The score at each iteration on a held-out validation set. The score\n    reported is the R2 score. Only available if `early_stopping=True`,\n    otherwise the attribute is set to `None`.\n    Only accessible when solver='sgd' or 'adam'.\n\nbest_validation_score_ : float or None\n    The best validation score (i.e. R2 score) that triggered the\n    early stopping. Only available if `early_stopping=True`, otherwise the\n    attribute is set to `None`.\n    Only accessible when solver='sgd' or 'adam'.\n\nt_ : int\n    The number of training samples seen by the solver during fitting.\n    Mathematically equals `n_iters * X.shape[0]`, it means\n    `time_step` and it is used by optimizer's learning rate scheduler.\n\ncoefs_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the weight matrix corresponding\n    to layer i.\n\nintercepts_ : list of shape (n_layers - 1,)\n    The ith element in the list represents the bias vector corresponding to\n    layer i + 1.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    The number of iterations the solver has run.\n\nn_layers_ : int\n    Number of layers.\n\nn_outputs_ : int\n    Number of outputs.\n\nout_activation_ : str\n    Name of the output activation function.\n\nSee Also\n--------\nBernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\nMLPClassifier : Multi-layer Perceptron classifier.\nsklearn.linear_model.SGDRegressor : Linear model fitted by minimizing\n    a regularized empirical loss with SGD.\n\nNotes\n-----\nMLPRegressor trains iteratively since at each time step\nthe partial derivatives of the loss function with respect to the model\nparameters are computed to update the parameters.\n\nIt can also have a regularization term added to the loss function\nthat shrinks model parameters to prevent overfitting.\n\nThis implementation works with data represented as dense and sparse numpy\narrays of floating point values.\n\nReferences\n----------\nHinton, Geoffrey E. \"Connectionist learning procedures.\"\nArtificial intelligence 40.1 (1989): 185-234.\n\nGlorot, Xavier, and Yoshua Bengio.\n\"Understanding the difficulty of training deep feedforward neural networks.\"\nInternational Conference on Artificial Intelligence and Statistics. 2010.\n\n:arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification.\" <1502.01852>`\n\n:arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\n\"Adam: A method for stochastic optimization.\" <1412.6980>`\n\nExamples\n--------\n>>> from sklearn.neural_network import MLPRegressor\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_regression(n_samples=200, random_state=1)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n...                                                     random_state=1)\n>>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n>>> regr.predict(X_test[:2])\narray([-0.9..., -7.1...])\n>>> regr.score(X_test, y_test)\n0.4..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Make a scorer from a performance metric or loss function.\n\nA scorer is a wrapper around an arbitrary metric or loss function that is called\nwith the signature `scorer(estimator, X, y_true, **kwargs)`.\n\nIt is accepted in all scikit-learn estimators or functions allowing a `scoring`\nparameter.\n\nThe parameter `response_method` allows to specify which method of the estimator\nshould be used to feed the scoring/loss function.\n\nRead more in the :ref:`User Guide <scoring>`.\n\nParameters\n----------\nscore_func : callable\n    Score function (or loss function) with signature\n    ``score_func(y, y_pred, **kwargs)``.\n\nresponse_method : {\"predict_proba\", \"decision_function\", \"predict\"} or             list/tuple of such str, default=None\n\n    Specifies the response method to use get prediction from an estimator\n    (i.e. :term:`predict_proba`, :term:`decision_function` or\n    :term:`predict`). Possible choices are:\n\n    - if `str`, it corresponds to the name to the method to return;\n    - if a list or tuple of `str`, it provides the method names in order of\n      preference. The method returned corresponds to the first method in\n      the list and which is implemented by `estimator`.\n    - if `None`, it is equivalent to `\"predict\"`.\n\n    .. versionadded:: 1.4\n\ngreater_is_better : bool, default=True\n    Whether `score_func` is a score function (default), meaning high is\n    good, or a loss function, meaning low is good. In the latter case, the\n    scorer object will sign-flip the outcome of the `score_func`.\n\nneeds_proba : bool, default=False\n    Whether `score_func` requires `predict_proba` to get probability\n    estimates out of a classifier.\n\n    If True, for binary `y_true`, the score function is supposed to accept\n    a 1D `y_pred` (i.e., probability of the positive class, shape\n    `(n_samples,)`).\n\n    .. deprecated:: 1.4\n       `needs_proba` is deprecated in version 1.4 and will be removed in\n       1.6. Use `response_method=\"predict_proba\"` instead.\n\nneeds_threshold : bool, default=False\n    Whether `score_func` takes a continuous decision certainty.\n    This only works for binary classification using estimators that\n    have either a `decision_function` or `predict_proba` method.\n\n    If True, for binary `y_true`, the score function is supposed to accept\n    a 1D `y_pred` (i.e., probability of the positive class or the decision\n    function, shape `(n_samples,)`).\n\n    For example `average_precision` or the area under the roc curve\n    can not be computed using discrete predictions alone.\n\n    .. deprecated:: 1.4\n       `needs_threshold` is deprecated in version 1.4 and will be removed\n       in 1.6. Use `response_method=(\"decision_function\", \"predict_proba\")`\n       instead to preserve the same behaviour.\n\n**kwargs : additional arguments\n    Additional parameters to be passed to `score_func`.\n\nReturns\n-------\nscorer : callable\n    Callable object that returns a scalar score; greater is better.\n\nExamples\n--------\n>>> from sklearn.metrics import fbeta_score, make_scorer\n>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n>>> ftwo_scorer\nmake_scorer(fbeta_score, response_method='predict', beta=2)\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.svm import LinearSVC\n>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n...                     scoring=ftwo_scorer)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MatthewsCorrcoefMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the Matthews correlation coefficient (MCC).\n\nThe Matthews correlation coefficient is used in machine learning as a\nmeasure of the quality of binary and multiclass classifications. It takes\ninto account true and false positives and negatives and is generally\nregarded as a balanced measure which can be used even if the classes are of\nvery different sizes. The MCC is in essence a correlation coefficient value\nbetween -1 and +1. A coefficient of +1 represents a perfect prediction, 0\nan average random prediction and -1 an inverse prediction.  The statistic\nis also known as the phi coefficient. [source: Wikipedia]\n\nBinary and multiclass labels are supported.  Only in the binary case does\nthis relate to information about true and false positives and negatives.\nSee references below.\n\nRead more in the :ref:`User Guide <matthews_corrcoef>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated targets as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nmcc : float\n    The Matthews correlation coefficient (+1 represents a perfect\n    prediction, 0 an average random prediction and -1 and inverse\n    prediction).\n\nReferences\n----------\n.. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n   accuracy of prediction algorithms for classification: an overview.\n   <10.1093/bioinformatics/16.5.412>`\n\n.. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n   <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.\n\n.. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n    K-category correlation coefficient\n    <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n\n.. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n    Error Measures in MultiClass Prediction\n    <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n\nExamples\n--------\n>>> from sklearn.metrics import matthews_corrcoef\n>>> y_true = [+1, +1, +1, -1]\n>>> y_pred = [+1, -1, +1, +1]\n>>> matthews_corrcoef(y_true, y_pred)\n-0.33..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Scale each feature by its maximum absolute value.\n\nThis estimator scales and translates each feature individually such\nthat the maximal absolute value of each feature in the\ntraining set will be 1.0. It does not shift/center the data, and\nthus does not destroy any sparsity.\n\nThis scaler can also be applied to sparse CSR or CSC matrices.\n\n`MaxAbsScaler` doesn't reduce the effect of outliers; it only linearly\nscales them down. For an example visualization, refer to :ref:`Compare\nMaxAbsScaler with other scalers <plot_all_scaling_max_abs_scaler_section>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\ncopy : bool, default=True\n    Set to False to perform inplace scaling and avoid a copy (if the input\n    is already a numpy array).\n\nAttributes\n----------\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data.\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\nmax_abs_ : ndarray of shape (n_features,)\n    Per feature maximum absolute value.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator. Will be reset on\n    new calls to fit, but increments across ``partial_fit`` calls.\n\nSee Also\n--------\nmaxabs_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MaxAbsScaler\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> transformer = MaxAbsScaler().fit(X)\n>>> transformer\nMaxAbsScaler()\n>>> transformer.transform(X)\narray([[ 0.5, -1. ,  1. ],\n       [ 1. ,  0. ,  0. ],\n       [ 0. ,  1. , -0.5]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "The max_error metric calculates the maximum residual error.\n\nRead more in the :ref:`User Guide <max_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values.\n\nReturns\n-------\nmax_error : float\n    A positive floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import max_error\n>>> y_true = [3, 2, 7, 1]\n>>> y_pred = [4, 2, 7, 1]\n>>> max_error(y_true, y_pred)\n1"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean absolute error regression loss.\n\nRead more in the :ref:`User Guide <mean_absolute_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats\n    If multioutput is 'raw_values', then mean absolute error is returned\n    for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\n    MAE output is non-negative floating point. The best value is 0.0.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_absolute_error(y_true, y_pred)\n0.5\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_absolute_error(y_true, y_pred)\n0.75\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.85..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean absolute percentage error (MAPE) regression loss.\n\nNote here that the output is not a percentage in the range [0, 100]\nand a value of 100 does not mean 100% but 1e2. Furthermore, the output\ncan be arbitrarily high when `y_true` is small (which is specific to the\nmetric) or when `abs(y_true - y_pred)` is large (which is common for most\nregression metrics). Read more in the\n:ref:`User Guide <mean_absolute_percentage_error>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n    If input is list then the shape must be (n_outputs,).\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats\n    If multioutput is 'raw_values', then mean absolute percentage error\n    is returned for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\n    MAPE output is non-negative floating point. The best value is 0.0.\n    But note that bad predictions can lead to arbitrarily large\n    MAPE values, especially if some `y_true` values are very close to zero.\n    Note that we return a large value instead of `inf` when `y_true` is zero.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_absolute_percentage_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_absolute_percentage_error(y_true, y_pred)\n0.3273...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_absolute_percentage_error(y_true, y_pred)\n0.5515...\n>>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.6198...\n>>> # the value when some element of the y_true is zero is arbitrarily high because\n>>> # of the division by epsilon\n>>> y_true = [1., 0., 2.4, 7.]\n>>> y_pred = [1.2, 0.1, 2.4, 8.]\n>>> mean_absolute_percentage_error(y_true, y_pred)\n112589990684262.48"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanGammaDevianceMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean Gamma deviance regression loss.\n\nGamma deviance is equivalent to the Tweedie deviance with\nthe power parameter `power=2`. It is invariant to scaling of\nthe target variable, and measures relative errors.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values. Requires y_true > 0.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values. Requires y_pred > 0.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n    A non-negative floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import mean_gamma_deviance\n>>> y_true = [2, 0.5, 1, 4]\n>>> y_pred = [0.5, 0.5, 2., 2.]\n>>> mean_gamma_deviance(y_true, y_pred)\n1.0568..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Pinball loss for quantile regression.\n\nRead more in the :ref:`User Guide <pinball_loss>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nalpha : float, slope of the pinball loss, default=0.5,\n    This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,\n    `alpha=0.95` is minimized by estimators of the 95th percentile.\n\nmultioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats\n    If multioutput is 'raw_values', then mean absolute error is returned\n    for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\n    The pinball loss output is a non-negative floating point. The best\n    value is 0.0.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_pinball_loss\n>>> y_true = [1, 2, 3]\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n0.03...\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n0.3...\n>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n0.3...\n>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n0.03...\n>>> mean_pinball_loss(y_true, y_true, alpha=0.1)\n0.0\n>>> mean_pinball_loss(y_true, y_true, alpha=0.9)\n0.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPoissonDevianceMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean Poisson deviance regression loss.\n\nPoisson deviance is equivalent to the Tweedie deviance with\nthe power parameter `power=1`.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values. Requires y_true >= 0.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values. Requires y_pred > 0.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float\n    A non-negative floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import mean_poisson_deviance\n>>> y_true = [2, 0, 1, 4]\n>>> y_pred = [0.5, 0.5, 2., 2.]\n>>> mean_poisson_deviance(y_true, y_pred)\n1.4260..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\nbandwidth : float, default=None\n    Bandwidth used in the flat kernel.\n\n    If not given, the bandwidth is estimated using\n    sklearn.cluster.estimate_bandwidth; see the documentation for that\n    function for hints on scalability (see also the Notes, below).\n\nseeds : array-like of shape (n_samples, n_features), default=None\n    Seeds used to initialize kernels. If not set,\n    the seeds are calculated by clustering.get_bin_seeds\n    with bandwidth as the grid size and default values for\n    other parameters.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    The default value is False.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. The following tasks benefit\n    from the parallelization:\n\n    - The search of nearest neighbors for bandwidth estimation and label\n      assignments. See the details in the docstring of the\n      ``NearestNeighbors`` class.\n    - Hill-climbing optimization for all seeds.\n\n    See :term:`Glossary <n_jobs>` for more details.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\nn_iter_ : int\n    Maximum number of iterations performed on each seed.\n\n    .. versionadded:: 0.22\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKMeans : K-Means clustering.\n\nNotes\n-----\n\nScalability:\n\nBecause this implementation uses a flat kernel and\na Ball Tree to look up members of each kernel, the complexity will tend\ntowards O(T*n*log(n)) in lower dimensions, with n the number of samples\nand T the number of points. In higher dimensions the complexity will\ntend towards O(T*n^2).\n\nScalability can be boosted by using fewer seeds, for example by using\na higher value of min_bin_freq in the get_bin_seeds function.\n\nNote that the estimate_bandwidth function is much less scalable than the\nmean shift algorithm and will be the bottleneck if it is used.\n\nReferences\n----------\n\nDorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\nfeature space analysis\". IEEE Transactions on Pattern Analysis and\nMachine Intelligence. 2002. pp. 603-619.\n\nExamples\n--------\n>>> from sklearn.cluster import MeanShift\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = MeanShift(bandwidth=2).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering.predict([[0, 0], [5, 5]])\narray([1, 0])\n>>> clustering\nMeanShift(bandwidth=2)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean squared error regression loss.\n\nRead more in the :ref:`User Guide <mean_squared_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nsquared : bool, default=True\n    If True returns MSE value, if False returns RMSE value.\n\n    .. deprecated:: 1.4\n       `squared` is deprecated in 1.4 and will be removed in 1.6.\n       Use :func:`~sklearn.metrics.root_mean_squared_error`\n       instead to calculate the root mean squared error.\n\nReturns\n-------\nloss : float or ndarray of floats\n    A non-negative floating point value (the best value is 0.0), or an\n    array of floating point values, one for each individual target.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_squared_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_squared_error(y_true, y_pred)\n0.375\n>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n>>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n>>> mean_squared_error(y_true, y_pred)\n0.708...\n>>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\narray([0.41666667, 1.        ])\n>>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.825..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean squared logarithmic error regression loss.\n\nRead more in the :ref:`User Guide <mean_squared_log_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors when the input is of multioutput\n        format.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nsquared : bool, default=True\n    If True returns MSLE (mean squared log error) value.\n    If False returns RMSLE (root mean squared log error) value.\n\n    .. deprecated:: 1.4\n       `squared` is deprecated in 1.4 and will be removed in 1.6.\n       Use :func:`~sklearn.metrics.root_mean_squared_log_error`\n       instead to calculate the root mean squared logarithmic error.\n\nReturns\n-------\nloss : float or ndarray of floats\n    A non-negative floating point value (the best value is 0.0), or an\n    array of floating point values, one for each individual target.\n\nExamples\n--------\n>>> from sklearn.metrics import mean_squared_log_error\n>>> y_true = [3, 5, 2.5, 7]\n>>> y_pred = [2.5, 5, 4, 8]\n>>> mean_squared_log_error(y_true, y_pred)\n0.039...\n>>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n>>> mean_squared_log_error(y_true, y_pred)\n0.044...\n>>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\narray([0.00462428, 0.08377444])\n>>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.060..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mean Tweedie deviance regression loss.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\npower : float, default=0\n    Tweedie power parameter. Either power <= 0 or power >= 1.\n\n    The higher `p` the less weight is given to extreme\n    deviations between true and predicted targets.\n\n    - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n    - power = 0 : Normal distribution, output corresponds to\n      mean_squared_error. y_true and y_pred can be any real numbers.\n    - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n      y_pred > 0.\n    - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n      and y_pred > 0.\n    - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n    - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n      and y_pred > 0.\n    - otherwise : Positive stable distribution. Requires: y_true > 0\n      and y_pred > 0.\n\nReturns\n-------\nloss : float\n    A non-negative floating point value (the best value is 0.0).\n\nExamples\n--------\n>>> from sklearn.metrics import mean_tweedie_deviance\n>>> y_true = [2, 0, 1, 4]\n>>> y_pred = [0.5, 0.5, 2., 2.]\n>>> mean_tweedie_deviance(y_true, y_pred, power=1)\n1.4260..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Median absolute error regression loss.\n\nMedian absolute error output is non-negative floating point. The best value\nis 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values. Array-like value defines\n    weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nloss : float or ndarray of floats\n    If multioutput is 'raw_values', then mean absolute error is returned\n    for each output separately.\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\n    weighted average of all output errors is returned.\n\nExamples\n--------\n>>> from sklearn.metrics import median_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> median_absolute_error(y_true, y_pred)\n0.5\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> median_absolute_error(y_true, y_pred)\n0.75\n>>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\n>>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n0.85"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, e.g. between\nzero and one.\n\nThe transformation is given by::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n    X_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\n`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\nscales them down into a fixed range, where the largest occurring data point\ncorresponds to the maximum value and the smallest one corresponds to the\nminimum value. For an example visualization, refer to :ref:`Compare\nMinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nfeature_range : tuple (min, max), default=(0, 1)\n    Desired range of transformed data.\n\ncopy : bool, default=True\n    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array).\n\nclip : bool, default=False\n    Set to True to clip transformed values of held-out data to\n    provided `feature range`.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nmin_ : ndarray of shape (n_features,)\n    Per feature adjustment for minimum. Equivalent to\n    ``min - X.min(axis=0) * self.scale_``\n\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data. Equivalent to\n    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\ndata_min_ : ndarray of shape (n_features,)\n    Per feature minimum seen in the data\n\n    .. versionadded:: 0.17\n       *data_min_*\n\ndata_max_ : ndarray of shape (n_features,)\n    Per feature maximum seen in the data\n\n    .. versionadded:: 0.17\n       *data_max_*\n\ndata_range_ : ndarray of shape (n_features,)\n    Per feature range ``(data_max_ - data_min_)`` seen in the data\n\n    .. versionadded:: 0.17\n       *data_range_*\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator.\n    It will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nminmax_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n>>> scaler = MinMaxScaler()\n>>> print(scaler.fit(data))\nMinMaxScaler()\n>>> print(scaler.data_max_)\n[ 1. 18.]\n>>> print(scaler.transform(data))\n[[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [1.   1.  ]]\n>>> print(scaler.transform([[2, 2]]))\n[[1.5 0. ]]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mini-batch dictionary learning.\n\nFinds a dictionary (a set of atoms) that performs well at sparsely\nencoding the fitted data.\n\nSolves the optimization problem::\n\n   (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                (U,V)\n                with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\nthe entry-wise matrix norm which is the sum of the absolute values\nof all the entries in the matrix.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of dictionary elements to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter.\n\nmax_iter : int, default=1_000\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion heuristics.\n\n    .. versionadded:: 1.1\n\n    .. deprecated:: 1.4\n       `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.\n       Use the default value (i.e. `1_000`) instead.\n\nfit_algorithm : {'lars', 'cd'}, default='lars'\n    The algorithm used:\n\n    - `'lars'`: uses the least angle regression method to solve the lasso\n      problem (`linear_model.lars_path`)\n    - `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nbatch_size : int, default=256\n    Number of samples in each mini-batch.\n\n    .. versionchanged:: 1.3\n       The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples before forming batches.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial value of the dictionary for warm restart scenarios.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n      if the estimated components are sparse.\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution.\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and\n    `algorithm='omp'`. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `None`, defaults to `alpha`.\n\n    .. versionchanged:: 1.2\n        When None, default value changed from 1.0 to `alpha`.\n\nverbose : bool or int, default=False\n    To control the verbosity of the procedure.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\n    .. versionadded:: 0.22\n\ncallback : callable, default=None\n    A callable that gets invoked at the end of each iteration.\n\n    .. versionadded:: 1.1\n\ntol : float, default=1e-3\n    Control early stopping based on the norm of the differences in the\n    dictionary between 2 steps.\n\n    To disable early stopping based on changes in the dictionary, set\n    `tol` to 0.0.\n\n    .. versionadded:: 1.1\n\nmax_no_improvement : int, default=10\n    Control early stopping based on the consecutive number of mini batches\n    that does not yield an improvement on the smoothed cost function.\n\n    To disable convergence detection based on cost function, set\n    `max_no_improvement` to None.\n\n    .. versionadded:: 1.1\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components extracted from the data.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of iterations over the full dataset.\n\nn_steps_ : int\n    Number of mini-batches processed.\n\n    .. versionadded:: 1.1\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nSparseCoder : Find a sparse representation of data from a fixed,\n    precomputed dictionary.\nSparsePCA : Sparse Principal Components Analysis.\n\nReferences\n----------\n\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\nfor sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import MiniBatchDictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42)\n>>> dict_learner = MiniBatchDictionaryLearning(\n...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n...     transform_alpha=0.1, max_iter=20, random_state=42)\n>>> X_transformed = dict_learner.fit_transform(X)\n\nWe can check the level of sparsity of `X_transformed`:\n\n>>> np.mean(X_transformed == 0) > 0.5\nTrue\n\nWe can compare the average squared euclidean norm of the reconstruction\nerror of the sparse coded signal relative to the squared euclidean norm of\nthe original signal:\n\n>>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n0.052..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centroids using sampling based on\n    an empirical probability distribution of the points' contribution to the\n    overall inertia. This technique speeds up convergence. The algorithm\n    implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n    by making several trials at each sampling step and choosing the best centroid\n    among them.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nmax_iter : int, default=100\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion heuristics.\n\nbatch_size : int, default=1024\n    Size of the mini batches.\n    For faster computations, you can set the ``batch_size`` greater than\n    256 * number of cores to enable parallelism on all cores.\n\n    .. versionchanged:: 1.0\n       `batch_size` default changed from 100 to 1024.\n\nverbose : int, default=0\n    Verbosity mode.\n\ncompute_labels : bool, default=True\n    Compute label assignment and inertia for the complete dataset\n    once the minibatch optimization has converged in fit.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization and\n    random reassignment. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.0\n    Control early stopping based on the relative center changes as\n    measured by a smoothed, variance-normalized of the mean center\n    squared position changes. This early stopping heuristics is\n    closer to the one used for the batch variant of the algorithms\n    but induces a slight computational and memory overhead over the\n    inertia heuristic.\n\n    To disable convergence detection based on normalized center\n    change, set tol to 0.0 (default).\n\nmax_no_improvement : int, default=10\n    Control early stopping based on the consecutive number of mini\n    batches that does not yield an improvement on the smoothed inertia.\n\n    To disable convergence detection based on inertia, set\n    max_no_improvement to None.\n\ninit_size : int, default=None\n    Number of samples to randomly sample for speeding up the\n    initialization (sometimes at the expense of accuracy): the\n    only algorithm is initialized by running a batch KMeans on a\n    random subset of the data. This needs to be larger than n_clusters.\n\n    If `None`, the heuristic is `init_size = 3 * batch_size` if\n    `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n\nn_init : 'auto' or int, default=\"auto\"\n    Number of random initializations that are tried.\n    In contrast to KMeans, the algorithm is only run once, using the best of\n    the `n_init` initializations as measured by inertia. Several runs are\n    recommended for sparse high-dimensional problems (see\n    :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    3 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'` in version.\n\nreassignment_ratio : float, default=0.01\n    Control the fraction of the maximum number of counts for a center to\n    be reassigned. A higher value means that low count centers are more\n    easily reassigned, which means that the model will take longer to\n    converge, but should converge in a better clustering. However, too high\n    a value may cause convergence issues, especially with a small batch\n    size.\n\nAttributes\n----------\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point (if compute_labels is set to True).\n\ninertia_ : float\n    The value of the inertia criterion associated with the chosen\n    partition if compute_labels is set to True. If compute_labels is set to\n    False, it's an approximation of the inertia based on an exponentially\n    weighted average of the batch inertiae.\n    The inertia is defined as the sum of square distances of samples to\n    their cluster center, weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations over the full dataset.\n\nn_steps_ : int\n    Number of minibatches processed.\n\n    .. versionadded:: 1.0\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKMeans : The classic implementation of the clustering method based on the\n    Lloyd's algorithm. It consumes the whole set of input data at each\n    iteration.\n\nNotes\n-----\nSee https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\nWhen there are too few points in the dataset, some centers may be\nduplicated, which means that a proper clustering in terms of the number\nof requesting clusters and the number of returned clusters will not\nalways match. One solution is to set `reassignment_ratio=0`, which\nprevents reassignments of clusters that are too small.\n\nExamples\n--------\n>>> from sklearn.cluster import MiniBatchKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 0], [4, 4],\n...               [4, 5], [0, 1], [2, 2],\n...               [3, 2], [5, 5], [1, -1]])\n>>> # manually fit on batches\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6,\n...                          n_init=\"auto\")\n>>> kmeans = kmeans.partial_fit(X[0:6,:])\n>>> kmeans = kmeans.partial_fit(X[6:12,:])\n>>> kmeans.cluster_centers_\narray([[3.375, 3.  ],\n       [0.75 , 0.5 ]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([1, 0], dtype=int32)\n>>> # fit on the whole data\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6,\n...                          max_iter=10,\n...                          n_init=\"auto\").fit(X)\n>>> kmeans.cluster_centers_\narray([[3.55102041, 2.48979592],\n       [1.06896552, 1.        ]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([1, 0], dtype=int32)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mini-Batch Non-Negative Matrix Factorization (NMF).\n\n.. versionadded:: 1.1\n\nFind two non-negative matrices, i.e. matrices with all non-negative elements,\n(`W`, `H`) whose product approximates the non-negative matrix `X`. This\nfactorization can be used for example for dimensionality reduction, source\nseparation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n        &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n        &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n        &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n        &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}^2` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe objective function is minimized with an alternating minimization of `W`\nand `H`.\n\nNote that the transformed data is named `W` and the components matrix is\nnamed `H`. In the NMF literature, the naming convention is usually the opposite\nsince the data matrix `X` is transposed.\n\nRead more in the :ref:`User Guide <MiniBatchNMF>`.\n\nParameters\n----------\nn_components : int or {'auto'} or None, default=None\n    Number of components, if `n_components` is not set all features\n    are kept.\n    If `n_components='auto'`, the number of components is automatically inferred\n    from W or H shapes.\n\n    .. versionchanged:: 1.4\n        Added `'auto'` value.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n    Valid options:\n\n    - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,\n      otherwise random.\n\n    - `'random'`: non-negative random matrices, scaled with:\n      `sqrt(X.mean() / n_components)`\n\n    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n      initialization (better for sparseness).\n\n    - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n      (better when sparsity is not desired).\n\n    - `'nndsvdar'` NNDSVD with zeros filled with small random values\n      (generally faster, less accurate alternative to NNDSVDa\n      for when sparsity is not desired).\n\n    - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\nbatch_size : int, default=1024\n    Number of samples in each mini-batch. Large batch sizes\n    give better long-term convergence at the cost of a slower start.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between `X`\n    and the dot product `WH`. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input\n    matrix `X` cannot contain zeros.\n\ntol : float, default=1e-4\n    Control early stopping based on the norm of the differences in `H`\n    between 2 steps. To disable early stopping based on changes in `H`, set\n    `tol` to 0.0.\n\nmax_no_improvement : int, default=10\n    Control early stopping based on the consecutive number of mini batches\n    that does not yield an improvement on the smoothed cost function.\n    To disable convergence detection based on cost function, set\n    `max_no_improvement` to None.\n\nmax_iter : int, default=200\n    Maximum number of iterations over the complete dataset before\n    timing out.\n\nalpha_W : float, default=0.0\n    Constant that multiplies the regularization terms of `W`. Set it to zero\n    (default) to have no regularization on `W`.\n\nalpha_H : float or \"same\", default=\"same\"\n    Constant that multiplies the regularization terms of `H`. Set it to zero to\n    have no regularization on `H`. If \"same\" (default), it takes the same value as\n    `alpha_W`.\n\nl1_ratio : float, default=0.0\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\nforget_factor : float, default=0.7\n    Amount of rescaling of past information. Its value could be 1 with\n    finite datasets. Choosing values < 1 is recommended with online\n    learning as more recent batches will weight more than past batches.\n\nfresh_restarts : bool, default=False\n    Whether to completely solve for W at each step. Doing fresh restarts will likely\n    lead to a better solution for a same number of iterations but it is much slower.\n\nfresh_restarts_max_iter : int, default=30\n    Maximum number of iterations when solving for W at each step. Only used when\n    doing fresh restarts. These iterations may be stopped early based on a small\n    change of W controlled by `tol`.\n\ntransform_max_iter : int, default=None\n    Maximum number of iterations when solving for W at transform time.\n    If None, it defaults to `max_iter`.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : bool, default=False\n    Whether to be verbose.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Factorization matrix, sometimes called 'dictionary'.\n\nn_components_ : int\n    The number of components. It is same as the `n_components` parameter\n    if it was given. Otherwise, it will be same as the number of\n    features.\n\nreconstruction_err_ : float\n    Frobenius norm of the matrix difference, or beta-divergence, between\n    the training data `X` and the reconstructed data `WH` from\n    the fitted model.\n\nn_iter_ : int\n    Actual number of started iterations over the whole dataset.\n\nn_steps_ : int\n    Number of mini-batches processed.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\nSee Also\n--------\nNMF : Non-negative matrix factorization.\nMiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n    data using a sparse code.\n\nReferences\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n   factorizations\" <10.1587/transfun.E92.A.708>`\n   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n   of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n.. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n   beta-divergence\" <10.1162/NECO_a_00168>`\n   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n.. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n   Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n   Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import MiniBatchNMF\n>>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mini-batch Sparse Principal Components Analysis.\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nFor an example comparing sparse PCA to PCA, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of sparse atoms to extract. If None, then ``n_components``\n    is set to ``n_features``.\n\nalpha : int, default=1\n    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n\nridge_alpha : float, default=0.01\n    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n\nmax_iter : int, default=1_000\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion heuristics.\n\n    .. versionadded:: 1.2\n\n    .. deprecated:: 1.4\n       `max_iter=None` is deprecated in 1.4 and will be removed in 1.6.\n       Use the default value (i.e. `100`) instead.\n\ncallback : callable, default=None\n    Callable that gets invoked every five iterations.\n\nbatch_size : int, default=3\n    The number of features to take in each mini batch.\n\nverbose : int or bool, default=False\n    Controls the verbosity; the higher, the more messages. Defaults to 0.\n\nshuffle : bool, default=True\n    Whether to shuffle the data before splitting it in batches.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmethod : {'lars', 'cd'}, default='lars'\n    Method to be used for optimization.\n    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for random shuffling when ``shuffle`` is set to ``True``,\n    during online dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=1e-3\n    Control early stopping based on the norm of the differences in the\n    dictionary between 2 steps.\n\n    To disable early stopping based on changes in the dictionary, set\n    `tol` to 0.0.\n\n    .. versionadded:: 1.1\n\nmax_no_improvement : int or None, default=10\n    Control early stopping based on the consecutive number of mini batches\n    that does not yield an improvement on the smoothed cost function.\n\n    To disable convergence detection based on cost function, set\n    `max_no_improvement` to `None`.\n\n    .. versionadded:: 1.1\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Sparse components extracted from the data.\n\nn_components_ : int\n    Estimated number of components.\n\n    .. versionadded:: 0.23\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to ``X.mean(axis=0)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nIncrementalPCA : Incremental principal components analysis.\nPCA : Principal component analysis.\nSparsePCA : Sparse Principal Components Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import MiniBatchSparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n...                                  max_iter=10, random_state=0)\n>>> transformer.fit(X)\nMiniBatchSparsePCA(...)\n>>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\n0.9..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Binary indicators for missing values.\n\nNote that this component typically should not be used in a vanilla\n:class:`~sklearn.pipeline.Pipeline` consisting of transformers and a\nclassifier, but rather could be added using a\n:class:`~sklearn.pipeline.FeatureUnion` or\n:class:`~sklearn.compose.ColumnTransformer`.\n\nRead more in the :ref:`User Guide <impute>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nmissing_values : int, float, str, np.nan or None, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.\n\nfeatures : {'missing-only', 'all'}, default='missing-only'\n    Whether the imputer mask should represent all or a subset of\n    features.\n\n    - If `'missing-only'` (default), the imputer mask will only represent\n      features containing missing values during fit time.\n    - If `'all'`, the imputer mask will represent all features.\n\nsparse : bool or 'auto', default='auto'\n    Whether the imputer mask format should be sparse or dense.\n\n    - If `'auto'` (default), the imputer mask will be of same type as\n      input.\n    - If `True`, the imputer mask will be a sparse matrix.\n    - If `False`, the imputer mask will be a numpy array.\n\nerror_on_new : bool, default=True\n    If `True`, :meth:`transform` will raise an error when there are\n    features with missing values that have no missing values in\n    :meth:`fit`. This is applicable only when `features='missing-only'`.\n\nAttributes\n----------\nfeatures_ : ndarray of shape (n_missing_features,) or (n_features,)\n    The features indices which will be returned when calling\n    :meth:`transform`. They are computed during :meth:`fit`. If\n    `features='all'`, `features_` is equal to `range(n_features)`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nSimpleImputer : Univariate imputation of missing values.\nIterativeImputer : Multivariate imputation of missing values.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.impute import MissingIndicator\n>>> X1 = np.array([[np.nan, 1, 3],\n...                [4, 0, np.nan],\n...                [8, 1, 0]])\n>>> X2 = np.array([[5, 1, np.nan],\n...                [np.nan, 2, 3],\n...                [2, 4, 0]])\n>>> indicator = MissingIndicator()\n>>> indicator.fit(X1)\nMissingIndicator()\n>>> X2_tr = indicator.transform(X2)\n>>> X2_tr\narray([[False,  True],\n       [ True, False],\n       [False, False]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelection",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transform between iterable of iterables and a multilabel format.\n\nAlthough a list of sets or tuples is a very intuitive format for multilabel\ndata, it is unwieldy to process. This transformer converts between this\nintuitive format and the supported multilabel format: a (samples x classes)\nbinary matrix indicating the presence of a class label.\n\nParameters\n----------\nclasses : array-like of shape (n_classes,), default=None\n    Indicates an ordering for the class labels.\n    All entries should be unique (cannot contain duplicate classes).\n\nsparse_output : bool, default=False\n    Set to True if output binary array is desired in CSR sparse format.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    A copy of the `classes` parameter when provided.\n    Otherwise it corresponds to the sorted set of classes found\n    when fitting.\n\nSee Also\n--------\nOneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n    scheme.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MultiLabelBinarizer\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit_transform([(1, 2), (3,)])\narray([[1, 1, 0],\n       [0, 0, 1]])\n>>> mlb.classes_\narray([1, 2, 3])\n\n>>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\narray([[0, 1, 1],\n       [1, 0, 0]])\n>>> list(mlb.classes_)\n['comedy', 'sci-fi', 'thriller']\n\nA common mistake is to pass in a list, which leads to the following issue:\n\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit(['sci-fi', 'thriller', 'comedy'])\nMultiLabelBinarizer()\n>>> mlb.classes_\narray(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't',\n    'y'], dtype=object)\n\nTo correct this, the list of labels should be passed in as:\n\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit([['sci-fi', 'thriller', 'comedy']])\nMultiLabelBinarizer()\n>>> mlb.classes_\narray(['comedy', 'sci-fi', 'thriller'], dtype=object)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe optimization objective for MultiTaskElasticNet is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n    + alpha * l1_ratio * ||W||_21\n    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_elastic_net>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nl1_ratio : float or list of float, default=0.5\n    The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n    is an L2 penalty.\n    For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n    This parameter can be a list, in which case the different\n    values are tested by cross-validation and the one giving the best\n    prediction score is used. Note that a good choice of list of\n    values for l1_ratio is often to put more values close to 1\n    (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n    .9, .95, .99, 1]``.\n\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : array-like, default=None\n    List of alphas where to compute the models.\n    If not provided, set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nverbose : bool or int, default=0\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation. Note that this is\n    used only if multiple values for l1_ratio are given.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nintercept_ : ndarray of shape (n_targets,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_targets, n_features)\n    Parameter vector (W in the cost function formula).\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\nmse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n    Mean square error for the test set on each fold, varying alpha.\n\nalphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n    The grid of alphas used for fitting, for each l1_ratio.\n\nl1_ratio_ : float\n    Best l1_ratio obtained by cross-validation.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\ndual_gap_ : float\n    The dual gap at the end of the optimization for the optimal alpha.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\nElasticNetCV : Elastic net model with best model selection by\n    cross-validation.\nMultiTaskLassoCV : Multi-task Lasso model trained with L1 norm\n    as regularizer and built-in cross-validation.\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nIn `fit`, once the best parameters `l1_ratio` and `alpha` are found through\ncross-validation, the model is fit again using the entire training set.\n\nTo avoid unnecessary memory duplication the `X` and `y` arguments of the\n`fit` method should be directly passed as Fortran-contiguous numpy arrays.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n>>> clf.fit([[0,0], [1, 1], [2, 2]],\n...         [[0, 0], [1, 1], [2, 2]])\nMultiTaskElasticNetCV(cv=3)\n>>> print(clf.coef_)\n[[0.52875032 0.46958558]\n [0.52875032 0.46958558]]\n>>> print(clf.intercept_)\n[0.00166409 0.00166409]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\n\nThe optimization objective for MultiTaskElasticNet is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n    + alpha * l1_ratio * ||W||_21\n    + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n    ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n\ni.e. the sum of norms of each row.\n\nRead more in the :ref:`User Guide <multi_task_elastic_net>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the L1/L2 term. Defaults to 1.0.\n\nl1_ratio : float, default=0.5\n    The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n    For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n    is an L2 penalty.\n    For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nintercept_ : ndarray of shape (n_targets,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_targets, n_features)\n    Parameter vector (W in the cost function formula). If a 1D y is\n    passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\ndual_gap_ : float\n    The dual gaps at the end of the optimization.\n\neps_ : float\n    The tolerance scaled scaled by the variance of the target `y`.\n\nsparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n    Sparse representation of the `coef_`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n    cross-validation.\nElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\nMultiTaskLasso : Multi-task Lasso model trained with L1/L2\n    mixed-norm as regularizer.\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X and y arguments of the fit\nmethod should be directly passed as Fortran-contiguous numpy arrays.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\nMultiTaskElasticNet(alpha=0.1)\n>>> print(clf.coef_)\n[[0.45663524 0.45612256]\n [0.45663524 0.45612256]]\n>>> print(clf.intercept_)\n[0.0872422 0.0872422]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nThe optimization objective for MultiTaskLasso is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\neps : float, default=1e-3\n    Length of the path. ``eps=1e-3`` means that\n    ``alpha_min / alpha_max = 1e-3``.\n\nn_alphas : int, default=100\n    Number of alphas along the regularization path.\n\nalphas : array-like, default=None\n    List of alphas where to compute the models.\n    If not provided, set automatically.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - int, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : bool or int, default=False\n    Amount of verbosity.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation. Note that this is\n    used only if multiple values for l1_ratio are given.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\nintercept_ : ndarray of shape (n_targets,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_targets, n_features)\n    Parameter vector (W in the cost function formula).\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nalpha_ : float\n    The amount of penalization chosen by cross validation.\n\nmse_path_ : ndarray of shape (n_alphas, n_folds)\n    Mean square error for the test set on each fold, varying alpha.\n\nalphas_ : ndarray of shape (n_alphas,)\n    The grid of alphas used for fitting.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance for the optimal alpha.\n\ndual_gap_ : float\n    The dual gap at the end of the optimization for the optimal alpha.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\n    mixed-norm as regularizer.\nElasticNetCV : Elastic net model with best model selection by\n    cross-validation.\nMultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n    cross-validation.\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nIn `fit`, once the best parameter `alpha` is found through\ncross-validation, the model is fit again using the entire training set.\n\nTo avoid unnecessary memory duplication the `X` and `y` arguments of the\n`fit` method should be directly passed as Fortran-contiguous numpy arrays.\n\nExamples\n--------\n>>> from sklearn.linear_model import MultiTaskLassoCV\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.metrics import r2_score\n>>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n>>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n>>> r2_score(y, reg.predict(X))\n0.9994...\n>>> reg.alpha_\n0.5713...\n>>> reg.predict(X[:1,])\narray([[153.7971...,  94.9015...]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\nThe optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\nWhere::\n\n    ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Constant that multiplies the L1/L2 term. Defaults to 1.0.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If ``True``, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=1000\n    The maximum number of iterations.\n\ntol : float, default=1e-4\n    The tolerance for the optimization: if the updates are\n    smaller than ``tol``, the optimization code checks the\n    dual gap for optimality and continues until it is smaller\n    than ``tol``.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\nrandom_state : int, RandomState instance, default=None\n    The seed of the pseudo random number generator that selects a random\n    feature to update. Used when ``selection`` == 'random'.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nselection : {'cyclic', 'random'}, default='cyclic'\n    If set to 'random', a random coefficient is updated every iteration\n    rather than looping over features sequentially by default. This\n    (setting to 'random') often leads to significantly faster convergence\n    especially when tol is higher than 1e-4.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_targets, n_features)\n    Parameter vector (W in the cost function formula).\n    Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\nintercept_ : ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : int\n    Number of iterations run by the coordinate descent solver to reach\n    the specified tolerance.\n\ndual_gap_ : ndarray of shape (n_alphas,)\n    The dual gaps at the end of the optimization for each alpha.\n\neps_ : float\n    The tolerance scaled scaled by the variance of the target `y`.\n\nsparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n    Sparse representation of the `coef_`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nLasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).\nMultiTaskLassoCV: Multi-task L1 regularized linear model with built-in\n    cross-validation.\nMultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\nNotes\n-----\nThe algorithm used to fit the model is coordinate descent.\n\nTo avoid unnecessary memory duplication the X and y arguments of the fit\nmethod should be directly passed as Fortran-contiguous numpy arrays.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n>>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\nMultiTaskLasso(alpha=0.1)\n>>> print(clf.coef_)\n[[0.         0.60809415]\n[0.         0.94592424]]\n>>> print(clf.intercept_)\n[-0.41888636 -0.87382323]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelClassification",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute a confusion matrix for each class or sample.\n\n.. versionadded:: 0.21\n\nCompute class-wise (default) or sample-wise (samplewise=True) multilabel\nconfusion matrix to evaluate the accuracy of a classification, and output\nconfusion matrices for each class or sample.\n\nIn multilabel confusion matrix :math:`MCM`, the count of true negatives\nis :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\ntrue positives is :math:`MCM_{:,1,1}` and false positives is\n:math:`MCM_{:,0,1}`.\n\nMulticlass data will be treated as if binarized under a one-vs-rest\ntransformation. Returned confusion matrices will be in the order of\nsorted unique labels in the union of (y_true, y_pred).\n\nRead more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n\nParameters\n----------\ny_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n    Ground truth (correct) target values.\n\ny_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n    Estimated targets as returned by a classifier.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nlabels : array-like of shape (n_classes,), default=None\n    A list of classes or column indices to select some (or to force\n    inclusion of classes absent from the data).\n\nsamplewise : bool, default=False\n    In the multilabel case, this calculates a confusion matrix per sample.\n\nReturns\n-------\nmulti_confusion : ndarray of shape (n_outputs, 2, 2)\n    A 2x2 confusion matrix corresponding to each output in the input.\n    When calculating class-wise multi_confusion (default), then\n    n_outputs = n_labels; when calculating sample-wise multi_confusion\n    (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n    the results will be returned in the order specified in ``labels``,\n    otherwise the results will be returned in sorted order by default.\n\nSee Also\n--------\nconfusion_matrix : Compute confusion matrix to evaluate the accuracy of a\n    classifier.\n\nNotes\n-----\nThe `multilabel_confusion_matrix` calculates class-wise or sample-wise\nmultilabel confusion matrices, and in multiclass tasks, labels are\nbinarized under a one-vs-rest way; while\n:func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix\nfor confusion between every two classes.\n\nExamples\n--------\nMultilabel-indicator case:\n\n>>> import numpy as np\n>>> from sklearn.metrics import multilabel_confusion_matrix\n>>> y_true = np.array([[1, 0, 1],\n...                    [0, 1, 0]])\n>>> y_pred = np.array([[1, 0, 0],\n...                    [0, 1, 1]])\n>>> multilabel_confusion_matrix(y_true, y_pred)\narray([[[1, 0],\n        [0, 1]],\n<BLANKLINE>\n       [[1, 0],\n        [0, 1]],\n<BLANKLINE>\n       [[0, 1],\n        [1, 0]]])\n\nMulticlass case:\n\n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> multilabel_confusion_matrix(y_true, y_pred,\n...                             labels=[\"ant\", \"bird\", \"cat\"])\narray([[[3, 1],\n        [0, 2]],\n<BLANKLINE>\n       [[5, 0],\n        [1, 0]],\n<BLANKLINE>\n       [[2, 1],\n        [1, 2]]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Naive Bayes classifier for multinomial models.\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.\n\nParameters\n----------\nalpha : float or array-like of shape (n_features,), default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (set alpha=0 and force_alpha=True, for no smoothing).\n\nforce_alpha : bool, default=True\n    If False and alpha is less than 1e-10, it will set alpha to\n    1e-10. If True, alpha will remain unchanged. This may cause\n    numerical errors if alpha is too close to 0.\n\n    .. versionadded:: 1.2\n    .. versionchanged:: 1.4\n       The default value of `force_alpha` changed to `True`.\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified, the priors are not\n    adjusted according to the data.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes,)\n    Smoothed empirical log probability for each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical log probability of features\n    given a class, ``P(x_i|y)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nBernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\nCategoricalNB : Naive Bayes classifier for categorical features.\nComplementNB : Complement Naive Bayes classifier.\nGaussianNB : Gaussian Naive Bayes.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> clf = MultinomialNB()\n>>> clf.fit(X, y)\nMultinomialNB()\n>>> print(clf.predict(X[2:3]))\n[3]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Feature matrix.\n\ny : array-like of shape (n_samples,)\n    Target vector.\n\ndiscrete_features : 'auto', bool or array-like, default='auto'\n    If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`.\n\nn_neighbors : int, default=3\n    Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias.\n\ncopy : bool, default=True\n    Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nmi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target in\n    nat units.\n\nNotes\n-----\n1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\n\nExamples\n--------\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.feature_selection import mutual_info_classif\n>>> X, y = make_classification(\n...     n_samples=100, n_features=10, n_informative=2, n_clusters_per_class=1,\n...     shuffle=False, random_state=42\n... )\n>>> mutual_info_classif(X, y)\narray([0.58..., 0.10..., 0.19..., 0.09... , 0.        ,\n       0.     , 0.     , 0.     , 0.      , 0.        ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Estimate mutual information for a continuous target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Feature matrix.\n\ny : array-like of shape (n_samples,)\n    Target vector.\n\ndiscrete_features : {'auto', bool, array-like}, default='auto'\n    If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`.\n\nn_neighbors : int, default=3\n    Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias.\n\ncopy : bool, default=True\n    Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nmi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target in\n    nat units.\n\nNotes\n-----\n1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\n\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.feature_selection import mutual_info_regression\n>>> X, y = make_regression(\n...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42\n... )\n>>> mutual_info_regression(X, y)\narray([0.1..., 2.6...  , 0.0...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Mutual Information between two clusterings.\n\nThe Mutual Information is a measure of the similarity between two labels\nof the same data. Where :math:`|U_i|` is the number of the samples\nin cluster :math:`U_i` and :math:`|V_j|` is the number of the\nsamples in cluster :math:`V_j`, the Mutual Information\nbetween clusterings :math:`U` and :math:`V` is given as:\n\n.. math::\n\n    MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n    \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching :math:`U` (i.e\n``label_true``) with :math:`V` (i.e. ``label_pred``) will return the\nsame score value. This can be useful to measure the agreement of two\nindependent label assignments strategies on the same dataset when the\nreal ground truth is not known.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=integral\n    A clustering of the data into disjoint subsets, called :math:`U` in\n    the above formula.\n\nlabels_pred : array-like of shape (n_samples,), dtype=integral\n    A clustering of the data into disjoint subsets, called :math:`V` in\n    the above formula.\n\ncontingency : {array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n    A contingency matrix given by the\n    :func:`~sklearn.metrics.cluster.contingency_matrix` function. If value\n    is ``None``, it will be computed, otherwise the given value is used,\n    with ``labels_true`` and ``labels_pred`` ignored.\n\nReturns\n-------\nmi : float\n   Mutual information, a non-negative value, measured in nats using the\n   natural logarithm.\n\nSee Also\n--------\nadjusted_mutual_info_score : Adjusted against chance Mutual Information.\nnormalized_mutual_info_score : Normalized Mutual Information.\n\nNotes\n-----\nThe logarithm used is the natural logarithm (base-e).\n\nExamples\n--------\n>>> from sklearn.metrics import mutual_info_score\n>>> labels_true = [0, 1, 1, 0, 1, 0]\n>>> labels_pred = [0, 1, 0, 0, 1, 1]\n>>> mutual_info_score(labels_true, labels_pred)\n0.056..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\nwhose product approximates the non-negative matrix X. This factorization can be used\nfor example for dimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n        &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n        &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n        &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n        &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n`H` to keep their impact balanced with respect to one another and to the data fit\nterm as independent as possible of the size `n_samples` of the training set.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nNote that the transformed data is named W and the components matrix is named H. In\nthe NMF literature, the naming convention is usually the opposite since the data\nmatrix X is transposed.\n\nRead more in the :ref:`User Guide <NMF>`.\n\nParameters\n----------\nn_components : int or {'auto'} or None, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n    If `n_components='auto'`, the number of components is automatically inferred\n    from W or H shapes.\n\n    .. versionchanged:: 1.4\n        Added `'auto'` value.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n    Valid options:\n\n    - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n      otherwise random.\n\n    - `'random'`: non-negative random matrices, scaled with:\n      `sqrt(X.mean() / n_components)`\n\n    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n      initialization (better for sparseness)\n\n    - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n      (better when sparsity is not desired)\n\n    - `'nndsvdar'` NNDSVD with zeros filled with small random values\n      (generally faster, less accurate alternative to NNDSVDa\n      for when sparsity is not desired)\n\n    - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    .. versionchanged:: 1.1\n        When `init=None` and n_components is less than n_samples and n_features\n        defaults to `nndsvda` instead of `nndsvd`.\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n\n    - 'cd' is a Coordinate Descent solver.\n    - 'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nalpha_W : float, default=0.0\n    Constant that multiplies the regularization terms of `W`. Set it to zero\n    (default) to have no regularization on `W`.\n\n    .. versionadded:: 1.0\n\nalpha_H : float or \"same\", default=\"same\"\n    Constant that multiplies the regularization terms of `H`. Set it to zero to\n    have no regularization on `H`. If \"same\" (default), it takes the same value as\n    `alpha_W`.\n\n    .. versionadded:: 1.0\n\nl1_ratio : float, default=0.0\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    .. versionadded:: 0.17\n       Regularization parameter *l1_ratio* used in the Coordinate Descent\n       solver.\n\nverbose : int, default=0\n    Whether to be verbose.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\n    .. versionadded:: 0.17\n       *shuffle* parameter used in the Coordinate Descent solver.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Factorization matrix, sometimes called 'dictionary'.\n\nn_components_ : int\n    The number of components. It is same as the `n_components` parameter\n    if it was given. Otherwise, it will be same as the number of\n    features.\n\nreconstruction_err_ : float\n    Frobenius norm of the matrix difference, or beta-divergence, between\n    the training data ``X`` and the reconstructed data ``WH`` from\n    the fitted model.\n\nn_iter_ : int\n    Actual number of iterations.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nPCA : Principal component analysis.\nSparseCoder : Find a sparse representation of data from a fixed,\n    precomputed dictionary.\nSparsePCA : Sparse Principal Components Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\n\nReferences\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n   factorizations\" <10.1587/transfun.E92.A.708>`\n   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n   of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n.. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n   beta-divergence\" <10.1162/NECO_a_00168>`\n   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NaiveBayesModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Calculate the euclidean distances in the presence of missing values.\n\nCompute the euclidean distance between each pair of samples in X and Y,\nwhere Y=X is assumed if Y=None. When calculating the distance between a\npair of samples, this formulation ignores feature coordinates with a\nmissing value in either sample and scales up the weight of the remaining\ncoordinates:\n\n    dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n    where,\n    weight = Total # of coordinates / # of present coordinates\n\nFor example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\nis:\n\n    .. math::\n        \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n\nIf all the coordinates are missing or if there are no common present\ncoordinates then NaN is returned for that pair.\n\nRead more in the :ref:`User Guide <metrics>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nX : array-like of shape (n_samples_X, n_features)\n    An array where each row is a sample and each column is a feature.\n\nY : array-like of shape (n_samples_Y, n_features), default=None\n    An array where each row is a sample and each column is a feature.\n    If `None`, method uses `Y=X`.\n\nsquared : bool, default=False\n    Return squared Euclidean distances.\n\nmissing_values : np.nan, float or int, default=np.nan\n    Representation of missing value.\n\ncopy : bool, default=True\n    Make and use a deep copy of X and Y (if Y exists).\n\nReturns\n-------\ndistances : ndarray of shape (n_samples_X, n_samples_Y)\n    Returns the distances between the row vectors of `X`\n    and the row vectors of `Y`.\n\nSee Also\n--------\npaired_distances : Distances between pairs of elements of X and Y.\n\nReferences\n----------\n* John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n  IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n  10, pp. 617 - 621, Oct. 1979.\n  http://ieeexplore.ieee.org/abstract/document/4310090/\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import nan_euclidean_distances\n>>> nan = float(\"NaN\")\n>>> X = [[0, 1], [1, nan]]\n>>> nan_euclidean_distances(X, X) # distance between rows of X\narray([[0.        , 1.41421356],\n       [1.41421356, 0.        ]])\n\n>>> # get distance to origin\n>>> nan_euclidean_distances(X, [[0, 0]])\narray([[1.        ],\n       [1.41421356]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Normalized Discounted Cumulative Gain.\n\nSum the true scores ranked in the order induced by the predicted scores,\nafter applying a logarithmic discount. Then divide by the best possible\nscore (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n0 and 1.\n\nThis ranking metric returns a high value if true labels are ranked high by\n``y_score``.\n\nParameters\n----------\ny_true : array-like of shape (n_samples, n_labels)\n    True targets of multilabel classification, or true scores of entities\n    to be ranked. Negative values in `y_true` may result in an output\n    that is not between 0 and 1.\n\ny_score : array-like of shape (n_samples, n_labels)\n    Target scores, can either be probability estimates, confidence values,\n    or non-thresholded measure of decisions (as returned by\n    \"decision_function\" on some classifiers).\n\nk : int, default=None\n    Only consider the highest k scores in the ranking. If `None`, use all\n    outputs.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If `None`, all samples are given the same weight.\n\nignore_ties : bool, default=False\n    Assume that there are no ties in y_score (which is likely to be the\n    case if y_score is continuous) for efficiency gains.\n\nReturns\n-------\nnormalized_discounted_cumulative_gain : float in [0., 1.]\n    The averaged NDCG scores for all samples.\n\nSee Also\n--------\ndcg_score : Discounted Cumulative Gain (not normalized).\n\nReferences\n----------\n`Wikipedia entry for Discounted Cumulative Gain\n<https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\nJarvelin, K., & Kekalainen, J. (2002).\nCumulated gain-based evaluation of IR techniques. ACM Transactions on\nInformation Systems (TOIS), 20(4), 422-446.\n\nWang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\nA theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\nAnnual Conference on Learning Theory (COLT 2013)\n\nMcSherry, F., & Najork, M. (2008, March). Computing information retrieval\nperformance measures efficiently in the presence of tied scores. In\nEuropean conference on information retrieval (pp. 414-421). Springer,\nBerlin, Heidelberg.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import ndcg_score\n>>> # we have groud-truth relevance of some answers to a query:\n>>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n>>> # we predict some scores (relevance) for the answers\n>>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n>>> ndcg_score(true_relevance, scores)\n0.69...\n>>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n>>> ndcg_score(true_relevance, scores)\n0.49...\n>>> # we can set k to truncate the sum; only top k answers contribute.\n>>> ndcg_score(true_relevance, scores, k=4)\n0.35...\n>>> # the normalization takes k into account so a perfect answer\n>>> # would still get 1.0\n>>> ndcg_score(true_relevance, true_relevance, k=4)\n1.0...\n>>> # now we have some ties in our prediction\n>>> scores = np.asarray([[1, 0, 0, 0, 1]])\n>>> # by default ties are averaged, so here we get the average (normalized)\n>>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n>>> ndcg_score(true_relevance, scores, k=1)\n0.75...\n>>> # we can choose to ignore ties for faster results, but only\n>>> # if we know there aren't ties in our scores, otherwise we get\n>>> # wrong results:\n>>> ndcg_score(true_relevance,\n...           scores, k=1, ignore_ties=True)\n0.5..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Nearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to\nthe class with the nearest centroid.\n\nRead more in the :ref:`User Guide <nearest_centroid_classifier>`.\n\nParameters\n----------\nmetric : str or callable, default=\"euclidean\"\n    Metric to use for distance computation. See the documentation of\n    `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values. Note that \"wminkowski\", \"seuclidean\" and \"mahalanobis\" are not\n    supported.\n\n    The centroids for the samples corresponding to each class is\n    the point from which the sum of the distances (according to the metric)\n    of all samples that belong to that particular class are minimized.\n    If the `\"manhattan\"` metric is provided, this centroid is the median\n    and for all other metrics, the centroid is now set to be the mean.\n\n    .. deprecated:: 1.3\n        Support for metrics other than `euclidean` and `manhattan` and for\n        callables was deprecated in version 1.3 and will be removed in\n        version 1.5.\n\n    .. versionchanged:: 0.19\n        `metric='precomputed'` was deprecated and now raises an error\n\nshrink_threshold : float, default=None\n    Threshold for shrinking centroids to remove features.\n\nAttributes\n----------\ncentroids_ : array-like of shape (n_classes, n_features)\n    Centroid of each class.\n\nclasses_ : array of shape (n_classes,)\n    The unique classes labels.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKNeighborsClassifier : Nearest neighbors classifier.\n\nNotes\n-----\nWhen used for text classification with tf-idf vectors, this classifier is\nalso known as the Rocchio classifier.\n\nReferences\n----------\nTibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\nmultiple cancer types by shrunken centroids of gene expression. Proceedings\nof the National Academy of Sciences of the United States of America,\n99(10), 6567-6572. The National Academy of Sciences.\n\nExamples\n--------\n>>> from sklearn.neighbors import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)\nNearestCentroid()\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Unsupervised learner for implementing neighbor searches.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\nn_neighbors : int, default=5\n    Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nradius : float, default=1.0\n    Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\np : float (positive), default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\neffective_metric_ : str\n    Metric used to compute distances to neighbors.\n\neffective_metric_params_ : dict\n    Parameters for the metric used to compute distances to neighbors.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nSee Also\n--------\nKNeighborsClassifier : Classifier implementing the k-nearest neighbors\n    vote.\nRadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n    within a given radius.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nRadiusNeighborsRegressor : Regression based on neighbors within a fixed\n    radius.\nBallTree : Space partitioning data structure for organizing points in a\n    multi-dimensional space, used for nearest neighbor search.\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.neighbors import NearestNeighbors\n>>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n>>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n>>> neigh.fit(samples)\nNearestNeighbors(...)\n>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\narray([[2, 0]]...)\n>>> nbrs = neigh.radius_neighbors(\n...    [[0, 0, 1.3]], 0.4, return_distance=False\n... )\n>>> np.asarray(nbrs[0][0])\narray(2)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Neighborhood Components Analysis.\n\nNeighborhood Component Analysis (NCA) is a machine learning algorithm for\nmetric learning. It learns a linear transformation in a supervised fashion\nto improve the classification accuracy of a stochastic nearest neighbors\nrule in the transformed space.\n\nRead more in the :ref:`User Guide <nca>`.\n\nParameters\n----------\nn_components : int, default=None\n    Preferred dimensionality of the projected space.\n    If None it will be set to `n_features`.\n\ninit : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'\n    Initialization of the linear transformation. Possible options are\n    `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy\n    array of shape `(n_features_a, n_features_b)`.\n\n    - `'auto'`\n        Depending on `n_components`, the most reasonable initialization\n        will be chosen. If `n_components <= n_classes` we use `'lda'`, as\n        it uses labels information. If not, but\n        `n_components < min(n_features, n_samples)`, we use `'pca'`, as\n        it projects data in meaningful directions (those of higher\n        variance). Otherwise, we just use `'identity'`.\n\n    - `'pca'`\n        `n_components` principal components of the inputs passed\n        to :meth:`fit` will be used to initialize the transformation.\n        (See :class:`~sklearn.decomposition.PCA`)\n\n    - `'lda'`\n        `min(n_components, n_classes)` most discriminative\n        components of the inputs passed to :meth:`fit` will be used to\n        initialize the transformation. (If `n_components > n_classes`,\n        the rest of the components will be zero.) (See\n        :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n\n    - `'identity'`\n        If `n_components` is strictly smaller than the\n        dimensionality of the inputs passed to :meth:`fit`, the identity\n        matrix will be truncated to the first `n_components` rows.\n\n    - `'random'`\n        The initial transformation will be a random array of shape\n        `(n_components, n_features)`. Each value is sampled from the\n        standard normal distribution.\n\n    - numpy array\n        `n_features_b` must match the dimensionality of the inputs passed\n        to :meth:`fit` and n_features_a must be less than or equal to that.\n        If `n_components` is not `None`, `n_features_a` must match it.\n\nwarm_start : bool, default=False\n    If `True` and :meth:`fit` has been called before, the solution of the\n    previous call to :meth:`fit` is used as the initial linear\n    transformation (`n_components` and `init` will be ignored).\n\nmax_iter : int, default=50\n    Maximum number of iterations in the optimization.\n\ntol : float, default=1e-5\n    Convergence tolerance for the optimization.\n\ncallback : callable, default=None\n    If not `None`, this function is called after every iteration of the\n    optimizer, taking as arguments the current solution (flattened\n    transformation matrix) and the number of iterations. This might be\n    useful in case one wants to examine or store the transformation\n    found after each iteration.\n\nverbose : int, default=0\n    If 0, no progress messages will be printed.\n    If 1, progress messages will be printed to stdout.\n    If > 1, progress messages will be printed and the `disp`\n    parameter of :func:`scipy.optimize.minimize` will be set to\n    `verbose - 2`.\n\nrandom_state : int or numpy.RandomState, default=None\n    A pseudo random number generator object or a seed for it if int. If\n    `init='random'`, `random_state` is used to initialize the random\n    transformation. If `init='pca'`, `random_state` is passed as an\n    argument to PCA when initializing the transformation. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The linear transformation learned during fitting.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nn_iter_ : int\n    Counts the number of iterations performed by the optimizer.\n\nrandom_state_ : numpy.RandomState\n    Pseudo random number generator object used during initialization.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis : Linear\n    Discriminant Analysis.\nsklearn.decomposition.PCA : Principal component analysis (PCA).\n\nReferences\n----------\n.. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n       \"Neighbourhood Components Analysis\". Advances in Neural Information\n       Processing Systems. 17, 513-520, 2005.\n       http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n.. [2] Wikipedia entry on Neighborhood Components Analysis\n       https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\n\nExamples\n--------\n>>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> nca.fit(X_train, y_train)\nNeighborhoodComponentsAnalysis(...)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> knn.fit(X_train, y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(X_test, y_test))\n0.933333...\n>>> knn.fit(nca.transform(X_train), y_train)\nKNeighborsClassifier(...)\n>>> print(knn.score(nca.transform(X_test), y_test))\n0.961904..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeuralNetworkModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Normalized Mutual Information between two clusterings.\n\nNormalized Mutual Information (NMI) is a normalization of the Mutual\nInformation (MI) score to scale the results between 0 (no mutual\ninformation) and 1 (perfect correlation). In this function, mutual\ninformation is normalized by some generalized mean of ``H(labels_true)``\nand ``H(labels_pred))``, defined by the `average_method`.\n\nThis measure is not adjusted for chance. Therefore\n:func:`adjusted_mutual_info_score` might be preferred.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n\nParameters\n----------\nlabels_true : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets.\n\nlabels_pred : int array-like of shape (n_samples,)\n    A clustering of the data into disjoint subsets.\n\naverage_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'\n    How to compute the normalizer in the denominator.\n\n    .. versionadded:: 0.20\n\n    .. versionchanged:: 0.22\n       The default value of ``average_method`` changed from 'geometric' to\n       'arithmetic'.\n\nReturns\n-------\nnmi : float\n   Score between 0.0 and 1.0 in normalized nats (based on the natural\n   logarithm). 1.0 stands for perfectly complete labeling.\n\nSee Also\n--------\nv_measure_score : V-Measure (NMI with arithmetic mean option).\nadjusted_rand_score : Adjusted Rand Index.\nadjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n    against chance).\n\nExamples\n--------\n\nPerfect labelings are both homogeneous and complete, hence have\nscore 1.0::\n\n  >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n  >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n  ... # doctest: +SKIP\n  1.0\n  >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n  ... # doctest: +SKIP\n  1.0\n\nIf classes members are completely split across different clusters,\nthe assignment is totally in-complete, hence the NMI is null::\n\n  >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n  ... # doctest: +SKIP\n  0.0"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Normalize samples individually to unit norm.\n\nEach sample (i.e. each row of the data matrix) with at least one\nnon zero component is rescaled independently of other samples so\nthat its norm (l1, l2 or inf) equals one.\n\nThis transformer is able to work both with dense numpy arrays and\nscipy.sparse matrix (use CSR format if you want to avoid the burden of\na copy / conversion).\n\nScaling inputs to unit norms is a common operation for text\nclassification or clustering for instance. For instance the dot\nproduct of two l2-normalized TF-IDF vectors is the cosine similarity\nof the vectors and is the base similarity metric for the Vector\nSpace Model commonly used by the Information Retrieval community.\n\nFor an example visualization, refer to :ref:`Compare Normalizer with other\nscalers <plot_all_scaling_normalizer_section>`.\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n\nParameters\n----------\nnorm : {'l1', 'l2', 'max'}, default='l2'\n    The norm to use to normalize each non zero sample. If norm='max'\n    is used, values will be rescaled by the maximum of the absolute\n    values.\n\ncopy : bool, default=True\n    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array or a scipy.sparse\n    CSR matrix).\n\nAttributes\n----------\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nnormalize : Equivalent function without the estimator API.\n\nNotes\n-----\nThis estimator is :term:`stateless` and does not need to be fitted.\nHowever, we recommend to call :meth:`fit_transform` instead of\n:meth:`transform`, as parameter validation is only performed in\n:meth:`fit`.\n\nExamples\n--------\n>>> from sklearn.preprocessing import Normalizer\n>>> X = [[4, 1, 2, 2],\n...      [1, 3, 9, 3],\n...      [5, 7, 5, 1]]\n>>> transformer = Normalizer().fit(X)  # fit does nothing.\n>>> transformer\nNormalizer()\n>>> transformer.transform(X)\narray([[0.8, 0.2, 0.4, 0.4],\n       [0.1, 0.3, 0.9, 0.3],\n       [0.5, 0.7, 0.5, 0.1]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Nu-Support Vector Classification.\n\nSimilar to SVC but uses a parameter to control the number of support\nvectors.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n\nParameters\n----------\nnu : float, default=0.5\n    An upper bound on the fraction of margin errors (see :ref:`User Guide\n    <nu_svc>`) and a lower bound of the fraction of support vectors.\n    Should be in the interval (0, 1].\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n    Specifies the kernel type to be used in the algorithm.\n    If none is given, 'rbf' will be used. If a callable is given it is\n    used to precompute the kernel matrix. For an intuitive\n    visualization of different kernel types see\n    :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Must be non-negative. Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features\n    - if float, must be non-negative.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\nprobability : bool, default=False\n    Whether to enable probability estimates. This must be enabled prior\n    to calling `fit`, will slow down that method as it internally uses\n    5-fold cross-validation, and `predict_proba` may be inconsistent with\n    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nclass_weight : {dict, 'balanced'}, default=None\n    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one. The \"balanced\" mode uses the values of y to automatically\n    adjust weights inversely proportional to class frequencies as\n    ``n_samples / (n_classes * np.bincount(y))``.\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\ndecision_function_shape : {'ovo', 'ovr'}, default='ovr'\n    Whether to return a one-vs-rest ('ovr') decision function of shape\n    (n_samples, n_classes) as all other classifiers, or the original\n    one-vs-one ('ovo') decision function of libsvm which has shape\n    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n    ('ovo') is always used as multi-class strategy. The parameter is\n    ignored for binary classification.\n\n    .. versionchanged:: 0.19\n        decision_function_shape is 'ovr' by default.\n\n    .. versionadded:: 0.17\n       *decision_function_shape='ovr'* is recommended.\n\n    .. versionchanged:: 0.17\n       Deprecated *decision_function_shape='ovo' and None*.\n\nbreak_ties : bool, default=False\n    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n    :term:`predict` will break ties according to the confidence values of\n    :term:`decision_function`; otherwise the first class among the tied\n    classes is returned. Please note that breaking ties comes at a\n    relatively high computational cost compared to a simple predict.\n\n    .. versionadded:: 0.22\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data for\n    probability estimates. Ignored when `probability` is False.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C of each class.\n    Computed based on the ``class_weight`` parameter.\n\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\ncoef_ : ndarray of shape (n_classes * (n_classes -1) / 2, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (n_classes - 1, n_SV)\n    Dual coefficients of the support vector in the decision\n    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n    their targets.\n    For multiclass, coefficient for all 1-vs-1 classifiers.\n    The layout of the coefficients in the multiclass case is somewhat\n    non-trivial. See the :ref:`multi-class section of the User Guide\n    <svm_multi_class>` for details.\n\nfit_status_ : int\n    0 if correctly fitted, 1 if the algorithm did not converge.\n\nintercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n    Number of iterations run by the optimization routine to fit the model.\n    The shape of this attribute depends on the number of models optimized\n    which in turn depends on the number of classes.\n\n    .. versionadded:: 1.1\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\nfit_status_ : int\n    0 if correctly fitted, 1 if the algorithm did not converge.\n\nprobA_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n\nprobB_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    If `probability=True`, it corresponds to the parameters learned in\n    Platt scaling to produce probability estimates from decision values.\n    If `probability=False`, it's an empty array. Platt scaling uses the\n    logistic function\n    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n    more information on the multiclass case and training procedure see\n    section 8 of [1]_.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nSee Also\n--------\nSVC : Support Vector Machine for classification using libsvm.\n\nLinearSVC : Scalable linear Support Vector Machine for classification using\n    liblinear.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n    Machines and Comparisons to Regularized Likelihood Methods\"\n    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> y = np.array([1, 1, 2, 2])\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.svm import NuSVC\n>>> clf = make_pipeline(StandardScaler(), NuSVC())\n>>> clf.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Nu Support Vector Regression.\n\nSimilar to NuSVC, for regression, uses a parameter nu to control\nthe number of support vectors. However, unlike NuSVC, where nu\nreplaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n\nParameters\n----------\nnu : float, default=0.5\n    An upper bound on the fraction of training errors and a lower bound of\n    the fraction of support vectors. Should be in the interval (0, 1].  By\n    default 0.5 will be taken.\n\nC : float, default=1.0\n    Penalty parameter C of the error term.\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Must be non-negative. Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features\n    - if float, must be non-negative.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (1, n_SV)\n    Coefficients of the support vector in the decision function.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (1,)\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of iterations run by the optimization routine to fit the model.\n\n    .. versionadded:: 1.1\n\nn_support_ : ndarray of shape (1,), dtype=int32\n    Number of support vectors.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nSee Also\n--------\nNuSVC : Support Vector Machine for classification implemented with libsvm\n    with a parameter to control the number of support vectors.\n\nSVR : Epsilon Support Vector Machine for regression implemented with\n    libsvm.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n    Machines and Comparisons to Regularized Likelihood Methods\"\n    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\nExamples\n--------\n>>> from sklearn.svm import NuSVR\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> np.random.seed(0)\n>>> y = np.random.randn(n_samples)\n>>> X = np.random.randn(n_samples, n_features)\n>>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))\n>>> regr.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('nusvr', NuSVR(nu=0.1))])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nmin_samples : int > 1 or float between 0 and 1, default=5\n    The number of samples in a neighborhood for a point to be considered as\n    a core point. Also, up and down steep regions can't have more than\n    ``min_samples`` consecutive non-steep points. Expressed as an absolute\n    number or a fraction of the number of samples (rounded to be at least\n    2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", `X` is assumed to be a distance matrix and must be\n    square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    Sparse matrices are only supported by scikit-learn metrics.\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\n    .. note::\n       `'kulsinski'` is deprecated from SciPy 1.9 and will removed in SciPy 1.11.\n\np : float, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncluster_method : str, default='xi'\n    The extraction method used to extract clusters using the calculated\n    reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\neps : float, default=None\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. By default it assumes the same value\n    as ``max_eps``.\n    Used only when ``cluster_method='dbscan'``.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n    Used only when ``cluster_method='xi'``.\n\npredecessor_correction : bool, default=True\n    Correct clusters according to the predecessors calculated by OPTICS\n    [2]_. This parameter has minimal effect on most datasets.\n    Used only when ``cluster_method='xi'``.\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n    Used only when ``cluster_method='xi'``.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`.\n    - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`.\n    - 'brute' will use a brute-force search.\n    - 'auto' (default) will attempt to decide the most appropriate\n      algorithm based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`~sklearn.neighbors.BallTree` or\n    :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the\n    construction and query, as well as the memory required to store the\n    tree. The optimal value depends on the nature of the problem.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nlabels_ : ndarray of shape (n_samples,)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples and points which are not included in a leaf cluster\n    of ``cluster_hierarchy_`` are labeled as -1.\n\nreachability_ : ndarray of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\nordering_ : ndarray of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : ndarray of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : ndarray of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\ncluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to\n    ``(end, -start)`` (ascending) so that larger clusters encompassing\n    smaller clusters come after those smaller ones. Since ``labels_`` does\n    not reflect the hierarchy, usually\n    ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n    note that these indices are of the ``ordering_``, i.e.\n    ``X[ordering_][start:end + 1]`` form a cluster.\n    Only available when ``cluster_method='xi'``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDBSCAN : A similar clustering for a specified neighborhood radius (eps).\n    Our implementation is optimized for runtime.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J√∂rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n.. [2] Schubert, Erich, Michael Gertz.\n   \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n   the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\nExamples\n--------\n>>> from sklearn.cluster import OPTICS\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> clustering = OPTICS(min_samples=2).fit(X)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])\n\nFor a more detailed example see\n:ref:`sphx_glr_auto_examples_cluster_plot_optics.py`."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Unsupervised Outlier Detection.\n\nEstimate the support of a high-dimensional distribution.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <outlier_detection>`.\n\nParameters\n----------\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Must be non-negative. Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features\n    - if float, must be non-negative.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\nnu : float, default=0.5\n    An upper bound on the fraction of training\n    errors and a lower bound of the fraction of support\n    vectors. Should be in the interval (0, 1]. By default 0.5\n    will be taken.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (1, n_SV)\n    Coefficients of the support vectors in the decision function.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (1,)\n    Constant in the decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of iterations run by the optimization routine to fit the model.\n\n    .. versionadded:: 1.1\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores.\n    We have the relation: decision_function = score_samples - `offset_`.\n    The offset is the opposite of `intercept_` and is provided for\n    consistency with other outlier detection algorithms.\n\n    .. versionadded:: 0.20\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nSee Also\n--------\nsklearn.linear_model.SGDOneClassSVM : Solves linear One-Class SVM using\n    Stochastic Gradient Descent.\nsklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection using\n    Local Outlier Factor (LOF).\nsklearn.ensemble.IsolationForest : Isolation Forest Algorithm.\n\nExamples\n--------\n>>> from sklearn.svm import OneClassSVM\n>>> X = [[0], [0.44], [0.45], [0.46], [1]]\n>>> clf = OneClassSVM(gamma='auto').fit(X)\n>>> clf.predict(X)\narray([-1,  1,  1,  1, -1])\n>>> clf.score_samples(X)\narray([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Encode categorical features as a one-hot numeric array.\n\nThe input to this transformer should be an array-like of integers or\nstrings, denoting the values taken on by categorical (discrete) features.\nThe features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\nencoding scheme. This creates a binary column for each category and\nreturns a sparse matrix or dense array (depending on the ``sparse_output``\nparameter).\n\nBy default, the encoder derives the categories based on the unique values\nin each feature. Alternatively, you can also specify the `categories`\nmanually.\n\nThis encoding is needed for feeding categorical data to many scikit-learn\nestimators, notably linear models and SVMs with the standard kernels.\n\nNote: a one-hot encoding of y labels should use a LabelBinarizer\ninstead.\n\nRead more in the :ref:`User Guide <preprocessing_categorical_features>`.\nFor a comparison of different encoders, refer to:\n:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.\n\nParameters\n----------\ncategories : 'auto' or a list of array-like, default='auto'\n    Categories (unique values) per feature:\n\n    - 'auto' : Determine categories automatically from the training data.\n    - list : ``categories[i]`` holds the categories expected in the ith\n      column. The passed categories should not mix strings and numeric\n      values within a single feature, and should be sorted in case of\n      numeric values.\n\n    The used categories can be found in the ``categories_`` attribute.\n\n    .. versionadded:: 0.20\n\ndrop : {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None\n    Specifies a methodology to use to drop one of the categories per\n    feature. This is useful in situations where perfectly collinear\n    features cause problems, such as when feeding the resulting data\n    into an unregularized linear regression model.\n\n    However, dropping one category breaks the symmetry of the original\n    representation and can therefore induce a bias in downstream models,\n    for instance for penalized linear classification or regression models.\n\n    - None : retain all features (the default).\n    - 'first' : drop the first category in each feature. If only one\n      category is present, the feature will be dropped entirely.\n    - 'if_binary' : drop the first category in each feature with two\n      categories. Features with 1 or more than 2 categories are\n      left intact.\n    - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n      should be dropped.\n\n    When `max_categories` or `min_frequency` is configured to group\n    infrequent categories, the dropping behavior is handled after the\n    grouping.\n\n    .. versionadded:: 0.21\n       The parameter `drop` was added in 0.21.\n\n    .. versionchanged:: 0.23\n       The option `drop='if_binary'` was added in 0.23.\n\n    .. versionchanged:: 1.1\n        Support for dropping infrequent categories.\n\nsparse_output : bool, default=True\n    When ``True``, it returns a :class:`scipy.sparse.csr_matrix`,\n    i.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.\n\n    .. versionadded:: 1.2\n       `sparse` was renamed to `sparse_output`\n\ndtype : number type, default=np.float64\n    Desired dtype of output.\n\nhandle_unknown : {'error', 'ignore', 'infrequent_if_exist'},                      default='error'\n    Specifies the way unknown categories are handled during :meth:`transform`.\n\n    - 'error' : Raise an error if an unknown category is present during transform.\n    - 'ignore' : When an unknown category is encountered during\n      transform, the resulting one-hot encoded columns for this feature\n      will be all zeros. In the inverse transform, an unknown category\n      will be denoted as None.\n    - 'infrequent_if_exist' : When an unknown category is encountered\n      during transform, the resulting one-hot encoded columns for this\n      feature will map to the infrequent category if it exists. The\n      infrequent category will be mapped to the last position in the\n      encoding. During inverse transform, an unknown category will be\n      mapped to the category denoted `'infrequent'` if it exists. If the\n      `'infrequent'` category does not exist, then :meth:`transform` and\n      :meth:`inverse_transform` will handle an unknown category as with\n      `handle_unknown='ignore'`. Infrequent categories exist based on\n      `min_frequency` and `max_categories`. Read more in the\n      :ref:`User Guide <encoder_infrequent_categories>`.\n\n    .. versionchanged:: 1.1\n        `'infrequent_if_exist'` was added to automatically handle unknown\n        categories and infrequent categories.\n\nmin_frequency : int or float, default=None\n    Specifies the minimum frequency below which a category will be\n    considered infrequent.\n\n    - If `int`, categories with a smaller cardinality will be considered\n      infrequent.\n\n    - If `float`, categories with a smaller cardinality than\n      `min_frequency * n_samples`  will be considered infrequent.\n\n    .. versionadded:: 1.1\n        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n\nmax_categories : int, default=None\n    Specifies an upper limit to the number of output features for each input\n    feature when considering infrequent categories. If there are infrequent\n    categories, `max_categories` includes the category representing the\n    infrequent categories along with the frequent categories. If `None`,\n    there is no limit to the number of output features.\n\n    .. versionadded:: 1.1\n        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n\nfeature_name_combiner : \"concat\" or callable, default=\"concat\"\n    Callable with signature `def callable(input_feature, category)` that returns a\n    string. This is used to create feature names to be returned by\n    :meth:`get_feature_names_out`.\n\n    `\"concat\"` concatenates encoded feature name and category with\n    `feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create\n    feature names `X_1, X_6, X_7`.\n\n    .. versionadded:: 1.3\n\nAttributes\n----------\ncategories_ : list of arrays\n    The categories of each feature determined during fitting\n    (in order of the features in X and corresponding with the output\n    of ``transform``). This includes the category specified in ``drop``\n    (if any).\n\ndrop_idx_ : array of shape (n_features,)\n    - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n      to be dropped for each feature.\n    - ``drop_idx_[i] = None`` if no category is to be dropped from the\n      feature with index ``i``, e.g. when `drop='if_binary'` and the\n      feature isn't binary.\n    - ``drop_idx_ = None`` if all the transformed features will be\n      retained.\n\n    If infrequent categories are enabled by setting `min_frequency` or\n    `max_categories` to a non-default value and `drop_idx[i]` corresponds\n    to a infrequent category, then the entire infrequent category is\n    dropped.\n\n    .. versionchanged:: 0.23\n       Added the possibility to contain `None` values.\n\ninfrequent_categories_ : list of ndarray\n    Defined only if infrequent categories are enabled by setting\n    `min_frequency` or `max_categories` to a non-default value.\n    `infrequent_categories_[i]` are the infrequent categories for feature\n    `i`. If the feature `i` has no infrequent categories\n    `infrequent_categories_[i]` is None.\n\n    .. versionadded:: 1.1\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 1.0\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nfeature_name_combiner : callable or None\n    Callable with signature `def callable(input_feature, category)` that returns a\n    string. This is used to create feature names to be returned by\n    :meth:`get_feature_names_out`.\n\n    .. versionadded:: 1.3\n\nSee Also\n--------\nOrdinalEncoder : Performs an ordinal (integer)\n  encoding of the categorical features.\nTargetEncoder : Encodes categorical features using the target.\nsklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n  dictionary items (also handles string-valued features).\nsklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n  encoding of dictionary items or strings.\nLabelBinarizer : Binarizes labels in a one-vs-all\n  fashion.\nMultiLabelBinarizer : Transforms between iterable of\n  iterables and a multilabel format, e.g. a (samples x classes) binary\n  matrix indicating the presence of a class label.\n\nExamples\n--------\nGiven a dataset with two features, we let the encoder find the unique\nvalues per feature and transform the data to a binary one-hot encoding.\n\n>>> from sklearn.preprocessing import OneHotEncoder\n\nOne can discard categories not seen during `fit`:\n\n>>> enc = OneHotEncoder(handle_unknown='ignore')\n>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n>>> enc.fit(X)\nOneHotEncoder(handle_unknown='ignore')\n>>> enc.categories_\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n>>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\narray([[1., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n>>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\narray([['Male', 1],\n       [None, 2]], dtype=object)\n>>> enc.get_feature_names_out(['gender', 'group'])\narray(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)\n\nOne can always drop the first column for each feature:\n\n>>> drop_enc = OneHotEncoder(drop='first').fit(X)\n>>> drop_enc.categories_\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n>>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\narray([[0., 0., 0.],\n       [1., 1., 0.]])\n\nOr drop a column for feature only having 2 categories:\n\n>>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n>>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\narray([[0., 1., 0., 0.],\n       [1., 0., 1., 0.]])\n\nOne can change the way feature names are created.\n\n>>> def custom_combiner(feature, category):\n...     return str(feature) + \"_\" + type(category).__name__ + \"_\" + str(category)\n>>> custom_fnames_enc = OneHotEncoder(feature_name_combiner=custom_combiner).fit(X)\n>>> custom_fnames_enc.get_feature_names_out()\narray(['x0_str_Female', 'x0_str_Male', 'x1_int_1', 'x1_int_2', 'x1_int_3'],\n      dtype=object)\n\nInfrequent categories are enabled by setting `max_categories` or `min_frequency`.\n\n>>> import numpy as np\n>>> X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n>>> ohe = OneHotEncoder(max_categories=3, sparse_output=False).fit(X)\n>>> ohe.infrequent_categories_\n[array(['a', 'd'], dtype=object)]\n>>> ohe.transform([[\"a\"], [\"b\"]])\narray([[0., 0., 1.],\n       [1., 0., 0.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "One-vs-one multiclass strategy.\n\nThis strategy consists in fitting one classifier per class pair.\nAt prediction time, the class which received the most votes is selected.\nSince it requires to fit `n_classes * (n_classes - 1) / 2` classifiers,\nthis method is usually slower than one-vs-the-rest, due to its\nO(n_classes^2) complexity. However, this method may be advantageous for\nalgorithms such as kernel algorithms which don't scale well with\n`n_samples`. This is because each individual learning problem only involves\na small subset of the data whereas, with one-vs-the-rest, the complete\ndataset is used `n_classes` times.\n\nRead more in the :ref:`User Guide <ovo_classification>`.\n\nParameters\n----------\nestimator : estimator object\n    A regressor or a classifier that implements :term:`fit`.\n    When a classifier is passed, :term:`decision_function` will be used\n    in priority and it will fallback to :term:`predict_proba` if it is not\n    available.\n    When a regressor is passed, :term:`predict` is used.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the `n_classes * (\n    n_classes - 1) / 2` OVO problems are computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nestimators_ : list of ``n_classes * (n_classes - 1) / 2`` estimators\n    Estimators used for predictions.\n\nclasses_ : numpy array of shape [n_classes]\n    Array containing labels.\n\nn_classes_ : int\n    Number of classes.\n\npairwise_indices_ : list, length = ``len(estimators_)``, or ``None``\n    Indices of samples used when training the estimators.\n    ``None`` when ``estimator``'s `pairwise` tag is False.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nOneVsRestClassifier : One-vs-all multiclass strategy.\nOutputCodeClassifier : (Error-Correcting) Output-Code multiclass strategy.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.multiclass import OneVsOneClassifier\n>>> from sklearn.svm import LinearSVC\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, shuffle=True, random_state=0)\n>>> clf = OneVsOneClassifier(\n...     LinearSVC(dual=\"auto\", random_state=0)).fit(X_train, y_train)\n>>> clf.predict(X_test[:10])\narray([2, 1, 0, 2, 0, 2, 0, 1, 1, 1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "One-vs-the-rest (OvR) multiclass strategy.\n\nAlso known as one-vs-all, this strategy consists in fitting one classifier\nper class. For each classifier, the class is fitted against all the other\nclasses. In addition to its computational efficiency (only `n_classes`\nclassifiers are needed), one advantage of this approach is its\ninterpretability. Since each class is represented by one and one classifier\nonly, it is possible to gain knowledge about the class by inspecting its\ncorresponding classifier. This is the most commonly used strategy for\nmulticlass classification and is a fair default choice.\n\nOneVsRestClassifier can also be used for multilabel classification. To use\nthis feature, provide an indicator matrix for the target `y` when calling\n`.fit`. In other words, the target labels should be formatted as a 2D\nbinary (0/1) matrix, where [i, j] == 1 indicates the presence of label j\nin sample i. This estimator uses the binary relevance method to perform\nmultilabel classification, which involves training one binary classifier\nindependently for each label.\n\nRead more in the :ref:`User Guide <ovr_classification>`.\n\nParameters\n----------\nestimator : estimator object\n    A regressor or a classifier that implements :term:`fit`.\n    When a classifier is passed, :term:`decision_function` will be used\n    in priority and it will fallback to :term:`predict_proba` if it is not\n    available.\n    When a regressor is passed, :term:`predict` is used.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the `n_classes`\n    one-vs-rest problems are computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: 0.20\n       `n_jobs` default changed from 1 to None\n\nverbose : int, default=0\n    The verbosity level, if non zero, progress messages are printed.\n    Below 50, the output is sent to stderr. Otherwise, the output is sent\n    to stdout. The frequency of the messages increases with the verbosity\n    level, reporting all iterations at 10. See :class:`joblib.Parallel` for\n    more details.\n\n    .. versionadded:: 1.1\n\nAttributes\n----------\nestimators_ : list of `n_classes` estimators\n    Estimators used for predictions.\n\nclasses_ : array, shape = [`n_classes`]\n    Class labels.\n\nn_classes_ : int\n    Number of classes.\n\nlabel_binarizer_ : LabelBinarizer object\n    Object used to transform multiclass labels to binary labels and\n    vice-versa.\n\nmultilabel_ : boolean\n    Whether a OneVsRestClassifier is a multilabel classifier.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nOneVsOneClassifier : One-vs-one multiclass strategy.\nOutputCodeClassifier : (Error-Correcting) Output-Code multiclass strategy.\nsklearn.multioutput.MultiOutputClassifier : Alternate way of extending an\n    estimator for multilabel classification.\nsklearn.preprocessing.MultiLabelBinarizer : Transform iterable of iterables\n    to binary indicator matrix.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.multiclass import OneVsRestClassifier\n>>> from sklearn.svm import SVC\n>>> X = np.array([\n...     [10, 10],\n...     [8, 10],\n...     [-5, 5.5],\n...     [-5.4, 5.5],\n...     [-20, -20],\n...     [-15, -20]\n... ])\n>>> y = np.array([0, 0, 1, 1, 2, 2])\n>>> clf = OneVsRestClassifier(SVC()).fit(X, y)\n>>> clf.predict([[-19, -20], [9, 9], [-5, 5]])\narray([2, 0, 1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Encode categorical features as an integer array.\n\nThe input to this transformer should be an array-like of integers or\nstrings, denoting the values taken on by categorical (discrete) features.\nThe features are converted to ordinal integers. This results in\na single column of integers (0 to n_categories - 1) per feature.\n\nRead more in the :ref:`User Guide <preprocessing_categorical_features>`.\nFor a comparison of different encoders, refer to:\n:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ncategories : 'auto' or a list of array-like, default='auto'\n    Categories (unique values) per feature:\n\n    - 'auto' : Determine categories automatically from the training data.\n    - list : ``categories[i]`` holds the categories expected in the ith\n      column. The passed categories should not mix strings and numeric\n      values, and should be sorted in case of numeric values.\n\n    The used categories can be found in the ``categories_`` attribute.\n\ndtype : number type, default=np.float64\n    Desired dtype of output.\n\nhandle_unknown : {'error', 'use_encoded_value'}, default='error'\n    When set to 'error' an error will be raised in case an unknown\n    categorical feature is present during transform. When set to\n    'use_encoded_value', the encoded value of unknown categories will be\n    set to the value given for the parameter `unknown_value`. In\n    :meth:`inverse_transform`, an unknown category will be denoted as None.\n\n    .. versionadded:: 0.24\n\nunknown_value : int or np.nan, default=None\n    When the parameter handle_unknown is set to 'use_encoded_value', this\n    parameter is required and will set the encoded value of unknown\n    categories. It has to be distinct from the values used to encode any of\n    the categories in `fit`. If set to np.nan, the `dtype` parameter must\n    be a float dtype.\n\n    .. versionadded:: 0.24\n\nencoded_missing_value : int or np.nan, default=np.nan\n    Encoded value of missing categories. If set to `np.nan`, then the `dtype`\n    parameter must be a float dtype.\n\n    .. versionadded:: 1.1\n\nmin_frequency : int or float, default=None\n    Specifies the minimum frequency below which a category will be\n    considered infrequent.\n\n    - If `int`, categories with a smaller cardinality will be considered\n      infrequent.\n\n    - If `float`, categories with a smaller cardinality than\n      `min_frequency * n_samples`  will be considered infrequent.\n\n    .. versionadded:: 1.3\n        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n\nmax_categories : int, default=None\n    Specifies an upper limit to the number of output categories for each input\n    feature when considering infrequent categories. If there are infrequent\n    categories, `max_categories` includes the category representing the\n    infrequent categories along with the frequent categories. If `None`,\n    there is no limit to the number of output features.\n\n    `max_categories` do **not** take into account missing or unknown\n    categories. Setting `unknown_value` or `encoded_missing_value` to an\n    integer will increase the number of unique integer codes by one each.\n    This can result in up to `max_categories + 2` integer codes.\n\n    .. versionadded:: 1.3\n        Read more in the :ref:`User Guide <encoder_infrequent_categories>`.\n\nAttributes\n----------\ncategories_ : list of arrays\n    The categories of each feature determined during ``fit`` (in order of\n    the features in X and corresponding with the output of ``transform``).\n    This does not include categories that weren't seen during ``fit``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 1.0\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\ninfrequent_categories_ : list of ndarray\n    Defined only if infrequent categories are enabled by setting\n    `min_frequency` or `max_categories` to a non-default value.\n    `infrequent_categories_[i]` are the infrequent categories for feature\n    `i`. If the feature `i` has no infrequent categories\n    `infrequent_categories_[i]` is None.\n\n    .. versionadded:: 1.3\n\nSee Also\n--------\nOneHotEncoder : Performs a one-hot encoding of categorical features. This encoding\n    is suitable for low to medium cardinality categorical variables, both in\n    supervised and unsupervised settings.\nTargetEncoder : Encodes categorical features using supervised signal\n    in a classification or regression pipeline. This encoding is typically\n    suitable for high cardinality categorical variables.\nLabelEncoder : Encodes target labels with values between 0 and\n    ``n_classes-1``.\n\nNotes\n-----\nWith a high proportion of `nan` values, inferring categories becomes slow with\nPython versions before 3.10. The handling of `nan` values was improved\nfrom Python 3.10 onwards, (c.f.\n`bpo-43475 <https://github.com/python/cpython/issues/87641>`_).\n\nExamples\n--------\nGiven a dataset with two features, we let the encoder find the unique\nvalues per feature and transform the data to an ordinal encoding.\n\n>>> from sklearn.preprocessing import OrdinalEncoder\n>>> enc = OrdinalEncoder()\n>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n>>> enc.fit(X)\nOrdinalEncoder()\n>>> enc.categories_\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n>>> enc.transform([['Female', 3], ['Male', 1]])\narray([[0., 2.],\n       [1., 0.]])\n\n>>> enc.inverse_transform([[1, 0], [0, 1]])\narray([['Male', 1],\n       ['Female', 2]], dtype=object)\n\nBy default, :class:`OrdinalEncoder` is lenient towards missing values by\npropagating them.\n\n>>> import numpy as np\n>>> X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n>>> enc.fit_transform(X)\narray([[ 1.,  0.],\n       [ 0.,  1.],\n       [ 0., nan]])\n\nYou can use the parameter `encoded_missing_value` to encode missing values.\n\n>>> enc.set_params(encoded_missing_value=-1).fit_transform(X)\narray([[ 1.,  0.],\n       [ 0.,  1.],\n       [ 0., -1.]])\n\nInfrequent categories are enabled by setting `max_categories` or `min_frequency`.\nIn the following example, \"a\" and \"d\" are considered infrequent and grouped\ntogether into a single category, \"b\" and \"c\" are their own categories, unknown\nvalues are encoded as 3 and missing values are encoded as 4.\n\n>>> X_train = np.array(\n...     [[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3 + [np.nan]],\n...     dtype=object).T\n>>> enc = OrdinalEncoder(\n...     handle_unknown=\"use_encoded_value\", unknown_value=3,\n...     max_categories=3, encoded_missing_value=4)\n>>> _ = enc.fit(X_train)\n>>> X_test = np.array([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [np.nan]], dtype=object)\n>>> enc.transform(X_test)\narray([[2.],\n       [0.],\n       [1.],\n       [2.],\n       [3.],\n       [4.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Cross-validated Orthogonal Matching Pursuit model (OMP).\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <omp>`.\n\nParameters\n----------\ncopy : bool, default=True\n    Whether the design matrix X must be copied by the algorithm. A false\n    value is only helpful if X is already Fortran-ordered, otherwise a\n    copy is made anyway.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nmax_iter : int, default=None\n    Maximum numbers of iterations to perform, therefore maximum features\n    to include. 10% of ``n_features`` but at least 5 if available.\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool or int, default=False\n    Sets the verbosity amount.\n\nAttributes\n----------\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the problem formulation).\n\nn_nonzero_coefs_ : int\n    Estimated number of non-zero coefficients giving the best mean squared\n    error over the cross-validation folds.\n\nn_iter_ : int or array-like\n    Number of active features across every target for the model refit with\n    the best hyperparameters got by cross-validating across all folds.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\northogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\northogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit\n    problems using only the Gram matrix X.T * X and the product X.T * y.\nlars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\nLars : Least Angle Regression model a.k.a. LAR.\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\nOrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\nLarsCV : Cross-validated Least Angle Regression model.\nLassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.\nsklearn.decomposition.sparse_encode : Generic sparse coding.\n    Each column of the result is the solution to a Lasso problem.\n\nNotes\n-----\nIn `fit`, once the optimal number of non-zero coefficients is found through\ncross-validation, the model is fit again using the entire training set.\n\nExamples\n--------\n>>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=100, n_informative=10,\n...                        noise=4, random_state=0)\n>>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n>>> reg.score(X, y)\n0.9991...\n>>> reg.n_nonzero_coefs_\n10\n>>> reg.predict(X[:1,])\narray([-78.3854...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Orthogonal Matching Pursuit model (OMP).\n\nRead more in the :ref:`User Guide <omp>`.\n\nParameters\n----------\nn_nonzero_coefs : int, default=None\n    Desired number of non-zero entries in the solution. If None (by\n    default) this value is set to 10% of n_features.\n\ntol : float, default=None\n    Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nprecompute : 'auto' or bool, default='auto'\n    Whether to use a precomputed Gram and Xy matrix to speed up\n    calculations. Improves performance when :term:`n_targets` or\n    :term:`n_samples` is very large. Note that if you already have such\n    matrices, you can pass them directly to the fit method.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Parameter vector (w in the formula).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function.\n\nn_iter_ : int or array-like\n    Number of active features across every target.\n\nn_nonzero_coefs_ : int\n    The number of non-zero coefficients in the solution. If\n    `n_nonzero_coefs` is None and `tol` is None this value is either set\n    to 10% of `n_features` or 1, whichever is greater.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\northogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\northogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit\n    problems using only the Gram matrix X.T * X and the product X.T * y.\nlars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\nLars : Least Angle Regression model a.k.a. LAR.\nLassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\nsklearn.decomposition.sparse_encode : Generic sparse coding.\n    Each column of the result is the solution to a Lasso problem.\nOrthogonalMatchingPursuitCV : Cross-validated\n    Orthogonal Matching Pursuit model (OMP).\n\nNotes\n-----\nOrthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\nMatching pursuits with time-frequency dictionaries, IEEE Transactions on\nSignal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n(https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n\nThis implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\nM., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\nMatching Pursuit Technical Report - CS Technion, April 2008.\nhttps://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\nExamples\n--------\n>>> from sklearn.linear_model import OrthogonalMatchingPursuit\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(noise=4, random_state=0)\n>>> reg = OrthogonalMatchingPursuit().fit(X, y)\n>>> reg.score(X, y)\n0.9991...\n>>> reg.predict(X[:1,])\narray([-78.3854...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "(Error-Correcting) Output-Code multiclass strategy.\n\nOutput-code based strategies consist in representing each class with a\nbinary code (an array of 0s and 1s). At fitting time, one binary\nclassifier per bit in the code book is fitted.  At prediction time, the\nclassifiers are used to project new points in the class space and the class\nclosest to the points is chosen. The main advantage of these strategies is\nthat the number of classifiers used can be controlled by the user, either\nfor compressing the model (0 < `code_size` < 1) or for making the model more\nrobust to errors (`code_size` > 1). See the documentation for more details.\n\nRead more in the :ref:`User Guide <ecoc>`.\n\nParameters\n----------\nestimator : estimator object\n    An estimator object implementing :term:`fit` and one of\n    :term:`decision_function` or :term:`predict_proba`.\n\ncode_size : float, default=1.5\n    Percentage of the number of classes to be used to create the code book.\n    A number between 0 and 1 will require fewer classifiers than\n    one-vs-the-rest. A number greater than 1 will require more classifiers\n    than one-vs-the-rest.\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to initialize the codebook.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation: the multiclass problems\n    are computed in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nestimators_ : list of `int(n_classes * code_size)` estimators\n    Estimators used for predictions.\n\nclasses_ : ndarray of shape (n_classes,)\n    Array containing labels.\n\ncode_book_ : ndarray of shape (n_classes, `len(estimators_)`)\n    Binary array containing the code of each class.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nOneVsRestClassifier : One-vs-all multiclass strategy.\nOneVsOneClassifier : One-vs-one multiclass strategy.\n\nReferences\n----------\n\n.. [1] \"Solving multiclass learning problems via error-correcting output\n   codes\",\n   Dietterich T., Bakiri G.,\n   Journal of Artificial Intelligence Research 2,\n   1995.\n\n.. [2] \"The error coding method and PICTs\",\n   James G., Hastie T.,\n   Journal of Computational and Graphical statistics 7,\n   1998.\n\n.. [3] \"The Elements of Statistical Learning\",\n   Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)\n   2008.\n\nExamples\n--------\n>>> from sklearn.multiclass import OutputCodeClassifier\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=100, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = OutputCodeClassifier(\n...     estimator=RandomForestClassifier(random_state=0),\n...     random_state=0).fit(X, y)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nFor a usage example, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\nRead more in the :ref:`User Guide <PCA>`.\n\nParameters\n----------\nn_components : int, float or 'mle', default=None\n    Number of components to keep.\n    if n_components is not set all components are kept::\n\n        n_components == min(n_samples, n_features)\n\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n\n    Hence, the None case results in::\n\n        n_components == min(n_samples, n_features) - 1\n\ncopy : bool, default=True\n    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n\nwhiten : bool, default=False\n    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n\nsvd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n    If auto :\n        The solver is selected by a default policy based on `X.shape` and\n        `n_components`: if the input data is larger than 500x500 and the\n        number of components to extract is lower than 80% of the smallest\n        dimension of the data, then the more efficient 'randomized'\n        method is enabled. Otherwise the exact full SVD is computed and\n        optionally truncated afterwards.\n    If full :\n        run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    If arpack :\n        run SVD truncated to n_components calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        0 < n_components < min(X.shape)\n    If randomized :\n        run randomized SVD by the method of Halko et al.\n\n    .. versionadded:: 0.18.0\n\ntol : float, default=0.0\n    Tolerance for singular values computed by svd_solver == 'arpack'.\n    Must be of range [0.0, infinity).\n\n    .. versionadded:: 0.18.0\n\niterated_power : int or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    Must be of range [0, infinity).\n\n    .. versionadded:: 0.18.0\n\nn_oversamples : int, default=10\n    This parameter is only relevant when `svd_solver=\"randomized\"`.\n    It corresponds to the additional number of random vectors to sample the\n    range of `X` so as to ensure proper conditioning. See\n    :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n    .. versionadded:: 1.1\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Power iteration normalizer for randomized SVD solver.\n    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n    for more details.\n\n    .. versionadded:: 1.1\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18.0\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. Equivalently, the right singular\n    vectors of the centered input data, parallel to its eigenvectors.\n    The components are sorted by decreasing ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The amount of variance explained by each of the selected components.\n    The variance estimation uses `n_samples - 1` degrees of freedom.\n\n    Equal to n_components largest eigenvalues\n    of the covariance matrix of X.\n\n    .. versionadded:: 0.18\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\n    If ``n_components`` is not set then all components are stored and the\n    sum of the ratios is equal to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\n    .. versionadded:: 0.19\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\n    Equal to `X.mean(axis=0)`.\n\nn_components_ : int\n    The estimated number of components. When n_components is set\n    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n    number is estimated from input data. Otherwise it equals the parameter\n    n_components, or the lesser value of n_features and n_samples\n    if n_components is None.\n\nn_samples_ : int\n    Number of samples in the training data.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n    compute the estimated data covariance and score samples.\n\n    Equal to the average of (min(n_features, n_samples) - n_components)\n    smallest eigenvalues of the covariance matrix of X.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKernelPCA : Kernel Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\nIncrementalPCA : Incremental Principal Component Analysis.\n\nReferences\n----------\nFor n_components == 'mle', this class uses the method from:\n`Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\nIn NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\nImplements the probabilistic PCA model from:\n`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\ncomponent analysis\". Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 61(3), 611-622.\n<http://www.miketipping.com/papers/met-mppca.pdf>`_\nvia the score and score_samples methods.\n\nFor svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\nFor svd_solver == 'randomized', see:\n:doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions\".\nSIAM review, 53(2), 217-288.\n<10.1137/090771806>`\nand also\n:doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\".\nApplied and Computational Harmonic Analysis, 30(1), 47-68.\n<10.1016/j.acha.2010.02.003>`\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)\nPCA(n_components=2)\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.0075...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=2, svd_solver='full')\n>>> pca.fit(X)\nPCA(n_components=2, svd_solver='full')\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.00755...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=1, svd_solver='arpack')\n>>> pca.fit(X)\nPCA(n_components=1, svd_solver='arpack')\n>>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairConfusionMatrixMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Pair confusion matrix arising from two clusterings [1]_.\n\nThe pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\nbetween two clusterings by considering all pairs of samples and counting\npairs that are assigned into the same or into different clusters under\nthe true and predicted clusterings.\n\nConsidering a pair of samples that is clustered together a positive pair,\nthen as in binary classification the count of true negatives is\n:math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n:math:`C_{11}` and false positives is :math:`C_{01}`.\n\nRead more in the :ref:`User Guide <pair_confusion_matrix>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=integral\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,), dtype=integral\n    Cluster labels to evaluate.\n\nReturns\n-------\nC : ndarray of shape (2, 2), dtype=np.int64\n    The contingency matrix.\n\nSee Also\n--------\nsklearn.metrics.rand_score : Rand Score.\nsklearn.metrics.adjusted_rand_score : Adjusted Rand Score.\nsklearn.metrics.adjusted_mutual_info_score : Adjusted Mutual Information.\n\nReferences\n----------\n.. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n       Journal of Classification 2, 193‚Äì218 (1985).\n       <10.1007/BF01908075>`\n\nExamples\n--------\nPerfectly matching labelings have all non-zero entries on the\ndiagonal regardless of actual label values:\n\n  >>> from sklearn.metrics.cluster import pair_confusion_matrix\n  >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n  array([[8, 0],\n         [0, 4]]...\n\nLabelings that assign all classes members to the same clusters\nare complete but may be not always pure, hence penalized, and\nhave some off-diagonal non-zero entries:\n\n  >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n  array([[8, 2],\n         [0, 2]]...\n\nNote that the matrix is not symmetric."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute minimum distances between one point and a set of points.\n\nThis function computes for each row in X, the index of the row of Y which\nis closest (according to the specified distance).\n\nThis is mostly equivalent to calling:\n\n    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n\nbut uses much less memory, and is faster for large arrays.\n\nThis function works with dense 2D arrays only.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    Array containing points.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n    Arrays containing points.\n\naxis : int, default=1\n    Axis along which the argmin and distances are to be computed.\n\nmetric : str or callable, default=\"euclidean\"\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\n    .. note::\n       `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n\n    .. note::\n       `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n\nmetric_kwargs : dict, default=None\n    Keyword arguments to pass to specified metric function.\n\nReturns\n-------\nargmin : numpy.ndarray\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n\nSee Also\n--------\npairwise_distances : Distances between every pair of samples of X and Y.\npairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\n    returns the distances.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import pairwise_distances_argmin\n>>> X = [[0, 0, 0], [1, 1, 1]]\n>>> Y = [[1, 0, 0], [1, 1, 0]]\n>>> pairwise_distances_argmin(X, Y)\narray([0, 1])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute minimum distances between one point and a set of points.\n\nThis function computes for each row in X, the index of the row of Y which\nis closest (according to the specified distance). The minimal distances are\nalso returned.\n\nThis is mostly equivalent to calling:\n\n    (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n     pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n\nbut uses much less memory, and is faster for large arrays.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n    Array containing points.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n    Array containing points.\n\naxis : int, default=1\n    Axis along which the argmin and distances are to be computed.\n\nmetric : str or callable, default='euclidean'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\n    .. note::\n       `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n\n    .. note::\n       `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n\nmetric_kwargs : dict, default=None\n    Keyword arguments to pass to specified metric function.\n\nReturns\n-------\nargmin : ndarray\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n\ndistances : ndarray\n    The array of minimum distances. `distances[i]` is the distance between\n    the i-th row in X and the argmin[i]-th row in Y.\n\nSee Also\n--------\npairwise_distances : Distances between every pair of samples of X and Y.\npairwise_distances_argmin : Same as `pairwise_distances_argmin_min` but only\n    returns the argmins.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n>>> X = [[0, 0, 0], [1, 1, 1]]\n>>> Y = [[1, 0, 0], [1, 1, 0]]\n>>> argmin, distances = pairwise_distances_argmin_min(X, Y)\n>>> argmin\narray([0, 1])\n>>> distances\narray([1., 1.])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Generate a distance matrix chunk by chunk with optional reduction.\n\nIn cases where not all of a pairwise distance matrix needs to be\nstored at once, this is used to calculate pairwise distances in\n``working_memory``-sized chunks.  If ``reduce_func`` is given, it is\nrun on each chunk and its return values are concatenated into lists,\narrays or sparse matrices.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n    Array of pairwise distances between samples, or a feature array.\n    The shape the array should be (n_samples_X, n_samples_X) if\n    metric='precomputed' and (n_samples_X, n_features) otherwise.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n    An optional second feature array. Only allowed if\n    metric != \"precomputed\".\n\nreduce_func : callable, default=None\n    The function which is applied on each chunk of the distance matrix,\n    reducing it to needed values.  ``reduce_func(D_chunk, start)``\n    is called repeatedly, where ``D_chunk`` is a contiguous vertical\n    slice of the pairwise distance matrix, starting at row ``start``.\n    It should return one of: None; an array, a list, or a sparse matrix\n    of length ``D_chunk.shape[0]``; or a tuple of such objects.\n    Returning None is useful for in-place operations, rather than\n    reductions.\n\n    If None, pairwise_distances_chunked returns a generator of vertical\n    chunks of the distance matrix.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by scipy.spatial.distance.pdist for its metric parameter,\n    or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n    If metric is \"precomputed\", X is assumed to be a distance matrix.\n    Alternatively, if metric is a callable function, it is called on\n    each pair of instances (rows) and the resulting value recorded.\n    The callable should take two arrays from X as input and return a\n    value indicating the distance between them.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by\n    breaking down the pairwise matrix into n_jobs even slices and\n    computing them in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nworking_memory : float, default=None\n    The sought maximum memory for temporary distance matrix chunks.\n    When None (default), the value of\n    ``sklearn.get_config()['working_memory']`` is used.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a scipy.spatial.distance metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nYields\n------\nD_chunk : {ndarray, sparse matrix}\n    A contiguous slice of distance matrix, optionally processed by\n    ``reduce_func``.\n\nExamples\n--------\nWithout reduce_func:\n\n>>> import numpy as np\n>>> from sklearn.metrics import pairwise_distances_chunked\n>>> X = np.random.RandomState(0).rand(5, 3)\n>>> D_chunk = next(pairwise_distances_chunked(X))\n>>> D_chunk\narray([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n\nRetrieve all neighbors and average distance within radius r:\n\n>>> r = .2\n>>> def reduce_func(D_chunk, start):\n...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n...     return neigh, avg_dist\n>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n>>> neigh, avg_dist = next(gen)\n>>> neigh\n[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n>>> avg_dist\narray([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n\nWhere r is defined per sample, we need to make use of ``start``:\n\n>>> r = [.2, .4, .4, .3, .1]\n>>> def reduce_func(D_chunk, start):\n...     neigh = [np.flatnonzero(d < r[i])\n...              for i, d in enumerate(D_chunk, start)]\n...     return neigh\n>>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n>>> neigh\n[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n\nForce row-by-row generation by reducing ``working_memory``:\n\n>>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n...                                  working_memory=0)\n>>> next(gen)\n[array([0, 3])]\n>>> next(gen)\n[array([0, 1])]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the distance matrix from a vector array X and optional Y.\n\nThis method takes either a vector array or a distance matrix, and returns\na distance matrix. If the input is a vector array, the distances are\ncomputed. If the input is a distances matrix, it is returned instead.\n\nThis method provides a safe way to take a distance matrix as input, while\npreserving compatibility with many other algorithms that take a vector\narray.\n\nIf Y is given (default is None), then the returned matrix is the pairwise\ndistance between the arrays from both X and Y.\n\nValid values for metric are:\n\n- From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n  'manhattan']. These metrics support sparse matrix\n  inputs.\n  ['nan_euclidean'] but it does not yet support sparse matrices.\n\n- From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n  'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n  'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n  See the documentation for scipy.spatial.distance for details on these\n  metrics. These metrics do not support sparse matrix inputs.\n\n.. note::\n    `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n\n.. note::\n    `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n\nNote that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\nvalid scipy.spatial.distance metrics), the scikit-learn implementation\nwill be used, which is faster and has support for sparse matrices (except\nfor 'cityblock'). For a verbose description of the metrics from\nscikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`\nfunction.\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n    Array of pairwise distances between samples, or a feature array.\n    The shape of the array should be (n_samples_X, n_samples_X) if\n    metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n    An optional second feature array. Only allowed if\n    metric != \"precomputed\".\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by scipy.spatial.distance.pdist for its metric parameter, or\n    a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n    If metric is \"precomputed\", X is assumed to be a distance matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays from X as input and return a value indicating\n    the distance between them.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nforce_all_finite : bool or 'allow-nan', default=True\n    Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n    for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n    possibilities are:\n\n    - True: Force all values of array to be finite.\n    - False: accepts np.inf, np.nan, pd.NA in array.\n    - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n      cannot be infinite.\n\n    .. versionadded:: 0.22\n       ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n    .. versionchanged:: 0.23\n       Accepts `pd.NA` and converts it into `np.nan`.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a scipy.spatial.distance metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nReturns\n-------\nD : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n    A distance matrix D such that D_{i, j} is the distance between the\n    ith and jth vectors of the given matrix X, if Y is None.\n    If Y is not None, then D_{i, j} is the distance between the ith array\n    from X and the jth array from Y.\n\nSee Also\n--------\npairwise_distances_chunked : Performs the same calculation as this\n    function, but returns a generator of chunks of the distance matrix, in\n    order to limit memory usage.\nsklearn.metrics.pairwise.paired_distances : Computes the distances between\n    corresponding elements of two arrays.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import pairwise_distances\n>>> X = [[0, 0, 0], [1, 1, 1]]\n>>> Y = [[1, 0, 0], [1, 1, 0]]\n>>> pairwise_distances(X, Y, metric='sqeuclidean')\narray([[1., 2.],\n       [2., 1.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the kernel between arrays X and optional array Y.\n\nThis method takes either a vector array or a kernel matrix, and returns\na kernel matrix. If the input is a vector array, the kernels are\ncomputed. If the input is a kernel matrix, it is returned instead.\n\nThis method provides a safe way to take a kernel matrix as input, while\npreserving compatibility with many other algorithms that take a vector\narray.\n\nIf Y is given (default is None), then the returned matrix is the pairwise\nkernel between the arrays from both X and Y.\n\nValid values for metric are:\n    ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n    'laplacian', 'sigmoid', 'cosine']\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters\n----------\nX : {array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n    Array of pairwise kernels between samples, or a feature array.\n    The shape of the array should be (n_samples_X, n_samples_X) if\n    metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n    A second feature array only if X has shape (n_samples_X, n_features).\n\nmetric : str or callable, default=\"linear\"\n    The metric to use when calculating kernel between instances in a\n    feature array. If metric is a string, it must be one of the metrics\n    in ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.\n    If metric is \"precomputed\", X is assumed to be a kernel matrix.\n    Alternatively, if metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two rows from X as input and return the corresponding\n    kernel value as a single number. This means that callables from\n    :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n    matrices, not single samples. Use the string identifying the kernel\n    instead.\n\nfilter_params : bool, default=False\n    Whether to filter invalid parameters or not.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the kernel function.\n\nReturns\n-------\nK : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)\n    A kernel matrix K such that K_{i, j} is the kernel between the\n    ith and jth vectors of the given matrix X, if Y is None.\n    If Y is not None, then K_{i, j} is the kernel between the ith array\n    from X and the jth array from Y.\n\nNotes\n-----\nIf metric is 'precomputed', Y is ignored and X is returned.\n\nExamples\n--------\n>>> from sklearn.metrics.pairwise import pairwise_kernels\n>>> X = [[0, 0, 0], [1, 1, 1]]\n>>> Y = [[1, 0, 0], [1, 1, 0]]\n>>> pairwise_kernels(X, Y, metric='linear')\narray([[0., 0.],\n       [1., 2.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Passive Aggressive Classifier.\n\nRead more in the :ref:`User Guide <passive_aggressive>`.\n\nParameters\n----------\nC : float, default=1.0\n    Maximum step size (regularization). Defaults to 1.0.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n\n    .. versionadded:: 0.19\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score is not improving by at least `tol` for\n    `n_iter_no_change` consecutive epochs.\n\n    .. versionadded:: 0.20\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n\nloss : str, default=\"hinge\"\n    The loss function to be used:\n    hinge: equivalent to PA-I in the reference paper.\n    squared_hinge: equivalent to PA-II in the reference paper.\n\nn_jobs : int or None, default=None\n    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n\nclass_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None\n    Preset for the class_weight fit parameter.\n\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\n    .. versionadded:: 0.17\n       parameter *class_weight* to automatically weight samples.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So average=10 will begin averaging after seeing 10 samples.\n\n    .. versionadded:: 0.19\n       parameter *average* to use weights averaging in SGD.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n    For multiclass fits, it is the maximum over every binary fit.\n\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples + 1)``.\n\nloss_function_ : callable\n    Loss function used by the algorithm.\n\nSee Also\n--------\nSGDClassifier : Incrementally trained logistic regression.\nPerceptron : Linear perceptron classifier.\n\nReferences\n----------\nOnline Passive-Aggressive Algorithms\n<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n\nExamples\n--------\n>>> from sklearn.linear_model import PassiveAggressiveClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n... tol=1e-3)\n>>> clf.fit(X, y)\nPassiveAggressiveClassifier(random_state=0)\n>>> print(clf.coef_)\n[[0.26642044 0.45070924 0.67251877 0.64185414]]\n>>> print(clf.intercept_)\n[1.84127814]\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Passive Aggressive Regressor.\n\nRead more in the :ref:`User Guide <passive_aggressive>`.\n\nParameters\n----------\n\nC : float, default=1.0\n    Maximum step size (regularization). Defaults to 1.0.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered. Defaults to True.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n\n    .. versionadded:: 0.19\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation.\n    score is not improving. If set to True, it will automatically set aside\n    a fraction of training data as validation and terminate\n    training when validation score is not improving by at least tol for\n    n_iter_no_change consecutive epochs.\n\n    .. versionadded:: 0.20\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n\nloss : str, default=\"epsilon_insensitive\"\n    The loss function to be used:\n    epsilon_insensitive: equivalent to PA-I in the reference paper.\n    squared_epsilon_insensitive: equivalent to PA-II in the reference\n    paper.\n\nepsilon : float, default=0.1\n    If the difference between the current prediction and the correct label\n    is below this threshold, the model is not updated.\n\nrandom_state : int, RandomState instance, default=None\n    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So average=10 will begin averaging after seeing 10 samples.\n\n    .. versionadded:: 0.19\n       parameter *average* to use weights averaging in SGD.\n\nAttributes\n----------\ncoef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n    Weights assigned to the features.\n\nintercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples + 1)``.\n\nSee Also\n--------\nSGDRegressor : Linear model fitted by minimizing a regularized\n    empirical loss with SGD.\n\nReferences\n----------\nOnline Passive-Aggressive Algorithms\n<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).\n\nExamples\n--------\n>>> from sklearn.linear_model import PassiveAggressiveRegressor\n>>> from sklearn.datasets import make_regression\n\n>>> X, y = make_regression(n_features=4, random_state=0)\n>>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n... tol=1e-3)\n>>> regr.fit(X, y)\nPassiveAggressiveRegressor(max_iter=100, random_state=0)\n>>> print(regr.coef_)\n[20.48736655 34.18818427 67.59122734 87.94731329]\n>>> print(regr.intercept_)\n[-0.02306214]\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-0.02306214]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear perceptron classifier.\n\nThe implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier`\nby fixing the `loss` and `learning_rate` parameters as::\n\n    SGDClassifier(loss=\"perceptron\", learning_rate=\"constant\")\n\nOther available parameters are described below and are forwarded to\n:class:`~sklearn.linear_model.SGDClassifier`.\n\nRead more in the :ref:`User Guide <perceptron>`.\n\nParameters\n----------\n\npenalty : {'l2','l1','elasticnet'}, default=None\n    The penalty (aka regularization term) to be used.\n\nalpha : float, default=0.0001\n    Constant that multiplies the regularization term if regularization is\n    used.\n\nl1_ratio : float, default=0.15\n    The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n    `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n    Only used if `penalty='elasticnet'`.\n\n    .. versionadded:: 0.24\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol).\n\n    .. versionadded:: 0.19\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n\neta0 : float, default=1\n    Constant by which the updates are multiplied.\n\nn_jobs : int, default=None\n    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance or None, default=0\n    Used to shuffle the training data, when ``shuffle`` is set to\n    ``True``. Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score is not improving by at least `tol` for\n    `n_iter_no_change` consecutive epochs.\n\n    .. versionadded:: 0.20\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if early_stopping is True.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before early stopping.\n\n    .. versionadded:: 0.20\n\nclass_weight : dict, {class_label: weight} or \"balanced\", default=None\n    Preset for the class_weight fit parameter.\n\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution. See\n    :term:`the Glossary <warm_start>`.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    The unique classes labels.\n\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nloss_function_ : concrete¬†LossFunction\n    The function that determines the loss, or difference between the\n    output of the algorithm and the target values.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n    For multiclass fits, it is the maximum over every binary fit.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples + 1)``.\n\nSee Also\n--------\nsklearn.linear_model.SGDClassifier : Linear classifiers\n    (SVM, logistic regression, etc.) with SGD training.\n\nNotes\n-----\n``Perceptron`` is a classification algorithm which shares the same\nunderlying implementation with ``SGDClassifier``. In fact,\n``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\neta0=1, learning_rate=\"constant\", penalty=None)`.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/Perceptron and references therein.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.linear_model import Perceptron\n>>> X, y = load_digits(return_X_y=True)\n>>> clf = Perceptron(tol=1e-3, random_state=0)\n>>> clf.fit(X, y)\nPerceptron()\n>>> clf.score(X, y)\n0.939..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculation",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Generalized Linear Model with a Poisson distribution.\n\nThis regressor uses the 'log' link function.\n\nRead more in the :ref:`User Guide <Generalized_linear_models>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\nalpha : float, default=1\n    Constant that multiplies the L2 penalty term and determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n    Values of `alpha` must be in the range `[0.0, inf)`.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (`X @ coef + intercept`).\n\nsolver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n    Algorithm to use in the optimization problem:\n\n    'lbfgs'\n        Calls scipy's L-BFGS-B optimizer.\n\n    'newton-cholesky'\n        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n        iterated reweighted least squares) with an inner Cholesky based solver.\n        This solver is a good choice for `n_samples` >> `n_features`, especially\n        with one-hot encoded categorical features with rare categories. Be aware\n        that the memory usage of this solver has a quadratic dependency on\n        `n_features` because it explicitly computes the Hessian matrix.\n\n        .. versionadded:: 1.2\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n    Values must be in the range `[1, inf)`.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n    Values must be in the range `(0.0, inf)`.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_`` .\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n    Values must be in the range `[0, inf)`.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X @ coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Actual number of iterations used in the solver.\n\nSee Also\n--------\nTweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.PoissonRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [12, 17, 22, 21]\n>>> clf.fit(X, y)\nPoissonRegressor()\n>>> clf.score(X, y)\n0.990...\n>>> clf.coef_\narray([0.121..., 0.158...])\n>>> clf.intercept_\n2.088...\n>>> clf.predict([[1, 1], [3, 4]])\narray([10.676..., 21.875...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Generate polynomial and interaction features.\n\nGenerate a new feature matrix consisting of all polynomial combinations\nof the features with degree less than or equal to the specified degree.\nFor example, if an input sample is two dimensional and of the form\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n\nRead more in the :ref:`User Guide <polynomial_features>`.\n\nParameters\n----------\ndegree : int or tuple (min_degree, max_degree), default=2\n    If a single int is given, it specifies the maximal degree of the\n    polynomial features. If a tuple `(min_degree, max_degree)` is passed,\n    then `min_degree` is the minimum and `max_degree` is the maximum\n    polynomial degree of the generated features. Note that `min_degree=0`\n    and `min_degree=1` are equivalent as outputting the degree zero term is\n    determined by `include_bias`.\n\ninteraction_only : bool, default=False\n    If `True`, only interaction features are produced: features that are\n    products of at most `degree` *distinct* input features, i.e. terms with\n    power of 2 or higher of the same input feature are excluded:\n\n        - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc.\n        - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.\n\ninclude_bias : bool, default=True\n    If `True` (default), then include a bias column, the feature in which\n    all polynomial powers are zero (i.e. a column of ones - acts as an\n    intercept term in a linear model).\n\norder : {'C', 'F'}, default='C'\n    Order of output array in the dense case. `'F'` order is faster to\n    compute, but may slow down subsequent estimators.\n\n    .. versionadded:: 0.21\n\nAttributes\n----------\npowers_ : ndarray of shape (`n_output_features_`, `n_features_in_`)\n    `powers_[i, j]` is the exponent of the jth input in the ith output.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_output_features_ : int\n    The total number of polynomial output features. The number of output\n    features is computed by iterating over all suitably sized combinations\n    of input features.\n\nSee Also\n--------\nSplineTransformer : Transformer that generates univariate B-spline bases\n    for features.\n\nNotes\n-----\nBe aware that the number of features in the output array scales\npolynomially in the number of features of the input array, and\nexponentially in the degree. High degrees can cause overfitting.\n\nSee :ref:`examples/linear_model/plot_polynomial_interpolation.py\n<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])\n>>> poly = PolynomialFeatures(interaction_only=True)\n>>> poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.],\n       [ 1.,  2.,  3.,  6.],\n       [ 1.,  4.,  5., 20.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Apply a power transform featurewise to make data more Gaussian-like.\n\nPower transforms are a family of parametric, monotonic transformations\nthat are applied to make data more Gaussian-like. This is useful for\nmodeling issues related to heteroscedasticity (non-constant variance),\nor other situations where normality is desired.\n\nCurrently, PowerTransformer supports the Box-Cox transform and the\nYeo-Johnson transform. The optimal parameter for stabilizing variance and\nminimizing skewness is estimated through maximum likelihood.\n\nBox-Cox requires input data to be strictly positive, while Yeo-Johnson\nsupports both positive or negative data.\n\nBy default, zero-mean, unit-variance normalization is applied to the\ntransformed data.\n\nFor an example visualization, refer to :ref:`Compare PowerTransformer with\nother scalers <plot_all_scaling_power_transformer_section>`. To see the\neffect of Box-Cox and Yeo-Johnson transformations on different\ndistributions, see:\n:ref:`sphx_glr_auto_examples_preprocessing_plot_map_data_to_normal.py`.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nmethod : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n    The power transform method. Available methods are:\n\n    - 'yeo-johnson' [1]_, works with positive and negative values\n    - 'box-cox' [2]_, only works with strictly positive values\n\nstandardize : bool, default=True\n    Set to True to apply zero-mean, unit-variance normalization to the\n    transformed output.\n\ncopy : bool, default=True\n    Set to False to perform inplace computation during transformation.\n\nAttributes\n----------\nlambdas_ : ndarray of float of shape (n_features,)\n    The parameters of the power transformation for the selected features.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\npower_transform : Equivalent function without the estimator API.\n\nQuantileTransformer : Maps data to a standard normal distribution with\n    the parameter `output_distribution='normal'`.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in ``fit``, and maintained\nin ``transform``.\n\nReferences\n----------\n\n.. [1] :doi:`I.K. Yeo and R.A. Johnson, \"A new family of power\n       transformations to improve normality or symmetry.\" Biometrika,\n       87(4), pp.954-959, (2000). <10.1093/biomet/87.4.954>`\n\n.. [2] :doi:`G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\",\n       Journal of the Royal Statistical Society B, 26, 211-252 (1964).\n       <10.1111/j.2517-6161.1964.tb00553.x>`\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import PowerTransformer\n>>> pt = PowerTransformer()\n>>> data = [[1, 2], [3, 2], [4, 5]]\n>>> print(pt.fit(data))\nPowerTransformer()\n>>> print(pt.lambdas_)\n[ 1.386... -3.100...]\n>>> print(pt.transform(data))\n[[-1.316... -0.707...]\n [ 0.209... -0.707...]\n [ 1.106...  1.414...]]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute precision-recall pairs for different probability thresholds.\n\nNote: this implementation is restricted to the binary classification task.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe last precision and recall values are 1. and 0. respectively and do not\nhave a corresponding threshold. This ensures that the graph starts on the\ny axis.\n\nThe first precision and recall values are precision=class balance and recall=1.0\nwhich corresponds to a classifier that always predicts the positive class.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\nprobas_pred : array-like of shape (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, or non-thresholded measure of decisions (as returned by\n    `decision_function` on some classifiers).\n\npos_label : int, float, bool or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndrop_intermediate : bool, default=False\n    Whether to drop some suboptimal thresholds which would not appear\n    on a plotted precision-recall curve. This is useful in order to create\n    lighter precision-recall curves.\n\n    .. versionadded:: 1.3\n\nReturns\n-------\nprecision : ndarray of shape (n_thresholds + 1,)\n    Precision values such that element i is the precision of\n    predictions with score >= thresholds[i] and the last element is 1.\n\nrecall : ndarray of shape (n_thresholds + 1,)\n    Decreasing recall values such that element i is the recall of\n    predictions with score >= thresholds[i] and the last element is 0.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Increasing thresholds on the decision function used to compute\n    precision and recall where `n_thresholds = len(np.unique(probas_pred))`.\n\nSee Also\n--------\nPrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n    a binary classifier.\nPrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n    using predictions from a binary classifier.\naverage_precision_score : Compute average precision from prediction scores.\ndet_curve: Compute error rates for different probability thresholds.\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import precision_recall_curve\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> precision, recall, thresholds = precision_recall_curve(\n...     y_true, y_scores)\n>>> precision\narray([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])\n>>> recall\narray([1. , 1. , 0.5, 0.5, 0. ])\n>>> thresholds\narray([0.1 , 0.35, 0.4 , 0.8 ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute precision, recall, F-measure and support for each class.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label a negative sample as\npositive.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of\nthe precision and recall, where an F-beta score reaches its best\nvalue at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of\n``beta``. ``beta == 1.0`` means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in ``y_true``.\n\nSupport beyond term:`binary` targets is achieved by treating :term:`multiclass`\nand :term:`multilabel` data as a collection of binary problems, one for each\nlabel. For the :term:`binary` case, setting `average='binary'` will return\nmetrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\nand metrics for both classes are computed, then averaged or both returned (when\n`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\nmetrics for all `labels` are either returned or averaged depending on the `average`\nparameter. Use `labels` specify the set of labels to calculate metrics for.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nbeta : float, default=1.0\n    The strength of recall versus precision in the F-score.\n\nlabels : array-like, default=None\n    The set of labels to include when `average != 'binary'`, and their\n    order if `average is None`. Labels present in the data can be\n    excluded, for example in multiclass classification to exclude a \"negative\n    class\". Labels not present in the data can be included and will be\n    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\npos_label : int, float, bool or str, default=1\n    The class to report if `average='binary'` and the data is binary,\n    otherwise this parameter is ignored.\n    For multiclass or multilabel targets, set `labels=[pos_label]` and\n    `average != 'binary'` to report metrics for one label only.\n\naverage : {'binary', 'micro', 'macro', 'samples', 'weighted'},             default=None\n    If ``None``, the metrics for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nwarn_for : list, tuple or set, for internal use\n    This determines which warnings will be made in the case that this\n    function is being used to return only one of its metrics.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n    Sets the value to return when there is a zero division:\n       - recall: when there are no positive labels\n       - precision: when there are no positive predictions\n       - f-score: both\n\n    Notes:\n    - If set to \"warn\", this acts like 0, but a warning is also raised.\n    - If set to `np.nan`, such values will be excluded from the average.\n\n    .. versionadded:: 1.3\n       `np.nan` option was added.\n\nReturns\n-------\nprecision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n    Precision score.\n\nrecall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n    Recall score.\n\nfbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n    F-beta score.\n\nsupport : None (if average is not None) or array of int, shape =        [n_unique_labels]\n    The number of occurrences of each label in ``y_true``.\n\nNotes\n-----\nWhen ``true positive + false positive == 0``, precision is undefined.\nWhen ``true positive + false negative == 0``, recall is undefined. When\n``true positive + false negative + false positive == 0``, f-score is\nundefined. In such cases, by default the metric will be set to 0, and\n``UndefinedMetricWarning`` will be raised. This behavior can be modified\nwith ``zero_division``.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Precision and recall\n       <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n\n.. [2] `Wikipedia entry for the F1-score\n       <https://en.wikipedia.org/wiki/F1_score>`_.\n\n.. [3] `Discriminative Methods for Multi-labeled Classification Advances\n       in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n       Godbole, Sunita Sarawagi\n       <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import precision_recall_fscore_support\n>>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n>>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n>>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n(0.22..., 0.33..., 0.26..., None)\n>>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n(0.33..., 0.33..., 0.33..., None)\n>>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n(0.22..., 0.33..., 0.26..., None)\n\nIt is possible to compute per-label precisions, recalls, F1-scores and\nsupports instead of averaging:\n\n>>> precision_recall_fscore_support(y_true, y_pred, average=None,\n... labels=['pig', 'dog', 'cat'])\n(array([0.        , 0.        , 0.66...]),\n array([0., 0., 1.]), array([0. , 0. , 0.8]),\n array([2, 2, 2]))"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the precision.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe best value is 1 and the worst value is 0.\n\nSupport beyond term:`binary` targets is achieved by treating :term:`multiclass`\nand :term:`multilabel` data as a collection of binary problems, one for each\nlabel. For the :term:`binary` case, setting `average='binary'` will return\nprecision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\nand precision for both classes are computed, then averaged or both returned (when\n`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\nprecision for all `labels` are either returned or averaged depending on the\n`average` parameter. Use `labels` specify the set of labels to calculate precision\nfor.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    The set of labels to include when `average != 'binary'`, and their\n    order if `average is None`. Labels present in the data can be\n    excluded, for example in multiclass classification to exclude a \"negative\n    class\". Labels not present in the data can be included and will be\n    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : int, float, bool or str, default=1\n    The class to report if `average='binary'` and the data is binary,\n    otherwise this parameter is ignored.\n    For multiclass or multilabel targets, set `labels=[pos_label]` and\n    `average != 'binary'` to report metrics for one label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n    Sets the value to return when there is a zero division.\n\n    Notes:\n    - If set to \"warn\", this acts like 0, but a warning is also raised.\n    - If set to `np.nan`, such values will be excluded from the average.\n\n    .. versionadded:: 1.3\n       `np.nan` option was added.\n\nReturns\n-------\nprecision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n    Precision of the positive class in binary classification or weighted\n    average of the precision of each class for the multiclass task.\n\nSee Also\n--------\nprecision_recall_fscore_support : Compute precision, recall, F-measure and\n    support for each class.\nrecall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n    number of true positives and ``fn`` the number of false negatives.\nPrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n    an estimator and some data.\nPrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n    binary class predictions.\nmultilabel_confusion_matrix : Compute a confusion matrix for each class or\n    sample.\n\nNotes\n-----\nWhen ``true positive + false positive == 0``, precision returns 0 and\nraises ``UndefinedMetricWarning``. This behavior can be\nmodified with ``zero_division``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import precision_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> precision_score(y_true, y_pred, average='macro')\n0.22...\n>>> precision_score(y_true, y_pred, average='micro')\n0.33...\n>>> precision_score(y_true, y_pred, average='weighted')\n0.22...\n>>> precision_score(y_true, y_pred, average=None)\narray([0.66..., 0.        , 0.        ])\n>>> y_pred = [0, 0, 0, 0, 0, 0]\n>>> precision_score(y_true, y_pred, average=None)\narray([0.33..., 0.        , 0.        ])\n>>> precision_score(y_true, y_pred, average=None, zero_division=1)\narray([0.33..., 1.        , 1.        ])\n>>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)\narray([0.33...,        nan,        nan])\n\n>>> # multilabel classification\n>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n>>> precision_score(y_true, y_pred, average=None)\narray([0.5, 1. , 1. ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PredefinedSplitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Predefined split cross-validator.\n\nProvides train/test indices to split data into train/test sets using a\npredefined scheme specified by the user with the ``test_fold`` parameter.\n\nRead more in the :ref:`User Guide <predefined_split>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\ntest_fold : array-like of shape (n_samples,)\n    The entry ``test_fold[i]`` represents the index of the test set that\n    sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n    any test set (i.e. include sample ``i`` in every training set) by\n    setting ``test_fold[i]`` equal to -1.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import PredefinedSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> test_fold = [0, 1, -1, 1]\n>>> ps = PredefinedSplit(test_fold)\n>>> ps.get_n_splits()\n2\n>>> print(ps)\nPredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n>>> for i, (train_index, test_index) in enumerate(ps.split()):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[1 2 3]\n  Test:  index=[0]\nFold 1:\n  Train: index=[0 2]\n  Test:  index=[1 3]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear regression model that predicts conditional quantiles.\n\nThe linear :class:`QuantileRegressor` optimizes the pinball loss for a\ndesired `quantile` and is robust to outliers.\n\nThis model uses an L1 regularization like\n:class:`~sklearn.linear_model.Lasso`.\n\nRead more in the :ref:`User Guide <quantile_regression>`.\n\n.. versionadded:: 1.0\n\nParameters\n----------\nquantile : float, default=0.5\n    The quantile that the model tries to predict. It must be strictly\n    between 0 and 1. If 0.5 (default), the model predicts the 50%\n    quantile, i.e. the median.\n\nalpha : float, default=1.0\n    Regularization constant that multiplies the L1 penalty term.\n\nfit_intercept : bool, default=True\n    Whether or not to fit the intercept.\n\nsolver : {'highs-ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='highs'\n    Method used by :func:`scipy.optimize.linprog` to solve the linear\n    programming formulation.\n\n    From `scipy>=1.6.0`, it is recommended to use the highs methods because\n    they are the fastest ones. Solvers \"highs-ds\", \"highs-ipm\" and \"highs\"\n    support sparse input data and, in fact, always convert to sparse csc.\n\n    From `scipy>=1.11.0`, \"interior-point\" is not available anymore.\n\n    .. versionchanged:: 1.4\n       The default of `solver` changed to `\"highs\"` in version 1.4.\n\nsolver_options : dict, default=None\n    Additional parameters passed to :func:`scipy.optimize.linprog` as\n    options. If `None` and if `solver='interior-point'`, then\n    `{\"lstsq\": True}` is passed to :func:`scipy.optimize.linprog` for the\n    sake of stability.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the features.\n\nintercept_ : float\n    The intercept of the model, aka bias term.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    The actual number of iterations performed by the solver.\n\nSee Also\n--------\nLasso : The Lasso is a linear model that estimates sparse coefficients\n    with l1 regularization.\nHuberRegressor : Linear regression model that is robust to outliers.\n\nExamples\n--------\n>>> from sklearn.linear_model import QuantileRegressor\n>>> import numpy as np\n>>> n_samples, n_features = 10, 2\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> # the two following lines are optional in practice\n>>> from sklearn.utils.fixes import sp_version, parse_version\n>>> solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n>>> reg = QuantileRegressor(quantile=0.8, solver=solver).fit(X, y)\n>>> np.mean(y <= reg.predict(X))\n0.8"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal\ndistribution. Therefore, for a given feature, this transformation tends\nto spread out the most frequent values. It also reduces the impact of\n(marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an\nestimate of the cumulative distribution function of a feature is\nused to map the original values to a uniform distribution. The obtained\nvalues are then mapped to the desired output distribution using the\nassociated quantile function. Features values of new/unseen data that fall\nbelow or above the fitted range will be mapped to the bounds of the output\ndistribution. Note that this transform is non-linear. It may distort linear\ncorrelations between variables measured at the same scale but renders\nvariables measured at different scales more directly comparable.\n\nFor example visualizations, refer to :ref:`Compare QuantileTransformer with\nother scalers <plot_all_scaling_quantile_transformer_section>`.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\n.. versionadded:: 0.19\n\nParameters\n----------\nn_quantiles : int, default=1000 or n_samples\n    Number of quantiles to be computed. It corresponds to the number\n    of landmarks used to discretize the cumulative distribution function.\n    If n_quantiles is larger than the number of samples, n_quantiles is set\n    to the number of samples as a larger number of quantiles does not give\n    a better approximation of the cumulative distribution function\n    estimator.\n\noutput_distribution : {'uniform', 'normal'}, default='uniform'\n    Marginal distribution for the transformed data. The choices are\n    'uniform' (default) or 'normal'.\n\nignore_implicit_zeros : bool, default=False\n    Only applies to sparse matrices. If True, the sparse entries of the\n    matrix are discarded to compute the quantile statistics. If False,\n    these entries are treated as zeros.\n\nsubsample : int, default=10_000\n    Maximum number of samples used to estimate the quantiles for\n    computational efficiency. Note that the subsampling procedure may\n    differ for value-identical sparse and dense matrices.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for subsampling and smoothing\n    noise.\n    Please see ``subsample`` for more details.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ncopy : bool, default=True\n    Set to False to perform inplace transformation and avoid a copy (if the\n    input is already a numpy array).\n\nAttributes\n----------\nn_quantiles_ : int\n    The actual number of quantiles used to discretize the cumulative\n    distribution function.\n\nquantiles_ : ndarray of shape (n_quantiles, n_features)\n    The values corresponding the quantiles of reference.\n\nreferences_ : ndarray of shape (n_quantiles, )\n    Quantiles of references.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nquantile_transform : Equivalent function without the estimator API.\nPowerTransformer : Perform mapping to a normal distribution using a power\n    transform.\nStandardScaler : Perform standardization that is faster, but less robust\n    to outliers.\nRobustScaler : Perform robust standardization that removes the influence\n    of outliers but does not put outliers and inliers on the same scale.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import QuantileTransformer\n>>> rng = np.random.RandomState(0)\n>>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n>>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n>>> qt.fit_transform(X)\narray([...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ":math:`R^2` (coefficient of determination) regression score function.\n\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). In the general case when the true y is\nnon-constant, a constant model that always predicts the average y\ndisregarding the input features would get a :math:`R^2` score of 0.0.\n\nIn the particular case when ``y_true`` is constant, the :math:`R^2` score\nis not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n(imperfect predictions). To prevent such non-finite numbers to pollute\nhigher-level experiments such as a grid search cross-validation, by default\nthese cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\npredictions) respectively. You can set ``force_finite`` to ``False`` to\nprevent this fix from happening.\n\nNote: when the prediction residuals have zero mean, the :math:`R^2` score\nis identical to the\n:func:`Explained Variance score <explained_variance_score>`.\n\nRead more in the :ref:`User Guide <r2_score>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n\n    Defines aggregating of multiple output scores.\n    Array-like value defines weights used to average scores.\n    Default is \"uniform_average\".\n\n    'raw_values' :\n        Returns a full set of scores in case of multioutput input.\n\n    'uniform_average' :\n        Scores of all outputs are averaged with uniform weight.\n\n    'variance_weighted' :\n        Scores of all outputs are averaged, weighted by the variances\n        of each individual output.\n\n    .. versionchanged:: 0.19\n        Default value of multioutput is 'uniform_average'.\n\nforce_finite : bool, default=True\n    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n    data should be replaced with real numbers (``1.0`` if prediction is\n    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n    for hyperparameters' search procedures (e.g. grid search\n    cross-validation).\n\n    .. versionadded:: 1.1\n\nReturns\n-------\nz : float or ndarray of floats\n    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n    'raw_values'.\n\nNotes\n-----\nThis is not a symmetric function.\n\nUnlike most other scores, :math:`R^2` score may be negative (it need not\nactually be the square of a quantity R).\n\nThis metric is not well-defined for single samples and will return a NaN\nvalue if n_samples is less than two.\n\nReferences\n----------\n.. [1] `Wikipedia entry on the Coefficient of determination\n        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n\nExamples\n--------\n>>> from sklearn.metrics import r2_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> r2_score(y_true, y_pred)\n0.948...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> r2_score(y_true, y_pred,\n...          multioutput='variance_weighted')\n0.938...\n>>> y_true = [1, 2, 3]\n>>> y_pred = [1, 2, 3]\n>>> r2_score(y_true, y_pred)\n1.0\n>>> y_true = [1, 2, 3]\n>>> y_pred = [2, 2, 2]\n>>> r2_score(y_true, y_pred)\n0.0\n>>> y_true = [1, 2, 3]\n>>> y_pred = [3, 2, 1]\n>>> r2_score(y_true, y_pred)\n-3.0\n>>> y_true = [-2, -2, -2]\n>>> y_pred = [-2, -2, -2]\n>>> r2_score(y_true, y_pred)\n1.0\n>>> r2_score(y_true, y_pred, force_finite=False)\nnan\n>>> y_true = [-2, -2, -2]\n>>> y_pred = [-2, -2, -2 + 1e-8]\n>>> r2_score(y_true, y_pred)\n0.0\n>>> r2_score(y_true, y_pred, force_finite=False)\n-inf"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "RANSAC (RANdom SAmple Consensus) algorithm.\n\nRANSAC is an iterative algorithm for the robust estimation of parameters\nfrom a subset of inliers from the complete data set.\n\nRead more in the :ref:`User Guide <ransac_regression>`.\n\nParameters\n----------\nestimator : object, default=None\n    Base estimator object which implements the following methods:\n\n     * `fit(X, y)`: Fit model to given training data and target values.\n     * `score(X, y)`: Returns the mean accuracy on the given test data,\n       which is used for the stop criterion defined by `stop_score`.\n       Additionally, the score is used to decide which of two equally\n       large consensus sets is chosen as the better one.\n     * `predict(X)`: Returns predicted values using the linear model,\n       which is used to compute residual error using loss function.\n\n    If `estimator` is None, then\n    :class:`~sklearn.linear_model.LinearRegression` is used for\n    target values of dtype float.\n\n    Note that the current implementation only supports regression\n    estimators.\n\nmin_samples : int (>= 1) or float ([0, 1]), default=None\n    Minimum number of samples chosen randomly from original data. Treated\n    as an absolute number of samples for `min_samples >= 1`, treated as a\n    relative number `ceil(min_samples * X.shape[0])` for\n    `min_samples < 1`. This is typically chosen as the minimal number of\n    samples necessary to estimate the given `estimator`. By default a\n    :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and\n    `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly\n    dependent upon the model, so if a `estimator` other than\n    :class:`~sklearn.linear_model.LinearRegression` is used, the user must\n    provide a value.\n\nresidual_threshold : float, default=None\n    Maximum residual for a data sample to be classified as an inlier.\n    By default the threshold is chosen as the MAD (median absolute\n    deviation) of the target values `y`. Points whose residuals are\n    strictly equal to the threshold are considered as inliers.\n\nis_data_valid : callable, default=None\n    This function is called with the randomly selected data before the\n    model is fitted to it: `is_data_valid(X, y)`. If its return value is\n    False the current randomly chosen sub-sample is skipped.\n\nis_model_valid : callable, default=None\n    This function is called with the estimated model and the randomly\n    selected data: `is_model_valid(model, X, y)`. If its return value is\n    False the current randomly chosen sub-sample is skipped.\n    Rejecting samples with this function is computationally costlier than\n    with `is_data_valid`. `is_model_valid` should therefore only be used if\n    the estimated model is needed for making the rejection decision.\n\nmax_trials : int, default=100\n    Maximum number of iterations for random sample selection.\n\nmax_skips : int, default=np.inf\n    Maximum number of iterations that can be skipped due to finding zero\n    inliers or invalid data defined by ``is_data_valid`` or invalid models\n    defined by ``is_model_valid``.\n\n    .. versionadded:: 0.19\n\nstop_n_inliers : int, default=np.inf\n    Stop iteration if at least this number of inliers are found.\n\nstop_score : float, default=np.inf\n    Stop iteration if score is greater equal than this threshold.\n\nstop_probability : float in range [0, 1], default=0.99\n    RANSAC iteration stops if at least one outlier-free set of the training\n    data is sampled in RANSAC. This requires to generate at least N\n    samples (iterations)::\n\n        N >= log(1 - probability) / log(1 - e**m)\n\n    where the probability (confidence) is typically set to high value such\n    as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n    the total number of samples.\n\nloss : str, callable, default='absolute_error'\n    String inputs, 'absolute_error' and 'squared_error' are supported which\n    find the absolute error and squared error per sample respectively.\n\n    If ``loss`` is a callable, then it should be a function that takes\n    two arrays as inputs, the true and predicted value and returns a 1-D\n    array with the i-th value of the array corresponding to the loss\n    on ``X[i]``.\n\n    If the loss on a sample is greater than the ``residual_threshold``,\n    then this sample is classified as an outlier.\n\n    .. versionadded:: 0.18\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to initialize the centers.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nestimator_ : object\n    Best fitted model (copy of the `estimator` object).\n\nn_trials_ : int\n    Number of random selection trials until one of the stop criteria is\n    met. It is always ``<= max_trials``.\n\ninlier_mask_ : bool array of shape [n_samples]\n    Boolean mask of inliers classified as ``True``.\n\nn_skips_no_inliers_ : int\n    Number of iterations skipped due to finding zero inliers.\n\n    .. versionadded:: 0.19\n\nn_skips_invalid_data_ : int\n    Number of iterations skipped due to invalid data defined by\n    ``is_data_valid``.\n\n    .. versionadded:: 0.19\n\nn_skips_invalid_model_ : int\n    Number of iterations skipped due to an invalid model defined by\n    ``is_model_valid``.\n\n    .. versionadded:: 0.19\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nHuberRegressor : Linear regression model that is robust to outliers.\nTheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\nSGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/RANSAC\n.. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf\n.. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n\nExamples\n--------\n>>> from sklearn.linear_model import RANSACRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = RANSACRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9885...\n>>> reg.predict(X[:1,])\narray([-31.9417...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Recursive feature elimination with cross-validation to select features.\n\nThe number of features selected is tuned automatically by fitting an :class:`RFE`\nselector on the different cross-validation splits (provided by the `cv` parameter).\nThe performance of the :class:`RFE` selector are evaluated using `scorer` for\ndifferent number of selected features and aggregated together. Finally, the scores\nare averaged across folds and the number of features selected is set to the number\nof features that maximize the cross-validation score.\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.\n\nParameters\n----------\nestimator : ``Estimator`` instance\n    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute.\n\nstep : int or float, default=1\n    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n    Note that the last iteration may remove fewer than ``step`` features in\n    order to reach ``min_features_to_select``.\n\nmin_features_to_select : int, default=1\n    The minimum number of features to be selected. This number of features\n    will always be scored, even if the difference between the original\n    feature count and ``min_features_to_select`` isn't divisible by\n    ``step``.\n\n    .. versionadded:: 0.20\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n    estimator is a classifier or if ``y`` is neither binary nor multiclass,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value of None changed from 3-fold to 5-fold.\n\nscoring : str, callable or None, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nverbose : int, default=0\n    Controls verbosity of output.\n\nn_jobs : int or None, default=None\n    Number of cores to run in parallel while fitting across folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance.\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. Only available when `estimator` is a classifier.\n\nestimator_ : ``Estimator`` instance\n    The fitted estimator used to select features.\n\ncv_results_ : dict of ndarrays\n    A dict with keys:\n\n    split(k)_test_score : ndarray of shape (n_subsets_of_features,)\n        The cross-validation scores across (k)th fold.\n\n    mean_test_score : ndarray of shape (n_subsets_of_features,)\n        Mean of scores over the folds.\n\n    std_test_score : ndarray of shape (n_subsets_of_features,)\n        Standard deviation of scores over the folds.\n\n    .. versionadded:: 1.0\n\nn_features_ : int\n    The number of selected features with cross-validation.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nranking_ : narray of shape (n_features,)\n    The feature ranking, such that `ranking_[i]`\n    corresponds to the ranking\n    position of the i-th feature.\n    Selected (i.e., estimated best)\n    features are assigned rank 1.\n\nsupport_ : ndarray of shape (n_features,)\n    The mask of selected features.\n\nSee Also\n--------\nRFE : Recursive feature elimination.\n\nNotes\n-----\nThe size of all values in ``cv_results_`` is equal to\n``ceil((n_features - min_features_to_select) / step) + 1``,\nwhere step is the number of features removed at each iteration.\n\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.\n\nExamples\n--------\nThe following example shows how to retrieve the a-priori not known 5\ninformative features in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFECV\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFECV(estimator, step=1, cv=5)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.\n\nParameters\n----------\nestimator : ``Estimator`` instance\n    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance\n    (e.g. `coef_`, `feature_importances_`).\n\nn_features_to_select : int or float, default=None\n    The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\n    .. versionchanged:: 0.24\n       Added float values for fractions.\n\nstep : int or float, default=1\n    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n\nverbose : int, default=0\n    Controls verbosity of output.\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. Only available when `estimator` is a classifier.\n\nestimator_ : ``Estimator`` instance\n    The fitted estimator used to select features.\n\nn_features_ : int\n    The number of selected features.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nranking_ : ndarray of shape (n_features,)\n    The feature ranking, such that ``ranking_[i]`` corresponds to the\n    ranking position of the i-th feature. Selected (i.e., estimated\n    best) features are assigned rank 1.\n\nsupport_ : ndarray of shape (n_features,)\n    The mask of selected features.\n\nSee Also\n--------\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.\n\nNotes\n-----\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.\n\nExamples\n--------\nThe following example shows how to retrieve the 5 most informative\nfeatures in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFE\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFE(estimator, n_features_to_select=5, step=1)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Classifier implementing a vote among neighbors within a given radius.\n\nRead more in the :ref:`User Guide <classification>`.\n\nParameters\n----------\nradius : float, default=1.0\n    Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries.\n\nweights : {'uniform', 'distance'}, callable or None, default='uniform'\n    Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : float, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\noutlier_label : {manual label, 'most_frequent'}, default=None\n    Label for outlier samples (samples with no neighbors in given radius).\n\n    - manual label: str or int label (should be the same type as y)\n      or list of manual labels if multi-output is used.\n    - 'most_frequent' : assign the most frequent label of y to outliers.\n    - None : when any outlier is detected, ValueError will be raised.\n\n    The outlier label should be selected from among the unique 'Y' labels.\n    If it is specified with a different value a warning will be raised and\n    all class probabilities of outliers will be assigned to be 0.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier.\n\neffective_metric_ : str or callable\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\noutlier_label_ : int or array-like of shape (n_class,)\n    Label which is given for outlier samples (samples with no neighbors\n    on given radius).\n\noutputs_2d_ : bool\n    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n    otherwise True.\n\nSee Also\n--------\nKNeighborsClassifier : Classifier implementing the k-nearest neighbors\n    vote.\nRadiusNeighborsRegressor : Regression based on neighbors within a\n    fixed radius.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nNearestNeighbors : Unsupervised learner for implementing neighbor\n    searches.\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsClassifier\n>>> neigh = RadiusNeighborsClassifier(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsClassifier(...)\n>>> print(neigh.predict([[1.5]]))\n[0]\n>>> print(neigh.predict_proba([[1.0]]))\n[[0.66666667 0.33333333]]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Regression based on neighbors within a fixed radius.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\nradius : float, default=1.0\n    Range of parameter space to use by default for :meth:`radius_neighbors`\n    queries.\n\nweights : {'uniform', 'distance'}, callable or None, default='uniform'\n    Weight function used in prediction.  Possible values:\n\n    - 'uniform' : uniform weights.  All points in each neighborhood\n      are weighted equally.\n    - 'distance' : weight points by the inverse of their distance.\n      in this case, closer neighbors of a query point will have a\n      greater influence than neighbors which are further away.\n    - [callable] : a user-defined function which accepts an\n      array of distances, and returns an array of the same shape\n      containing the weights.\n\n    Uniform weights are used by default.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\np : float, default=2\n    Power parameter for the Minkowski metric. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit. X may be a :term:`sparse graph`, in which\n    case only \"nonzero\" elements may be considered neighbors.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric to use. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nSee Also\n--------\nNearestNeighbors : Unsupervised learner for implementing neighbor searches.\nKNeighborsRegressor : Regression based on k-nearest neighbors.\nKNeighborsClassifier : Classifier based on the k-nearest neighbors.\nRadiusNeighborsClassifier : Classifier based on neighbors within a given radius.\n\nNotes\n-----\nSee :ref:`Nearest Neighbors <neighbors>` in the online documentation\nfor a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\nhttps://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n\nExamples\n--------\n>>> X = [[0], [1], [2], [3]]\n>>> y = [0, 0, 1, 1]\n>>> from sklearn.neighbors import RadiusNeighborsRegressor\n>>> neigh = RadiusNeighborsRegressor(radius=1.0)\n>>> neigh.fit(X, y)\nRadiusNeighborsRegressor(...)\n>>> print(neigh.predict([[1.5]]))\n[0.5]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transform X into a (weighted) graph of neighbors nearer than a radius.\n\nThe transformed data is a sparse graph as returned by\n`radius_neighbors_graph`.\n\nRead more in the :ref:`User Guide <neighbors_transformer>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nmode : {'distance', 'connectivity'}, default='distance'\n    Type of returned matrix: 'connectivity' will return the connectivity\n    matrix with ones and zeros, and 'distance' will return the distances\n    between neighbors according to the given metric.\n\nradius : float, default=1.0\n    Radius of neighborhood in the transformed sparse graph.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree.  This can affect the\n    speed of the construction and query, as well as the memory\n    required to store the tree.  The optimal value depends on the\n    nature of the problem.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2. See the\n    documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in\n    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n    values.\n\n    If metric is a callable function, it takes two arrays representing 1D\n    vectors as inputs and must return one value indicating the distance\n    between those vectors. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string.\n\n    Distance matrices are not supported.\n\np : float, default=2\n    Parameter for the Minkowski metric from\n    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    This parameter is expected to be positive.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    If ``-1``, then the number of jobs is set to the number of CPU cores.\n\nAttributes\n----------\neffective_metric_ : str or callable\n    The distance metric used. It will be same as the `metric` parameter\n    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n    'minkowski' and `p` parameter set to 2.\n\neffective_metric_params_ : dict\n    Additional keyword arguments for the metric function. For most metrics\n    will be same with `metric_params` parameter, but may also contain the\n    `p` parameter value if the `effective_metric_` attribute is set to\n    'minkowski'.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_fit_ : int\n    Number of samples in the fitted data.\n\nSee Also\n--------\nkneighbors_graph : Compute the weighted graph of k-neighbors for\n    points in X.\nKNeighborsTransformer : Transform X into a weighted graph of k\n    nearest neighbors.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import load_wine\n>>> from sklearn.cluster import DBSCAN\n>>> from sklearn.neighbors import RadiusNeighborsTransformer\n>>> from sklearn.pipeline import make_pipeline\n>>> X, _ = load_wine(return_X_y=True)\n>>> estimator = make_pipeline(\n...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n...     DBSCAN(eps=25.0, metric='precomputed'))\n>>> X_clustered = estimator.fit_predict(X)\n>>> clusters, counts = np.unique(X_clustered, return_counts=True)\n>>> print(counts)\n[ 29  15 111  11  12]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Rand index.\n\nThe Rand Index computes a similarity measure between two clusterings\nby considering all pairs of samples and counting pairs that are\nassigned in the same or different clusters in the predicted and\ntrue clusterings [1]_ [2]_.\n\nThe raw RI score [3]_ is:\n\n    RI = (number of agreeing pairs) / (number of pairs)\n\nRead more in the :ref:`User Guide <rand_score>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,), dtype=integral\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,), dtype=integral\n    Cluster labels to evaluate.\n\nReturns\n-------\nRI : float\n   Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n   perfect match.\n\nSee Also\n--------\nadjusted_rand_score: Adjusted Rand Score.\nadjusted_mutual_info_score: Adjusted Mutual Information.\n\nReferences\n----------\n.. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n   Journal of Classification 2, 193‚Äì218 (1985).\n   <10.1007/BF01908075>`.\n\n.. [2] `Wikipedia: Simple Matching Coefficient\n    <https://en.wikipedia.org/wiki/Simple_matching_coefficient>`_\n\n.. [3] `Wikipedia: Rand Index <https://en.wikipedia.org/wiki/Rand_index>`_\n\nExamples\n--------\nPerfectly matching labelings have a score of 1 even\n\n  >>> from sklearn.metrics.cluster import rand_score\n  >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nLabelings that assign all classes members to the same clusters\nare complete but may not always be pure, hence penalized:\n\n  >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n  0.83..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nTrees in the forest use the best split strategy, i.e. equivalent to passing\n`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nFor a comparison between tree-based ensemble models see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes > 2`),\n      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n    tree classifiers.\nsklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n    Boosting Classification Tree, very fast for big datasets (n_samples >=\n    10_000).\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n>>> clf.fit(X, y)\nRandomForestClassifier(...)\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "A random forest regressor.\n\nA random forest is a meta estimator that fits a number of decision tree\nregressors on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nTrees in the forest use the best split strategy, i.e. equivalent to passing\n`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nFor a comparison between tree-based ensemble models see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n    The function to measure the quality of a split. Supported criteria\n    are \"squared_error\" for the mean squared error, which is equal to\n    variance reduction as feature selection criterion and minimizes the L2\n    loss using the mean of each terminal node, \"friedman_mse\", which uses\n    mean squared error with Friedman's improvement score for potential\n    splits, \"absolute_error\" for the mean absolute error, which minimizes\n    the L1 loss using the median of each terminal node, and \"poisson\" which\n    uses reduction in Poisson deviance to find splits.\n    Training using \"absolute_error\" is significantly slower\n    than when using \"squared_error\".\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\n    .. versionadded:: 1.0\n       Poisson criterion.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None or 1.0, then `max_features=n_features`.\n\n    .. note::\n        The default of 1.0 is equivalent to bagged trees and more\n        randomness can be achieved by setting smaller values, e.g. 0.3.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to 1.0.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.r2_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonically increasing\n      - 0: no constraint\n      - -1: monotonically decreasing\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multioutput regressions (i.e. when `n_outputs_ > 1`),\n      - regressions trained on data with missing values.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of DecisionTreeRegressor\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n    Prediction computed with out-of-bag estimate on the training set.\n    This attribute exists only when ``oob_score`` is True.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nsklearn.tree.DecisionTreeRegressor : A decision tree regressor.\nsklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n    tree regressors.\nsklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient\n    Boosting Regression Tree, very fast for big datasets (n_samples >=\n    10_000).\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nThe default value ``max_features=1.0`` uses ``n_features``\nrather than ``n_features / 3``. The latter was originally suggested in\n[1], whereas the former was more recently justified empirically in [2].\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n.. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n       trees\", Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, n_informative=2,\n...                        random_state=0, shuffle=False)\n>>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n>>> regr.fit(X, y)\nRandomForestRegressor(...)\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-8.32987858]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "An ensemble of totally random trees.\n\nAn unsupervised transformation of a dataset to a high-dimensional\nsparse representation. A datapoint is coded according to which leaf of\neach tree it is sorted into. Using a one-hot encoding of the leaves,\nthis leads to a binary coding with as many ones as there are trees in\nthe forest.\n\nThe dimensionality of the resulting representation is\n``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\nthe number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\nRead more in the :ref:`User Guide <random_trees_embedding>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    Number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\nmax_depth : int, default=5\n    The maximum depth of each tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` is the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` is the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nsparse_output : bool, default=True\n    Whether or not to return a sparse CSR matrix, as default behavior,\n    or to return a dense array compatible with dense pipeline operators.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the generation of the random `y` used to fit the trees\n    and the draw of the splits for each feature at the trees' nodes.\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The feature importances (the higher, the more important the feature).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\none_hot_encoder_ : OneHotEncoder instance\n    One-hot encoder used to create the sparse embedding.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nExtraTreesClassifier : An extra-trees classifier.\nExtraTreesRegressor : An extra-trees regressor.\nRandomForestClassifier : A random forest classifier.\nRandomForestRegressor : A random forest regressor.\nsklearn.tree.ExtraTreeClassifier: An extremely randomized\n    tree classifier.\nsklearn.tree.ExtraTreeRegressor : An extremely randomized\n    tree regressor.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n.. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n       visual codebooks using randomized clustering forests\"\n       NIPS 2007\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomTreesEmbedding\n>>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n>>> random_trees = RandomTreesEmbedding(\n...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n>>> X_sparse_embedding = random_trees.transform(X)\n>>> X_sparse_embedding.toarray()\narray([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Randomized search on hyper parameters.\n\nRandomizedSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"score_samples\", \"predict\", \"predict_proba\",\n\"decision_function\", \"transform\" and \"inverse_transform\" if they are\nimplemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated search over parameter settings.\n\nIn contrast to GridSearchCV, not all parameter values are tried out, but\nrather a fixed number of parameter settings is sampled from the specified\ndistributions. The number of parameter settings that are tried is\ngiven by n_iter.\n\nIf all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nRead more in the :ref:`User Guide <randomized_parameter_search>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nestimator : estimator object\n    An object of that type is instantiated for each grid point.\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_distributions : dict or list of dicts\n    Dictionary with parameters names (`str`) as keys and distributions\n    or lists of parameters to try. Distributions must provide a ``rvs``\n    method for sampling (such as those from scipy.stats.distributions).\n    If a list is given, it is sampled uniformly.\n    If a list of dicts is given, first a dict is sampled uniformly, and\n    then a parameter is sampled using that dict as above.\n\nn_iter : int, default=10\n    Number of parameter settings that are sampled. n_iter trades\n    off runtime vs quality of the solution.\n\nscoring : str, callable, list, tuple or dict, default=None\n    Strategy to evaluate the performance of the cross-validated model on\n    the test set.\n\n    If `scoring` represents a single score, one can use:\n\n    - a single string (see :ref:`scoring_parameter`);\n    - a callable (see :ref:`scoring`) that returns a single value.\n\n    If `scoring` represents multiple scores, one can use:\n\n    - a list or tuple of unique strings;\n    - a callable returning a dictionary where the keys are the metric\n      names and the values are the metric scores;\n    - a dictionary with metric names as keys and callables a values.\n\n    See :ref:`multimetric_grid_search` for an example.\n\n    If None, the estimator's score method is used.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nrefit : bool, str, or callable, default=True\n    Refit an estimator using the best found parameters on the whole\n    dataset.\n\n    For multiple metric evaluation, this needs to be a `str` denoting the\n    scorer that would be used to find the best parameters for refitting\n    the estimator at the end.\n\n    Where there are considerations other than maximum score in\n    choosing a best estimator, ``refit`` can be set to a function which\n    returns the selected ``best_index_`` given the ``cv_results``. In that\n    case, the ``best_estimator_`` and ``best_params_`` will be set\n    according to the returned ``best_index_`` while the ``best_score_``\n    attribute will not be available.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``RandomizedSearchCV`` instance.\n\n    Also for multiple metric evaluation, the attributes ``best_index_``,\n    ``best_score_`` and ``best_params_`` will only be available if\n    ``refit`` is set and all of them will be determined w.r.t this specific\n    scorer.\n\n    See ``scoring`` parameter to know more about multiple metric\n    evaluation.\n\n    .. versionchanged:: 0.20\n        Support for callable added.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used. These splitters are instantiated\n    with `shuffle=False` so the splits will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\n    - >1 : the computation time for each fold and parameter candidate is\n      displayed;\n    - >2 : the score is also displayed;\n    - >3 : the fold and candidate parameter indexes are also displayed\n      together with the starting time of the computation.\n\npre_dispatch : int, or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n        - None, in which case all the jobs are immediately\n          created and spawned. Use this for lightweight and\n          fast-running jobs, to avoid delays due to on-demand\n          spawning of the jobs\n\n        - An int, giving the exact number of total jobs that are\n          spawned\n\n        - A str, giving an expression as a function of n_jobs,\n          as in '2*n_jobs'\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo random number generator state used for random uniform sampling\n    from lists of possible values instead of scipy.stats distributions.\n    Pass an int for reproducible output across multiple\n    function calls.\n    See :term:`Glossary <random_state>`.\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\n    .. versionadded:: 0.19\n\n    .. versionchanged:: 0.21\n        Default value was changed from ``True`` to ``False``\n\nAttributes\n----------\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``.\n\n    For instance the below given table\n\n    +--------------+-------------+-------------------+---+---------------+\n    | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n    +==============+=============+===================+===+===============+\n    |    'rbf'     |     0.1     |       0.80        |...|       1       |\n    +--------------+-------------+-------------------+---+---------------+\n    |    'rbf'     |     0.2     |       0.84        |...|       3       |\n    +--------------+-------------+-------------------+---+---------------+\n    |    'rbf'     |     0.3     |       0.70        |...|       2       |\n    +--------------+-------------+-------------------+---+---------------+\n\n    will be represented by a ``cv_results_`` dict of::\n\n        {\n        'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n                                      mask = False),\n        'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n        'split0_test_score'  : [0.80, 0.84, 0.70],\n        'split1_test_score'  : [0.82, 0.50, 0.70],\n        'mean_test_score'    : [0.81, 0.67, 0.70],\n        'std_test_score'     : [0.01, 0.24, 0.00],\n        'rank_test_score'    : [1, 3, 2],\n        'split0_train_score' : [0.80, 0.92, 0.70],\n        'split1_train_score' : [0.82, 0.55, 0.70],\n        'mean_train_score'   : [0.81, 0.74, 0.70],\n        'std_train_score'    : [0.01, 0.19, 0.00],\n        'mean_fit_time'      : [0.73, 0.63, 0.43],\n        'std_fit_time'       : [0.01, 0.02, 0.01],\n        'mean_score_time'    : [0.01, 0.06, 0.04],\n        'std_score_time'     : [0.00, 0.00, 0.00],\n        'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n        }\n\n    NOTE\n\n    The key ``'params'`` is used to store a list of parameter\n    settings dicts for all the parameter candidates.\n\n    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n    ``std_score_time`` are all in seconds.\n\n    For multi-metric evaluation, the scores for all the scorers are\n    available in the ``cv_results_`` dict at the keys ending with that\n    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n    above. ('split0_test_precision', 'mean_train_precision' etc.)\n\nbest_estimator_ : estimator\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\n    For multi-metric evaluation, this attribute is present only if\n    ``refit`` is specified.\n\n    See ``refit`` parameter for more information on allowed values.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator.\n\n    For multi-metric evaluation, this is not available if ``refit`` is\n    ``False``. See ``refit`` parameter for more information.\n\n    This attribute is not available if ``refit`` is a function.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\n    For multi-metric evaluation, this is not available if ``refit`` is\n    ``False``. See ``refit`` parameter for more information.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\n    For multi-metric evaluation, this is not available if ``refit`` is\n    ``False``. See ``refit`` parameter for more information.\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\n    For multi-metric evaluation, this attribute holds the validated\n    ``scoring`` dict which maps the scorer key to the scorer callable.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\n    .. versionadded:: 0.20\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. This is present only if ``refit`` is specified and\n    the underlying estimator is a classifier.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `n_features_in_` when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `feature_names_in_` when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nGridSearchCV : Does exhaustive search over a grid of parameters.\nParameterSampler : A generator over parameter settings, constructed from\n    param_distributions.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the held-out\ndata, according to the scoring parameter.\n\nIf `n_jobs` was set to a value higher than one, the data is copied for each\nparameter setting(and not `n_jobs` times). This is done for efficiency\nreasons if individual jobs take very little time, but may raise errors if\nthe dataset is large and not enough memory is available.  A workaround in\nthis case is to set `pre_dispatch`. Then, the memory is copied only\n`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\nn_jobs`.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import RandomizedSearchCV\n>>> from scipy.stats import uniform\n>>> iris = load_iris()\n>>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n...                               random_state=0)\n>>> distributions = dict(C=uniform(loc=0, scale=4),\n...                      penalty=['l2', 'l1'])\n>>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n>>> search = clf.fit(iris.data, iris.target)\n>>> search.best_params_\n{'C': 2..., 'penalty': 'l1'}"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the recall.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe best value is 1 and the worst value is 0.\n\nSupport beyond term:`binary` targets is achieved by treating :term:`multiclass`\nand :term:`multilabel` data as a collection of binary problems, one for each\nlabel. For the :term:`binary` case, setting `average='binary'` will return\nrecall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\nand recall for both classes are computed then averaged or both returned (when\n`average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\nrecall for all `labels` are either returned or averaged depending on the `average`\nparameter. Use `labels` specify the set of labels to calculate recall for.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Estimated targets as returned by a classifier.\n\nlabels : array-like, default=None\n    The set of labels to include when `average != 'binary'`, and their\n    order if `average is None`. Labels present in the data can be\n    excluded, for example in multiclass classification to exclude a \"negative\n    class\". Labels not present in the data can be included and will be\n    \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n    By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\n    .. versionchanged:: 0.17\n       Parameter `labels` improved for multiclass problem.\n\npos_label : int, float, bool or str, default=1\n    The class to report if `average='binary'` and the data is binary,\n    otherwise this parameter is ignored.\n    For multiclass or multilabel targets, set `labels=[pos_label]` and\n    `average != 'binary'` to report metrics for one label only.\n\naverage : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n    This parameter is required for multiclass/multilabel targets.\n    If ``None``, the scores for each class are returned. Otherwise, this\n    determines the type of averaging performed on the data:\n\n    ``'binary'``:\n        Only report results for the class specified by ``pos_label``.\n        This is applicable only if targets (``y_{true,pred}``) are binary.\n    ``'micro'``:\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This\n        alters 'macro' to account for label imbalance; it can result in an\n        F-score that is not between precision and recall. Weighted recall\n        is equal to accuracy.\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average (only\n        meaningful for multilabel classification where this differs from\n        :func:`accuracy_score`).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nzero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n    Sets the value to return when there is a zero division.\n\n    Notes:\n    - If set to \"warn\", this acts like 0, but a warning is also raised.\n    - If set to `np.nan`, such values will be excluded from the average.\n\n    .. versionadded:: 1.3\n       `np.nan` option was added.\n\nReturns\n-------\nrecall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n    Recall of the positive class in binary classification or weighted\n    average of the recall of each class for the multiclass task.\n\nSee Also\n--------\nprecision_recall_fscore_support : Compute precision, recall, F-measure and\n    support for each class.\nprecision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n    number of true positives and ``fp`` the number of false positives.\nbalanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n    datasets.\nmultilabel_confusion_matrix : Compute a confusion matrix for each class or\n    sample.\nPrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n    an estimator and some data.\nPrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n    binary class predictions.\n\nNotes\n-----\nWhen ``true positive + false negative == 0``, recall returns 0 and raises\n``UndefinedMetricWarning``. This behavior can be modified with\n``zero_division``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import recall_score\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> recall_score(y_true, y_pred, average='macro')\n0.33...\n>>> recall_score(y_true, y_pred, average='micro')\n0.33...\n>>> recall_score(y_true, y_pred, average='weighted')\n0.33...\n>>> recall_score(y_true, y_pred, average=None)\narray([1., 0., 0.])\n>>> y_true = [0, 0, 0, 0, 0, 0]\n>>> recall_score(y_true, y_pred, average=None)\narray([0.5, 0. , 0. ])\n>>> recall_score(y_true, y_pred, average=None, zero_division=1)\narray([0.5, 1. , 1. ])\n>>> recall_score(y_true, y_pred, average=None, zero_division=np.nan)\narray([0.5, nan, nan])\n\n>>> # multilabel classification\n>>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n>>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n>>> recall_score(y_true, y_pred, average=None)\narray([1. , 1. , 0.5])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Repeated K-Fold cross validator.\n\nRepeats K-Fold n times with different randomization in each repetition.\n\nRead more in the :ref:`User Guide <repeated_k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\nn_repeats : int, default=10\n    Number of times cross-validator needs to be repeated.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of each repeated cross-validation instance.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import RepeatedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n>>> rkf.get_n_splits(X, y)\n4\n>>> print(rkf)\nRepeatedKFold(n_repeats=2, n_splits=2, random_state=2652124)\n>>> for i, (train_index, test_index) in enumerate(rkf.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\n...\nFold 0:\n  Train: index=[0 1]\n  Test:  index=[2 3]\nFold 1:\n  Train: index=[2 3]\n  Test:  index=[0 1]\nFold 2:\n  Train: index=[1 2]\n  Test:  index=[0 3]\nFold 3:\n  Train: index=[0 3]\n  Test:  index=[1 2]\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nRepeatedStratifiedKFold : Repeats Stratified K-Fold n times."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Repeated Stratified K-Fold cross validator.\n\nRepeats Stratified K-Fold n times with different randomization in each\nrepetition.\n\nRead more in the :ref:`User Guide <repeated_k_fold>`.\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\nn_repeats : int, default=10\n    Number of times cross-validator needs to be repeated.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the generation of the random states for each repetition.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import RepeatedStratifiedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n...     random_state=36851234)\n>>> rskf.get_n_splits(X, y)\n4\n>>> print(rskf)\nRepeatedStratifiedKFold(n_repeats=2, n_splits=2, random_state=36851234)\n>>> for i, (train_index, test_index) in enumerate(rskf.split(X, y)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\n...\nFold 0:\n  Train: index=[1 2]\n  Test:  index=[0 3]\nFold 1:\n  Train: index=[0 3]\n  Test:  index=[1 2]\nFold 2:\n  Train: index=[1 3]\n  Test:  index=[0 2]\nFold 3:\n  Train: index=[0 2]\n  Test:  index=[1 3]\n\nNotes\n-----\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nRepeatedKFold : Repeats K-Fold n times."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Ridge regression with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nBy default, it performs efficient Leave-One-Out Cross-Validation.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n    Array of alpha values to try.\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n    If using Leave-One-Out cross-validation, alphas must be positive.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nscoring : str, callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n    If None, the negative mean squared error if cv is 'auto' or None\n    (i.e. when using leave-one-out cross-validation), and r2 score\n    otherwise.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the efficient Leave-One-Out cross-validation\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\ngcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n    Flag indicating which strategy to use when performing\n    Leave-One-Out Cross-Validation. Options are::\n\n        'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n        'svd' : force use of singular value decomposition of X when X is\n            dense, eigenvalue decomposition of X^T.X when X is sparse.\n        'eigen' : force computation via eigendecomposition of X.X^T\n\n    The 'auto' mode is the default and is intended to pick the cheaper\n    option of the two depending on the shape of the training data.\n\nstore_cv_values : bool, default=False\n    Flag indicating if the cross-validation values corresponding to\n    each alpha should be stored in the ``cv_values_`` attribute (see\n    below). This flag is only compatible with ``cv=None`` (i.e. using\n    Leave-One-Out Cross-Validation).\n\nalpha_per_target : bool, default=False\n    Flag indicating whether to optimize the alpha value (picked from the\n    `alphas` parameter list) for each target separately (for multi-output\n    settings: multiple prediction targets). When set to `True`, after\n    fitting, the `alpha_` attribute will contain a value for each target.\n    When set to `False`, a single alpha is used for all targets.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncv_values_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\n    Cross-validation values for each alpha (only available if\n    ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n    called, this attribute will contain the mean squared errors if\n    `scoring is None` otherwise it will contain standardized per point\n    prediction values.\n\ncoef_ : ndarray of shape (n_features) or (n_targets, n_features)\n    Weight vector(s).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nalpha_ : float or ndarray of shape (n_targets,)\n    Estimated regularization parameter, or, if ``alpha_per_target=True``,\n    the estimated regularization parameter for each target.\n\nbest_score_ : float or ndarray of shape (n_targets,)\n    Score of base estimator with best alpha, or, if\n    ``alpha_per_target=True``, a score for each target.\n\n    .. versionadded:: 0.23\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\nRidgeClassifierCV : Ridge classifier with built-in cross validation.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> X, y = load_diabetes(return_X_y=True)\n>>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.5166..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Ridge classifier with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nBy default, it performs Leave-One-Out Cross-Validation. Currently,\nonly the n_features > n_samples case is handled efficiently.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n    Array of alpha values to try.\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nscoring : str, callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the efficient Leave-One-Out cross-validation\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nstore_cv_values : bool, default=False\n    Flag indicating if the cross-validation values corresponding to\n    each alpha should be stored in the ``cv_values_`` attribute (see\n    below). This flag is only compatible with ``cv=None`` (i.e. using\n    Leave-One-Out Cross-Validation).\n\nAttributes\n----------\ncv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n    Cross-validation values for each alpha (only if ``store_cv_values=True`` and\n    ``cv=None``). After ``fit()`` has been called, this attribute will\n    contain the mean squared errors if `scoring is None` otherwise it\n    will contain standardized per point prediction values.\n\ncoef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n    Coefficient of the features in the decision function.\n\n    ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nalpha_ : float\n    Estimated regularization parameter.\n\nbest_score_ : float\n    Score of base estimator with best alpha.\n\n    .. versionadded:: 0.23\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifier : Ridge classifier.\nRidgeCV : Ridge regression with built-in cross validation.\n\nNotes\n-----\nFor multi-class classification, n_class classifiers are trained in\na one-versus-all approach. Concretely, this is implemented by taking\nadvantage of the multi-variate response support in Ridge.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import RidgeClassifierCV\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.9630..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Classifier using Ridge regression.\n\nThis classifier first converts the target values into ``{-1, 1}`` and\nthen treats the problem as a regression task (multi-output regression in\nthe multiclass case).\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set to false, no\n    intercept will be used in calculations (e.g. data is expected to be\n    already centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=None\n    Maximum number of iterations for conjugate gradient solver.\n    The default value is determined by scipy.sparse.linalg.\n\ntol : float, default=1e-4\n    The precision of the solution (`coef_`) is determined by `tol` which\n    specifies a different convergence criterion for each solver:\n\n    - 'svd': `tol` has no impact.\n\n    - 'cholesky': `tol` has no impact.\n\n    - 'sparse_cg': norm of residuals smaller than `tol`.\n\n    - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n      which control the norm of the residual vector in terms of the norms of\n      matrix and coefficients.\n\n    - 'sag' and 'saga': relative change of coef smaller than `tol`.\n\n    - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n      smaller than `tol`.\n\n    .. versionchanged:: 1.2\n       Default value changed from 1e-3 to 1e-4 for consistency with other linear\n       models.\n\nclass_weight : dict or 'balanced', default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n    Solver to use in the computational routines:\n\n    - 'auto' chooses the solver automatically based on the type of data.\n\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n      coefficients. It is the most stable solver, in particular more stable\n      for singular matrices than 'cholesky' at the cost of being slower.\n\n    - 'cholesky' uses the standard scipy.linalg.solve function to\n      obtain a closed-form solution.\n\n    - 'sparse_cg' uses the conjugate gradient solver as found in\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n      more appropriate than 'cholesky' for large-scale data\n      (possibility to set `tol` and `max_iter`).\n\n    - 'lsqr' uses the dedicated regularized least-squares routine\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n      procedure.\n\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n      its unbiased and more flexible version named SAGA. Both methods\n      use an iterative procedure, and are often faster than other solvers\n      when both n_samples and n_features are large. Note that 'sag' and\n      'saga' fast convergence is only guaranteed on features with\n      approximately the same scale. You can preprocess the data with a\n      scaler from sklearn.preprocessing.\n\n      .. versionadded:: 0.17\n         Stochastic Average Gradient descent solver.\n      .. versionadded:: 0.19\n         SAGA solver.\n\n    - 'lbfgs' uses L-BFGS-B algorithm implemented in\n      `scipy.optimize.minimize`. It can be used only when `positive`\n      is True.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n    Only 'lbfgs' solver is supported in this case.\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n    See :term:`Glossary <random_state>` for details.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n    Coefficient of the features in the decision function.\n\n    ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nn_iter_ : None or ndarray of shape (n_targets,)\n    Actual number of iterations for each target. Available only for\n    sag and lsqr solvers. Other solvers will return None.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifierCV :  Ridge classifier with built-in cross validation.\n\nNotes\n-----\nFor multi-class classification, n_class classifiers are trained in\na one-versus-all approach. Concretely, this is implemented by taking\nadvantage of the multi-variate response support in Ridge.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import RidgeClassifier\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = RidgeClassifier().fit(X, y)\n>>> clf.score(X, y)\n0.9595..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear least squares with l2 regularization.\n\nMinimizes the objective function::\n\n||y - Xw||^2_2 + alpha * ||w||^2_2\n\nThis model solves a regression model where the loss function is\nthe linear least squares function and regularization is given by\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalpha : {float, ndarray of shape (n_targets,)}, default=1.0\n    Constant that multiplies the L2 term, controlling regularization\n    strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n\n    When `alpha = 0`, the objective is equivalent to ordinary least\n    squares, solved by the :class:`LinearRegression` object. For numerical\n    reasons, using `alpha = 0` with the `Ridge` object is not advised.\n    Instead, you should use the :class:`LinearRegression` object.\n\n    If an array is passed, penalties are assumed to be specific to the\n    targets. Hence they must correspond in number.\n\nfit_intercept : bool, default=True\n    Whether to fit the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. ``X`` and ``y`` are expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nmax_iter : int, default=None\n    Maximum number of iterations for conjugate gradient solver.\n    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n    For 'lbfgs' solver, the default value is 15000.\n\ntol : float, default=1e-4\n    The precision of the solution (`coef_`) is determined by `tol` which\n    specifies a different convergence criterion for each solver:\n\n    - 'svd': `tol` has no impact.\n\n    - 'cholesky': `tol` has no impact.\n\n    - 'sparse_cg': norm of residuals smaller than `tol`.\n\n    - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n      which control the norm of the residual vector in terms of the norms of\n      matrix and coefficients.\n\n    - 'sag' and 'saga': relative change of coef smaller than `tol`.\n\n    - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n      smaller than `tol`.\n\n    .. versionchanged:: 1.2\n       Default value changed from 1e-3 to 1e-4 for consistency with other linear\n       models.\n\nsolver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n    Solver to use in the computational routines:\n\n    - 'auto' chooses the solver automatically based on the type of data.\n\n    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n      coefficients. It is the most stable solver, in particular more stable\n      for singular matrices than 'cholesky' at the cost of being slower.\n\n    - 'cholesky' uses the standard scipy.linalg.solve function to\n      obtain a closed-form solution.\n\n    - 'sparse_cg' uses the conjugate gradient solver as found in\n      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n      more appropriate than 'cholesky' for large-scale data\n      (possibility to set `tol` and `max_iter`).\n\n    - 'lsqr' uses the dedicated regularized least-squares routine\n      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n      procedure.\n\n    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n      its improved, unbiased version named SAGA. Both methods also use an\n      iterative procedure, and are often faster than other solvers when\n      both n_samples and n_features are large. Note that 'sag' and\n      'saga' fast convergence is only guaranteed on features with\n      approximately the same scale. You can preprocess the data with a\n      scaler from sklearn.preprocessing.\n\n    - 'lbfgs' uses L-BFGS-B algorithm implemented in\n      `scipy.optimize.minimize`. It can be used only when `positive`\n      is True.\n\n    All solvers except 'svd' support both dense and sparse data. However, only\n    'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n    `fit_intercept` is True.\n\n    .. versionadded:: 0.17\n       Stochastic Average Gradient descent solver.\n    .. versionadded:: 0.19\n       SAGA solver.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive.\n    Only 'lbfgs' solver is supported in this case.\n\nrandom_state : int, RandomState instance, default=None\n    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n    See :term:`Glossary <random_state>` for details.\n\n    .. versionadded:: 0.17\n       `random_state` to support Stochastic Average Gradient.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n    Weight vector(s).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nn_iter_ : None or ndarray of shape (n_targets,)\n    Actual number of iterations for each target. Available only for\n    sag and lsqr solvers. Other solvers will return None.\n\n    .. versionadded:: 0.17\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidgeClassifier : Ridge classifier.\nRidgeCV : Ridge regression with built-in cross validation.\n:class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n    combines ridge regression with the kernel trick.\n\nNotes\n-----\nRegularization improves the conditioning of the problem and\nreduces the variance of the estimates. Larger values specify stronger\nregularization. Alpha corresponds to ``1 / (2C)`` in other linear\nmodels such as :class:`~sklearn.linear_model.LogisticRegression` or\n:class:`~sklearn.svm.LinearSVC`.\n\nExamples\n--------\n>>> from sklearn.linear_model import Ridge\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> clf = Ridge(alpha=1.0)\n>>> clf.fit(X, y)\nRidge()"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Scale features using statistics that are robust to outliers.\n\nThis Scaler removes the median and scales the data according to\nthe quantile range (defaults to IQR: Interquartile Range).\nThe IQR is the range between the 1st quartile (25th quantile)\nand the 3rd quartile (75th quantile).\n\nCentering and scaling happen independently on each feature by\ncomputing the relevant statistics on the samples in the training\nset. Median and interquartile range are then stored to be used on\nlater data using the :meth:`transform` method.\n\nStandardization of a dataset is a common preprocessing for many machine\nlearning estimators. Typically this is done by removing the mean and\nscaling to unit variance. However, outliers can often influence the sample\nmean / variance in a negative way. In such cases, using the median and the\ninterquartile range often give better results. For an example visualization\nand comparison to other scalers, refer to :ref:`Compare RobustScaler with\nother scalers <plot_all_scaling_robust_scaler_section>`.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nwith_centering : bool, default=True\n    If `True`, center the data before scaling.\n    This will cause :meth:`transform` to raise an exception when attempted\n    on sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n\nwith_scaling : bool, default=True\n    If `True`, scale the data to interquartile range.\n\nquantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0)\n    Quantile range used to calculate `scale_`. By default this is equal to\n    the IQR, i.e., `q_min` is the first quantile and `q_max` is the third\n    quantile.\n\n    .. versionadded:: 0.18\n\ncopy : bool, default=True\n    If `False`, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n\nunit_variance : bool, default=False\n    If `True`, scale data so that normally distributed features have a\n    variance of 1. In general, if the difference between the x-values of\n    `q_max` and `q_min` for a standard normal distribution is greater\n    than 1, the dataset will be scaled down. If less than 1, the dataset\n    will be scaled up.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncenter_ : array of floats\n    The median value for each feature in the training set.\n\nscale_ : array of floats\n    The (scaled) interquartile range for each feature in the training set.\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nrobust_scale : Equivalent function without the estimator API.\nsklearn.decomposition.PCA : Further removes the linear correlation across\n    features with 'whiten=True'.\n\nNotes\n-----\n\nhttps://en.wikipedia.org/wiki/Median\nhttps://en.wikipedia.org/wiki/Interquartile_range\n\nExamples\n--------\n>>> from sklearn.preprocessing import RobustScaler\n>>> X = [[ 1., -2.,  2.],\n...      [ -2.,  1.,  3.],\n...      [ 4.,  1., -2.]]\n>>> transformer = RobustScaler().fit(X)\n>>> transformer\nRobustScaler()\n>>> transformer.transform(X)\narray([[ 0. , -2. ,  0. ],\n       [-1. ,  0. ,  0.4],\n       [ 1. ,  0. , -1.6]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\n\nNote: this implementation can be used with binary, multiclass and\nmultilabel classification, but some restrictions apply (see Parameters).\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n    True labels or binary label indicators. The binary and multiclass cases\n    expect labels with shape (n_samples,) while the multilabel case expects\n    binary label indicators with shape (n_samples, n_classes).\n\ny_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n    Target scores.\n\n    * In the binary case, it corresponds to an array of shape\n      `(n_samples,)`. Both probability estimates and non-thresholded\n      decision values can be provided. The probability estimates correspond\n      to the **probability of the class with the greater label**,\n      i.e. `estimator.classes_[1]` and thus\n      `estimator.predict_proba(X, y)[:, 1]`. The decision values\n      corresponds to the output of `estimator.decision_function(X, y)`.\n      See more information in the :ref:`User guide <roc_auc_binary>`;\n    * In the multiclass case, it corresponds to an array of shape\n      `(n_samples, n_classes)` of probability estimates provided by the\n      `predict_proba` method. The probability estimates **must**\n      sum to 1 across the possible classes. In addition, the order of the\n      class scores must correspond to the order of ``labels``,\n      if provided, or else to the numerical or lexicographical order of\n      the labels in ``y_true``. See more information in the\n      :ref:`User guide <roc_auc_multiclass>`;\n    * In the multilabel case, it corresponds to an array of shape\n      `(n_samples, n_classes)`. Probability estimates are provided by the\n      `predict_proba` method and the non-thresholded decision values by\n      the `decision_function` method. The probability estimates correspond\n      to the **probability of the class with the greater label for each\n      output** of the classifier. See more information in the\n      :ref:`User guide <roc_auc_multilabel>`.\n\naverage : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n    If ``None``, the scores for each class are returned.\n    Otherwise, this determines the type of averaging performed on the data.\n    Note: multiclass ROC AUC currently only handles the 'macro' and\n    'weighted' averages. For multiclass targets, `average=None` is only\n    implemented for `multi_class='ovr'` and `average='micro'` is only\n    implemented for `multi_class='ovr'`.\n\n    ``'micro'``:\n        Calculate metrics globally by considering each element of the label\n        indicator matrix as a label.\n    ``'macro'``:\n        Calculate metrics for each label, and find their unweighted\n        mean.  This does not take label imbalance into account.\n    ``'weighted'``:\n        Calculate metrics for each label, and find their average, weighted\n        by support (the number of true instances for each label).\n    ``'samples'``:\n        Calculate metrics for each instance, and find their average.\n\n    Will be ignored when ``y_true`` is binary.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmax_fpr : float > 0 and <= 1, default=None\n    If not ``None``, the standardized partial AUC [2]_ over the range\n    [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n    should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n    computation currently is not supported for multiclass.\n\nmulti_class : {'raise', 'ovr', 'ovo'}, default='raise'\n    Only used for multiclass targets. Determines the type of configuration\n    to use. The default value raises an error, so either\n    ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n\n    ``'ovr'``:\n        Stands for One-vs-rest. Computes the AUC of each class\n        against the rest [3]_ [4]_. This\n        treats the multiclass case in the same way as the multilabel case.\n        Sensitive to class imbalance even when ``average == 'macro'``,\n        because class imbalance affects the composition of each of the\n        'rest' groupings.\n    ``'ovo'``:\n        Stands for One-vs-one. Computes the average AUC of all\n        possible pairwise combinations of classes [5]_.\n        Insensitive to class imbalance when\n        ``average == 'macro'``.\n\nlabels : array-like of shape (n_classes,), default=None\n    Only used for multiclass targets. List of labels that index the\n    classes in ``y_score``. If ``None``, the numerical or lexicographical\n    order of the labels in ``y_true`` is used.\n\nReturns\n-------\nauc : float\n    Area Under the Curve score.\n\nSee Also\n--------\naverage_precision_score : Area under the precision-recall curve.\nroc_curve : Compute Receiver operating characteristic (ROC) curve.\nRocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n    (ROC) curve given an estimator and some data.\nRocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n    (ROC) curve given the true and predicted values.\n\nNotes\n-----\nThe Gini Coefficient is a summary measure of the ranking ability of binary\nclassifiers. It is expressed using the area under of the ROC as follows:\n\nG = 2 * AUC - 1\n\nWhere G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation\nwill ensure that random guessing will yield a score of 0 in expectation, and it is\nupper bounded by 1.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Receiver operating characteristic\n        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n.. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n\n.. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n       probability estimation trees (Section 6.2), CeDER Working Paper\n       #IS-00-04, Stern School of Business, New York University.\n\n.. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n        Recognition Letters, 27(8), 861-874.\n        <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n\n.. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n        Under the ROC Curve for Multiple Class Classification Problems.\n        Machine Learning, 45(2), 171-186.\n        <http://link.springer.com/article/10.1023/A:1010920819831>`_\n.. [6] `Wikipedia entry for the Gini coefficient\n        <https://en.wikipedia.org/wiki/Gini_coefficient>`_\n\nExamples\n--------\nBinary case:\n\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.metrics import roc_auc_score\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n>>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n0.99...\n>>> roc_auc_score(y, clf.decision_function(X))\n0.99...\n\nMulticlass case:\n\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n>>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n0.99...\n\nMultilabel case:\n\n>>> import numpy as np\n>>> from sklearn.datasets import make_multilabel_classification\n>>> from sklearn.multioutput import MultiOutputClassifier\n>>> X, y = make_multilabel_classification(random_state=0)\n>>> clf = MultiOutputClassifier(clf).fit(X, y)\n>>> # get a list of n_output containing probability arrays of shape\n>>> # (n_samples, n_classes)\n>>> y_pred = clf.predict_proba(X)\n>>> # extract the positive columns for each output\n>>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n>>> roc_auc_score(y, y_pred, average=None)\narray([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n>>> from sklearn.linear_model import RidgeClassifierCV\n>>> clf = RidgeClassifierCV().fit(X, y)\n>>> roc_auc_score(y, clf.decision_function(X), average=None)\narray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute Receiver operating characteristic (ROC).\n\nNote: this implementation is restricted to the binary classification task.\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\ny_score : array-like of shape (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\npos_label : int, float, bool or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndrop_intermediate : bool, default=True\n    Whether to drop some suboptimal thresholds which would not appear\n    on a plotted ROC curve. This is useful in order to create lighter\n    ROC curves.\n\n    .. versionadded:: 0.17\n       parameter *drop_intermediate*.\n\nReturns\n-------\nfpr : ndarray of shape (>2,)\n    Increasing false positive rates such that element i is the false\n    positive rate of predictions with score >= `thresholds[i]`.\n\ntpr : ndarray of shape (>2,)\n    Increasing true positive rates such that element `i` is the true\n    positive rate of predictions with score >= `thresholds[i]`.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Decreasing thresholds on the decision function used to compute\n    fpr and tpr. `thresholds[0]` represents no instances being predicted\n    and is arbitrarily set to `np.inf`.\n\nSee Also\n--------\nRocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n    (ROC) curve given an estimator and some data.\nRocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n    (ROC) curve given the true and predicted values.\ndet_curve: Compute error rates for different probability thresholds.\nroc_auc_score : Compute the area under the ROC curve.\n\nNotes\n-----\nSince the thresholds are sorted from low to high values, they\nare reversed upon returning them to ensure they correspond to both ``fpr``\nand ``tpr``, which are sorted in reversed order during their calculation.\n\nAn arbitrary threshold is added for the case `tpr=0` and `fpr=0` to\nensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n`np.inf`.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Receiver operating characteristic\n        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n.. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n       Letters, 2006, 27(8):861-874.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>> tpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>> thresholds\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Root mean squared error regression loss.\n\nRead more in the :ref:`User Guide <mean_squared_error>`.\n\n.. versionadded:: 1.4\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors in case of multioutput input.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats\n    A non-negative floating point value (the best value is 0.0), or an\n    array of floating point values, one for each individual target.\n\nExamples\n--------\n>>> from sklearn.metrics import root_mean_squared_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> root_mean_squared_error(y_true, y_pred)\n0.612...\n>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n>>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n>>> root_mean_squared_error(y_true, y_pred)\n0.822..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Root mean squared logarithmic error regression loss.\n\nRead more in the :ref:`User Guide <mean_squared_log_error>`.\n\n.. versionadded:: 1.4\n\nParameters\n----------\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\n\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n\n    Defines aggregating of multiple output values.\n    Array-like value defines weights used to average errors.\n\n    'raw_values' :\n        Returns a full set of errors when the input is of multioutput\n        format.\n\n    'uniform_average' :\n        Errors of all outputs are averaged with uniform weight.\n\nReturns\n-------\nloss : float or ndarray of floats\n    A non-negative floating point value (the best value is 0.0), or an\n    array of floating point values, one for each individual target.\n\nExamples\n--------\n>>> from sklearn.metrics import root_mean_squared_log_error\n>>> y_true = [3, 5, 2.5, 7]\n>>> y_pred = [2.5, 5, 4, 8]\n>>> root_mean_squared_log_error(y_true, y_pred)\n0.199..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n\nThis estimator implements regularized linear models with stochastic\ngradient descent (SGD) learning: the gradient of the loss is estimated\neach sample at a time and the model is updated along the way with a\ndecreasing strength schedule (aka learning rate). SGD allows minibatch\n(online/out-of-core) learning via the `partial_fit` method.\nFor best results using the default learning rate schedule, the data should\nhave zero mean and unit variance.\n\nThis implementation works with data represented as dense or sparse arrays\nof floating point values for the features. The model it fits can be\ncontrolled with the loss parameter; by default, it fits a linear support\nvector machine (SVM).\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\n\nRead more in the :ref:`User Guide <sgd>`.\n\nParameters\n----------\nloss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'\n    The loss function to be used.\n\n    - 'hinge' gives a linear SVM.\n    - 'log_loss' gives logistic regression, a probabilistic classifier.\n    - 'modified_huber' is another smooth loss that brings tolerance to\n      outliers as well as probability estimates.\n    - 'squared_hinge' is like hinge but is quadratically penalized.\n    - 'perceptron' is the linear loss used by the perceptron algorithm.\n    - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n      'squared_epsilon_insensitive' are designed for regression but can be useful\n      in classification as well; see\n      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n\n    More details about the losses formulas can be found in the\n    :ref:`User Guide <sgd_mathematical_formulation>`.\n\npenalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n    The penalty (aka regularization term) to be used. Defaults to 'l2'\n    which is the standard regularizer for linear SVM models. 'l1' and\n    'elasticnet' might bring sparsity to the model (feature selection)\n    not achievable with 'l2'. No penalty is added when set to `None`.\n\nalpha : float, default=0.0001\n    Constant that multiplies the regularization term. The higher the\n    value, the stronger the regularization. Also used to compute the\n    learning rate when `learning_rate` is set to 'optimal'.\n    Values must be in the range `[0.0, inf)`.\n\nl1_ratio : float, default=0.15\n    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n    Only used if `penalty` is 'elasticnet'.\n    Values must be in the range `[0.0, 1.0]`.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n    Values must be in the range `[1, inf)`.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, training will stop\n    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n    epochs.\n    Convergence is checked against the training loss or the\n    validation loss depending on the `early_stopping` parameter.\n    Values must be in the range `[0.0, inf)`.\n\n    .. versionadded:: 0.19\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n    Values must be in the range `[0, inf)`.\n\nepsilon : float, default=0.1\n    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    For 'huber', determines the threshold at which it becomes less\n    important to get the prediction exactly right.\n    For epsilon-insensitive, any differences between the current prediction\n    and the correct label are ignored if they are less than this threshold.\n    Values must be in the range `[0.0, inf)`.\n\nn_jobs : int, default=None\n    The number of CPUs to use to do the OVA (One Versus All, for\n    multi-class problems) computation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nrandom_state : int, RandomState instance, default=None\n    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n    Integer values must be in the range `[0, 2**32 - 1]`.\n\nlearning_rate : str, default='optimal'\n    The learning rate schedule:\n\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where `t0` is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      `early_stopping` is `True`, the current learning rate is divided by 5.\n\n        .. versionadded:: 0.20\n            Added 'adaptive' option\n\neta0 : float, default=0.0\n    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n    the default schedule 'optimal'.\n    Values must be in the range `[0.0, inf)`.\n\npower_t : float, default=0.5\n    The exponent for inverse scaling learning rate.\n    Values must be in the range `(-inf, inf)`.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to `True`, it will automatically set aside\n    a stratified fraction of training data as validation and terminate\n    training when validation score returned by the `score` method is not\n    improving by at least tol for n_iter_no_change consecutive epochs.\n\n    .. versionadded:: 0.20\n        Added 'early_stopping' option\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if `early_stopping` is True.\n    Values must be in the range `(0.0, 1.0)`.\n\n    .. versionadded:: 0.20\n        Added 'validation_fraction' option\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before stopping\n    fitting.\n    Convergence is checked against the training loss or the\n    validation loss depending on the `early_stopping` parameter.\n    Integer values must be in the range `[1, max_iter)`.\n\n    .. versionadded:: 0.20\n        Added 'n_iter_no_change' option\n\nclass_weight : dict, {class_label: weight} or \"balanced\", default=None\n    Preset for the class_weight fit parameter.\n\n    Weights associated with classes. If not given, all classes\n    are supposed to have weight one.\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit`` will result in increasing the\n    existing counter.\n\naverage : bool or int, default=False\n    When set to `True`, computes the averaged SGD weights across all\n    updates and stores the result in the ``coef_`` attribute. If set to\n    an int greater than 1, averaging will begin once the total number of\n    samples seen reaches `average`. So ``average=10`` will begin\n    averaging after seeing 10 samples.\n    Integer values must be in the range `[1, n_samples]`.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n    Constants in decision function.\n\nn_iter_ : int\n    The actual number of iterations before reaching the stopping criterion.\n    For multiclass fits, it is the maximum over every binary fit.\n\nloss_function_ : concrete ``LossFunction``\n\n    .. deprecated:: 1.4\n        Attribute `loss_function_` was deprecated in version 1.4 and will be\n        removed in 1.6.\n\nclasses_ : array of shape (n_classes,)\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples + 1)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.svm.LinearSVC : Linear support vector classification.\nLogisticRegression : Logistic regression.\nPerceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n    ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n    penalty=None)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import SGDClassifier\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> Y = np.array([1, 1, 2, 2])\n>>> # Always scale the input. The most convenient way is to use a pipeline.\n>>> clf = make_pipeline(StandardScaler(),\n...                     SGDClassifier(max_iter=1000, tol=1e-3))\n>>> clf.fit(X, Y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('sgdclassifier', SGDClassifier())])\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Solves linear One-Class SVM using Stochastic Gradient Descent.\n\nThis implementation is meant to be used with a kernel approximation\ntechnique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results\nsimilar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by\ndefault.\n\nRead more in the :ref:`User Guide <sgd_online_one_class_svm>`.\n\n.. versionadded:: 1.0\n\nParameters\n----------\nnu : float, default=0.5\n    The nu parameter of the One Class SVM: an upper bound on the\n    fraction of training errors and a lower bound of the fraction of\n    support vectors. Should be in the interval (0, 1]. By default 0.5\n    will be taken.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. Defaults to True.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    `partial_fit`. Defaults to 1000.\n    Values must be in the range `[1, inf)`.\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, the iterations will stop\n    when (loss > previous_loss - tol). Defaults to 1e-3.\n    Values must be in the range `[0.0, inf)`.\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n    Defaults to True.\n\nverbose : int, default=0\n    The verbosity level.\n\nrandom_state : int, RandomState instance or None, default=None\n    The seed of the pseudo random number generator to use when shuffling\n    the data.  If int, random_state is the seed used by the random number\n    generator; If RandomState instance, random_state is the random number\n    generator; If None, the random number generator is the RandomState\n    instance used by `np.random`.\n\nlearning_rate : {'constant', 'optimal', 'invscaling', 'adaptive'}, default='optimal'\n    The learning rate schedule to use with `fit`. (If using `partial_fit`,\n    learning rate must be controlled directly).\n\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where t0 is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      early_stopping is True, the current learning rate is divided by 5.\n\neta0 : float, default=0.0\n    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n    the default schedule 'optimal'.\n    Values must be in the range `[0.0, inf)`.\n\npower_t : float, default=0.5\n    The exponent for inverse scaling learning rate.\n    Values must be in the range `(-inf, inf)`.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit``  will result in increasing the\n    existing counter.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights and stores the\n    result in the ``coef_`` attribute. If set to an int greater than 1,\n    averaging will begin once the total number of samples seen reaches\n    average. So ``average=10`` will begin averaging after seeing 10\n    samples.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features.\n\noffset_ : ndarray of shape (1,)\n    Offset used to define the decision function from the raw scores.\n    We have the relation: decision_function = score_samples - offset.\n\nn_iter_ : int\n    The actual number of iterations to reach the stopping criterion.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples + 1)``.\n\nloss_function_ : concrete ``LossFunction``\n\n    .. deprecated:: 1.4\n        ``loss_function_`` was deprecated in version 1.4 and will be removed in\n        1.6.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n\nNotes\n-----\nThis estimator has a linear complexity in the number of training samples\nand is thus better suited than the `sklearn.svm.OneClassSVM`\nimplementation for datasets with a large number of training samples (say\n> 10,000).\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import linear_model\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> clf = linear_model.SGDOneClassSVM(random_state=42)\n>>> clf.fit(X)\nSGDOneClassSVM(random_state=42)\n\n>>> print(clf.predict([[4, 4]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Linear model fitted by minimizing a regularized empirical loss with SGD.\n\nSGD stands for Stochastic Gradient Descent: the gradient of the loss is\nestimated each sample at a time and the model is updated along the way with\na decreasing strength schedule (aka learning rate).\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\n\nThis implementation works with data represented as dense numpy arrays of\nfloating point values for the features.\n\nRead more in the :ref:`User Guide <sgd>`.\n\nParameters\n----------\nloss : str, default='squared_error'\n    The loss function to be used. The possible values are 'squared_error',\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n\n    The 'squared_error' refers to the ordinary least squares fit.\n    'huber' modifies 'squared_error' to focus less on getting outliers\n    correct by switching from squared to linear loss past a distance of\n    epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n    linear past that; this is the loss function used in SVR.\n    'squared_epsilon_insensitive' is the same but becomes squared loss past\n    a tolerance of epsilon.\n\n    More details about the losses formulas can be found in the\n    :ref:`User Guide <sgd_mathematical_formulation>`.\n\npenalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n    The penalty (aka regularization term) to be used. Defaults to 'l2'\n    which is the standard regularizer for linear SVM models. 'l1' and\n    'elasticnet' might bring sparsity to the model (feature selection)\n    not achievable with 'l2'. No penalty is added when set to `None`.\n\nalpha : float, default=0.0001\n    Constant that multiplies the regularization term. The higher the\n    value, the stronger the regularization. Also used to compute the\n    learning rate when `learning_rate` is set to 'optimal'.\n    Values must be in the range `[0.0, inf)`.\n\nl1_ratio : float, default=0.15\n    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n    Only used if `penalty` is 'elasticnet'.\n    Values must be in the range `[0.0, 1.0]`.\n\nfit_intercept : bool, default=True\n    Whether the intercept should be estimated or not. If False, the\n    data is assumed to be already centered.\n\nmax_iter : int, default=1000\n    The maximum number of passes over the training data (aka epochs).\n    It only impacts the behavior in the ``fit`` method, and not the\n    :meth:`partial_fit` method.\n    Values must be in the range `[1, inf)`.\n\n    .. versionadded:: 0.19\n\ntol : float or None, default=1e-3\n    The stopping criterion. If it is not None, training will stop\n    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n    epochs.\n    Convergence is checked against the training loss or the\n    validation loss depending on the `early_stopping` parameter.\n    Values must be in the range `[0.0, inf)`.\n\n    .. versionadded:: 0.19\n\nshuffle : bool, default=True\n    Whether or not the training data should be shuffled after each epoch.\n\nverbose : int, default=0\n    The verbosity level.\n    Values must be in the range `[0, inf)`.\n\nepsilon : float, default=0.1\n    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n    For 'huber', determines the threshold at which it becomes less\n    important to get the prediction exactly right.\n    For epsilon-insensitive, any differences between the current prediction\n    and the correct label are ignored if they are less than this threshold.\n    Values must be in the range `[0.0, inf)`.\n\nrandom_state : int, RandomState instance, default=None\n    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nlearning_rate : str, default='invscaling'\n    The learning rate schedule:\n\n    - 'constant': `eta = eta0`\n    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n      where t0 is chosen by a heuristic proposed by Leon Bottou.\n    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n    - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n      Each time n_iter_no_change consecutive epochs fail to decrease the\n      training loss by tol or fail to increase validation score by tol if\n      early_stopping is True, the current learning rate is divided by 5.\n\n        .. versionadded:: 0.20\n            Added 'adaptive' option\n\neta0 : float, default=0.01\n    The initial learning rate for the 'constant', 'invscaling' or\n    'adaptive' schedules. The default value is 0.01.\n    Values must be in the range `[0.0, inf)`.\n\npower_t : float, default=0.25\n    The exponent for inverse scaling learning rate.\n    Values must be in the range `(-inf, inf)`.\n\nearly_stopping : bool, default=False\n    Whether to use early stopping to terminate training when validation\n    score is not improving. If set to True, it will automatically set aside\n    a fraction of training data as validation and terminate\n    training when validation score returned by the `score` method is not\n    improving by at least `tol` for `n_iter_no_change` consecutive\n    epochs.\n\n    .. versionadded:: 0.20\n        Added 'early_stopping' option\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if `early_stopping` is True.\n    Values must be in the range `(0.0, 1.0)`.\n\n    .. versionadded:: 0.20\n        Added 'validation_fraction' option\n\nn_iter_no_change : int, default=5\n    Number of iterations with no improvement to wait before stopping\n    fitting.\n    Convergence is checked against the training loss or the\n    validation loss depending on the `early_stopping` parameter.\n    Integer values must be in the range `[1, max_iter)`.\n\n    .. versionadded:: 0.20\n        Added 'n_iter_no_change' option\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit as\n    initialization, otherwise, just erase the previous solution.\n    See :term:`the Glossary <warm_start>`.\n\n    Repeatedly calling fit or partial_fit when warm_start is True can\n    result in a different solution than when calling fit a single time\n    because of the way the data is shuffled.\n    If a dynamic learning rate is used, the learning rate is adapted\n    depending on the number of samples already seen. Calling ``fit`` resets\n    this counter, while ``partial_fit``  will result in increasing the\n    existing counter.\n\naverage : bool or int, default=False\n    When set to True, computes the averaged SGD weights across all\n    updates and stores the result in the ``coef_`` attribute. If set to\n    an int greater than 1, averaging will begin once the total number of\n    samples seen reaches `average`. So ``average=10`` will begin\n    averaging after seeing 10 samples.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,)\n    Weights assigned to the features.\n\nintercept_ : ndarray of shape (1,)\n    The intercept term.\n\nn_iter_ : int\n    The actual number of iterations before reaching the stopping criterion.\n\nt_ : int\n    Number of weight updates performed during training.\n    Same as ``(n_iter_ * n_samples + 1)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nHuberRegressor : Linear regression model that is robust to outliers.\nLars : Least Angle Regression model.\nLasso : Linear Model trained with L1 prior as regularizer.\nRANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\nRidge : Linear least squares with l2 regularization.\nsklearn.svm.SVR : Epsilon-Support Vector Regression.\nTheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import SGDRegressor\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> # Always scale the input. The most convenient way is to use a pipeline.\n>>> reg = make_pipeline(StandardScaler(),\n...                     SGDRegressor(max_iter=1000, tol=1e-3))\n>>> reg.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('sgdregressor', SGDRegressor())])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "C-Support Vector Classification.\n\nThe implementation is based on libsvm. The fit time scales at least\nquadratically with the number of samples and may be impractical\nbeyond tens of thousands of samples. For large datasets\nconsider using :class:`~sklearn.svm.LinearSVC` or\n:class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n:class:`~sklearn.kernel_approximation.Nystroem` transformer or\nother :ref:`kernel_approximation`.\n\nThe multiclass support is handled according to a one-vs-one scheme.\n\nFor details on the precise mathematical formulation of the provided\nkernel functions and how `gamma`, `coef0` and `degree` affect each\nother, see the corresponding section in the narrative documentation:\n:ref:`svm_kernels`.\n\nTo learn how to tune SVC's hyperparameters, see the following example:\n:ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`\n\nRead more in the :ref:`User Guide <svm_classification>`.\n\nParameters\n----------\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive. The penalty\n    is a squared l2 penalty.\n\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n    Specifies the kernel type to be used in the algorithm. If\n    none is given, 'rbf' will be used. If a callable is given it is used to\n    pre-compute the kernel matrix from data matrices; that matrix should be\n    an array of shape ``(n_samples, n_samples)``. For an intuitive\n    visualization of different kernel types see\n    :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Must be non-negative. Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features\n    - if float, must be non-negative.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\nprobability : bool, default=False\n    Whether to enable probability estimates. This must be enabled prior\n    to calling `fit`, will slow down that method as it internally uses\n    5-fold cross-validation, and `predict_proba` may be inconsistent with\n    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nclass_weight : dict or 'balanced', default=None\n    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one.\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``.\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\ndecision_function_shape : {'ovo', 'ovr'}, default='ovr'\n    Whether to return a one-vs-rest ('ovr') decision function of shape\n    (n_samples, n_classes) as all other classifiers, or the original\n    one-vs-one ('ovo') decision function of libsvm which has shape\n    (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n    internally, one-vs-one ('ovo') is always used as a multi-class strategy\n    to train models; an ovr matrix is only constructed from the ovo matrix.\n    The parameter is ignored for binary classification.\n\n    .. versionchanged:: 0.19\n        decision_function_shape is 'ovr' by default.\n\n    .. versionadded:: 0.17\n       *decision_function_shape='ovr'* is recommended.\n\n    .. versionchanged:: 0.17\n       Deprecated *decision_function_shape='ovo' and None*.\n\nbreak_ties : bool, default=False\n    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n    :term:`predict` will break ties according to the confidence values of\n    :term:`decision_function`; otherwise the first class among the tied\n    classes is returned. Please note that breaking ties comes at a\n    relatively high computational cost compared to a simple predict.\n\n    .. versionadded:: 0.22\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo random number generation for shuffling the data for\n    probability estimates. Ignored when `probability` is False.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclass_weight_ : ndarray of shape (n_classes,)\n    Multipliers of parameter C for each class.\n    Computed based on the ``class_weight`` parameter.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\ncoef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is a readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (n_classes -1, n_SV)\n    Dual coefficients of the support vector in the decision\n    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n    their targets.\n    For multiclass, coefficient for all 1-vs-1 classifiers.\n    The layout of the coefficients in the multiclass case is somewhat\n    non-trivial. See the :ref:`multi-class section of the User Guide\n    <svm_multi_class>` for details.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n    Number of iterations run by the optimization routine to fit the model.\n    The shape of this attribute depends on the number of models optimized\n    which in turn depends on the number of classes.\n\n    .. versionadded:: 1.1\n\nsupport_ : ndarray of shape (n_SV)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors. An empty array if kernel is precomputed.\n\nn_support_ : ndarray of shape (n_classes,), dtype=int32\n    Number of support vectors for each class.\n\nprobA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\nprobB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n    If `probability=True`, it corresponds to the parameters learned in\n    Platt scaling to produce probability estimates from decision values.\n    If `probability=False`, it's an empty array. Platt scaling uses the\n    logistic function\n    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n    more information on the multiclass case and training procedure see\n    section 8 of [1]_.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nSee Also\n--------\nSVR : Support Vector Machine for Regression implemented using libsvm.\n\nLinearSVC : Scalable Linear Support Vector Machine for classification\n    implemented using liblinear. Check the See Also section of\n    LinearSVC for more comparison element.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n    Machines and Comparisons to Regularized Likelihood Methods\"\n    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n>>> y = np.array([1, 1, 2, 2])\n>>> from sklearn.svm import SVC\n>>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n>>> clf.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(gamma='auto'))])\n\n>>> print(clf.predict([[-0.8, -1]]))\n[1]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Epsilon-Support Vector Regression.\n\nThe free parameters in the model are C and epsilon.\n\nThe implementation is based on libsvm. The fit time complexity\nis more than quadratic with the number of samples which makes it hard\nto scale to datasets with more than a couple of 10000 samples. For large\ndatasets consider using :class:`~sklearn.svm.LinearSVR` or\n:class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a\n:class:`~sklearn.kernel_approximation.Nystroem` transformer or\nother :ref:`kernel_approximation`.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n\nParameters\n----------\nkernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n     Specifies the kernel type to be used in the algorithm.\n     If none is given, 'rbf' will be used. If a callable is given it is\n     used to precompute the kernel matrix.\n\ndegree : int, default=3\n    Degree of the polynomial kernel function ('poly').\n    Must be non-negative. Ignored by all other kernels.\n\ngamma : {'scale', 'auto'} or float, default='scale'\n    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n    - if ``gamma='scale'`` (default) is passed then it uses\n      1 / (n_features * X.var()) as value of gamma,\n    - if 'auto', uses 1 / n_features\n    - if float, must be non-negative.\n\n    .. versionchanged:: 0.22\n       The default value of ``gamma`` changed from 'auto' to 'scale'.\n\ncoef0 : float, default=0.0\n    Independent term in kernel function.\n    It is only significant in 'poly' and 'sigmoid'.\n\ntol : float, default=1e-3\n    Tolerance for stopping criterion.\n\nC : float, default=1.0\n    Regularization parameter. The strength of the regularization is\n    inversely proportional to C. Must be strictly positive.\n    The penalty is a squared l2 penalty.\n\nepsilon : float, default=0.1\n     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n     within which no penalty is associated in the training loss function\n     with points predicted within a distance epsilon from the actual\n     value. Must be non-negative.\n\nshrinking : bool, default=True\n    Whether to use the shrinking heuristic.\n    See the :ref:`User Guide <shrinking_svm>`.\n\ncache_size : float, default=200\n    Specify the size of the kernel cache (in MB).\n\nverbose : bool, default=False\n    Enable verbose output. Note that this setting takes advantage of a\n    per-process runtime setting in libsvm that, if enabled, may not work\n    properly in a multithreaded context.\n\nmax_iter : int, default=-1\n    Hard limit on iterations within solver, or -1 for no limit.\n\nAttributes\n----------\ncoef_ : ndarray of shape (1, n_features)\n    Weights assigned to the features (coefficients in the primal\n    problem). This is only available in the case of a linear kernel.\n\n    `coef_` is readonly property derived from `dual_coef_` and\n    `support_vectors_`.\n\ndual_coef_ : ndarray of shape (1, n_SV)\n    Coefficients of the support vector in the decision function.\n\nfit_status_ : int\n    0 if correctly fitted, 1 otherwise (will raise warning)\n\nintercept_ : ndarray of shape (1,)\n    Constants in decision function.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_iter_ : int\n    Number of iterations run by the optimization routine to fit the model.\n\n    .. versionadded:: 1.1\n\nn_support_ : ndarray of shape (1,), dtype=int32\n    Number of support vectors.\n\nshape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n    Array dimensions of training vector ``X``.\n\nsupport_ : ndarray of shape (n_SV,)\n    Indices of support vectors.\n\nsupport_vectors_ : ndarray of shape (n_SV, n_features)\n    Support vectors.\n\nSee Also\n--------\nNuSVR : Support Vector Machine for regression implemented using libsvm\n    using a parameter to control the number of support vectors.\n\nLinearSVR : Scalable Linear Support Vector Machine for regression\n    implemented using liblinear.\n\nReferences\n----------\n.. [1] `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n.. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n    Machines and Comparisons to Regularized Likelihood Methods\"\n    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\nExamples\n--------\n>>> from sklearn.svm import SVR\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> import numpy as np\n>>> n_samples, n_features = 10, 5\n>>> rng = np.random.RandomState(0)\n>>> y = rng.randn(n_samples)\n>>> X = rng.randn(n_samples, n_features)\n>>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n>>> regr.fit(X, y)\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svr', SVR(epsilon=0.2))])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Filter: Select the p-values for an estimated false discovery rate.\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest uncorrected p-value for features to keep.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/False_discovery_rate\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFdr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    Features with p-values less than `alpha` are selected.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFpr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.\n\nParameters\n----------\nestimator : object\n    The base estimator from which the transformer is built.\n    This can be both a fitted (if ``prefit`` is set to True)\n    or a non-fitted estimator. The estimator should have a\n    ``feature_importances_`` or ``coef_`` attribute after fitting.\n    Otherwise, the ``importance_getter`` parameter should be used.\n\nthreshold : str or float, default=None\n    The threshold value to use for feature selection. Features whose\n    absolute importance value is greater or equal are kept while the others\n    are discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value\n    is the median (resp. the mean) of the feature importances. A scaling\n    factor (e.g., \"1.25*mean\") may also be used. If None and if the\n    estimator has a parameter penalty set to l1, either explicitly\n    or implicitly (e.g, Lasso), the threshold used is 1e-5.\n    Otherwise, \"mean\" is used by default.\n\nprefit : bool, default=False\n    Whether a prefit model is expected to be passed into the constructor\n    directly or not.\n    If `True`, `estimator` must be a fitted estimator.\n    If `False`, `estimator` is fitted and updated by calling\n    `fit` and `partial_fit`, respectively.\n\nnorm_order : non-zero int, inf, -inf, default=1\n    Order of the norm used to filter the vectors of coefficients below\n    ``threshold`` in the case where the ``coef_`` attribute of the\n    estimator is of dimension 2.\n\nmax_features : int, callable, default=None\n    The maximum number of features to select.\n\n    - If an integer, then it specifies the maximum number of features to\n      allow.\n    - If a callable, then it specifies how to calculate the maximum number of\n      features allowed by using the output of `max_features(X)`.\n    - If `None`, then all features are kept.\n\n    To only select based on ``max_features``, set ``threshold=-np.inf``.\n\n    .. versionadded:: 0.20\n    .. versionchanged:: 1.1\n       `max_features` accepts a callable.\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a ``coef_``\n    attribute or ``feature_importances_`` attribute of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : estimator\n    The base estimator from which the transformer is built. This attribute\n    exist only when `fit` has been called.\n\n    - If `prefit=True`, it is a deep copy of `estimator`.\n    - If `prefit=False`, it is a clone of `estimator` and fit on the data\n      passed to `fit` or `partial_fit`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nmax_features_ : int\n    Maximum number of features calculated during :term:`fit`. Only defined\n    if the ``max_features`` is not `None`.\n\n    - If `max_features` is an `int`, then `max_features_ = max_features`.\n    - If `max_features` is a callable, then `max_features_ = max_features(X)`.\n\n    .. versionadded:: 1.1\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nthreshold_ : float\n    The threshold value used for feature selection.\n\nSee Also\n--------\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.\n\nNotes\n-----\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nExamples\n--------\n>>> from sklearn.feature_selection import SelectFromModel\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[ 0.87, -1.34,  0.31 ],\n...      [-2.79, -0.02, -0.85 ],\n...      [-1.34, -0.48, -2.55 ],\n...      [ 1.92,  1.48,  0.65 ]]\n>>> y = [0, 1, 0, 1]\n>>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n>>> selector.estimator_.coef_\narray([[-0.3252...,  0.8345...,  0.4976...]])\n>>> selector.threshold_\n0.55249...\n>>> selector.get_support()\narray([False,  True, False])\n>>> selector.transform(X)\narray([[-1.34],\n       [-0.02],\n       [-0.48],\n       [ 1.48]])\n\nUsing a callable to create a selector that can use no more than half\nof the input features.\n\n>>> def half_callable(X):\n...     return round(len(X[0]) / 2)\n>>> half_selector = SelectFromModel(estimator=LogisticRegression(),\n...                                 max_features=half_callable)\n>>> _ = half_selector.fit(X, y)\n>>> half_selector.max_features_\n2"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Filter: Select the p-values corresponding to Family-wise error rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest uncorrected p-value for features to keep.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFwe, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 15)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18\n\nk : int or \"all\", default=10\n    Number of top features to select.\n    The \"all\" option bypasses selection, for use in a parameter search.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned only scores.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nf_classif: ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nchi2: Chi-squared stats of non-negative features for classification tasks.\nf_regression: F-value between label/feature for regression tasks.\nmutual_info_regression: Mutual information for a continuous target.\nSelectPercentile: Select features based on percentile of the highest\n    scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nNotes\n-----\nTies between features with equal scores will be broken in an unspecified\nway.\n\nThis filter supports unsupervised feature selection that only requests `X` for\ncomputing the scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n>>> X_new.shape\n(1797, 20)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18\n\npercentile : int, default=10\n    Percent of features to keep.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned only scores.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.\n\nNotes\n-----\nTies between features with equal scores will be broken in an unspecified\nway.\n\nThis filter supports unsupervised feature selection that only requests `X` for\ncomputing the scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectPercentile, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n>>> X_new.shape\n(1797, 7)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator. In\nthe case of unsupervised learning, this Sequential Feature Selector\nlooks only at the features (X), not the desired outputs (y).\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nestimator : estimator instance\n    An unfitted estimator.\n\nn_features_to_select : \"auto\", int or float, default=\"auto\"\n    If `\"auto\"`, the behaviour depends on the `tol` parameter:\n\n    - if `tol` is not `None`, then features are selected while the score\n      change does not exceed `tol`.\n    - otherwise, half of the features are selected.\n\n    If integer, the parameter is the absolute number of features to select.\n    If float between 0 and 1, it is the fraction of features to select.\n\n    .. versionadded:: 1.1\n       The option `\"auto\"` was added in version 1.1.\n\n    .. versionchanged:: 1.3\n       The default changed from `\"warn\"` to `\"auto\"` in 1.3.\n\ntol : float, default=None\n    If the score is not incremented by at least `tol` between two\n    consecutive feature additions or removals, stop adding or removing.\n\n    `tol` can be negative when removing features using `direction=\"backward\"`.\n    It can be useful to reduce the number of features at the cost of a small\n    decrease in the score.\n\n    `tol` is enabled only when `n_features_to_select` is `\"auto\"`.\n\n    .. versionadded:: 1.1\n\ndirection : {'forward', 'backward'}, default='forward'\n    Whether to perform forward selection or backward selection.\n\nscoring : str or callable, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    NOTE that when using a custom scorer, it should return a single\n    value.\n\n    If None, the estimator's score method is used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other\n    cases, :class:`~sklearn.model_selection.KFold` is used. These splitters\n    are instantiated with `shuffle=False` so the splits will be the same\n    across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. When evaluating a new feature to\n    add or remove, the cross-validation procedure is parallel over the\n    folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying estimator exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_features_to_select_ : int\n    The number of features that were selected.\n\nsupport_ : ndarray of shape (n_features,), dtype=bool\n    The mask of selected features.\n\nSee Also\n--------\nGenericUnivariateSelect : Univariate feature selector with configurable\n    strategy.\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination based on importance weights, with\n    automatic selection of the number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\n\nExamples\n--------\n>>> from sklearn.feature_selection import SequentialFeatureSelector\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n>>> sfs.fit(X, y)\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=3)\n>>> sfs.get_support()\narray([ True, False,  True,  True])\n>>> sfs.transform(X).shape\n(150, 3)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Random permutation cross-validator.\n\nYields indices to split data into training and test sets.\n\nNote: contrary to other cross-validation strategies, random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <ShuffleSplit>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=10\n    Number of re-shuffling & splitting iterations.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.1.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the training and testing indices produced.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import ShuffleSplit\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n>>> y = np.array([1, 2, 1, 2, 1, 2])\n>>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n>>> rs.get_n_splits(X)\n5\n>>> print(rs)\nShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n>>> for i, (train_index, test_index) in enumerate(rs.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[1 3 0 4]\n  Test:  index=[5 2]\nFold 1:\n  Train: index=[4 0 2 5]\n  Test:  index=[1 3]\nFold 2:\n  Train: index=[1 2 4 0]\n  Test:  index=[3 5]\nFold 3:\n  Train: index=[3 4 1 0]\n  Test:  index=[5 2]\nFold 4:\n  Train: index=[3 5 1 0]\n  Test:  index=[2 4]\n>>> # Specify train and test size\n>>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n...                   random_state=0)\n>>> for i, (train_index, test_index) in enumerate(rs.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[1 3 0]\n  Test:  index=[5 2]\nFold 1:\n  Train: index=[4 0 2]\n  Test:  index=[1 3]\nFold 2:\n  Train: index=[1 2 4]\n  Test:  index=[3 5]\nFold 3:\n  Train: index=[3 4 1]\n  Test:  index=[5 2]\nFold 4:\n  Train: index=[3 5 1]\n  Test:  index=[2 4]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the Silhouette Coefficient for each sample.\n\nThe Silhouette Coefficient is a measure of how well samples are clustered\nwith samples that are similar to themselves. Clustering models with a high\nSilhouette Coefficient are said to be dense, where samples in the same\ncluster are similar to each other, and well separated, where samples in\ndifferent clusters are not very similar to each other.\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\nb)``.\nNote that Silhouette Coefficient is only defined if number of labels\nis 2 ``<= n_labels <= n_samples - 1``.\n\nThis function returns the Silhouette Coefficient for each sample.\n\nThe best value is 1 and the worst value is -1. Values near 0 indicate\noverlapping clusters.\n\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n    An array of pairwise distances between samples, or a feature array. If\n    a sparse matrix is provided, CSR format should be favoured avoiding\n    an additional copy.\n\nlabels : array-like of shape (n_samples,)\n    Label values for each sample.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by :func:`~sklearn.metrics.pairwise_distances`.\n    If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n    Precomputed distance matrices must have 0 along the diagonal.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a ``scipy.spatial.distance`` metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nReturns\n-------\nsilhouette : array-like of shape (n_samples,)\n    Silhouette Coefficients for each sample.\n\nReferences\n----------\n\n.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n   Interpretation and Validation of Cluster Analysis\". Computational\n   and Applied Mathematics 20: 53-65.\n   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n\n.. [2] `Wikipedia entry on the Silhouette Coefficient\n   <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n\nExamples\n--------\n>>> from sklearn.metrics import silhouette_samples\n>>> from sklearn.datasets import make_blobs\n>>> from sklearn.cluster import KMeans\n>>> X, y = make_blobs(n_samples=50, random_state=42)\n>>> kmeans = KMeans(n_clusters=3, random_state=42)\n>>> labels = kmeans.fit_predict(X)\n>>> silhouette_samples(X, labels)\narray([...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Compute the mean Silhouette Coefficient of all samples.\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\nb)``.  To clarify, ``b`` is the distance between a sample and the nearest\ncluster that the sample is not a part of.\nNote that Silhouette Coefficient is only defined if number of labels\nis ``2 <= n_labels <= n_samples - 1``.\n\nThis function returns the mean Silhouette Coefficient over all samples.\nTo obtain the values for each sample, use :func:`silhouette_samples`.\n\nThe best value is 1 and the worst value is -1. Values near 0 indicate\noverlapping clusters. Negative values generally indicate that a sample has\nbeen assigned to the wrong cluster, as a different cluster is more similar.\n\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n    An array of pairwise distances between samples, or a feature array.\n\nlabels : array-like of shape (n_samples,)\n    Predicted labels for each sample.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by :func:`~sklearn.metrics.pairwise_distances`. If ``X`` is\n    the distance array itself, use ``metric=\"precomputed\"``.\n\nsample_size : int, default=None\n    The size of the sample to use when computing the Silhouette Coefficient\n    on a random subset of the data.\n    If ``sample_size is None``, no sampling is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for selecting a subset of samples.\n    Used when ``sample_size is not None``.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n**kwds : optional keyword parameters\n    Any further parameters are passed directly to the distance function.\n    If using a scipy.spatial.distance metric, the parameters are still\n    metric dependent. See the scipy docs for usage examples.\n\nReturns\n-------\nsilhouette : float\n    Mean Silhouette Coefficient for all samples.\n\nReferences\n----------\n\n.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n   Interpretation and Validation of Cluster Analysis\". Computational\n   and Applied Mathematics 20: 53-65.\n   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n\n.. [2] `Wikipedia entry on the Silhouette Coefficient\n       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n\nExamples\n--------\n>>> from sklearn.datasets import make_blobs\n>>> from sklearn.cluster import KMeans\n>>> from sklearn.metrics import silhouette_score\n>>> X, y = make_blobs(random_state=42)\n>>> kmeans = KMeans(n_clusters=2, random_state=42)\n>>> silhouette_score(X, kmeans.fit_predict(X))\n0.49..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Univariate imputer for completing missing values with simple strategies.\n\nReplace missing values using a descriptive statistic (e.g. mean, median, or\nmost frequent) along each column, or using a constant value.\n\nRead more in the :ref:`User Guide <impute>`.\n\n.. versionadded:: 0.20\n   `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`\n   estimator which is now removed.\n\nParameters\n----------\nmissing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan\n    The placeholder for the missing values. All occurrences of\n    `missing_values` will be imputed. For pandas' dataframes with\n    nullable integer dtypes with missing values, `missing_values`\n    can be set to either `np.nan` or `pd.NA`.\n\nstrategy : str, default='mean'\n    The imputation strategy.\n\n    - If \"mean\", then replace missing values using the mean along\n      each column. Can only be used with numeric data.\n    - If \"median\", then replace missing values using the median along\n      each column. Can only be used with numeric data.\n    - If \"most_frequent\", then replace missing using the most frequent\n      value along each column. Can be used with strings or numeric data.\n      If there is more than one such value, only the smallest is returned.\n    - If \"constant\", then replace missing values with fill_value. Can be\n      used with strings or numeric data.\n\n    .. versionadded:: 0.20\n       strategy=\"constant\" for fixed value imputation.\n\nfill_value : str or numerical value, default=None\n    When strategy == \"constant\", `fill_value` is used to replace all\n    occurrences of missing_values. For string or object data types,\n    `fill_value` must be a string.\n    If `None`, `fill_value` will be 0 when imputing numerical\n    data and \"missing_value\" for strings or object data types.\n\ncopy : bool, default=True\n    If True, a copy of X will be created. If False, imputation will\n    be done in-place whenever possible. Note that, in the following cases,\n    a new copy will always be made, even if `copy=False`:\n\n    - If `X` is not an array of floating values;\n    - If `X` is encoded as a CSR matrix;\n    - If `add_indicator=True`.\n\nadd_indicator : bool, default=False\n    If True, a :class:`MissingIndicator` transform will stack onto output\n    of the imputer's transform. This allows a predictive estimator\n    to account for missingness despite imputation. If a feature has no\n    missing values at fit/train time, the feature won't appear on\n    the missing indicator even if there are missing values at\n    transform/test time.\n\nkeep_empty_features : bool, default=False\n    If True, features that consist exclusively of missing values when\n    `fit` is called are returned in results when `transform` is called.\n    The imputed value is always `0` except when `strategy=\"constant\"`\n    in which case `fill_value` will be used instead.\n\n    .. versionadded:: 1.2\n\nAttributes\n----------\nstatistics_ : array of shape (n_features,)\n    The imputation fill value for each feature.\n    Computing statistics can result in `np.nan` values.\n    During :meth:`transform`, features corresponding to `np.nan`\n    statistics will be discarded.\n\nindicator_ : :class:`~sklearn.impute.MissingIndicator`\n    Indicator used to add binary indicators for missing values.\n    `None` if `add_indicator=False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nIterativeImputer : Multivariate imputer that estimates values to impute for\n    each feature with missing values from all the others.\nKNNImputer : Multivariate imputer that estimates missing features using\n    nearest samples.\n\nNotes\n-----\nColumns which only contained missing values at :meth:`fit` are discarded\nupon :meth:`transform` if strategy is not `\"constant\"`.\n\nIn a prediction context, simple imputation usually performs poorly when\nassociated with a weak learner. However, with a powerful learner, it can\nlead to as good or better performance than complex imputation such as\n:class:`~sklearn.impute.IterativeImputer` or :class:`~sklearn.impute.KNNImputer`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.impute import SimpleImputer\n>>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n>>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\nSimpleImputer()\n>>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n>>> print(imp_mean.transform(X))\n[[ 7.   2.   3. ]\n [ 4.   3.5  6. ]\n [10.   3.5  9. ]]\n\nFor a more detailed example see\n:ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ImputeModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#Module"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Sparse coding.\n\nFinds a sparse representation of data against a fixed, precomputed\ndictionary.\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n\nParameters\n----------\ndictionary : ndarray of shape (n_components, n_features)\n    The dictionary atoms used for sparse coding. Lines are assumed to be\n    normalized to unit norm.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution;\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n      the estimated components are sparse;\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution;\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `lasso_lars`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nn_components_ : int\n    Number of atoms.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nMiniBatchDictionaryLearning : A faster, less accurate, version of the\n    dictionary learning algorithm.\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nSparsePCA : Sparse Principal Components Analysis.\nsparse_encode : Sparse coding where each row of the result is the solution\n    to a sparse coding problem.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import SparseCoder\n>>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n>>> dictionary = np.array(\n...     [[0, 1, 0],\n...      [-1, -1, 2],\n...      [1, 1, 1],\n...      [0, 1, 1],\n...      [0, 2, 1]],\n...    dtype=np.float64\n... )\n>>> coder = SparseCoder(\n...     dictionary=dictionary, transform_algorithm='lasso_lars',\n...     transform_alpha=1e-10,\n... )\n>>> coder.transform(X)\narray([[ 0.,  0., -1.,  0.,  0.],\n       [ 0.,  1.,  1.,  0.,  0.]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Sparse Principal Components Analysis (SparsePCA).\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of sparse atoms to extract. If None, then ``n_components``\n    is set to ``n_features``.\n\nalpha : float, default=1\n    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n\nridge_alpha : float, default=0.01\n    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for the stopping condition.\n\nmethod : {'lars', 'cd'}, default='lars'\n    Method to be used for optimization.\n    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nU_init : ndarray of shape (n_samples, n_components), default=None\n    Initial values for the loadings for warm restart scenarios. Only used\n    if `U_init` and `V_init` are not None.\n\nV_init : ndarray of shape (n_components, n_features), default=None\n    Initial values for the components for warm restart scenarios. Only used\n    if `U_init` and `V_init` are not None.\n\nverbose : int or bool, default=False\n    Controls the verbosity; the higher, the more messages. Defaults to 0.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used during dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Sparse components extracted from the data.\n\nerror_ : ndarray\n    Vector of errors at each iteration.\n\nn_components_ : int\n    Estimated number of components.\n\n    .. versionadded:: 0.23\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to ``X.mean(axis=0)``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nPCA : Principal Component Analysis implementation.\nMiniBatchSparsePCA : Mini batch variant of `SparsePCA` that is faster but less\n    accurate.\nDictionaryLearning : Generic dictionary learning problem using a sparse code.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import SparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = SparsePCA(n_components=5, random_state=0)\n>>> transformer.fit(X)\nSparsePCA(...)\n>>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\n0.9666..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.\n\nParameters\n----------\nn_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n    The number of row and column clusters in the checkerboard\n    structure.\n\nmethod : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n    Method of normalizing and converting singular vectors into\n    biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n    The authors recommend using 'log'. If the data is sparse,\n    however, log normalization will not work, which is why the\n    default is 'bistochastic'.\n\n    .. warning::\n       if `method='log'`, the data must not be sparse.\n\nn_components : int, default=6\n    Number of singular vectors to check.\n\nn_best : int, default=3\n    Number of best singular vectors to which to project the data\n    for clustering.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', uses\n    :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', uses\n    `scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random'} or ndarray of shape (n_clusters, n_features),             default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    Row partition labels.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    Column partition labels.\n\nbiclusters_ : tuple of two ndarrays\n    The tuple contains the `rows_` and `columns_` arrays.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nSpectralCoclustering : Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nReferences\n----------\n\n* :doi:`Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray\n  data: coclustering genes and conditions.\n  <10.1101/gr.648603>`\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralBiclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_\narray([1, 0], dtype=int32)\n>>> clustering\nSpectralBiclustering(n_clusters=2, random_state=0)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts [1]_, [2]_.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=8\n    The dimension of the projection subspace.\n\neigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used. See [4]_ for more details regarding `'lobpcg'`.\n\nn_components : int, default=None\n    Number of eigenvectors to use for the spectral embedding. If None,\n    defaults to `n_clusters`.\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization\n    of the lobpcg eigenvectors decomposition when `eigen_solver ==\n    'amg'`, and for the K-Means initialization. Use an int to make\n    the results deterministic across calls (See\n    :term:`Glossary <random_state>`).\n\n    .. note::\n        When using `eigen_solver == 'amg'`,\n        it is necessary to also fix the global numpy seed with\n        `np.random.seed(int)` to get deterministic results. See\n        https://github.com/pyamg/pyamg/issues/139 for further\n        information.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of n_init\n    consecutive runs in terms of inertia. Only used if\n    ``assign_labels='kmeans'``.\n\ngamma : float, default=1.0\n    Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n    Ignored for ``affinity='nearest_neighbors'``.\n\naffinity : str or callable, default='rbf'\n    How to construct the affinity matrix.\n     - 'nearest_neighbors': construct the affinity matrix by computing a\n       graph of nearest neighbors.\n     - 'rbf': construct the affinity matrix using a radial basis function\n       (RBF) kernel.\n     - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n       where larger values indicate greater similarity between instances.\n     - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n       of precomputed distances, and construct a binary affinity matrix\n       from the ``n_neighbors`` nearest neighbors of each instance.\n     - one of the kernels supported by\n       :func:`~sklearn.metrics.pairwise.pairwise_kernels`.\n\n    Only kernels that produce similarity scores (non-negative values that\n    increase with similarity) should be used. This property is not checked\n    by the clustering algorithm.\n\nn_neighbors : int, default=10\n    Number of neighbors to use when constructing the affinity matrix using\n    the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\neigen_tol : float, default=\"auto\"\n    Stopping criterion for eigen decomposition of the Laplacian matrix.\n    If `eigen_tol=\"auto\"` then the passed tolerance will depend on the\n    `eigen_solver`:\n\n    - If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n    - If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n      `eigen_tol=None` which configures the underlying `lobpcg` solver to\n      automatically resolve the value according to their heuristics. See,\n      :func:`scipy.sparse.linalg.lobpcg` for details.\n\n    Note that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`\n    values of `tol<1e-5` may lead to convergence issues and should be\n    avoided.\n\n    .. versionadded:: 1.2\n       Added 'auto' option.\n\nassign_labels : {'kmeans', 'discretize', 'cluster_qr'}, default='kmeans'\n    The strategy for assigning labels in the embedding space. There are two\n    ways to assign labels after the Laplacian embedding. k-means is a\n    popular choice, but it can be sensitive to initialization.\n    Discretization is another approach which is less sensitive to random\n    initialization [3]_.\n    The cluster_qr method [5]_ directly extract clusters from eigenvectors\n    in spectral clustering. In contrast to k-means and discretization, cluster_qr\n    has no tuning parameters and runs no iterations, yet may outperform\n    k-means and discretization in terms of both quality and speed.\n\n    .. versionchanged:: 1.1\n       Added new labeling method 'cluster_qr'.\n\ndegree : float, default=3\n    Degree of the polynomial kernel. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Zero coefficient for polynomial and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict of str to any, default=None\n    Parameters (keyword arguments) and values for kernel passed as\n    callable object. Ignored by other kernels.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run when `affinity='nearest_neighbors'`\n    or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n    will be done in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\naffinity_matrix_ : array-like of shape (n_samples, n_samples)\n    Affinity matrix used for clustering. Available only after calling\n    ``fit``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nsklearn.cluster.KMeans : K-Means clustering.\nsklearn.cluster.DBSCAN : Density-Based Spatial Clustering of\n    Applications with Noise.\n\nNotes\n-----\nA distance matrix for which 0 indicates identical elements and high values\nindicate very dissimilar elements can be transformed into an affinity /\nsimilarity matrix that is well-suited for the algorithm by\napplying the Gaussian (aka RBF, heat) kernel::\n\n    np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\nwhere ``delta`` is a free parameter representing the width of the Gaussian\nkernel.\n\nAn alternative is to take a symmetric version of the k-nearest neighbors\nconnectivity matrix of the points.\n\nIf the pyamg package is installed, it is used: this greatly\nspeeds up computation.\n\nReferences\n----------\n.. [1] :doi:`Normalized cuts and image segmentation, 2000\n       Jianbo Shi, Jitendra Malik\n       <10.1109/34.868688>`\n\n.. [2] :doi:`A Tutorial on Spectral Clustering, 2007\n       Ulrike von Luxburg\n       <10.1007/s11222-007-9033-z>`\n\n.. [3] `Multiclass spectral clustering, 2003\n       Stella X. Yu, Jianbo Shi\n       <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n\n.. [4] :doi:`Toward the Optimal Preconditioned Eigensolver:\n       Locally Optimal Block Preconditioned Conjugate Gradient Method, 2001\n       A. V. Knyazev\n       SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541.\n       <10.1137/S1064827500366124>`\n\n.. [5] :doi:`Simple, direct, and efficient multi-way spectral clustering, 2019\n       Anil Damle, Victor Minden, Lexing Ying\n       <10.1093/imaiai/iay008>`\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralClustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralClustering(n_clusters=2,\n...         assign_labels='discretize',\n...         random_state=0).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering\nSpectralClustering(assign_labels='discretize', n_clusters=2,\n    random_state=0)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.\n\nParameters\n----------\nn_clusters : int, default=3\n    The number of biclusters to find.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', use\n    :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', use\n    :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    The bicluster label of each row.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    The bicluster label of each column.\n\nbiclusters_ : tuple of two ndarrays\n    The tuple contains the `rows_` and `columns_` arrays.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nSpectralBiclustering : Partitions rows and columns under the assumption\n    that the data has an underlying checkerboard structure.\n\nReferences\n----------\n* :doi:`Dhillon, Inderjit S, 2001. Co-clustering documents and words using\n  bipartite spectral graph partitioning.\n  <10.1145/502512.502550>`\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralCoclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_ #doctest: +SKIP\narray([0, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_ #doctest: +SKIP\narray([0, 0], dtype=int32)\n>>> clustering\nSpectralCoclustering(n_clusters=2, random_state=0)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClusterModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Generate univariate B-spline bases for features.\n\nGenerate a new feature matrix consisting of\n`n_splines=n_knots + degree - 1` (`n_knots - 1` for\n`extrapolation=\"periodic\"`) spline basis functions\n(B-splines) of polynomial order=`degree` for each feature.\n\nIn order to learn more about the SplineTransformer class go to:\n:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`\n\nRead more in the :ref:`User Guide <spline_transformer>`.\n\n.. versionadded:: 1.0\n\nParameters\n----------\nn_knots : int, default=5\n    Number of knots of the splines if `knots` equals one of\n    {'uniform', 'quantile'}. Must be larger or equal 2. Ignored if `knots`\n    is array-like.\n\ndegree : int, default=3\n    The polynomial degree of the spline basis. Must be a non-negative\n    integer.\n\nknots : {'uniform', 'quantile'} or array-like of shape         (n_knots, n_features), default='uniform'\n    Set knot positions such that first knot <= features <= last knot.\n\n    - If 'uniform', `n_knots` number of knots are distributed uniformly\n      from min to max values of the features.\n    - If 'quantile', they are distributed uniformly along the quantiles of\n      the features.\n    - If an array-like is given, it directly specifies the sorted knot\n      positions including the boundary knots. Note that, internally,\n      `degree` number of knots are added before the first knot, the same\n      after the last knot.\n\nextrapolation : {'error', 'constant', 'linear', 'continue', 'periodic'},         default='constant'\n    If 'error', values outside the min and max values of the training\n    features raises a `ValueError`. If 'constant', the value of the\n    splines at minimum and maximum value of the features is used as\n    constant extrapolation. If 'linear', a linear extrapolation is used.\n    If 'continue', the splines are extrapolated as is, i.e. option\n    `extrapolate=True` in :class:`scipy.interpolate.BSpline`. If\n    'periodic', periodic splines with a periodicity equal to the distance\n    between the first and last knot are used. Periodic splines enforce\n    equal function values and derivatives at the first and last knot.\n    For example, this makes it possible to avoid introducing an arbitrary\n    jump between Dec 31st and Jan 1st in spline features derived from a\n    naturally periodic \"day-of-year\" input feature. In this case it is\n    recommended to manually set the knot values to control the period.\n\ninclude_bias : bool, default=True\n    If False, then the last spline element inside the data range\n    of a feature is dropped. As B-splines sum to one over the spline basis\n    functions for each data point, they implicitly include a bias term,\n    i.e. a column of ones. It acts as an intercept term in a linear models.\n\norder : {'C', 'F'}, default='C'\n    Order of output array in the dense case. `'F'` order is faster to compute, but\n    may slow down subsequent estimators.\n\nsparse_output : bool, default=False\n    Will return sparse CSR matrix if set True else will return an array. This\n    option is only available with `scipy>=1.8`.\n\n    .. versionadded:: 1.2\n\nAttributes\n----------\nbsplines_ : list of shape (n_features,)\n    List of BSplines objects, one for each feature.\n\nn_features_in_ : int\n    The total number of input features.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_features_out_ : int\n    The total number of output features, which is computed as\n    `n_features * n_splines`, where `n_splines` is\n    the number of bases elements of the B-splines,\n    `n_knots + degree - 1` for non-periodic splines and\n    `n_knots - 1` for periodic ones.\n    If `include_bias=False`, then it is only\n    `n_features * (n_splines - 1)`.\n\nSee Also\n--------\nKBinsDiscretizer : Transformer that bins continuous data into intervals.\n\nPolynomialFeatures : Transformer that generates polynomial and interaction\n    features.\n\nNotes\n-----\nHigh degrees and a high number of knots can cause overfitting.\n\nSee :ref:`examples/linear_model/plot_polynomial_interpolation.py\n<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.preprocessing import SplineTransformer\n>>> X = np.arange(6).reshape(6, 1)\n>>> spline = SplineTransformer(degree=2, n_knots=3)\n>>> spline.fit_transform(X)\narray([[0.5 , 0.5 , 0.  , 0.  ],\n       [0.18, 0.74, 0.08, 0.  ],\n       [0.02, 0.66, 0.32, 0.  ],\n       [0.  , 0.32, 0.66, 0.02],\n       [0.  , 0.08, 0.74, 0.18],\n       [0.  , 0.  , 0.5 , 0.5 ]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : ""
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Stack of estimators with a final classifier.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nestimators : list of (str, estimator)\n    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n\n    The type of estimator is generally expected to be a classifier.\n    However, one can pass a regressor for some use case (e.g. ordinal\n    regression).\n\nfinal_estimator : estimator, default=None\n    A classifier which will be used to combine the base estimators.\n    The default classifier is a\n    :class:`~sklearn.linear_model.LogisticRegression`.\n\ncv : int, cross-validation generator, iterable, or \"prefit\", default=None\n    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits,\n    * `\"prefit\"` to assume the `estimators` are prefit. In this case, the\n      estimators will not be refitted.\n\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used.\n    In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n    These splitters are instantiated with `shuffle=False` so the splits\n    will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    If \"prefit\" is passed, it is assumed that all `estimators` have\n    been fitted already. The `final_estimator_` is trained on the `estimators`\n    predictions on the full training set and are **not** cross validated\n    predictions. Please note that if the models have been trained on the same\n    data to train the stacking model, there is a very high risk of overfitting.\n\n    .. versionadded:: 1.1\n        The 'prefit' option was added in 1.1\n\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n\nstack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n    Methods called for each base estimator. It can be:\n\n    * if 'auto', it will try to invoke, for each estimator,\n      `'predict_proba'`, `'decision_function'` or `'predict'` in that\n      order.\n    * otherwise, one of `'predict_proba'`, `'decision_function'` or\n      `'predict'`. If the method is not implemented by the estimator, it\n      will raise an error.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel all `estimators` `fit`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n\npassthrough : bool, default=False\n    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n\nverbose : int, default=0\n    Verbosity level.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray if `y`         is of type `\"multilabel-indicator\"`.\n    Class labels.\n\nestimators_ : list of estimators\n    The elements of the `estimators` parameter, having been fitted on the\n    training data. If an estimator has been set to `'drop'`, it\n    will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n    is set to `estimators` and is not fitted again.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying classifier exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if the\n    underlying estimators expose such an attribute when fit.\n\n    .. versionadded:: 1.0\n\nfinal_estimator_ : estimator\n    The classifier which predicts given the output of `estimators_`.\n\nstack_method_ : list of str\n    The method used by each base estimator.\n\nSee Also\n--------\nStackingRegressor : Stack of estimators with a final regressor.\n\nNotes\n-----\nWhen `predict_proba` is used by each estimator (i.e. most of the time for\n`stack_method='auto'` or specifically for `stack_method='predict_proba'`),\nThe first column predicted by each estimator will be dropped in the case\nof a binary classification problem. Indeed, both feature will be perfectly\ncollinear.\n\nIn some cases (e.g. ordinal regression), one can pass regressors as the\nfirst layer of the :class:`StackingClassifier`. However, note that `y` will\nbe internally encoded in a numerically increasing order or lexicographic\norder. If this ordering is not adequate, one should manually numerically\nencode the classes in the desired order.\n\nReferences\n----------\n.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.ensemble import StackingClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> estimators = [\n...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n...     ('svr', make_pipeline(StandardScaler(),\n...                           LinearSVC(dual=\"auto\", random_state=42)))\n... ]\n>>> clf = StackingClassifier(\n...     estimators=estimators, final_estimator=LogisticRegression()\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, stratify=y, random_state=42\n... )\n>>> clf.fit(X_train, y_train).score(X_test, y_test)\n0.9..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Stack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a regressor to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nestimators : list of (str, estimator)\n    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n\nfinal_estimator : estimator, default=None\n    A regressor which will be used to combine the base estimators.\n    The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.\n\ncv : int, cross-validation generator, iterable, or \"prefit\", default=None\n    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n    * \"prefit\" to assume the `estimators` are prefit, and skip cross validation\n\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used.\n    In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n    These splitters are instantiated with `shuffle=False` so the splits\n    will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    If \"prefit\" is passed, it is assumed that all `estimators` have\n    been fitted already. The `final_estimator_` is trained on the `estimators`\n    predictions on the full training set and are **not** cross validated\n    predictions. Please note that if the models have been trained on the same\n    data to train the stacking model, there is a very high risk of overfitting.\n\n    .. versionadded:: 1.1\n        The 'prefit' option was added in 1.1\n\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for `fit` of all `estimators`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n\npassthrough : bool, default=False\n    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n\nverbose : int, default=0\n    Verbosity level.\n\nAttributes\n----------\nestimators_ : list of estimator\n    The elements of the `estimators` parameter, having been fitted on the\n    training data. If an estimator has been set to `'drop'`, it\n    will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n    is set to `estimators` and is not fitted again.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying regressor exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if the\n    underlying estimators expose such an attribute when fit.\n\n    .. versionadded:: 1.0\n\nfinal_estimator_ : estimator\n    The regressor to stacked the base estimators fitted.\n\nstack_method_ : list of str\n    The method used by each base estimator.\n\nSee Also\n--------\nStackingClassifier : Stack of estimators with a final classifier.\n\nReferences\n----------\n.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import StackingRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> estimators = [\n...     ('lr', RidgeCV()),\n...     ('svr', LinearSVR(dual=\"auto\", random_state=42))\n... ]\n>>> reg = StackingRegressor(\n...     estimators=estimators,\n...     final_estimator=RandomForestRegressor(n_estimators=10,\n...                                           random_state=42)\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42\n... )\n>>> reg.fit(X_train, y_train).score(X_test, y_test)\n0.3..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Standardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample `x` is calculated as:\n\n    z = (x - u) / s\n\nwhere `u` is the mean of the training samples or zero if `with_mean=False`,\nand `s` is the standard deviation of the training samples or one if\n`with_std=False`.\n\nCentering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using\n:meth:`transform`.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthan others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\n`StandardScaler` is sensitive to outliers, and the features may scale\ndifferently from each other in the presence of outliers. For an example\nvisualization, refer to :ref:`Compare StandardScaler with other scalers\n<plot_all_scaling_standard_scaler_section>`.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing\n`with_mean=False` to avoid breaking the sparsity structure of the data.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\ncopy : bool, default=True\n    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n\nwith_mean : bool, default=True\n    If True, center the data before scaling.\n    This does not work (and will raise an exception) when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n\nwith_std : bool, default=True\n    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n\nAttributes\n----------\nscale_ : ndarray of shape (n_features,) or None\n    Per feature relative scaling of the data to achieve zero mean and unit\n    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n    variance is zero, we can't achieve unit variance, and the data is left\n    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n    when `with_std=False`.\n\n    .. versionadded:: 0.17\n       *scale_*\n\nmean_ : ndarray of shape (n_features,) or None\n    The mean value for each feature in the training set.\n    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\nvar_ : ndarray of shape (n_features,) or None\n    The variance for each feature in the training set. Used to compute\n    `scale_`. Equal to ``None`` when ``with_mean=False`` and\n    ``with_std=False``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_seen_ : int or ndarray of shape (n_features,)\n    The number of samples processed by the estimator for each feature.\n    If there are no missing samples, the ``n_samples_seen`` will be an\n    integer, otherwise it will be an array of dtype int. If\n    `sample_weights` are used it will be a float (if no missing data)\n    or an array of dtype float that sums the weights seen so far.\n    Will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nSee Also\n--------\nscale : Equivalent function without the estimator API.\n\n:class:`~sklearn.decomposition.PCA` : Further removes the linear\n    correlation across features with 'whiten=True'.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nWe use a biased estimator for the standard deviation, equivalent to\n`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\naffect model performance.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler\n>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n>>> scaler = StandardScaler()\n>>> print(scaler.fit(data))\nStandardScaler()\n>>> print(scaler.mean_)\n[0.5 0.5]\n>>> print(scaler.transform(data))\n[[-1. -1.]\n [-1. -1.]\n [ 1.  1.]\n [ 1.  1.]]\n>>> print(scaler.transform([[2, 2]]))\n[[3. 3.]]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Stratified K-Fold iterator variant with non-overlapping groups.\n\nThis cross-validation object is a variation of StratifiedKFold attempts to\nreturn stratified folds with non-overlapping groups. The folds are made by\npreserving the percentage of samples for each class.\n\nEach group will appear exactly once in the test set across all folds (the\nnumber of distinct groups has to be at least equal to the number of folds).\n\nThe difference between :class:`~sklearn.model_selection.GroupKFold`\nand :class:`~sklearn.model_selection.StratifiedGroupKFold` is that\nthe former attempts to create balanced folds such that the number of\ndistinct groups is approximately the same in each fold, whereas\nStratifiedGroupKFold attempts to create folds which preserve the\npercentage of samples for each class as much as possible given the\nconstraint of non-overlapping groups between splits.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\nshuffle : bool, default=False\n    Whether to shuffle each class's samples before splitting into batches.\n    Note that the samples within each split will not be shuffled.\n    This implementation can only shuffle groups that have approximately the\n    same y distribution, no global shuffle will be performed.\n\nrandom_state : int or RandomState instance, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold for each class.\n    Otherwise, leave `random_state` as `None`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import StratifiedGroupKFold\n>>> X = np.ones((17, 2))\n>>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n>>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n>>> sgkf = StratifiedGroupKFold(n_splits=3)\n>>> sgkf.get_n_splits(X, y)\n3\n>>> print(sgkf)\nStratifiedGroupKFold(n_splits=3, random_state=None, shuffle=False)\n>>> for i, (train_index, test_index) in enumerate(sgkf.split(X, y, groups)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"         group={groups[train_index]}\")\n...     print(f\"  Test:  index={test_index}\")\n...     print(f\"         group={groups[test_index]}\")\nFold 0:\n  Train: index=[ 0  1  2  3  7  8  9 10 11 15 16]\n         group=[1 1 2 2 4 5 5 5 5 8 8]\n  Test:  index=[ 4  5  6 12 13 14]\n         group=[3 3 3 6 6 7]\nFold 1:\n  Train: index=[ 4  5  6  7  8  9 10 11 12 13 14]\n         group=[3 3 3 4 5 5 5 5 6 6 7]\n  Test:  index=[ 0  1  2  3 15 16]\n         group=[1 1 2 2 8 8]\nFold 2:\n  Train: index=[ 0  1  2  3  4  5  6 12 13 14 15 16]\n         group=[1 1 2 2 3 3 3 6 6 7 8 8]\n  Test:  index=[ 7  8  9 10 11]\n         group=[4 5 5 5 5]\n\nNotes\n-----\nThe implementation is designed to:\n\n* Mimic the behavior of StratifiedKFold as much as possible for trivial\n  groups (e.g. when each group contains only one sample).\n* Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n  ``y = [1, 0]`` should not change the indices generated.\n* Stratify based on samples as much as possible while keeping\n  non-overlapping groups constraint. That means that in some cases when\n  there is a small number of groups containing a large number of samples\n  the stratification will not be possible and the behavior will be close\n  to GroupKFold.\n\nSee also\n--------\nStratifiedKFold: Takes class information into account to build folds which\n    retain class distributions (for binary or multiclass classification\n    tasks).\n\nGroupKFold: K-fold iterator variant with non-overlapping groups."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Stratified K-Fold cross-validator.\n\nProvides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a variation of KFold that returns\nstratified folds. The folds are made by preserving the percentage of\nsamples for each class.\n\nRead more in the :ref:`User Guide <stratified_k_fold>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nshuffle : bool, default=False\n    Whether to shuffle each class's samples before splitting into batches.\n    Note that the samples within each split will not be shuffled.\n\nrandom_state : int, RandomState instance or None, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold for each class.\n    Otherwise, leave `random_state` as `None`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import StratifiedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> skf = StratifiedKFold(n_splits=2)\n>>> skf.get_n_splits(X, y)\n2\n>>> print(skf)\nStratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n>>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[1 3]\n  Test:  index=[0 2]\nFold 1:\n  Train: index=[0 2]\n  Test:  index=[1 3]\n\nNotes\n-----\nThe implementation is designed to:\n\n* Generate test sets such that all contain the same distribution of\n  classes, or as close as possible.\n* Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n  ``y = [1, 0]`` should not change the indices generated.\n* Preserve order dependencies in the dataset ordering, when\n  ``shuffle=False``: all samples from class k in some test set were\n  contiguous in y, or separated in y by samples from classes other than k.\n* Generate test sets where the smallest and largest differ by at most one\n  sample.\n\n.. versionchanged:: 0.22\n    The previous implementation did not follow the last constraint.\n\nSee Also\n--------\nRepeatedStratifiedKFold : Repeats Stratified K-Fold n times."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Stratified ShuffleSplit cross-validator.\n\nProvides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a merge of StratifiedKFold and\nShuffleSplit, which returns stratified randomized folds. The folds\nare made by preserving the percentage of samples for each class.\n\nNote: like the ShuffleSplit strategy, stratified random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <stratified_shuffle_split>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=10\n    Number of re-shuffling & splitting iterations.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.1.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the training and testing indices produced.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import StratifiedShuffleSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 0, 1, 1, 1])\n>>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n>>> sss.get_n_splits(X, y)\n5\n>>> print(sss)\nStratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n>>> for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[5 2 3]\n  Test:  index=[4 1 0]\nFold 1:\n  Train: index=[5 1 4]\n  Test:  index=[0 2 3]\nFold 2:\n  Train: index=[5 0 2]\n  Test:  index=[4 3 1]\nFold 3:\n  Train: index=[4 1 0]\n  Test:  index=[2 3 5]\nFold 4:\n  Train: index=[0 5 1]\n  Test:  index=[3 4 2]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SupportVectorRegression",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SvmModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Target Encoder for regression and classification targets.\n\nEach category is encoded based on a shrunk estimate of the average target\nvalues for observations belonging to the category. The encoding scheme mixes\nthe global target mean with the target mean conditioned on the value of the\ncategory (see [MIC]_).\n\nWhen the target type is \"multiclass\", encodings are based\non the conditional probability estimate for each class. The target is first\nbinarized using the \"one-vs-all\" scheme via\n:class:`~sklearn.preprocessing.LabelBinarizer`, then the average target\nvalue for each class and each category is used for encoding, resulting in\n`n_features` * `n_classes` encoded output features.\n\n:class:`TargetEncoder` considers missing values, such as `np.nan` or `None`,\nas another category and encodes them like any other category. Categories\nthat are not seen during :meth:`fit` are encoded with the target mean, i.e.\n`target_mean_`.\n\nFor a demo on the importance of the `TargetEncoder` internal cross-fitting,\nsee\n:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder_cross_val.py`.\nFor a comparison of different encoders, refer to\n:ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`. Read\nmore in the :ref:`User Guide <target_encoder>`.\n\n.. note::\n    `fit(X, y).transform(X)` does not equal `fit_transform(X, y)` because a\n    :term:`cross fitting` scheme is used in `fit_transform` for encoding.\n    See the :ref:`User Guide <target_encoder>` for details.\n\n.. versionadded:: 1.3\n\nParameters\n----------\ncategories : \"auto\" or list of shape (n_features,) of array-like, default=\"auto\"\n    Categories (unique values) per feature:\n\n    - `\"auto\"` : Determine categories automatically from the training data.\n    - list : `categories[i]` holds the categories expected in the i-th column. The\n      passed categories should not mix strings and numeric values within a single\n      feature, and should be sorted in case of numeric values.\n\n    The used categories are stored in the `categories_` fitted attribute.\n\ntarget_type : {\"auto\", \"continuous\", \"binary\", \"multiclass\"}, default=\"auto\"\n    Type of target.\n\n    - `\"auto\"` : Type of target is inferred with\n      :func:`~sklearn.utils.multiclass.type_of_target`.\n    - `\"continuous\"` : Continuous target\n    - `\"binary\"` : Binary target\n    - `\"multiclass\"` : Multiclass target\n\n    .. note::\n        The type of target inferred with `\"auto\"` may not be the desired target\n        type used for modeling. For example, if the target consisted of integers\n        between 0 and 100, then :func:`~sklearn.utils.multiclass.type_of_target`\n        will infer the target as `\"multiclass\"`. In this case, setting\n        `target_type=\"continuous\"` will specify the target as a regression\n        problem. The `target_type_` attribute gives the target type used by the\n        encoder.\n\n    .. versionchanged:: 1.4\n       Added the option 'multiclass'.\n\nsmooth : \"auto\" or float, default=\"auto\"\n    The amount of mixing of the target mean conditioned on the value of the\n    category with the global target mean. A larger `smooth` value will put\n    more weight on the global target mean.\n    If `\"auto\"`, then `smooth` is set to an empirical Bayes estimate.\n\ncv : int, default=5\n    Determines the number of folds in the :term:`cross fitting` strategy used in\n    :meth:`fit_transform`. For classification targets, `StratifiedKFold` is used\n    and for continuous targets, `KFold` is used.\n\nshuffle : bool, default=True\n    Whether to shuffle the data in :meth:`fit_transform` before splitting into\n    folds. Note that the samples within each split will not be shuffled.\n\nrandom_state : int, RandomState instance or None, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold. Otherwise, this\n    parameter has no effect.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nencodings_ : list of shape (n_features,) or (n_features * n_classes) of                     ndarray\n    Encodings learnt on all of `X`.\n    For feature `i`, `encodings_[i]` are the encodings matching the\n    categories listed in `categories_[i]`. When `target_type_` is\n    \"multiclass\", the encoding for feature `i` and class `j` is stored in\n    `encodings_[j + (i * len(classes_))]`. E.g., for 2 features (f) and\n    3 classes (c), encodings are ordered:\n    f0_c0, f0_c1, f0_c2, f1_c0, f1_c1, f1_c2,\n\ncategories_ : list of shape (n_features,) of ndarray\n    The categories of each input feature determined during fitting or\n    specified in `categories`\n    (in order of the features in `X` and corresponding with the output\n    of :meth:`transform`).\n\ntarget_type_ : str\n    Type of target.\n\ntarget_mean_ : float\n    The overall mean of the target. This value is only used in :meth:`transform`\n    to encode categories.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\nclasses_ : ndarray or None\n    If `target_type_` is 'binary' or 'multiclass', holds the label for each class,\n    otherwise `None`.\n\nSee Also\n--------\nOrdinalEncoder : Performs an ordinal (integer) encoding of the categorical features.\n    Contrary to TargetEncoder, this encoding is not supervised. Treating the\n    resulting encoding as a numerical features therefore lead arbitrarily\n    ordered values and therefore typically lead to lower predictive performance\n    when used as preprocessing for a classifier or regressor.\nOneHotEncoder : Performs a one-hot encoding of categorical features. This\n    unsupervised encoding is better suited for low cardinality categorical\n    variables as it generate one new feature per unique category.\n\nReferences\n----------\n.. [MIC] :doi:`Micci-Barreca, Daniele. \"A preprocessing scheme for high-cardinality\n   categorical attributes in classification and prediction problems\"\n   SIGKDD Explor. Newsl. 3, 1 (July 2001), 27‚Äì32. <10.1145/507533.507538>`\n\nExamples\n--------\nWith `smooth=\"auto\"`, the smoothing parameter is set to an empirical Bayes estimate:\n\n>>> import numpy as np\n>>> from sklearn.preprocessing import TargetEncoder\n>>> X = np.array([[\"dog\"] * 20 + [\"cat\"] * 30 + [\"snake\"] * 38], dtype=object).T\n>>> y = [90.3] * 5 + [80.1] * 15 + [20.4] * 5 + [20.1] * 25 + [21.2] * 8 + [49] * 30\n>>> enc_auto = TargetEncoder(smooth=\"auto\")\n>>> X_trans = enc_auto.fit_transform(X, y)\n\n>>> # A high `smooth` parameter puts more weight on global mean on the categorical\n>>> # encodings:\n>>> enc_high_smooth = TargetEncoder(smooth=5000.0).fit(X, y)\n>>> enc_high_smooth.target_mean_\n44...\n>>> enc_high_smooth.encodings_\n[array([44..., 44..., 44...])]\n\n>>> # On the other hand, a low `smooth` parameter puts more weight on target\n>>> # conditioned on the value of the categorical:\n>>> enc_low_smooth = TargetEncoder(smooth=1.0).fit(X, y)\n>>> enc_low_smooth.encodings_\n[array([20..., 80..., 43...])]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PreprocessingModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Theil-Sen Estimator: robust multivariate regression model.\n\nThe algorithm calculates least square solutions on subsets with size\nn_subsamples of the samples in X. Any value of n_subsamples between the\nnumber of features and samples leads to an estimator with a compromise\nbetween robustness and efficiency. Since the number of least square\nsolutions is \"n_samples choose n_subsamples\", it can be extremely large\nand can therefore be limited with max_subpopulation. If this limit is\nreached, the subsets are chosen randomly. In a final step, the spatial\nmedian (or L1 median) is calculated of all least square solutions.\n\nRead more in the :ref:`User Guide <theil_sen_regression>`.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations.\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nmax_subpopulation : int, default=1e4\n    Instead of computing with a set of cardinality 'n choose k', where n is\n    the number of samples and k is the number of subsamples (at least\n    number of features), consider only a stochastic subpopulation of a\n    given maximal size if 'n choose k' is larger than max_subpopulation.\n    For other than small problem sizes this parameter will determine\n    memory usage and runtime if n_subsamples is not changed. Note that the\n    data type should be int but floats such as 1e4 can be accepted too.\n\nn_subsamples : int, default=None\n    Number of samples to calculate the parameters. This is at least the\n    number of features (plus 1 if fit_intercept=True) and the number of\n    samples as a maximum. A lower number leads to a higher breakdown\n    point and a low efficiency while a high number leads to a low\n    breakdown point and a high efficiency. If None, take the\n    minimum number of subsamples leading to maximal robustness.\n    If n_subsamples is set to n_samples, Theil-Sen is identical to least\n    squares.\n\nmax_iter : int, default=300\n    Maximum number of iterations for the calculation of spatial median.\n\ntol : float, default=1e-3\n    Tolerance when calculating spatial median.\n\nrandom_state : int, RandomState instance or None, default=None\n    A random number generator instance to define the state of the random\n    permutations generator. Pass an int for reproducible output across\n    multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    Number of CPUs to use during the cross validation.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    Verbose mode when fitting the model.\n\nAttributes\n----------\ncoef_ : ndarray of shape (n_features,)\n    Coefficients of the regression model (median of distribution).\n\nintercept_ : float\n    Estimated intercept of regression model.\n\nbreakdown_ : float\n    Approximated breakdown point.\n\nn_iter_ : int\n    Number of iterations needed for the spatial median.\n\nn_subpopulation_ : int\n    Number of combinations taken into account from 'n choose k', where n is\n    the number of samples and k is the number of subsamples.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nHuberRegressor : Linear regression model that is robust to outliers.\nRANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\nSGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n\nReferences\n----------\n- Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n  Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n  http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n\nExamples\n--------\n>>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])\narray([-31.5871...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Time Series cross-validator.\n\nProvides train/test indices to split time series data samples\nthat are observed at fixed time intervals, in train/test sets.\nIn each split, test indices must be higher than before, and thus shuffling\nin cross validator is inappropriate.\n\nThis cross-validation object is a variation of :class:`KFold`.\nIn the kth split, it returns first k folds as train set and the\n(k+1)th fold as test set.\n\nNote that unlike standard cross-validation methods, successive\ntraining sets are supersets of those that come before them.\n\nRead more in the :ref:`User Guide <time_series_split>`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\n.. versionadded:: 0.18\n\nParameters\n----------\nn_splits : int, default=5\n    Number of splits. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nmax_train_size : int, default=None\n    Maximum size for a single training set.\n\ntest_size : int, default=None\n    Used to limit the size of the test set. Defaults to\n    ``n_samples // (n_splits + 1)``, which is the maximum allowed value\n    with ``gap=0``.\n\n    .. versionadded:: 0.24\n\ngap : int, default=0\n    Number of samples to exclude from the end of each train set before\n    the test set.\n\n    .. versionadded:: 0.24\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import TimeSeriesSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> tscv = TimeSeriesSplit()\n>>> print(tscv)\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n>>> for i, (train_index, test_index) in enumerate(tscv.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[0]\n  Test:  index=[1]\nFold 1:\n  Train: index=[0 1]\n  Test:  index=[2]\nFold 2:\n  Train: index=[0 1 2]\n  Test:  index=[3]\nFold 3:\n  Train: index=[0 1 2 3]\n  Test:  index=[4]\nFold 4:\n  Train: index=[0 1 2 3 4]\n  Test:  index=[5]\n>>> # Fix test_size to 2 with 12 samples\n>>> X = np.random.randn(12, 2)\n>>> y = np.random.randint(0, 2, 12)\n>>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n>>> for i, (train_index, test_index) in enumerate(tscv.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[0 1 2 3 4 5]\n  Test:  index=[6 7]\nFold 1:\n  Train: index=[0 1 2 3 4 5 6 7]\n  Test:  index=[8 9]\nFold 2:\n  Train: index=[0 1 2 3 4 5 6 7 8 9]\n  Test:  index=[10 11]\n>>> # Add in a 2 period gap\n>>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n>>> for i, (train_index, test_index) in enumerate(tscv.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[0 1 2 3]\n  Test:  index=[6 7]\nFold 1:\n  Train: index=[0 1 2 3 4 5]\n  Test:  index=[8 9]\nFold 2:\n  Train: index=[0 1 2 3 4 5 6 7]\n  Test:  index=[10 11]\n\nFor a more extended example see\n:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`.\n\nNotes\n-----\nThe training set has size ``i * n_samples // (n_splits + 1)\n+ n_samples % (n_splits + 1)`` in the ``i`` th split,\nwith a test set of size ``n_samples//(n_splits + 1)`` by default,\nwhere ``n_samples`` is the number of samples."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Top-k Accuracy classification score.\n\nThis metric computes the number of times where the correct label is among\nthe top `k` labels predicted (ranked by predicted scores). Note that the\nmultilabel case isn't covered here.\n\nRead more in the :ref:`User Guide <top_k_accuracy_score>`\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True labels.\n\ny_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n    Target scores. These can be either probability estimates or\n    non-thresholded decision values (as returned by\n    :term:`decision_function` on some classifiers).\n    The binary case expects scores with shape (n_samples,) while the\n    multiclass case expects scores with shape (n_samples, n_classes).\n    In the multiclass case, the order of the class scores must\n    correspond to the order of ``labels``, if provided, or else to\n    the numerical or lexicographical order of the labels in ``y_true``.\n    If ``y_true`` does not contain all the labels, ``labels`` must be\n    provided.\n\nk : int, default=2\n    Number of most likely outcomes considered to find the correct label.\n\nnormalize : bool, default=True\n    If `True`, return the fraction of correctly classified samples.\n    Otherwise, return the number of correctly classified samples.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If `None`, all samples are given the same weight.\n\nlabels : array-like of shape (n_classes,), default=None\n    Multiclass only. List of labels that index the classes in ``y_score``.\n    If ``None``, the numerical or lexicographical order of the labels in\n    ``y_true`` is used. If ``y_true`` does not contain all the labels,\n    ``labels`` must be provided.\n\nReturns\n-------\nscore : float\n    The top-k accuracy score. The best performance is 1 with\n    `normalize == True` and the number of samples with\n    `normalize == False`.\n\nSee Also\n--------\naccuracy_score : Compute the accuracy score. By default, the function will\n    return the fraction of correct predictions divided by the total number\n    of predictions.\n\nNotes\n-----\nIn cases where two or more labels are assigned equal predicted scores,\nthe labels with the highest indices will be chosen first. This might\nimpact the result if the correct label falls after the threshold because\nof that.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.metrics import top_k_accuracy_score\n>>> y_true = np.array([0, 1, 2, 2])\n>>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n>>> top_k_accuracy_score(y_true, y_score, k=2)\n0.75\n>>> # Not normalizing gives the number of \"correctly\" classified samples\n>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n3"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation,\n``next(ShuffleSplit().split(X, y))``, and application to input data\ninto a single call for splitting (and optionally subsampling) data into a\none-liner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\n*arrays : sequence of indexables with same length / shape[0]\n    Allowed inputs are lists, numpy arrays, scipy-sparse\n    matrices or pandas dataframes.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.25.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the shuffling applied to the data before applying the split.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data before splitting. If shuffle=False\n    then stratify must be None.\n\nstratify : array-like, default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n    Read more in the :ref:`User Guide <stratification>`.\n\nReturns\n-------\nsplitting : list, length=2 * len(arrays)\n    List containing train-test split of inputs.\n\n    .. versionadded:: 0.16\n        If the input is sparse, the output will be a\n        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n        input type.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = np.arange(10).reshape((5, 2)), range(5)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(y)\n[0, 1, 2, 3, 4]\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, random_state=42)\n...\n>>> X_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> y_train\n[2, 0, 3]\n>>> X_test\narray([[2, 3],\n       [8, 9]])\n>>> y_test\n[1, 4]\n\n>>> train_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TreeModule",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SklearnModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Dimensionality reduction using truncated SVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of\ntruncated singular value decomposition (SVD). Contrary to PCA, this\nestimator does not center the data before computing the singular value\ndecomposition. This means it can work with sparse matrices\nefficiently.\n\nIn particular, truncated SVD works on term count/tf-idf matrices as\nreturned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\nthat context, it is known as latent semantic analysis (LSA).\n\nThis estimator supports two algorithms: a fast randomized SVD solver, and\na \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n`X.T * X`, whichever is more efficient.\n\nRead more in the :ref:`User Guide <LSA>`.\n\nParameters\n----------\nn_components : int, default=2\n    Desired dimensionality of output data.\n    If algorithm='arpack', must be strictly less than the number of features.\n    If algorithm='randomized', must be less than or equal to the number of features.\n    The default value is useful for visualisation. For LSA, a value of\n    100 is recommended.\n\nalgorithm : {'arpack', 'randomized'}, default='randomized'\n    SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n    (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n    algorithm due to Halko (2009).\n\nn_iter : int, default=5\n    Number of iterations for randomized SVD solver. Not used by ARPACK. The\n    default is larger than the default in\n    :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n    matrices that may have large slowly decaying spectrum.\n\nn_oversamples : int, default=10\n    Number of oversamples for randomized SVD solver. Not used by ARPACK.\n    See :func:`~sklearn.utils.extmath.randomized_svd` for a complete\n    description.\n\n    .. versionadded:: 1.1\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Power iteration normalizer for randomized SVD solver.\n    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n    for more details.\n\n    .. versionadded:: 1.1\n\nrandom_state : int, RandomState instance or None, default=None\n    Used during randomized svd. Pass an int for reproducible results across\n    multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.0\n    Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n    SVD solver.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The right singular vectors of the input data.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The variance of the training samples transformed by a projection to\n    each component.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nFactorAnalysis : A simple linear generative model with\n    Gaussian latent variables.\nIncrementalPCA : Incremental principal components analysis.\nKernelPCA : Kernel Principal component analysis.\nNMF : Non-Negative Matrix Factorization.\nPCA : Principal component analysis.\n\nNotes\n-----\nSVD suffers from a problem called \"sign indeterminacy\", which means the\nsign of the ``components_`` and the output from transform depend on the\nalgorithm and random state. To work around this, fit instances of this\nclass to data once, then keep the instance around to do transformations.\n\nReferences\n----------\n:arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\nStochastic algorithms for constructing approximate matrix decompositions\"\n<0909.4061>`\n\nExamples\n--------\n>>> from sklearn.decomposition import TruncatedSVD\n>>> from scipy.sparse import csr_matrix\n>>> import numpy as np\n>>> np.random.seed(0)\n>>> X_dense = np.random.rand(100, 100)\n>>> X_dense[:, 2 * np.arange(50)] = 0\n>>> X = csr_matrix(X_dense)\n>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> svd.fit(X)\nTruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> print(svd.explained_variance_ratio_)\n[0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n>>> print(svd.explained_variance_ratio_.sum())\n0.2102...\n>>> print(svd.singular_values_)\n[35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecompositionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Generalized Linear Model with a Tweedie distribution.\n\nThis estimator can be used to model different GLMs depending on the\n``power`` parameter, which determines the underlying distribution.\n\nRead more in the :ref:`User Guide <Generalized_linear_models>`.\n\n.. versionadded:: 0.23\n\nParameters\n----------\npower : float, default=0\n        The power determines the underlying target distribution according\n        to the following table:\n\n        +-------+------------------------+\n        | Power | Distribution           |\n        +=======+========================+\n        | 0     | Normal                 |\n        +-------+------------------------+\n        | 1     | Poisson                |\n        +-------+------------------------+\n        | (1,2) | Compound Poisson Gamma |\n        +-------+------------------------+\n        | 2     | Gamma                  |\n        +-------+------------------------+\n        | 3     | Inverse Gaussian       |\n        +-------+------------------------+\n\n        For ``0 < power < 1``, no distribution exists.\n\nalpha : float, default=1\n    Constant that multiplies the L2 penalty term and determines the\n    regularization strength. ``alpha = 0`` is equivalent to unpenalized\n    GLMs. In this case, the design matrix `X` must have full column rank\n    (no collinearities).\n    Values of `alpha` must be in the range `[0.0, inf)`.\n\nfit_intercept : bool, default=True\n    Specifies if a constant (a.k.a. bias or intercept) should be\n    added to the linear predictor (`X @ coef + intercept`).\n\nlink : {'auto', 'identity', 'log'}, default='auto'\n    The link function of the GLM, i.e. mapping from linear predictor\n    `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n    the link depending on the chosen `power` parameter as follows:\n\n    - 'identity' for ``power <= 0``, e.g. for the Normal distribution\n    - 'log' for ``power > 0``, e.g. for Poisson, Gamma and Inverse Gaussian\n      distributions\n\nsolver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n    Algorithm to use in the optimization problem:\n\n    'lbfgs'\n        Calls scipy's L-BFGS-B optimizer.\n\n    'newton-cholesky'\n        Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n        iterated reweighted least squares) with an inner Cholesky based solver.\n        This solver is a good choice for `n_samples` >> `n_features`, especially\n        with one-hot encoded categorical features with rare categories. Be aware\n        that the memory usage of this solver has a quadratic dependency on\n        `n_features` because it explicitly computes the Hessian matrix.\n\n        .. versionadded:: 1.2\n\nmax_iter : int, default=100\n    The maximal number of iterations for the solver.\n    Values must be in the range `[1, inf)`.\n\ntol : float, default=1e-4\n    Stopping criterion. For the lbfgs solver,\n    the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n    where ``g_j`` is the j-th component of the gradient (derivative) of\n    the objective function.\n    Values must be in the range `(0.0, inf)`.\n\nwarm_start : bool, default=False\n    If set to ``True``, reuse the solution of the previous call to ``fit``\n    as initialization for ``coef_`` and ``intercept_`` .\n\nverbose : int, default=0\n    For the lbfgs solver set verbose to any positive number for verbosity.\n    Values must be in the range `[0, inf)`.\n\nAttributes\n----------\ncoef_ : array of shape (n_features,)\n    Estimated coefficients for the linear predictor (`X @ coef_ +\n    intercept_`) in the GLM.\n\nintercept_ : float\n    Intercept (a.k.a. bias) added to linear predictor.\n\nn_iter_ : int\n    Actual number of iterations used in the solver.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nPoissonRegressor : Generalized Linear Model with a Poisson distribution.\nGammaRegressor : Generalized Linear Model with a Gamma distribution.\n\nExamples\n--------\n>>> from sklearn import linear_model\n>>> clf = linear_model.TweedieRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [2, 3.5, 5, 5.5]\n>>> clf.fit(X, y)\nTweedieRegressor()\n>>> clf.score(X, y)\n0.839...\n>>> clf.coef_\narray([0.599..., 0.299...])\n>>> clf.intercept_\n1.600...\n>>> clf.predict([[1, 1], [3, 4]])\narray([2.500..., 4.599...])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GlmModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearModelModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "V-measure cluster labeling given a ground truth.\n\nThis score is identical to :func:`normalized_mutual_info_score` with\nthe ``'arithmetic'`` option for averaging.\n\nThe V-measure is the harmonic mean between homogeneity and completeness::\n\n    v = (1 + beta) * homogeneity * completeness\n         / (beta * homogeneity + completeness)\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n\nParameters\n----------\nlabels_true : array-like of shape (n_samples,)\n    Ground truth class labels to be used as a reference.\n\nlabels_pred : array-like of shape (n_samples,)\n    Cluster labels to evaluate.\n\nbeta : float, default=1.0\n    Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n    If ``beta`` is greater than 1, ``completeness`` is weighted more\n    strongly in the calculation. If ``beta`` is less than 1,\n    ``homogeneity`` is weighted more strongly.\n\nReturns\n-------\nv_measure : float\n   Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n\nSee Also\n--------\nhomogeneity_score : Homogeneity metric of cluster labeling.\ncompleteness_score : Completeness metric of cluster labeling.\nnormalized_mutual_info_score : Normalized Mutual Information.\n\nReferences\n----------\n\n.. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n   conditional entropy-based external cluster evaluation measure\n   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n\nExamples\n--------\nPerfect labelings are both homogeneous and complete, hence have score 1.0::\n\n  >>> from sklearn.metrics.cluster import v_measure_score\n  >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n  1.0\n  >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n  1.0\n\nLabelings that assign all classes members to the same clusters\nare complete but not homogeneous, hence penalized::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n  0.8...\n  >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n  0.66...\n\nLabelings that have pure clusters with members coming from the same\nclasses are homogeneous but un-necessary splits harm completeness\nand thus penalize V-measure as well::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n  0.8...\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n  0.66...\n\nIf classes members are completely split across different clusters,\nthe assignment is totally incomplete, hence the V-Measure is null::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n  0.0...\n\nClusters that include samples from totally different classes totally\ndestroy the homogeneity of the labeling, hence::\n\n  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n  0.0..."
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Validation Curve visualization.\n\nIt is recommended to use\n:meth:`~sklearn.model_selection.ValidationCurveDisplay.from_estimator` to\ncreate a :class:`~sklearn.model_selection.ValidationCurveDisplay` instance.\nAll parameters are stored as attributes.\n\nRead more in the :ref:`User Guide <visualizations>` for general information\nabout the visualization API and :ref:`detailed documentation\n<validation_curve>` regarding the validation curve visualization.\n\n.. versionadded:: 1.3\n\nParameters\n----------\nparam_name : str\n    Name of the parameter that has been varied.\n\nparam_range : array-like of shape (n_ticks,)\n    The values of the parameter that have been evaluated.\n\ntrain_scores : ndarray of shape (n_ticks, n_cv_folds)\n    Scores on training sets.\n\ntest_scores : ndarray of shape (n_ticks, n_cv_folds)\n    Scores on test set.\n\nscore_name : str, default=None\n    The name of the score used in `validation_curve`. It will override the name\n    inferred from the `scoring` parameter. If `score` is `None`, we use `\"Score\"` if\n    `negate_score` is `False` and `\"Negative score\"` otherwise. If `scoring` is a\n    string or a callable, we infer the name. We replace `_` by spaces and capitalize\n    the first letter. We remove `neg_` and replace it by `\"Negative\"` if\n    `negate_score` is `False` or just remove it otherwise.\n\nAttributes\n----------\nax_ : matplotlib Axes\n    Axes with the validation curve.\n\nfigure_ : matplotlib Figure\n    Figure containing the validation curve.\n\nerrorbar_ : list of matplotlib Artist or None\n    When the `std_display_style` is `\"errorbar\"`, this is a list of\n    `matplotlib.container.ErrorbarContainer` objects. If another style is\n    used, `errorbar_` is `None`.\n\nlines_ : list of matplotlib Artist or None\n    When the `std_display_style` is `\"fill_between\"`, this is a list of\n    `matplotlib.lines.Line2D` objects corresponding to the mean train and\n    test scores. If another style is used, `line_` is `None`.\n\nfill_between_ : list of matplotlib Artist or None\n    When the `std_display_style` is `\"fill_between\"`, this is a list of\n    `matplotlib.collections.PolyCollection` objects. If another style is\n    used, `fill_between_` is `None`.\n\nSee Also\n--------\nsklearn.model_selection.validation_curve : Compute the validation curve.\n\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.model_selection import ValidationCurveDisplay, validation_curve\n>>> from sklearn.linear_model import LogisticRegression\n>>> X, y = make_classification(n_samples=1_000, random_state=0)\n>>> logistic_regression = LogisticRegression()\n>>> param_name, param_range = \"C\", np.logspace(-8, 3, 10)\n>>> train_scores, test_scores = validation_curve(\n...     logistic_regression, X, y, param_name=param_name, param_range=param_range\n... )\n>>> display = ValidationCurveDisplay(\n...     param_name=param_name, param_range=param_range,\n...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\"\n... )\n>>> display.plot()\n<...>\n>>> plt.show()"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplittingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelectionModule"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.\n\nParameters\n----------\nthreshold : float, default=0\n    Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n\nAttributes\n----------\nvariances_ : array, shape (n_features,)\n    Variances of individual features.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nSelectFromModel: Meta-transformer for selecting features based on\n    importance weights.\nSelectPercentile : Select features according to a percentile of the highest\n    scores.\nSequentialFeatureSelector : Transformer that performs Sequential Feature\n    Selection.\n\nNotes\n-----\nAllows NaN in the input.\nRaises ValueError if no feature in X meets the variance threshold.\n\nExamples\n--------\nThe following dataset has integer features, two of which are the same\nin every sample. These are removed with the default setting for threshold::\n\n    >>> from sklearn.feature_selection import VarianceThreshold\n    >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n    >>> selector = VarianceThreshold()\n    >>> selector.fit_transform(X)\n    array([[2, 0],\n           [1, 4],\n           [1, 1]])"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelectionModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Soft Voting/Majority Rule classifier for unfitted estimators.\n\nRead more in the :ref:`User Guide <voting_classifier>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nestimators : list of (str, estimator) tuples\n    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'`` using\n    :meth:`set_params`.\n\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n        support was removed in 0.24.\n\nvoting : {'hard', 'soft'}, default='hard'\n    If 'hard', uses predicted class labels for majority rule voting.\n    Else if 'soft', predicts the class label based on the argmax of\n    the sums of the predicted probabilities, which is recommended for\n    an ensemble of well-calibrated classifiers.\n\nweights : array-like of shape (n_classifiers,), default=None\n    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted class labels (`hard` voting) or class probabilities\n    before averaging (`soft` voting). Uses uniform weights if `None`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nflatten_transform : bool, default=True\n    Affects shape of transform output only when voting='soft'\n    If voting='soft' and flatten_transform=True, transform method returns\n    matrix with shape (n_samples, n_classifiers * n_classes). If\n    flatten_transform=False, it returns\n    (n_classifiers, n_samples, n_classes).\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting will be printed as it\n    is completed.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators as defined in ``estimators``\n    that are not 'drop'.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\n    .. versionadded:: 0.20\n\nle_ : :class:`~sklearn.preprocessing.LabelEncoder`\n    Transformer used to encode the labels during fit and decode during\n    prediction.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying classifier exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if the\n    underlying estimators expose such an attribute when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nVotingRegressor : Prediction voting regressor.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = VotingClassifier(estimators=[\n...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n...                eclf1.named_estimators_['lr'].predict(X))\nTrue\n>>> eclf2 = VotingClassifier(estimators=[\n...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...         voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n\nTo drop an estimator, :meth:`set_params` can be used to remove it. Here we\ndropped one of the estimators, resulting in 2 fitted estimators:\n\n>>> eclf2 = eclf2.set_params(lr='drop')\n>>> eclf2 = eclf2.fit(X, y)\n>>> len(eclf2.estimators_)\n2\n\nSetting `flatten_transform=True` with `voting='soft'` flattens output shape of\n`transform`:\n\n>>> eclf3 = VotingClassifier(estimators=[\n...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...        voting='soft', weights=[2,1,1],\n...        flatten_transform=True)\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>> print(eclf3.transform(X).shape)\n(6, 6)"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Prediction voting regressor for unfitted estimators.\n\nA voting regressor is an ensemble meta-estimator that fits several base\nregressors, each on the whole dataset. Then it averages the individual\npredictions to form a final prediction.\n\nRead more in the :ref:`User Guide <voting_regressor>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nestimators : list of (str, estimator) tuples\n    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'`` using\n    :meth:`set_params`.\n\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n        support was removed in 0.24.\n\nweights : array-like of shape (n_regressors,), default=None\n    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted values before averaging. Uses uniform weights if `None`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting will be printed as it\n    is completed.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nestimators_ : list of regressors\n    The collection of fitted sub-estimators as defined in ``estimators``\n    that are not 'drop'.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\n    .. versionadded:: 0.20\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if the\n    underlying regressor exposes such an attribute when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if the\n    underlying estimators expose such an attribute when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nVotingClassifier : Soft Voting/Majority Rule classifier.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import VotingRegressor\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>> r1 = LinearRegression()\n>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n>>> r3 = KNeighborsRegressor()\n>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n>>> y = np.array([2, 6, 12, 20, 30, 42])\n>>> er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])\n>>> print(er.fit(X, y).predict(X))\n[ 6.8...  8.4... 12.5... 17.8... 26...  34...]\n\nIn the following example, we drop the `'lr'` estimator with\n:meth:`~VotingRegressor.set_params` and fit the remaining two estimators:\n\n>>> er = er.set_params(lr='drop')\n>>> er = er.fit(X, y)\n>>> len(er.estimators_)\n2"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EnsembleModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#Class" ],
  "http://www.w3.org/2000/01/rdf-schema#comment" : [ {
    "@value" : "Zero-one classification loss.\n\nIf normalize is ``True``, return the fraction of misclassifications\n(float), else it returns the number of misclassifications (int). The best\nperformance is 0.\n\nRead more in the :ref:`User Guide <zero_one_loss>`.\n\nParameters\n----------\ny_true : 1d array-like, or label indicator array / sparse matrix\n    Ground truth (correct) labels.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n    Predicted labels, as returned by a classifier.\n\nnormalize : bool, default=True\n    If ``False``, return the number of misclassifications.\n    Otherwise, return the fraction of misclassifications.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nloss : float or int,\n    If ``normalize == True``, return the fraction of misclassifications\n    (float), else it returns the number of misclassifications (int).\n\nSee Also\n--------\naccuracy_score : Compute the accuracy score. By default, the function will\n    return the fraction of correct predictions divided by the total number\n    of predictions.\nhamming_loss : Compute the average Hamming loss or Hamming distance between\n    two sets of samples.\njaccard_score : Compute the Jaccard similarity coefficient score.\n\nNotes\n-----\nIn multilabel classification, the zero_one_loss function corresponds to\nthe subset zero-one loss: for each sample, the entire set of labels must be\ncorrectly predicted, otherwise the loss for that sample is equal to one.\n\nExamples\n--------\n>>> from sklearn.metrics import zero_one_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> zero_one_loss(y_true, y_pred)\n0.25\n>>> zero_one_loss(y_true, y_pred, normalize=False)\n1.0\n\nIn the multilabel case with binary label indicators:\n\n>>> import numpy as np\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subClassOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#AtomicMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MetricsModule"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBayesianRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBinaryClassificationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinaryClassification"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasBoostingRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BoostingRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasClusteringMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Clustering"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BallTreeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KDTreeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataProcessingMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataProcessing"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelCentererMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataSplittingMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DataSplitting"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneGroupOutMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeaveOneOutMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PredefinedSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDataSplittingMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDecisionTreeRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasDecompositionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Decomposition"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasFeatureSelectionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureSelection"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#Chi2Method"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FClassifMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasInstanceBasedRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#InstanceBasedRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasLeastAngleRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeastAngleRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasModelSelectionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModelSelection"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMulticlassClassificationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MulticlassClassification"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasMultilabelClassificationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelClassification"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamA",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAcceptSparse",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamActivation",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAddIndicator",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAdjusted",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAffinity",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAggressiveElimination",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlgorithm",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAllowNone",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAllowSingleCluster",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha1",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlpha2",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaH",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaPerTarget",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphaW",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlphas",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAlternateSign",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAssignLabels",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAtol",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverage",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAverageMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamAxis",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamB",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBandwidth",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBatchSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta1",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBeta2",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBetaLoss",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBinSeeding",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBinarize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBisectingStrategy",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBootstrap",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBootstrapFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBranchingFactor",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBreadthFirst",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamBreakTies",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamC",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCacheSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCallback",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCategoricalFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCategories",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCcpAlpha",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCenter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCheckInverse",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClassWeight",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClip",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterAll",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterSelectionEpsilon",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamClusterSelectionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCodeInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCodeSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCoef0",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeDistances",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeFullTree",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeLabels",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamComputeScore",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamConnectivity",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamContamination",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamContingency",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamConvergenceIter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopy",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxAbsScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCopyX",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCriterion",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamCv",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDamping",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDecisionFunctionShape",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDegree",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDictInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDictionary",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDigits",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDirection",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDiscreteFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDistanceThreshold",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDocTopicPrior",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDrop",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDropIntermediate",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDtype",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamDual",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEarlyStopping",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEigenSolver",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEigenTol",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEncode",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEncodedMissingValue",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEps",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEpsilon",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamErrorOnNew",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamErrorScore",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimator",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEstimators",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEta0",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamEvaluateEvery",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamExtrapolation",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFactor",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureNameCombiner",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureNamesOut",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatureRange",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MinMaxScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFillValue",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFilterParams",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFinalEstimator",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitAlgorithm",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitIntercept",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitInverseTransform",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPath",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFitPrior",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFlattenTransform",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceAllFinite",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceAlpha",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultinomialNBMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForceFinite",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamForgetFactor",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFreshRestarts",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFreshRestartsMaxIter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFun",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFunArgs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamFunc",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGamma",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGap",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGcvMode",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamGreaterIsBetter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamHandleUnknown",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIgnoreImplicitZeros",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIgnoreTies",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamImportanceGetter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIncludeBias",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInitSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInputType",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInteractionCst",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInteractionOnly",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInterceptScaling",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInvKwArgs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamInverseFunc",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsDataValid",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIsModelValid",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamIteratedPower",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamJitter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamK",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKeepEmptyFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKernel",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKernelParams",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKnots",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamKwArgs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratio",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL1Ratios",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamL2Regularization",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda1",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambda2",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLambdaInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLeafSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningDecay",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningOffset",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRate",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLearningRateInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLink",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLinkage",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLogBase",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamLoss",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxBins",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxCategories",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxClusterSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxDepth",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxDocUpdateIter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxEps",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxFun",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxIter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxLeafNodes",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNAlphas",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxNoImprovement",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxResources",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSamples",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSkips",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxSubpopulation",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrainSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMaxTrials",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMeanChangeTol",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMemory",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetric",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetricKwargs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMetricParams",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinBinFreq",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinCategories",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CategoricalNBMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinClusterSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinFeaturesToSelect",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinFrequency",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinImpurityDecrease",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinResources",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamples",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamplesLeaf",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinSamplesSplit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMinWeightFractionLeaf",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMiniBatch",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMissingValues",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMode",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMomentum",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMonotonicCst",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultiClass",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamMultioutput",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNAlphas",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNBest",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNBins",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNCandidates",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNClusters",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AgglomerativeClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNComponents",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNEstimators",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNFeatures",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureHasherMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNFeaturesToSelect",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNGroups",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePGroupsOutMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNIterNoChange",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNJobs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsOneClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNKnots",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNeighbors",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNNonzeroCoefs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNOversamples",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNQuantiles",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNRepeats",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSplits",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSubsamples",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNSvdVecs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNeedsProba",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNeedsThreshold",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNegLabel",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNesterovsMomentum",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNoiseVariance",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNorm",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ComplementNBMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNormOrder",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNormalize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNovelty",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamNu",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOobScore",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOrder",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PolynomialFeaturesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutlierLabel",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutputDict",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamOutputDistribution",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamP",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DBSCANMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LeavePOutMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LocalOutlierFactorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParam",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamDistributions",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamGrid",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamParamName",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPassthrough",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPenalty",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPercentile",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPerpTol",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPoolingFunc",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FeatureAgglomerationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPosLabel",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositive",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositiveCode",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPositiveDict",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPower",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerIterationNormalizer",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPowerT",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPreDispatch",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrecompute",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPredDecision",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPredecessorCorrection",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OPTICSMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPreference",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamPrefit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamProbability",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantile",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamQuantileRange",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRadius",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestNeighborsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRaiseWarning",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRandomState",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdaBoostRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoClassifMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OutputCodeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RepeatedStratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReassignmentRatio",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReduceFunc",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRefit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRemoveZeroEig",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResidualThreshold",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResource",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamResponseMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamReturnTrainScore",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRidgeAlpha",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRotation",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamRtol",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelDensityMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSampleSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSamplewise",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoreFunc",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GenericUnivariateSelectMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFdrMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFprMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFweMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectKBestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectPercentileMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoreName",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamScoring",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSeeds",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanShiftMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSelection",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSeparator",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShrinkThreshold",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NearestCentroidMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShrinking",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamShuffle",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedGroupKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedKFoldMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSimilarity",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSmooth",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolver",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSolverOptions",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSort",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSparse",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictVectorizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MissingIndicatorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSparseOutput",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelBinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiLabelBinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneHotEncoderMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SplineTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSplitSign",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSplitter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DecisionTreeRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreeRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSquared",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStackMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStandardize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PowerTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStep",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopNInliers",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStopScore",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCenters",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HDBSCANMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStoreCvValues",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierCVMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStrategy",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleImputerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamStratify",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSubsample",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KBinsDiscretizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSvdMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralBiclusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralCoclusteringMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamSvdSolver",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTargetType",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TargetEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTestScores",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTestSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TimeSeriesSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThreshold",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BinarizerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BirchMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SelectFromModelMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VarianceThresholdMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamThresholdLambda",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTol",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FactorAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KernelPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SequentialFeatureSelectorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TruncatedSVDMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTopicWordPrior",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTotalSamples",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTrainScores",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LearningCurveDisplayMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ValidationCurveDisplayMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTrainSize",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GroupShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StratifiedShuffleSplitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TrainTestSplitMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformAlgorithm",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformAlpha",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformMaxIter",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamTransformNNonzeroCoefs",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparseCoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUnitVariance",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamUnknownValue",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrdinalEncoderMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidate",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FunctionTransformerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamValidationFraction",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVarSmoothing",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GaussianNBMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVerbose",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ARDRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AffinityPropagationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BayesianRidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BernoulliRBMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BisectingKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingGridSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HalvingRandomSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LatentDirichletAllocationMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchDictionaryLearningMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchKMeansMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchNMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MiniBatchSparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NMFMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneVsRestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFECVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RFEMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomizedSearchCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVCMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SparsePCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SpectralClusteringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamVoting",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingClassifierMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWInit",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarmStart",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HistGradientBoostingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IsolationForestMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogisticRegressionMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NeighborhoodComponentsAnalysisMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerceptronMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomTreesEmbeddingMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWarnFor",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWeights",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNNImputerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#KNeighborsRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsClassifierMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RadiusNeighborsRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWhiten",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#IncrementalPCAMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PCAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWhitenSolver",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FastICAMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithCentering",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithMean",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithScaling",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RobustScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWithStd",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StandardScalerMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#boolean"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamWorkingMemory",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#float"
  }, {
    "@id" : "http://www.w3.org/2001/XMLSchema#int"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasParamZeroDivision",
  "@type" : [ "http://www.w3.org/2002/07/owl#DatatypeProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "http://www.w3.org/2001/XMLSchema#string"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ds_exeKGOntology.ttl#hasParameter"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPerformanceCalculationMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PerformanceCalculation"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AccuracyScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedMutualInfoScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AdjustedRandScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AucMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#AveragePrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BalancedAccuracyScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BrierScoreLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CalinskiHarabaszScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CheckScoringMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassLikelihoodRatiosMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ClassificationReportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CohenKappaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CompletenessScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConfusionMatrixMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ConsensusScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#CoverageErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2AbsoluteErrorScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2PinballScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#D2TweedieScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DaviesBouldinScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DcgScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#DetCurveMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#EuclideanDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExplainedVarianceScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#F1ScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FbetaScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#FowlkesMallowsScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GetScorerNamesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HammingLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HingeLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityCompletenessVMeasureMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HomogeneityScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#JaccardScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingAveragePrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LabelRankingLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LogLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MakeScorerMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MatthewsCorrcoefMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MaxErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsoluteErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanAbsolutePercentageErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanGammaDevianceMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPinballLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanPoissonDevianceMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanSquaredLogErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MeanTweedieDevianceMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MedianAbsoluteErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultilabelConfusionMatrixMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MutualInfoScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NanEuclideanDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NdcgScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NormalizedMutualInfoScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairConfusionMatrixMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesArgminMinMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesChunkedMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseDistancesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PairwiseKernelsMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallCurveMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionRecallFscoreSupportMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PrecisionScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#R2ScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RecallScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocAucScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RocCurveMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RootMeanSquaredLogErrorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteSamplesMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SilhouetteScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TopKAccuracyScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VMeasureScoreMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ZeroOneLossMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPerformanceCalculationMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasPrepareTransformerMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRandomForestRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ExtraTreesRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RandomForestRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasRegularizedRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RegularizedRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#BaggingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#GammaRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#HuberRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoLarsICMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MLPRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#ModifiedHuberMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskElasticNetMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#MultiTaskLassoMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#OrthogonalMatchingPursuitMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PassiveAggressiveRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#PoissonRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#QuantileRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RANSACRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeCVMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#RidgeMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDOneClassSVMMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SGDRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SquaredLossMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#StackingRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TheilSenRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#TweedieRegressorMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#VotingRegressorMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSimpleRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SimpleRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearRegressionMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasSupportVectorRegressionMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ],
  "http://www.w3.org/2000/01/rdf-schema#domain" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SupportVectorRegression"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#range" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#LinearSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#NuSVRMethod"
  }, {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#SVRMethod"
  } ],
  "http://www.w3.org/2000/01/rdf-schema#subPropertyOf" : [ {
    "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod"
  } ]
}, {
  "@id" : "https://raw.githubusercontent.com/nsai-uio/ExeKGOntology/main/ml_exeKGOntology.ttl#hasTrainMethod",
  "@type" : [ "http://www.w3.org/2002/07/owl#ObjectProperty" ]
} ]